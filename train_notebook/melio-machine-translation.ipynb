{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e6151f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T14:37:02.475071Z",
     "iopub.status.busy": "2024-08-27T14:37:02.474743Z",
     "iopub.status.idle": "2024-08-27T14:37:18.206072Z",
     "shell.execute_reply": "2024-08-27T14:37:18.204834Z"
    },
    "papermill": {
     "duration": 15.740711,
     "end_time": "2024-08-27T14:37:18.208578",
     "exception": false,
     "start_time": "2024-08-27T14:37:02.467867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q joeynmt==2.3.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1c0edfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T14:37:18.220738Z",
     "iopub.status.busy": "2024-08-27T14:37:18.220008Z",
     "iopub.status.idle": "2024-08-27T14:37:30.308500Z",
     "shell.execute_reply": "2024-08-27T14:37:30.307381Z"
    },
    "papermill": {
     "duration": 12.097022,
     "end_time": "2024-08-27T14:37:30.310891",
     "exception": false,
     "start_time": "2024-08-27T14:37:18.213869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\r\n",
      "Version: 2.1.2\r\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\r\n",
      "Home-page: https://pytorch.org/\r\n",
      "Author: PyTorch Team\r\n",
      "Author-email: packages@pytorch.org\r\n",
      "License: BSD-3\r\n",
      "Location: /opt/conda/lib/python3.10/site-packages\r\n",
      "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\r\n",
      "Required-by: accelerate, catalyst, easyocr, fastai, joeynmt, kornia, pytorch-ignite, pytorch-lightning, stable-baselines3, timm, torchaudio, torchdata, torchmetrics, torchtext, torchvision\r\n"
     ]
    }
   ],
   "source": [
    "!pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "226f1e6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T14:37:30.322750Z",
     "iopub.status.busy": "2024-08-27T14:37:30.322400Z",
     "iopub.status.idle": "2024-08-27T14:37:30.327411Z",
     "shell.execute_reply": "2024-08-27T14:37:30.326558Z"
    },
    "papermill": {
     "duration": 0.013327,
     "end_time": "2024-08-27T14:37:30.329379",
     "exception": false,
     "start_time": "2024-08-27T14:37:30.316052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os,re\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f3ea6dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T14:37:30.340855Z",
     "iopub.status.busy": "2024-08-27T14:37:30.339921Z",
     "iopub.status.idle": "2024-08-27T14:37:30.498236Z",
     "shell.execute_reply": "2024-08-27T14:37:30.497472Z"
    },
    "papermill": {
     "duration": 0.166512,
     "end_time": "2024-08-27T14:37:30.500737",
     "exception": false,
     "start_time": "2024-08-27T14:37:30.334225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71903a93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T14:37:30.513436Z",
     "iopub.status.busy": "2024-08-27T14:37:30.512614Z",
     "iopub.status.idle": "2024-08-27T14:37:30.519010Z",
     "shell.execute_reply": "2024-08-27T14:37:30.518137Z"
    },
    "papermill": {
     "duration": 0.014965,
     "end_time": "2024-08-27T14:37:30.520980",
     "exception": false,
     "start_time": "2024-08-27T14:37:30.506015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_lang = 'dyu'\n",
    "trg_lang = \"fr\"\n",
    "chars_to_remove_regex = '[!\"&\\(\\),-./:;=?+.\\n\\[\\]]'\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub(chars_to_remove_regex, ' ', text.lower())\n",
    "    return text.strip()\n",
    "\n",
    "def clean_text(batch):\n",
    "    # process source text\n",
    "    batch['translation'][src_lang] = remove_special_characters(batch['translation'][src_lang])\n",
    "    # process target text\n",
    "    batch['translation'][trg_lang] = remove_special_characters(batch['translation'][trg_lang])\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "354c91c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T14:37:30.532681Z",
     "iopub.status.busy": "2024-08-27T14:37:30.532358Z",
     "iopub.status.idle": "2024-08-27T14:37:37.335915Z",
     "shell.execute_reply": "2024-08-27T14:37:37.334943Z"
    },
    "papermill": {
     "duration": 6.811706,
     "end_time": "2024-08-27T14:37:37.337904",
     "exception": false,
     "start_time": "2024-08-27T14:37:30.526198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cfa9a2318340d38ad27644e8ebf9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a821e4b10bb84eec839950193a0444ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/530k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939d0e7b29c44302a5f3f2811efc67ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/102k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4758071a18ee488ebcb00b6ccd31c72a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/55.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1e45dccf094b7ebcd71d039bb7d7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8065 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84edb701364f4d5caf664fe9fb0beb33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17ceeacb33247c5a91fcc857067e19d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1393 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a33aab5bfa74b8da550945528cc8415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8065 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1d45496dbc4c26ba649d24c15cf4ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac31f04a700436c9f092db9acfbeaee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1393 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"uvci/Koumankan_mt_dyu_fr\", token=HF_TOKEN)\n",
    "data = data.map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc072375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T14:37:37.352986Z",
     "iopub.status.busy": "2024-08-27T14:37:37.352261Z",
     "iopub.status.idle": "2024-08-27T14:37:37.361831Z",
     "shell.execute_reply": "2024-08-27T14:37:37.360998Z"
    },
    "papermill": {
     "duration": 0.018883,
     "end_time": "2024-08-27T14:37:37.363709",
     "exception": false,
     "start_time": "2024-08-27T14:37:37.344826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ID', 'translation'],\n",
       "    num_rows: 8065\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ID', 'translation'],\n",
       "    num_rows: 1471\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ID', 'translation'],\n",
       "    num_rows: 1393\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data['train'])\n",
    "display(data['validation'])\n",
    "display(data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6420ad7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T14:37:37.380227Z",
     "iopub.status.busy": "2024-08-27T14:37:37.379723Z",
     "iopub.status.idle": "2024-08-27T14:37:37.451700Z",
     "shell.execute_reply": "2024-08-27T14:37:37.450835Z"
    },
    "papermill": {
     "duration": 0.082831,
     "end_time": "2024-08-27T14:37:37.453725",
     "exception": false,
     "start_time": "2024-08-27T14:37:37.370894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bdd06931934549839af18fd421a022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8065 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc69ab656ccb4ed0a4f0c851cf281bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3689f3c88909451193bd9c1ae64292af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1393 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dir = \"./data/dyu_fr\"\n",
    "data.save_to_disk(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0479a6b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T14:37:37.470740Z",
     "iopub.status.busy": "2024-08-27T14:37:37.469915Z",
     "iopub.status.idle": "2024-08-27T14:37:37.477191Z",
     "shell.execute_reply": "2024-08-27T14:37:37.476360Z"
    },
    "papermill": {
     "duration": 0.017708,
     "end_time": "2024-08-27T14:37:37.479158",
     "exception": false,
     "start_time": "2024-08-27T14:37:37.461450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# model dir\n",
    "model_dir = \"./saved_model/dyu_fr\"\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"dyu_fr_transformer-sp\"\n",
    "joeynmt_version: \"2.3.0\"\n",
    "model_dir: \"{model_dir}\"\n",
    "use_cuda: True # False for CPU training\n",
    "fp16: False\n",
    "\n",
    "data:\n",
    "    train: \"{data_dir}\"\n",
    "    dev: \"{data_dir}\"\n",
    "    test: \"{data_dir}\"\n",
    "    dataset_type: \"huggingface\"\n",
    "    dataset_cfg:\n",
    "        name: \"dyu-fr\"\n",
    "    sample_dev_subset: 1460\n",
    "    src:\n",
    "        lang: \"dyu\"\n",
    "        max_length: 100\n",
    "        lowercase: False\n",
    "        normalize: False\n",
    "        level: \"bpe\"\n",
    "        voc_limit: 4000\n",
    "        voc_min_freq: 1\n",
    "        voc_file: \"{data_dir}/vocab.txt\"\n",
    "        tokenizer_type: \"sentencepiece\"\n",
    "        tokenizer_cfg:\n",
    "            model_file: \"{data_dir}/sp.model\"\n",
    "    trg:\n",
    "        lang: \"fr\"\n",
    "        max_length: 100\n",
    "        lowercase: False\n",
    "        normalize: False\n",
    "        level: \"bpe\"\n",
    "        voc_limit: 4000\n",
    "        voc_min_freq: 1\n",
    "        voc_file: \"{data_dir}/vocab.txt\"\n",
    "        tokenizer_type: \"sentencepiece\"\n",
    "        tokenizer_cfg:\n",
    "            model_file: \"{data_dir}/sp.model\"\n",
    "    special_symbols:\n",
    "        unk_token: \"<unk>\"\n",
    "        unk_id: 0\n",
    "        pad_token: \"<pad>\"\n",
    "        pad_id: 1\n",
    "        bos_token: \"<s>\"\n",
    "        bos_id: 2\n",
    "        eos_token: \"</s>\"\n",
    "        eos_id: 3\n",
    "\n",
    "\"\"\".format(data_dir=data_dir, model_dir=model_dir)\n",
    "with (Path(data_dir) / \"config.yaml\").open('w') as f:\n",
    "    f.write(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c25d488b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T14:37:37.497144Z",
     "iopub.status.busy": "2024-08-27T14:37:37.496584Z",
     "iopub.status.idle": "2024-08-27T14:37:39.820646Z",
     "shell.execute_reply": "2024-08-27T14:37:39.819279Z"
    },
    "papermill": {
     "duration": 2.335333,
     "end_time": "2024-08-27T14:37:39.823147",
     "exception": false,
     "start_time": "2024-08-27T14:37:37.487814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-27 14:37:38--  https://raw.githubusercontent.com/joeynmt/joeynmt/v2.3/scripts/build_vocab.py\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 13170 (13K) [text/plain]\r\n",
      "Saving to: 'build_vocab.py'\r\n",
      "\r\n",
      "build_vocab.py      100%[===================>]  12.86K  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2024-08-27 14:37:38 (62.1 MB/s) - 'build_vocab.py' saved [13170/13170]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/joeynmt/joeynmt/v2.3/scripts/build_vocab.py\n",
    "! sudo chmod 777 build_vocab.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "187277e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T14:37:39.841585Z",
     "iopub.status.busy": "2024-08-27T14:37:39.840750Z",
     "iopub.status.idle": "2024-08-27T14:38:02.725758Z",
     "shell.execute_reply": "2024-08-27T14:38:02.724763Z"
    },
    "papermill": {
     "duration": 22.896725,
     "end_time": "2024-08-27T14:38:02.728178",
     "exception": false,
     "start_time": "2024-08-27T14:37:39.831453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-27 14:37:46.860276: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-08-27 14:37:46.860406: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-08-27 14:37:46.993799: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Dropping NaN...: 100%|████████████| 8065/8065 [00:00<00:00, 69112.25 examples/s]\r\n",
      "Preprocessing...: 100%|████████████| 8065/8065 [00:00<00:00, 8512.55 examples/s]\r\n",
      "Filter: 100%|█████████████████████| 8065/8065 [00:00<00:00, 20384.11 examples/s]\r\n",
      "### Training sentencepiece...\r\n",
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=/tmp/sentencepiece_5x44_tcl.txt --model_prefix=data/dyu_fr/sp --model_type=unigram --vocab_size=4000 --character_coverage=1.0 --accept_language=dyu,fr --unk_piece=<unk> --bos_piece=<s> --eos_piece=</s> --pad_piece=<pad> --unk_id=0 --bos_id=2 --eos_id=3 --pad_id=1 --vocabulary_output_piece_score=false\r\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \r\n",
      "trainer_spec {\r\n",
      "  input: /tmp/sentencepiece_5x44_tcl.txt\r\n",
      "  input_format: \r\n",
      "  model_prefix: data/dyu_fr/sp\r\n",
      "  model_type: UNIGRAM\r\n",
      "  vocab_size: 4000\r\n",
      "  accept_language: dyu\r\n",
      "  accept_language: fr\r\n",
      "  self_test_sample_size: 0\r\n",
      "  character_coverage: 1\r\n",
      "  input_sentence_size: 0\r\n",
      "  shuffle_input_sentence: 1\r\n",
      "  seed_sentencepiece_size: 1000000\r\n",
      "  shrinking_factor: 0.75\r\n",
      "  max_sentence_length: 4192\r\n",
      "  num_threads: 16\r\n",
      "  num_sub_iterations: 2\r\n",
      "  max_sentencepiece_length: 16\r\n",
      "  split_by_unicode_script: 1\r\n",
      "  split_by_number: 1\r\n",
      "  split_by_whitespace: 1\r\n",
      "  split_digits: 0\r\n",
      "  pretokenization_delimiter: \r\n",
      "  treat_whitespace_as_suffix: 0\r\n",
      "  allow_whitespace_only_pieces: 0\r\n",
      "  required_chars: \r\n",
      "  byte_fallback: 0\r\n",
      "  vocabulary_output_piece_score: 0\r\n",
      "  train_extremely_large_corpus: 0\r\n",
      "  seed_sentencepieces_file: \r\n",
      "  hard_vocab_limit: 1\r\n",
      "  use_all_vocab: 0\r\n",
      "  unk_id: 0\r\n",
      "  bos_id: 2\r\n",
      "  eos_id: 3\r\n",
      "  pad_id: 1\r\n",
      "  unk_piece: <unk>\r\n",
      "  bos_piece: <s>\r\n",
      "  eos_piece: </s>\r\n",
      "  pad_piece: <pad>\r\n",
      "  unk_surface:  ⁇ \r\n",
      "  enable_differential_privacy: 0\r\n",
      "  differential_privacy_noise_level: 0\r\n",
      "  differential_privacy_clipping_threshold: 0\r\n",
      "}\r\n",
      "normalizer_spec {\r\n",
      "  name: nmt_nfkc\r\n",
      "  add_dummy_prefix: 1\r\n",
      "  remove_extra_whitespaces: 1\r\n",
      "  escape_whitespaces: 1\r\n",
      "  normalization_rule_tsv: \r\n",
      "}\r\n",
      "denormalizer_spec {}\r\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\r\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: /tmp/sentencepiece_5x44_tcl.txt\r\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 16130 sentences\r\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\r\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\r\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\r\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\r\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\r\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=470835\r\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=79\r\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\r\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 16130 sentences.\r\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\r\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=251398\r\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 32786 seed sentencepieces\r\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 16130\r\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 16906\r\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 16906 sentences for EM training\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12806 obj=10.5164 num_tokens=32859 num_tokens/piece=2.56591\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11184 obj=9.25634 num_tokens=33257 num_tokens/piece=2.97362\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8381 obj=9.28154 num_tokens=35600 num_tokens/piece=4.2477\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8359 obj=9.22704 num_tokens=35628 num_tokens/piece=4.26223\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6268 obj=9.41457 num_tokens=39002 num_tokens/piece=6.2224\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6268 obj=9.35124 num_tokens=39010 num_tokens/piece=6.22368\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4701 obj=9.58835 num_tokens=42619 num_tokens/piece=9.06594\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4701 obj=9.52127 num_tokens=42620 num_tokens/piece=9.06616\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4400 obj=9.57065 num_tokens=43419 num_tokens/piece=9.86795\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4400 obj=9.55719 num_tokens=43420 num_tokens/piece=9.86818\r\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: data/dyu_fr/sp.model\r\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: data/dyu_fr/sp.vocab\r\n",
      "### Copying data/dyu_fr/sp.vocab to data/dyu_fr/vocab.txt ...\r\n",
      "### Done.\r\n"
     ]
    }
   ],
   "source": [
    "!python build_vocab.py {data_dir}/config.yaml --joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbccc8e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T14:38:02.751236Z",
     "iopub.status.busy": "2024-08-27T14:38:02.750597Z",
     "iopub.status.idle": "2024-08-27T14:38:02.758376Z",
     "shell.execute_reply": "2024-08-27T14:38:02.757529Z"
    },
    "papermill": {
     "duration": 0.021409,
     "end_time": "2024-08-27T14:38:02.760246",
     "exception": false,
     "start_time": "2024-08-27T14:38:02.738837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config += \"\"\"\n",
    "testing:\n",
    "    #load_model: \"{model_dir}/best.ckpt\"\n",
    "    n_best: 1\n",
    "    beam_size: 5\n",
    "    beam_alpha: 1.0\n",
    "    batch_size: 256\n",
    "    batch_type: \"token\"\n",
    "    max_output_length: 100\n",
    "    eval_metrics: [\"bleu\"]\n",
    "    #return_prob: \"hyp\"\n",
    "    #return_attention: False\n",
    "    sacrebleu_cfg:\n",
    "        tokenize: \"13a\"\n",
    "\n",
    "training:\n",
    "    #load_model: \"{model_dir}/latest.ckpt\"\n",
    "    #reset_best_ckpt: False\n",
    "    #reset_scheduler: False\n",
    "    #reset_optimizer: False\n",
    "    #reset_iter_state: False\n",
    "    random_seed: 42\n",
    "    optimizer: \"adamw\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999]\n",
    "    scheduling: \"warmupinversesquareroot\"\n",
    "    learning_rate_warmup: 500\n",
    "    learning_rate: 0.0008\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.01\n",
    "    loss: \"crossentropy\"\n",
    "    batch_size: 512\n",
    "    batch_type: \"token\"\n",
    "    batch_multiplier: 4\n",
    "    early_stopping_metric: \"bleu\"\n",
    "    epochs: 300\n",
    "    validation_freq: 500\n",
    "    logging_freq: 5\n",
    "    overwrite: True\n",
    "    shuffle: True\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_best_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier_uniform\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier_uniform\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 8\n",
    "        num_heads: 4\n",
    "        embeddings:\n",
    "            embedding_dim: 256\n",
    "            scale: True\n",
    "            dropout: 0.1\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256\n",
    "        ff_size: 1024\n",
    "        dropout: 0.3\n",
    "        layer_norm: \"pre\"\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 8\n",
    "        embeddings:\n",
    "            embedding_dim: 256\n",
    "            scale: True\n",
    "            dropout: 0.0\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256\n",
    "        ff_size: 1024\n",
    "        dropout: 0.1\n",
    "        layer_norm: \"pre\"\n",
    "\n",
    "\"\"\".format(model_dir=model_dir)\n",
    "with (Path(data_dir) / \"config.yaml\").open('w') as f:\n",
    "    f.write(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8b0aef1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T14:38:02.783689Z",
     "iopub.status.busy": "2024-08-27T14:38:02.783268Z",
     "iopub.status.idle": "2024-08-27T16:48:04.179516Z",
     "shell.execute_reply": "2024-08-27T16:48:04.178345Z"
    },
    "papermill": {
     "duration": 7801.410198,
     "end_time": "2024-08-27T16:48:04.181909",
     "exception": false,
     "start_time": "2024-08-27T14:38:02.771711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-27 14:38:06.279194: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-08-27 14:38:06.279258: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-08-27 14:38:06.280680: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2024-08-27 14:38:10,135 - INFO - joeynmt.config -                           cfg.name : dyu_fr_transformer-sp\r\n",
      "2024-08-27 14:38:10,135 - INFO - joeynmt.config -                cfg.joeynmt_version : 2.3.0\r\n",
      "2024-08-27 14:38:10,135 - INFO - joeynmt.config -                      cfg.model_dir : ./saved_model/dyu_fr\r\n",
      "2024-08-27 14:38:10,135 - INFO - joeynmt.config -                       cfg.use_cuda : True\r\n",
      "2024-08-27 14:38:10,135 - INFO - joeynmt.config -                           cfg.fp16 : False\r\n",
      "2024-08-27 14:38:10,135 - INFO - joeynmt.config -                     cfg.data.train : ./data/dyu_fr\r\n",
      "2024-08-27 14:38:10,135 - INFO - joeynmt.config -                       cfg.data.dev : ./data/dyu_fr\r\n",
      "2024-08-27 14:38:10,135 - INFO - joeynmt.config -                      cfg.data.test : ./data/dyu_fr\r\n",
      "2024-08-27 14:38:10,135 - INFO - joeynmt.config -              cfg.data.dataset_type : huggingface\r\n",
      "2024-08-27 14:38:10,135 - INFO - joeynmt.config -          cfg.data.dataset_cfg.name : dyu-fr\r\n",
      "2024-08-27 14:38:10,136 - INFO - joeynmt.config -         cfg.data.sample_dev_subset : 1460\r\n",
      "2024-08-27 14:38:10,136 - INFO - joeynmt.config -                  cfg.data.src.lang : dyu\r\n",
      "2024-08-27 14:38:10,136 - INFO - joeynmt.config -            cfg.data.src.max_length : 100\r\n",
      "2024-08-27 14:38:10,136 - INFO - joeynmt.config -             cfg.data.src.lowercase : False\r\n",
      "2024-08-27 14:38:10,136 - INFO - joeynmt.config -             cfg.data.src.normalize : False\r\n",
      "2024-08-27 14:38:10,136 - INFO - joeynmt.config -                 cfg.data.src.level : bpe\r\n",
      "2024-08-27 14:38:10,136 - INFO - joeynmt.config -             cfg.data.src.voc_limit : 4000\r\n",
      "2024-08-27 14:38:10,136 - INFO - joeynmt.config -          cfg.data.src.voc_min_freq : 1\r\n",
      "2024-08-27 14:38:10,136 - INFO - joeynmt.config -              cfg.data.src.voc_file : ./data/dyu_fr/vocab.txt\r\n",
      "2024-08-27 14:38:10,136 - INFO - joeynmt.config -        cfg.data.src.tokenizer_type : sentencepiece\r\n",
      "2024-08-27 14:38:10,136 - INFO - joeynmt.config - cfg.data.src.tokenizer_cfg.model_file : ./data/dyu_fr/sp.model\r\n",
      "2024-08-27 14:38:10,136 - INFO - joeynmt.config -                  cfg.data.trg.lang : fr\r\n",
      "2024-08-27 14:38:10,137 - INFO - joeynmt.config -            cfg.data.trg.max_length : 100\r\n",
      "2024-08-27 14:38:10,137 - INFO - joeynmt.config -             cfg.data.trg.lowercase : False\r\n",
      "2024-08-27 14:38:10,137 - INFO - joeynmt.config -             cfg.data.trg.normalize : False\r\n",
      "2024-08-27 14:38:10,137 - INFO - joeynmt.config -                 cfg.data.trg.level : bpe\r\n",
      "2024-08-27 14:38:10,137 - INFO - joeynmt.config -             cfg.data.trg.voc_limit : 4000\r\n",
      "2024-08-27 14:38:10,137 - INFO - joeynmt.config -          cfg.data.trg.voc_min_freq : 1\r\n",
      "2024-08-27 14:38:10,137 - INFO - joeynmt.config -              cfg.data.trg.voc_file : ./data/dyu_fr/vocab.txt\r\n",
      "2024-08-27 14:38:10,137 - INFO - joeynmt.config -        cfg.data.trg.tokenizer_type : sentencepiece\r\n",
      "2024-08-27 14:38:10,137 - INFO - joeynmt.config - cfg.data.trg.tokenizer_cfg.model_file : ./data/dyu_fr/sp.model\r\n",
      "2024-08-27 14:38:10,137 - INFO - joeynmt.config - cfg.data.special_symbols.unk_token : <unk>\r\n",
      "2024-08-27 14:38:10,137 - INFO - joeynmt.config -    cfg.data.special_symbols.unk_id : 0\r\n",
      "2024-08-27 14:38:10,137 - INFO - joeynmt.config - cfg.data.special_symbols.pad_token : <pad>\r\n",
      "2024-08-27 14:38:10,138 - INFO - joeynmt.config -    cfg.data.special_symbols.pad_id : 1\r\n",
      "2024-08-27 14:38:10,138 - INFO - joeynmt.config - cfg.data.special_symbols.bos_token : <s>\r\n",
      "2024-08-27 14:38:10,138 - INFO - joeynmt.config -    cfg.data.special_symbols.bos_id : 2\r\n",
      "2024-08-27 14:38:10,138 - INFO - joeynmt.config - cfg.data.special_symbols.eos_token : </s>\r\n",
      "2024-08-27 14:38:10,138 - INFO - joeynmt.config -    cfg.data.special_symbols.eos_id : 3\r\n",
      "2024-08-27 14:38:10,138 - INFO - joeynmt.config -                 cfg.testing.n_best : 1\r\n",
      "2024-08-27 14:38:10,138 - INFO - joeynmt.config -              cfg.testing.beam_size : 5\r\n",
      "2024-08-27 14:38:10,138 - INFO - joeynmt.config -             cfg.testing.beam_alpha : 1.0\r\n",
      "2024-08-27 14:38:10,138 - INFO - joeynmt.config -             cfg.testing.batch_size : 256\r\n",
      "2024-08-27 14:38:10,138 - INFO - joeynmt.config -             cfg.testing.batch_type : token\r\n",
      "2024-08-27 14:38:10,138 - INFO - joeynmt.config -      cfg.testing.max_output_length : 100\r\n",
      "2024-08-27 14:38:10,138 - INFO - joeynmt.config -           cfg.testing.eval_metrics : ['bleu']\r\n",
      "2024-08-27 14:38:10,138 - INFO - joeynmt.config - cfg.testing.sacrebleu_cfg.tokenize : 13a\r\n",
      "2024-08-27 14:38:10,139 - INFO - joeynmt.config -           cfg.training.random_seed : 42\r\n",
      "2024-08-27 14:38:10,139 - INFO - joeynmt.config -             cfg.training.optimizer : adamw\r\n",
      "2024-08-27 14:38:10,139 - INFO - joeynmt.config -         cfg.training.normalization : tokens\r\n",
      "2024-08-27 14:38:10,139 - INFO - joeynmt.config -            cfg.training.adam_betas : [0.9, 0.999]\r\n",
      "2024-08-27 14:38:10,139 - INFO - joeynmt.config -            cfg.training.scheduling : warmupinversesquareroot\r\n",
      "2024-08-27 14:38:10,139 - INFO - joeynmt.config -  cfg.training.learning_rate_warmup : 500\r\n",
      "2024-08-27 14:38:10,139 - INFO - joeynmt.config -         cfg.training.learning_rate : 0.0008\r\n",
      "2024-08-27 14:38:10,139 - INFO - joeynmt.config -     cfg.training.learning_rate_min : 1e-08\r\n",
      "2024-08-27 14:38:10,139 - INFO - joeynmt.config -          cfg.training.weight_decay : 0.0\r\n",
      "2024-08-27 14:38:10,139 - INFO - joeynmt.config -       cfg.training.label_smoothing : 0.01\r\n",
      "2024-08-27 14:38:10,139 - INFO - joeynmt.config -                  cfg.training.loss : crossentropy\r\n",
      "2024-08-27 14:38:10,140 - INFO - joeynmt.config -            cfg.training.batch_size : 512\r\n",
      "2024-08-27 14:38:10,140 - INFO - joeynmt.config -            cfg.training.batch_type : token\r\n",
      "2024-08-27 14:38:10,140 - INFO - joeynmt.config -      cfg.training.batch_multiplier : 4\r\n",
      "2024-08-27 14:38:10,140 - INFO - joeynmt.config - cfg.training.early_stopping_metric : bleu\r\n",
      "2024-08-27 14:38:10,140 - INFO - joeynmt.config -                cfg.training.epochs : 300\r\n",
      "2024-08-27 14:38:10,140 - INFO - joeynmt.config -       cfg.training.validation_freq : 500\r\n",
      "2024-08-27 14:38:10,140 - INFO - joeynmt.config -          cfg.training.logging_freq : 5\r\n",
      "2024-08-27 14:38:10,140 - INFO - joeynmt.config -             cfg.training.overwrite : True\r\n",
      "2024-08-27 14:38:10,140 - INFO - joeynmt.config -               cfg.training.shuffle : True\r\n",
      "2024-08-27 14:38:10,140 - INFO - joeynmt.config -     cfg.training.print_valid_sents : [0, 1, 2, 3]\r\n",
      "2024-08-27 14:38:10,140 - INFO - joeynmt.config -       cfg.training.keep_best_ckpts : 3\r\n",
      "2024-08-27 14:38:10,140 - INFO - joeynmt.config -              cfg.model.initializer : xavier_uniform\r\n",
      "2024-08-27 14:38:10,140 - INFO - joeynmt.config -         cfg.model.bias_initializer : zeros\r\n",
      "2024-08-27 14:38:10,140 - INFO - joeynmt.config -                cfg.model.init_gain : 1.0\r\n",
      "2024-08-27 14:38:10,140 - INFO - joeynmt.config -        cfg.model.embed_initializer : xavier_uniform\r\n",
      "2024-08-27 14:38:10,140 - INFO - joeynmt.config -          cfg.model.embed_init_gain : 1.0\r\n",
      "2024-08-27 14:38:10,140 - INFO - joeynmt.config -          cfg.model.tied_embeddings : True\r\n",
      "2024-08-27 14:38:10,141 - INFO - joeynmt.config -             cfg.model.tied_softmax : True\r\n",
      "2024-08-27 14:38:10,141 - INFO - joeynmt.config -             cfg.model.encoder.type : transformer\r\n",
      "2024-08-27 14:38:10,141 - INFO - joeynmt.config -       cfg.model.encoder.num_layers : 8\r\n",
      "2024-08-27 14:38:10,141 - INFO - joeynmt.config -        cfg.model.encoder.num_heads : 4\r\n",
      "2024-08-27 14:38:10,141 - INFO - joeynmt.config - cfg.model.encoder.embeddings.embedding_dim : 256\r\n",
      "2024-08-27 14:38:10,141 - INFO - joeynmt.config - cfg.model.encoder.embeddings.scale : True\r\n",
      "2024-08-27 14:38:10,141 - INFO - joeynmt.config - cfg.model.encoder.embeddings.dropout : 0.1\r\n",
      "2024-08-27 14:38:10,141 - INFO - joeynmt.config -      cfg.model.encoder.hidden_size : 256\r\n",
      "2024-08-27 14:38:10,141 - INFO - joeynmt.config -          cfg.model.encoder.ff_size : 1024\r\n",
      "2024-08-27 14:38:10,141 - INFO - joeynmt.config -          cfg.model.encoder.dropout : 0.3\r\n",
      "2024-08-27 14:38:10,141 - INFO - joeynmt.config -       cfg.model.encoder.layer_norm : pre\r\n",
      "2024-08-27 14:38:10,141 - INFO - joeynmt.config -             cfg.model.decoder.type : transformer\r\n",
      "2024-08-27 14:38:10,141 - INFO - joeynmt.config -       cfg.model.decoder.num_layers : 6\r\n",
      "2024-08-27 14:38:10,141 - INFO - joeynmt.config -        cfg.model.decoder.num_heads : 8\r\n",
      "2024-08-27 14:38:10,141 - INFO - joeynmt.config - cfg.model.decoder.embeddings.embedding_dim : 256\r\n",
      "2024-08-27 14:38:10,141 - INFO - joeynmt.config - cfg.model.decoder.embeddings.scale : True\r\n",
      "2024-08-27 14:38:10,141 - INFO - joeynmt.config - cfg.model.decoder.embeddings.dropout : 0.0\r\n",
      "2024-08-27 14:38:10,141 - INFO - joeynmt.config -      cfg.model.decoder.hidden_size : 256\r\n",
      "2024-08-27 14:38:10,142 - INFO - joeynmt.config -          cfg.model.decoder.ff_size : 1024\r\n",
      "2024-08-27 14:38:10,142 - INFO - joeynmt.config -          cfg.model.decoder.dropout : 0.1\r\n",
      "2024-08-27 14:38:10,142 - INFO - joeynmt.config -       cfg.model.decoder.layer_norm : pre\r\n",
      "2024-08-27 14:38:10,213 - INFO - joeynmt.data - Building tokenizer...\r\n",
      "2024-08-27 14:38:10,226 - INFO - joeynmt.tokenizers - dyu tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\r\n",
      "2024-08-27 14:38:10,226 - INFO - joeynmt.tokenizers - fr tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\r\n",
      "2024-08-27 14:38:10,226 - INFO - joeynmt.data - Loading train set...\r\n",
      "Dropping NaN...: 100%|████████████| 8065/8065 [00:00<00:00, 67688.78 examples/s]\r\n",
      "Preprocessing...: 100%|████████████| 8065/8065 [00:00<00:00, 8863.57 examples/s]\r\n",
      "2024-08-27 14:38:11,616 - INFO - joeynmt.data - Building vocabulary...\r\n",
      "2024-08-27 14:38:11,836 - INFO - joeynmt.data - Loading dev set...\r\n",
      "Dropping NaN...: 100%|████████████| 1471/1471 [00:00<00:00, 62447.58 examples/s]\r\n",
      "Preprocessing...: 100%|████████████| 1471/1471 [00:00<00:00, 7493.85 examples/s]\r\n",
      "2024-08-27 14:38:12,321 - INFO - joeynmt.data - Loading test set...\r\n",
      "Dropping NaN...: 100%|████████████| 1393/1393 [00:00<00:00, 59854.79 examples/s]\r\n",
      "Preprocessing...: 100%|████████████| 1393/1393 [00:00<00:00, 8410.97 examples/s]\r\n",
      "2024-08-27 14:38:12,788 - INFO - joeynmt.data - Data loaded.\r\n",
      "2024-08-27 14:38:12,789 - INFO - joeynmt.data - Train dataset: HuggingfaceTranslationDataset(len=8065, src_lang=\"dyu\", trg_lang=\"fr\", has_trg=True, random_subset=-1, has_src_prompt=False, has_trg_prompt=False, name=dyu-fr, split=train)\r\n",
      "2024-08-27 14:38:12,789 - INFO - joeynmt.data - Valid dataset: HuggingfaceTranslationDataset(len=1471, src_lang=\"dyu\", trg_lang=\"fr\", has_trg=True, random_subset=1460, has_src_prompt=False, has_trg_prompt=False, name=dyu-fr, split=validation)\r\n",
      "2024-08-27 14:38:12,789 - INFO - joeynmt.data -  Test dataset: HuggingfaceTranslationDataset(len=1393, src_lang=\"dyu\", trg_lang=\"fr\", has_trg=True, random_subset=-1, has_src_prompt=False, has_trg_prompt=False, name=dyu-fr, split=test)\r\n",
      "2024-08-27 14:38:12,790 - INFO - joeynmt.data - First training example:\r\n",
      "\t[SRC] ▁a ▁bi ▁ji ▁min ▁na\r\n",
      "\t[TRG] ▁il ▁bo it ▁de ▁l ’ eau\r\n",
      "2024-08-27 14:38:12,790 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ▁a (5) s (6) ▁ka (7) ' (8) a (9) ▁la\r\n",
      "2024-08-27 14:38:12,790 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ▁a (5) s (6) ▁ka (7) ' (8) a (9) ▁la\r\n",
      "2024-08-27 14:38:12,790 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 4000\r\n",
      "2024-08-27 14:38:12,790 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 4000\r\n",
      "2024-08-27 14:38:12,796 - WARNING - joeynmt.tokenizers - /kaggle/working/saved_model/dyu_fr/sp.model already exists. Stop copying.\r\n",
      "2024-08-27 14:38:12,797 - INFO - joeynmt.model - Building an encoder-decoder model...\r\n",
      "2024-08-27 14:38:13,203 - INFO - joeynmt.model - Enc-dec model built.\r\n",
      "2024-08-27 14:38:13,208 - INFO - joeynmt.model - Total params: 13663744\r\n",
      "2024-08-27 14:38:13,385 - INFO - joeynmt.prediction - Model(\r\n",
      "\tencoder=TransformerEncoder(num_layers=8, num_heads=4, alpha=1.0, layer_norm=\"pre\", activation=ReLU()),\r\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=8, alpha=1.0, layer_norm=\"pre\", activation=ReLU()),\r\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4000),\r\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4000),\r\n",
      "\tloss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.01))\r\n",
      "2024-08-27 14:38:14,160 - INFO - joeynmt.builders - AdamW(lr=0.0008, weight_decay=0.0, betas=[0.9, 0.999])\r\n",
      "2024-08-27 14:38:14,161 - INFO - joeynmt.builders - WarmupInverseSquareRootScheduler(warmup=500, decay_rate=0.017889, peak_rate=0.0008, min_rate=1e-08)\r\n",
      "2024-08-27 14:38:14,161 - INFO - joeynmt.training - Train config:\r\n",
      "\tdevice: cuda\r\n",
      "\tn_gpu: 1\r\n",
      "\tddp training: False\r\n",
      "\t16-bits training: False\r\n",
      "\tgradient accumulation: 4\r\n",
      "\tbatch size per device: 512\r\n",
      "\teffective batch size (w. parallel & accumulation): 2048\r\n",
      "2024-08-27 14:38:14,161 - INFO - joeynmt.training - EPOCH 1\r\n",
      "2024-08-27 14:38:16,075 - INFO - joeynmt.training - Epoch   1, Step:        5, Batch Loss:     7.718401, Batch Acc: 0.080781, Tokens per Sec:     2119, Lr: 0.000008\r\n",
      "2024-08-27 14:38:17,248 - INFO - joeynmt.training - Epoch   1, Step:       10, Batch Loss:     7.699598, Batch Acc: 0.105026, Tokens per Sec:     3412, Lr: 0.000016\r\n",
      "2024-08-27 14:38:18,377 - INFO - joeynmt.training - Epoch   1, Step:       15, Batch Loss:     7.631328, Batch Acc: 0.098434, Tokens per Sec:     3564, Lr: 0.000024\r\n",
      "2024-08-27 14:38:19,490 - INFO - joeynmt.training - Epoch   1, Step:       20, Batch Loss:     7.556461, Batch Acc: 0.096645, Tokens per Sec:     3591, Lr: 0.000032\r\n",
      "2024-08-27 14:38:20,626 - INFO - joeynmt.training - Epoch   1, Step:       25, Batch Loss:     7.497436, Batch Acc: 0.105980, Tokens per Sec:     3624, Lr: 0.000040\r\n",
      "2024-08-27 14:38:21,752 - INFO - joeynmt.training - Epoch   1, Step:       30, Batch Loss:     7.444970, Batch Acc: 0.096631, Tokens per Sec:     3798, Lr: 0.000048\r\n",
      "2024-08-27 14:38:22,914 - INFO - joeynmt.training - Epoch   1, Step:       35, Batch Loss:     7.432101, Batch Acc: 0.093566, Tokens per Sec:     3656, Lr: 0.000056\r\n",
      "2024-08-27 14:38:24,035 - INFO - joeynmt.training - Epoch   1, Step:       40, Batch Loss:     7.332080, Batch Acc: 0.098639, Tokens per Sec:     3674, Lr: 0.000064\r\n",
      "2024-08-27 14:38:25,138 - INFO - joeynmt.training - Epoch   1, Step:       45, Batch Loss:     7.273354, Batch Acc: 0.097470, Tokens per Sec:     3870, Lr: 0.000072\r\n",
      "2024-08-27 14:38:26,240 - INFO - joeynmt.training - Epoch   1, Step:       50, Batch Loss:     7.179103, Batch Acc: 0.102200, Tokens per Sec:     3840, Lr: 0.000080\r\n",
      "2024-08-27 14:38:27,368 - INFO - joeynmt.training - Epoch   1, Step:       55, Batch Loss:     7.109608, Batch Acc: 0.101582, Tokens per Sec:     3474, Lr: 0.000088\r\n",
      "2024-08-27 14:38:28,501 - INFO - joeynmt.training - Epoch   1, Step:       60, Batch Loss:     7.023612, Batch Acc: 0.096063, Tokens per Sec:     3723, Lr: 0.000096\r\n",
      "2024-08-27 14:38:29,653 - INFO - joeynmt.training - Epoch   1, Step:       65, Batch Loss:     6.870697, Batch Acc: 0.101379, Tokens per Sec:     3778, Lr: 0.000104\r\n",
      "2024-08-27 14:38:30,806 - INFO - joeynmt.training - Epoch   1, Step:       70, Batch Loss:     6.784752, Batch Acc: 0.104206, Tokens per Sec:     3717, Lr: 0.000112\r\n",
      "2024-08-27 14:38:31,947 - INFO - joeynmt.training - Epoch   1, Step:       75, Batch Loss:     6.637016, Batch Acc: 0.100069, Tokens per Sec:     3795, Lr: 0.000120\r\n",
      "2024-08-27 14:38:33,070 - INFO - joeynmt.training - Epoch   1, Step:       80, Batch Loss:     6.567925, Batch Acc: 0.097847, Tokens per Sec:     3725, Lr: 0.000128\r\n",
      "2024-08-27 14:38:34,220 - INFO - joeynmt.training - Epoch   1, Step:       85, Batch Loss:     6.493937, Batch Acc: 0.099414, Tokens per Sec:     3860, Lr: 0.000136\r\n",
      "2024-08-27 14:38:35,359 - INFO - joeynmt.training - Epoch   1, Step:       90, Batch Loss:     6.444967, Batch Acc: 0.102172, Tokens per Sec:     3681, Lr: 0.000144\r\n",
      "2024-08-27 14:38:36,515 - INFO - joeynmt.training - Epoch   1, Step:       95, Batch Loss:     6.260778, Batch Acc: 0.104831, Tokens per Sec:     3796, Lr: 0.000152\r\n",
      "2024-08-27 14:38:36,847 - INFO - joeynmt.training - Epoch   1, total training loss: 684.64, num. of seqs: 8065, num. of tokens: 80463, 22.6680[sec]\r\n",
      "2024-08-27 14:38:36,847 - INFO - joeynmt.training - EPOCH 2\r\n",
      "2024-08-27 14:38:37,762 - INFO - joeynmt.training - Epoch   2, Step:      100, Batch Loss:     6.201378, Batch Acc: 0.102653, Tokens per Sec:     3813, Lr: 0.000160\r\n",
      "2024-08-27 14:38:38,872 - INFO - joeynmt.training - Epoch   2, Step:      105, Batch Loss:     6.243502, Batch Acc: 0.098476, Tokens per Sec:     3725, Lr: 0.000168\r\n",
      "2024-08-27 14:38:39,969 - INFO - joeynmt.training - Epoch   2, Step:      110, Batch Loss:     6.093081, Batch Acc: 0.106446, Tokens per Sec:     4021, Lr: 0.000176\r\n",
      "2024-08-27 14:38:41,074 - INFO - joeynmt.training - Epoch   2, Step:      115, Batch Loss:     6.086413, Batch Acc: 0.103246, Tokens per Sec:     4018, Lr: 0.000184\r\n",
      "2024-08-27 14:38:42,194 - INFO - joeynmt.training - Epoch   2, Step:      120, Batch Loss:     5.909479, Batch Acc: 0.106103, Tokens per Sec:     3863, Lr: 0.000192\r\n",
      "2024-08-27 14:38:43,332 - INFO - joeynmt.training - Epoch   2, Step:      125, Batch Loss:     6.040269, Batch Acc: 0.109975, Tokens per Sec:     3527, Lr: 0.000200\r\n",
      "2024-08-27 14:38:44,532 - INFO - joeynmt.training - Epoch   2, Step:      130, Batch Loss:     5.832957, Batch Acc: 0.114651, Tokens per Sec:     3455, Lr: 0.000208\r\n",
      "2024-08-27 14:38:45,656 - INFO - joeynmt.training - Epoch   2, Step:      135, Batch Loss:     5.882043, Batch Acc: 0.111266, Tokens per Sec:     3825, Lr: 0.000216\r\n",
      "2024-08-27 14:38:46,815 - INFO - joeynmt.training - Epoch   2, Step:      140, Batch Loss:     5.901101, Batch Acc: 0.127978, Tokens per Sec:     3516, Lr: 0.000224\r\n",
      "2024-08-27 14:38:47,923 - INFO - joeynmt.training - Epoch   2, Step:      145, Batch Loss:     5.903374, Batch Acc: 0.127127, Tokens per Sec:     3607, Lr: 0.000232\r\n",
      "2024-08-27 14:38:49,048 - INFO - joeynmt.training - Epoch   2, Step:      150, Batch Loss:     5.802353, Batch Acc: 0.130250, Tokens per Sec:     3772, Lr: 0.000240\r\n",
      "2024-08-27 14:38:50,173 - INFO - joeynmt.training - Epoch   2, Step:      155, Batch Loss:     5.852951, Batch Acc: 0.121637, Tokens per Sec:     3867, Lr: 0.000248\r\n",
      "2024-08-27 14:38:51,294 - INFO - joeynmt.training - Epoch   2, Step:      160, Batch Loss:     5.924385, Batch Acc: 0.124033, Tokens per Sec:     3691, Lr: 0.000256\r\n",
      "2024-08-27 14:38:52,427 - INFO - joeynmt.training - Epoch   2, Step:      165, Batch Loss:     5.859796, Batch Acc: 0.133613, Tokens per Sec:     3796, Lr: 0.000264\r\n",
      "2024-08-27 14:38:53,522 - INFO - joeynmt.training - Epoch   2, Step:      170, Batch Loss:     5.849961, Batch Acc: 0.122610, Tokens per Sec:     3822, Lr: 0.000272\r\n",
      "2024-08-27 14:38:54,594 - INFO - joeynmt.training - Epoch   2, Step:      175, Batch Loss:     5.708818, Batch Acc: 0.123428, Tokens per Sec:     4010, Lr: 0.000280\r\n",
      "2024-08-27 14:38:55,680 - INFO - joeynmt.training - Epoch   2, Step:      180, Batch Loss:     5.759852, Batch Acc: 0.131683, Tokens per Sec:     3946, Lr: 0.000288\r\n",
      "2024-08-27 14:38:56,889 - INFO - joeynmt.training - Epoch   2, Step:      185, Batch Loss:     5.719266, Batch Acc: 0.132315, Tokens per Sec:     3521, Lr: 0.000296\r\n",
      "2024-08-27 14:38:58,060 - INFO - joeynmt.training - Epoch   2, Step:      190, Batch Loss:     5.726038, Batch Acc: 0.144201, Tokens per Sec:     3543, Lr: 0.000304\r\n",
      "2024-08-27 14:38:58,344 - INFO - joeynmt.training - Epoch   2, total training loss: 562.78, num. of seqs: 8065, num. of tokens: 80463, 21.4771[sec]\r\n",
      "2024-08-27 14:38:58,344 - INFO - joeynmt.training - EPOCH 3\r\n",
      "2024-08-27 14:38:59,282 - INFO - joeynmt.training - Epoch   3, Step:      195, Batch Loss:     5.657017, Batch Acc: 0.135194, Tokens per Sec:     3456, Lr: 0.000312\r\n",
      "2024-08-27 14:39:00,412 - INFO - joeynmt.training - Epoch   3, Step:      200, Batch Loss:     5.614244, Batch Acc: 0.136899, Tokens per Sec:     3757, Lr: 0.000320\r\n",
      "2024-08-27 14:39:01,542 - INFO - joeynmt.training - Epoch   3, Step:      205, Batch Loss:     5.703935, Batch Acc: 0.143303, Tokens per Sec:     3689, Lr: 0.000328\r\n",
      "2024-08-27 14:39:02,651 - INFO - joeynmt.training - Epoch   3, Step:      210, Batch Loss:     5.616012, Batch Acc: 0.154501, Tokens per Sec:     3917, Lr: 0.000336\r\n",
      "2024-08-27 14:39:03,783 - INFO - joeynmt.training - Epoch   3, Step:      215, Batch Loss:     5.547676, Batch Acc: 0.150763, Tokens per Sec:     3707, Lr: 0.000344\r\n",
      "2024-08-27 14:39:04,926 - INFO - joeynmt.training - Epoch   3, Step:      220, Batch Loss:     5.538640, Batch Acc: 0.159800, Tokens per Sec:     3675, Lr: 0.000352\r\n",
      "2024-08-27 14:39:06,052 - INFO - joeynmt.training - Epoch   3, Step:      225, Batch Loss:     5.580638, Batch Acc: 0.150121, Tokens per Sec:     3672, Lr: 0.000360\r\n",
      "2024-08-27 14:39:07,205 - INFO - joeynmt.training - Epoch   3, Step:      230, Batch Loss:     5.526555, Batch Acc: 0.161798, Tokens per Sec:     3803, Lr: 0.000368\r\n",
      "2024-08-27 14:39:08,289 - INFO - joeynmt.training - Epoch   3, Step:      235, Batch Loss:     5.473674, Batch Acc: 0.166156, Tokens per Sec:     3916, Lr: 0.000376\r\n",
      "2024-08-27 14:39:09,380 - INFO - joeynmt.training - Epoch   3, Step:      240, Batch Loss:     5.378229, Batch Acc: 0.165975, Tokens per Sec:     3756, Lr: 0.000384\r\n",
      "2024-08-27 14:39:10,463 - INFO - joeynmt.training - Epoch   3, Step:      245, Batch Loss:     5.498674, Batch Acc: 0.168658, Tokens per Sec:     3946, Lr: 0.000392\r\n",
      "2024-08-27 14:39:11,585 - INFO - joeynmt.training - Epoch   3, Step:      250, Batch Loss:     5.482623, Batch Acc: 0.163469, Tokens per Sec:     3672, Lr: 0.000400\r\n",
      "2024-08-27 14:39:12,743 - INFO - joeynmt.training - Epoch   3, Step:      255, Batch Loss:     5.312853, Batch Acc: 0.169751, Tokens per Sec:     3504, Lr: 0.000408\r\n",
      "2024-08-27 14:39:13,902 - INFO - joeynmt.training - Epoch   3, Step:      260, Batch Loss:     5.545098, Batch Acc: 0.166326, Tokens per Sec:     3804, Lr: 0.000416\r\n",
      "2024-08-27 14:39:15,125 - INFO - joeynmt.training - Epoch   3, Step:      265, Batch Loss:     5.424301, Batch Acc: 0.174962, Tokens per Sec:     3220, Lr: 0.000424\r\n",
      "2024-08-27 14:39:16,239 - INFO - joeynmt.training - Epoch   3, Step:      270, Batch Loss:     5.130863, Batch Acc: 0.170623, Tokens per Sec:     3823, Lr: 0.000432\r\n",
      "2024-08-27 14:39:17,364 - INFO - joeynmt.training - Epoch   3, Step:      275, Batch Loss:     5.296248, Batch Acc: 0.172544, Tokens per Sec:     3533, Lr: 0.000440\r\n",
      "2024-08-27 14:39:18,479 - INFO - joeynmt.training - Epoch   3, Step:      280, Batch Loss:     5.157894, Batch Acc: 0.174741, Tokens per Sec:     3723, Lr: 0.000448\r\n",
      "2024-08-27 14:39:19,624 - INFO - joeynmt.training - Epoch   3, Step:      285, Batch Loss:     5.311334, Batch Acc: 0.164941, Tokens per Sec:     3799, Lr: 0.000456\r\n",
      "2024-08-27 14:39:20,079 - INFO - joeynmt.training - Epoch   3, total training loss: 521.64, num. of seqs: 8065, num. of tokens: 80463, 21.7167[sec]\r\n",
      "2024-08-27 14:39:20,079 - INFO - joeynmt.training - EPOCH 4\r\n",
      "2024-08-27 14:39:20,765 - INFO - joeynmt.training - Epoch   4, Step:      290, Batch Loss:     5.275478, Batch Acc: 0.178172, Tokens per Sec:     3800, Lr: 0.000464\r\n",
      "2024-08-27 14:39:21,908 - INFO - joeynmt.training - Epoch   4, Step:      295, Batch Loss:     5.117806, Batch Acc: 0.179424, Tokens per Sec:     3925, Lr: 0.000472\r\n",
      "2024-08-27 14:39:23,007 - INFO - joeynmt.training - Epoch   4, Step:      300, Batch Loss:     4.936431, Batch Acc: 0.184369, Tokens per Sec:     3925, Lr: 0.000480\r\n",
      "2024-08-27 14:39:24,102 - INFO - joeynmt.training - Epoch   4, Step:      305, Batch Loss:     5.139072, Batch Acc: 0.188163, Tokens per Sec:     3877, Lr: 0.000488\r\n",
      "2024-08-27 14:39:25,182 - INFO - joeynmt.training - Epoch   4, Step:      310, Batch Loss:     5.020082, Batch Acc: 0.181524, Tokens per Sec:     3999, Lr: 0.000496\r\n",
      "2024-08-27 14:39:26,314 - INFO - joeynmt.training - Epoch   4, Step:      315, Batch Loss:     4.952558, Batch Acc: 0.197449, Tokens per Sec:     3742, Lr: 0.000504\r\n",
      "2024-08-27 14:39:27,507 - INFO - joeynmt.training - Epoch   4, Step:      320, Batch Loss:     4.919631, Batch Acc: 0.196365, Tokens per Sec:     3509, Lr: 0.000512\r\n",
      "2024-08-27 14:39:28,646 - INFO - joeynmt.training - Epoch   4, Step:      325, Batch Loss:     4.979871, Batch Acc: 0.194107, Tokens per Sec:     3754, Lr: 0.000520\r\n",
      "2024-08-27 14:39:29,783 - INFO - joeynmt.training - Epoch   4, Step:      330, Batch Loss:     4.809591, Batch Acc: 0.204096, Tokens per Sec:     3782, Lr: 0.000528\r\n",
      "2024-08-27 14:39:30,913 - INFO - joeynmt.training - Epoch   4, Step:      335, Batch Loss:     4.933644, Batch Acc: 0.194366, Tokens per Sec:     3773, Lr: 0.000536\r\n",
      "2024-08-27 14:39:32,014 - INFO - joeynmt.training - Epoch   4, Step:      340, Batch Loss:     4.999483, Batch Acc: 0.208628, Tokens per Sec:     3729, Lr: 0.000544\r\n",
      "2024-08-27 14:39:33,137 - INFO - joeynmt.training - Epoch   4, Step:      345, Batch Loss:     4.912345, Batch Acc: 0.193459, Tokens per Sec:     3844, Lr: 0.000552\r\n",
      "2024-08-27 14:39:34,270 - INFO - joeynmt.training - Epoch   4, Step:      350, Batch Loss:     4.898313, Batch Acc: 0.207859, Tokens per Sec:     3797, Lr: 0.000560\r\n",
      "2024-08-27 14:39:35,387 - INFO - joeynmt.training - Epoch   4, Step:      355, Batch Loss:     5.067392, Batch Acc: 0.188095, Tokens per Sec:     3765, Lr: 0.000568\r\n",
      "2024-08-27 14:39:36,478 - INFO - joeynmt.training - Epoch   4, Step:      360, Batch Loss:     4.799327, Batch Acc: 0.193802, Tokens per Sec:     3728, Lr: 0.000576\r\n",
      "2024-08-27 14:39:37,600 - INFO - joeynmt.training - Epoch   4, Step:      365, Batch Loss:     4.990727, Batch Acc: 0.203781, Tokens per Sec:     3867, Lr: 0.000584\r\n",
      "2024-08-27 14:39:38,676 - INFO - joeynmt.training - Epoch   4, Step:      370, Batch Loss:     4.757086, Batch Acc: 0.195336, Tokens per Sec:     3710, Lr: 0.000592\r\n",
      "2024-08-27 14:39:39,766 - INFO - joeynmt.training - Epoch   4, Step:      375, Batch Loss:     4.686061, Batch Acc: 0.210564, Tokens per Sec:     3894, Lr: 0.000600\r\n",
      "2024-08-27 14:39:40,889 - INFO - joeynmt.training - Epoch   4, Step:      380, Batch Loss:     4.863984, Batch Acc: 0.193644, Tokens per Sec:     3896, Lr: 0.000608\r\n",
      "2024-08-27 14:39:41,279 - INFO - joeynmt.training - Epoch   4, total training loss: 466.48, num. of seqs: 8065, num. of tokens: 80463, 21.1819[sec]\r\n",
      "2024-08-27 14:39:41,279 - INFO - joeynmt.training - EPOCH 5\r\n",
      "2024-08-27 14:39:42,230 - INFO - joeynmt.training - Epoch   5, Step:      385, Batch Loss:     4.624754, Batch Acc: 0.220787, Tokens per Sec:     3596, Lr: 0.000616\r\n",
      "2024-08-27 14:39:43,354 - INFO - joeynmt.training - Epoch   5, Step:      390, Batch Loss:     4.824134, Batch Acc: 0.215454, Tokens per Sec:     3759, Lr: 0.000624\r\n",
      "2024-08-27 14:39:44,460 - INFO - joeynmt.training - Epoch   5, Step:      395, Batch Loss:     4.673131, Batch Acc: 0.203823, Tokens per Sec:     3926, Lr: 0.000632\r\n",
      "2024-08-27 14:39:45,587 - INFO - joeynmt.training - Epoch   5, Step:      400, Batch Loss:     4.545216, Batch Acc: 0.215349, Tokens per Sec:     3877, Lr: 0.000640\r\n",
      "2024-08-27 14:39:46,857 - INFO - joeynmt.training - Epoch   5, Step:      405, Batch Loss:     4.415812, Batch Acc: 0.227175, Tokens per Sec:     3287, Lr: 0.000648\r\n",
      "2024-08-27 14:39:47,982 - INFO - joeynmt.training - Epoch   5, Step:      410, Batch Loss:     4.503893, Batch Acc: 0.239310, Tokens per Sec:     3455, Lr: 0.000656\r\n",
      "2024-08-27 14:39:49,094 - INFO - joeynmt.training - Epoch   5, Step:      415, Batch Loss:     4.694587, Batch Acc: 0.220890, Tokens per Sec:     3599, Lr: 0.000664\r\n",
      "2024-08-27 14:39:50,204 - INFO - joeynmt.training - Epoch   5, Step:      420, Batch Loss:     4.576324, Batch Acc: 0.222524, Tokens per Sec:     3651, Lr: 0.000672\r\n",
      "2024-08-27 14:39:51,300 - INFO - joeynmt.training - Epoch   5, Step:      425, Batch Loss:     4.552108, Batch Acc: 0.228904, Tokens per Sec:     3842, Lr: 0.000680\r\n",
      "2024-08-27 14:39:52,384 - INFO - joeynmt.training - Epoch   5, Step:      430, Batch Loss:     4.525032, Batch Acc: 0.232168, Tokens per Sec:     3907, Lr: 0.000688\r\n",
      "2024-08-27 14:39:53,485 - INFO - joeynmt.training - Epoch   5, Step:      435, Batch Loss:     4.437235, Batch Acc: 0.219408, Tokens per Sec:     4058, Lr: 0.000696\r\n",
      "2024-08-27 14:39:54,585 - INFO - joeynmt.training - Epoch   5, Step:      440, Batch Loss:     4.609116, Batch Acc: 0.232052, Tokens per Sec:     3925, Lr: 0.000704\r\n",
      "2024-08-27 14:39:55,726 - INFO - joeynmt.training - Epoch   5, Step:      445, Batch Loss:     4.504756, Batch Acc: 0.222523, Tokens per Sec:     3889, Lr: 0.000712\r\n",
      "2024-08-27 14:39:56,941 - INFO - joeynmt.training - Epoch   5, Step:      450, Batch Loss:     4.513941, Batch Acc: 0.226561, Tokens per Sec:     3520, Lr: 0.000720\r\n",
      "2024-08-27 14:39:58,065 - INFO - joeynmt.training - Epoch   5, Step:      455, Batch Loss:     4.636655, Batch Acc: 0.229566, Tokens per Sec:     3758, Lr: 0.000728\r\n",
      "2024-08-27 14:39:59,191 - INFO - joeynmt.training - Epoch   5, Step:      460, Batch Loss:     4.845544, Batch Acc: 0.221931, Tokens per Sec:     3728, Lr: 0.000736\r\n",
      "2024-08-27 14:40:00,297 - INFO - joeynmt.training - Epoch   5, Step:      465, Batch Loss:     4.560014, Batch Acc: 0.221795, Tokens per Sec:     3760, Lr: 0.000744\r\n",
      "2024-08-27 14:40:01,409 - INFO - joeynmt.training - Epoch   5, Step:      470, Batch Loss:     4.593227, Batch Acc: 0.238615, Tokens per Sec:     3795, Lr: 0.000752\r\n",
      "2024-08-27 14:40:02,508 - INFO - joeynmt.training - Epoch   5, Step:      475, Batch Loss:     4.658475, Batch Acc: 0.225305, Tokens per Sec:     3806, Lr: 0.000760\r\n",
      "2024-08-27 14:40:02,839 - INFO - joeynmt.training - Epoch   5, total training loss: 437.92, num. of seqs: 8065, num. of tokens: 80463, 21.5415[sec]\r\n",
      "2024-08-27 14:40:02,840 - INFO - joeynmt.training - EPOCH 6\r\n",
      "2024-08-27 14:40:03,737 - INFO - joeynmt.training - Epoch   6, Step:      480, Batch Loss:     4.456407, Batch Acc: 0.247411, Tokens per Sec:     3893, Lr: 0.000768\r\n",
      "2024-08-27 14:40:04,836 - INFO - joeynmt.training - Epoch   6, Step:      485, Batch Loss:     4.169904, Batch Acc: 0.251484, Tokens per Sec:     3835, Lr: 0.000776\r\n",
      "2024-08-27 14:40:05,919 - INFO - joeynmt.training - Epoch   6, Step:      490, Batch Loss:     4.280671, Batch Acc: 0.244726, Tokens per Sec:     3941, Lr: 0.000784\r\n",
      "2024-08-27 14:40:07,046 - INFO - joeynmt.training - Epoch   6, Step:      495, Batch Loss:     4.175765, Batch Acc: 0.248108, Tokens per Sec:     3755, Lr: 0.000792\r\n",
      "2024-08-27 14:40:08,110 - INFO - joeynmt.training - Epoch   6, Step:      500, Batch Loss:     4.452656, Batch Acc: 0.249644, Tokens per Sec:     3958, Lr: 0.000800\r\n",
      "2024-08-27 14:40:08,111 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=542\r\n",
      "2024-08-27 14:40:08,111 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:55<00:00, 26.53it/s]\r\n",
      "2024-08-27 14:41:03,145 - INFO - joeynmt.prediction - Generation took 55.0317[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 45085.21 examples/s]\r\n",
      "2024-08-27 14:41:03,519 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 14:41:03,519 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   0.37, loss:   4.64, ppl: 103.55, acc:   0.21, 0.1460[sec]\r\n",
      "2024-08-27 14:41:03,520 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\r\n",
      "2024-08-27 14:41:04,067 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/500.ckpt.\r\n",
      "2024-08-27 14:41:04,068 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 14:41:04,213 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 14:41:04,214 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 14:41:04,214 - INFO - joeynmt.training - \tHypothesis: c’est ce que tu es sûr\r\n",
      "2024-08-27 14:41:04,508 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 14:41:04,653 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 14:41:04,654 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 14:41:04,654 - INFO - joeynmt.training - \tHypothesis: un un petits livres\r\n",
      "2024-08-27 14:41:04,944 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 14:41:05,090 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 14:41:05,090 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 14:41:05,090 - INFO - joeynmt.training - \tHypothesis: c’est un autre chose\r\n",
      "2024-08-27 14:41:05,379 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 14:41:05,525 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 14:41:05,526 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 14:41:05,526 - INFO - joeynmt.training - \tHypothesis: une petite petitessss\r\n",
      "2024-08-27 14:41:06,995 - INFO - joeynmt.training - Epoch   6, Step:      505, Batch Loss:     4.265695, Batch Acc: 0.248695, Tokens per Sec:     3577, Lr: 0.000796\r\n",
      "2024-08-27 14:41:08,158 - INFO - joeynmt.training - Epoch   6, Step:      510, Batch Loss:     4.455061, Batch Acc: 0.249165, Tokens per Sec:     3608, Lr: 0.000792\r\n",
      "2024-08-27 14:41:09,295 - INFO - joeynmt.training - Epoch   6, Step:      515, Batch Loss:     4.352741, Batch Acc: 0.245384, Tokens per Sec:     3622, Lr: 0.000788\r\n",
      "2024-08-27 14:41:10,405 - INFO - joeynmt.training - Epoch   6, Step:      520, Batch Loss:     4.505930, Batch Acc: 0.245168, Tokens per Sec:     3685, Lr: 0.000784\r\n",
      "2024-08-27 14:41:11,519 - INFO - joeynmt.training - Epoch   6, Step:      525, Batch Loss:     4.254503, Batch Acc: 0.244460, Tokens per Sec:     3934, Lr: 0.000781\r\n",
      "2024-08-27 14:41:12,633 - INFO - joeynmt.training - Epoch   6, Step:      530, Batch Loss:     4.327548, Batch Acc: 0.244127, Tokens per Sec:     3898, Lr: 0.000777\r\n",
      "2024-08-27 14:41:13,736 - INFO - joeynmt.training - Epoch   6, Step:      535, Batch Loss:     4.390296, Batch Acc: 0.246171, Tokens per Sec:     3969, Lr: 0.000773\r\n",
      "2024-08-27 14:41:14,848 - INFO - joeynmt.training - Epoch   6, Step:      540, Batch Loss:     4.403628, Batch Acc: 0.257831, Tokens per Sec:     3850, Lr: 0.000770\r\n",
      "2024-08-27 14:41:15,961 - INFO - joeynmt.training - Epoch   6, Step:      545, Batch Loss:     4.216337, Batch Acc: 0.254584, Tokens per Sec:     3826, Lr: 0.000766\r\n",
      "2024-08-27 14:41:17,122 - INFO - joeynmt.training - Epoch   6, Step:      550, Batch Loss:     4.338320, Batch Acc: 0.251814, Tokens per Sec:     3562, Lr: 0.000763\r\n",
      "2024-08-27 14:41:18,218 - INFO - joeynmt.training - Epoch   6, Step:      555, Batch Loss:     4.230304, Batch Acc: 0.264085, Tokens per Sec:     3891, Lr: 0.000759\r\n",
      "2024-08-27 14:41:19,297 - INFO - joeynmt.training - Epoch   6, Step:      560, Batch Loss:     4.144718, Batch Acc: 0.252309, Tokens per Sec:     3816, Lr: 0.000756\r\n",
      "2024-08-27 14:41:20,542 - INFO - joeynmt.training - Epoch   6, Step:      565, Batch Loss:     4.227450, Batch Acc: 0.254341, Tokens per Sec:     3287, Lr: 0.000753\r\n",
      "2024-08-27 14:41:21,756 - INFO - joeynmt.training - Epoch   6, Step:      570, Batch Loss:     4.192854, Batch Acc: 0.253289, Tokens per Sec:     3444, Lr: 0.000749\r\n",
      "2024-08-27 14:41:22,091 - INFO - joeynmt.training - Epoch   6, total training loss: 408.80, num. of seqs: 8065, num. of tokens: 80463, 21.5261[sec]\r\n",
      "2024-08-27 14:41:22,091 - INFO - joeynmt.training - EPOCH 7\r\n",
      "2024-08-27 14:41:23,008 - INFO - joeynmt.training - Epoch   7, Step:      575, Batch Loss:     4.069407, Batch Acc: 0.273990, Tokens per Sec:     3472, Lr: 0.000746\r\n",
      "2024-08-27 14:41:24,124 - INFO - joeynmt.training - Epoch   7, Step:      580, Batch Loss:     3.988603, Batch Acc: 0.282183, Tokens per Sec:     3843, Lr: 0.000743\r\n",
      "2024-08-27 14:41:25,240 - INFO - joeynmt.training - Epoch   7, Step:      585, Batch Loss:     3.981019, Batch Acc: 0.266996, Tokens per Sec:     3811, Lr: 0.000740\r\n",
      "2024-08-27 14:41:26,341 - INFO - joeynmt.training - Epoch   7, Step:      590, Batch Loss:     4.172950, Batch Acc: 0.274648, Tokens per Sec:     3742, Lr: 0.000736\r\n",
      "2024-08-27 14:41:27,482 - INFO - joeynmt.training - Epoch   7, Step:      595, Batch Loss:     3.893836, Batch Acc: 0.270721, Tokens per Sec:     3577, Lr: 0.000733\r\n",
      "2024-08-27 14:41:28,591 - INFO - joeynmt.training - Epoch   7, Step:      600, Batch Loss:     4.040068, Batch Acc: 0.277567, Tokens per Sec:     3797, Lr: 0.000730\r\n",
      "2024-08-27 14:41:29,690 - INFO - joeynmt.training - Epoch   7, Step:      605, Batch Loss:     3.953920, Batch Acc: 0.270078, Tokens per Sec:     3720, Lr: 0.000727\r\n",
      "2024-08-27 14:41:30,805 - INFO - joeynmt.training - Epoch   7, Step:      610, Batch Loss:     3.948743, Batch Acc: 0.274715, Tokens per Sec:     3859, Lr: 0.000724\r\n",
      "2024-08-27 14:41:31,895 - INFO - joeynmt.training - Epoch   7, Step:      615, Batch Loss:     3.958625, Batch Acc: 0.278796, Tokens per Sec:     3906, Lr: 0.000721\r\n",
      "2024-08-27 14:41:32,981 - INFO - joeynmt.training - Epoch   7, Step:      620, Batch Loss:     4.122873, Batch Acc: 0.268095, Tokens per Sec:     4086, Lr: 0.000718\r\n",
      "2024-08-27 14:41:34,068 - INFO - joeynmt.training - Epoch   7, Step:      625, Batch Loss:     3.983221, Batch Acc: 0.280894, Tokens per Sec:     3872, Lr: 0.000716\r\n",
      "2024-08-27 14:41:35,180 - INFO - joeynmt.training - Epoch   7, Step:      630, Batch Loss:     4.237326, Batch Acc: 0.269881, Tokens per Sec:     3566, Lr: 0.000713\r\n",
      "2024-08-27 14:41:36,331 - INFO - joeynmt.training - Epoch   7, Step:      635, Batch Loss:     4.017458, Batch Acc: 0.267576, Tokens per Sec:     3698, Lr: 0.000710\r\n",
      "2024-08-27 14:41:37,541 - INFO - joeynmt.training - Epoch   7, Step:      640, Batch Loss:     4.037114, Batch Acc: 0.261135, Tokens per Sec:     3526, Lr: 0.000707\r\n",
      "2024-08-27 14:41:38,673 - INFO - joeynmt.training - Epoch   7, Step:      645, Batch Loss:     4.027020, Batch Acc: 0.274356, Tokens per Sec:     3603, Lr: 0.000704\r\n",
      "2024-08-27 14:41:39,808 - INFO - joeynmt.training - Epoch   7, Step:      650, Batch Loss:     4.034342, Batch Acc: 0.272081, Tokens per Sec:     3724, Lr: 0.000702\r\n",
      "2024-08-27 14:41:40,925 - INFO - joeynmt.training - Epoch   7, Step:      655, Batch Loss:     4.303338, Batch Acc: 0.271274, Tokens per Sec:     3811, Lr: 0.000699\r\n",
      "2024-08-27 14:41:42,047 - INFO - joeynmt.training - Epoch   7, Step:      660, Batch Loss:     3.874906, Batch Acc: 0.281858, Tokens per Sec:     3668, Lr: 0.000696\r\n",
      "2024-08-27 14:41:43,159 - INFO - joeynmt.training - Epoch   7, Step:      665, Batch Loss:     4.068951, Batch Acc: 0.280065, Tokens per Sec:     3889, Lr: 0.000694\r\n",
      "2024-08-27 14:41:43,617 - INFO - joeynmt.training - Epoch   7, total training loss: 386.12, num. of seqs: 8065, num. of tokens: 80463, 21.5079[sec]\r\n",
      "2024-08-27 14:41:43,617 - INFO - joeynmt.training - EPOCH 8\r\n",
      "2024-08-27 14:41:44,277 - INFO - joeynmt.training - Epoch   8, Step:      670, Batch Loss:     3.850002, Batch Acc: 0.278175, Tokens per Sec:     3840, Lr: 0.000691\r\n",
      "2024-08-27 14:41:45,385 - INFO - joeynmt.training - Epoch   8, Step:      675, Batch Loss:     3.690777, Batch Acc: 0.297983, Tokens per Sec:     3806, Lr: 0.000689\r\n",
      "2024-08-27 14:41:46,463 - INFO - joeynmt.training - Epoch   8, Step:      680, Batch Loss:     3.819889, Batch Acc: 0.298994, Tokens per Sec:     3873, Lr: 0.000686\r\n",
      "2024-08-27 14:41:47,593 - INFO - joeynmt.training - Epoch   8, Step:      685, Batch Loss:     3.707189, Batch Acc: 0.294399, Tokens per Sec:     3891, Lr: 0.000683\r\n",
      "2024-08-27 14:41:48,664 - INFO - joeynmt.training - Epoch   8, Step:      690, Batch Loss:     3.691719, Batch Acc: 0.308019, Tokens per Sec:     3963, Lr: 0.000681\r\n",
      "2024-08-27 14:41:49,788 - INFO - joeynmt.training - Epoch   8, Step:      695, Batch Loss:     3.589730, Batch Acc: 0.322938, Tokens per Sec:     3537, Lr: 0.000679\r\n",
      "2024-08-27 14:41:50,927 - INFO - joeynmt.training - Epoch   8, Step:      700, Batch Loss:     3.852026, Batch Acc: 0.303777, Tokens per Sec:     3814, Lr: 0.000676\r\n",
      "2024-08-27 14:41:52,185 - INFO - joeynmt.training - Epoch   8, Step:      705, Batch Loss:     3.797360, Batch Acc: 0.292444, Tokens per Sec:     3326, Lr: 0.000674\r\n",
      "2024-08-27 14:41:53,328 - INFO - joeynmt.training - Epoch   8, Step:      710, Batch Loss:     3.603056, Batch Acc: 0.295704, Tokens per Sec:     3834, Lr: 0.000671\r\n",
      "2024-08-27 14:41:54,455 - INFO - joeynmt.training - Epoch   8, Step:      715, Batch Loss:     3.761799, Batch Acc: 0.300374, Tokens per Sec:     3799, Lr: 0.000669\r\n",
      "2024-08-27 14:41:55,570 - INFO - joeynmt.training - Epoch   8, Step:      720, Batch Loss:     3.906378, Batch Acc: 0.297929, Tokens per Sec:     3725, Lr: 0.000667\r\n",
      "2024-08-27 14:41:56,722 - INFO - joeynmt.training - Epoch   8, Step:      725, Batch Loss:     3.695255, Batch Acc: 0.299127, Tokens per Sec:     3580, Lr: 0.000664\r\n",
      "2024-08-27 14:41:57,848 - INFO - joeynmt.training - Epoch   8, Step:      730, Batch Loss:     3.605813, Batch Acc: 0.308660, Tokens per Sec:     3675, Lr: 0.000662\r\n",
      "2024-08-27 14:41:58,957 - INFO - joeynmt.training - Epoch   8, Step:      735, Batch Loss:     3.884399, Batch Acc: 0.291085, Tokens per Sec:     3814, Lr: 0.000660\r\n",
      "2024-08-27 14:42:00,056 - INFO - joeynmt.training - Epoch   8, Step:      740, Batch Loss:     3.807841, Batch Acc: 0.291841, Tokens per Sec:     3695, Lr: 0.000658\r\n",
      "2024-08-27 14:42:01,195 - INFO - joeynmt.training - Epoch   8, Step:      745, Batch Loss:     3.802805, Batch Acc: 0.290407, Tokens per Sec:     3690, Lr: 0.000655\r\n",
      "2024-08-27 14:42:02,287 - INFO - joeynmt.training - Epoch   8, Step:      750, Batch Loss:     3.717628, Batch Acc: 0.298826, Tokens per Sec:     3903, Lr: 0.000653\r\n",
      "2024-08-27 14:42:03,374 - INFO - joeynmt.training - Epoch   8, Step:      755, Batch Loss:     3.850421, Batch Acc: 0.298659, Tokens per Sec:     3914, Lr: 0.000651\r\n",
      "2024-08-27 14:42:04,505 - INFO - joeynmt.training - Epoch   8, Step:      760, Batch Loss:     3.793281, Batch Acc: 0.298979, Tokens per Sec:     3810, Lr: 0.000649\r\n",
      "2024-08-27 14:42:05,111 - INFO - joeynmt.training - Epoch   8, total training loss: 358.03, num. of seqs: 8065, num. of tokens: 80463, 21.4762[sec]\r\n",
      "2024-08-27 14:42:05,111 - INFO - joeynmt.training - EPOCH 9\r\n",
      "2024-08-27 14:42:05,793 - INFO - joeynmt.training - Epoch   9, Step:      765, Batch Loss:     3.399862, Batch Acc: 0.335013, Tokens per Sec:     3518, Lr: 0.000647\r\n",
      "2024-08-27 14:42:06,952 - INFO - joeynmt.training - Epoch   9, Step:      770, Batch Loss:     3.438100, Batch Acc: 0.334802, Tokens per Sec:     3526, Lr: 0.000645\r\n",
      "2024-08-27 14:42:08,061 - INFO - joeynmt.training - Epoch   9, Step:      775, Batch Loss:     3.382215, Batch Acc: 0.328493, Tokens per Sec:     3791, Lr: 0.000643\r\n",
      "2024-08-27 14:42:09,166 - INFO - joeynmt.training - Epoch   9, Step:      780, Batch Loss:     3.500538, Batch Acc: 0.328427, Tokens per Sec:     3754, Lr: 0.000641\r\n",
      "2024-08-27 14:42:10,280 - INFO - joeynmt.training - Epoch   9, Step:      785, Batch Loss:     3.503393, Batch Acc: 0.319366, Tokens per Sec:     3793, Lr: 0.000638\r\n",
      "2024-08-27 14:42:11,402 - INFO - joeynmt.training - Epoch   9, Step:      790, Batch Loss:     3.586488, Batch Acc: 0.323509, Tokens per Sec:     3875, Lr: 0.000636\r\n",
      "2024-08-27 14:42:12,526 - INFO - joeynmt.training - Epoch   9, Step:      795, Batch Loss:     3.374840, Batch Acc: 0.339663, Tokens per Sec:     3751, Lr: 0.000634\r\n",
      "2024-08-27 14:42:13,655 - INFO - joeynmt.training - Epoch   9, Step:      800, Batch Loss:     3.532299, Batch Acc: 0.322789, Tokens per Sec:     3698, Lr: 0.000632\r\n",
      "2024-08-27 14:42:14,764 - INFO - joeynmt.training - Epoch   9, Step:      805, Batch Loss:     3.762293, Batch Acc: 0.319719, Tokens per Sec:     3600, Lr: 0.000630\r\n",
      "2024-08-27 14:42:15,876 - INFO - joeynmt.training - Epoch   9, Step:      810, Batch Loss:     3.586900, Batch Acc: 0.337261, Tokens per Sec:     3820, Lr: 0.000629\r\n",
      "2024-08-27 14:42:16,999 - INFO - joeynmt.training - Epoch   9, Step:      815, Batch Loss:     3.635531, Batch Acc: 0.326248, Tokens per Sec:     3732, Lr: 0.000627\r\n",
      "2024-08-27 14:42:18,092 - INFO - joeynmt.training - Epoch   9, Step:      820, Batch Loss:     3.598889, Batch Acc: 0.322389, Tokens per Sec:     3846, Lr: 0.000625\r\n",
      "2024-08-27 14:42:19,241 - INFO - joeynmt.training - Epoch   9, Step:      825, Batch Loss:     3.505740, Batch Acc: 0.329368, Tokens per Sec:     3734, Lr: 0.000623\r\n",
      "2024-08-27 14:42:20,376 - INFO - joeynmt.training - Epoch   9, Step:      830, Batch Loss:     3.449082, Batch Acc: 0.316092, Tokens per Sec:     3684, Lr: 0.000621\r\n",
      "2024-08-27 14:42:21,482 - INFO - joeynmt.training - Epoch   9, Step:      835, Batch Loss:     3.700568, Batch Acc: 0.313194, Tokens per Sec:     3908, Lr: 0.000619\r\n",
      "2024-08-27 14:42:22,695 - INFO - joeynmt.training - Epoch   9, Step:      840, Batch Loss:     3.460521, Batch Acc: 0.313025, Tokens per Sec:     3534, Lr: 0.000617\r\n",
      "2024-08-27 14:42:23,951 - INFO - joeynmt.training - Epoch   9, Step:      845, Batch Loss:     3.697189, Batch Acc: 0.327157, Tokens per Sec:     3268, Lr: 0.000615\r\n",
      "2024-08-27 14:42:25,055 - INFO - joeynmt.training - Epoch   9, Step:      850, Batch Loss:     3.675428, Batch Acc: 0.323968, Tokens per Sec:     3646, Lr: 0.000614\r\n",
      "2024-08-27 14:42:26,178 - INFO - joeynmt.training - Epoch   9, Step:      855, Batch Loss:     3.625698, Batch Acc: 0.301747, Tokens per Sec:     3725, Lr: 0.000612\r\n",
      "2024-08-27 14:42:26,936 - INFO - joeynmt.training - Epoch   9, total training loss: 339.26, num. of seqs: 8065, num. of tokens: 80463, 21.8054[sec]\r\n",
      "2024-08-27 14:42:26,936 - INFO - joeynmt.training - EPOCH 10\r\n",
      "2024-08-27 14:42:27,391 - INFO - joeynmt.training - Epoch  10, Step:      860, Batch Loss:     3.461128, Batch Acc: 0.335958, Tokens per Sec:     4226, Lr: 0.000610\r\n",
      "2024-08-27 14:42:28,525 - INFO - joeynmt.training - Epoch  10, Step:      865, Batch Loss:     3.305371, Batch Acc: 0.361752, Tokens per Sec:     3747, Lr: 0.000608\r\n",
      "2024-08-27 14:42:29,594 - INFO - joeynmt.training - Epoch  10, Step:      870, Batch Loss:     3.070972, Batch Acc: 0.365413, Tokens per Sec:     3783, Lr: 0.000606\r\n",
      "2024-08-27 14:42:30,689 - INFO - joeynmt.training - Epoch  10, Step:      875, Batch Loss:     3.273678, Batch Acc: 0.348076, Tokens per Sec:     4133, Lr: 0.000605\r\n",
      "2024-08-27 14:42:31,783 - INFO - joeynmt.training - Epoch  10, Step:      880, Batch Loss:     3.102044, Batch Acc: 0.347070, Tokens per Sec:     3992, Lr: 0.000603\r\n",
      "2024-08-27 14:42:32,897 - INFO - joeynmt.training - Epoch  10, Step:      885, Batch Loss:     3.140951, Batch Acc: 0.352913, Tokens per Sec:     3734, Lr: 0.000601\r\n",
      "2024-08-27 14:42:34,034 - INFO - joeynmt.training - Epoch  10, Step:      890, Batch Loss:     3.225545, Batch Acc: 0.358202, Tokens per Sec:     3737, Lr: 0.000600\r\n",
      "2024-08-27 14:42:35,141 - INFO - joeynmt.training - Epoch  10, Step:      895, Batch Loss:     3.259906, Batch Acc: 0.340271, Tokens per Sec:     3866, Lr: 0.000598\r\n",
      "2024-08-27 14:42:36,245 - INFO - joeynmt.training - Epoch  10, Step:      900, Batch Loss:     3.353207, Batch Acc: 0.350157, Tokens per Sec:     4041, Lr: 0.000596\r\n",
      "2024-08-27 14:42:37,396 - INFO - joeynmt.training - Epoch  10, Step:      905, Batch Loss:     3.103016, Batch Acc: 0.349272, Tokens per Sec:     3581, Lr: 0.000595\r\n",
      "2024-08-27 14:42:38,517 - INFO - joeynmt.training - Epoch  10, Step:      910, Batch Loss:     3.434289, Batch Acc: 0.349754, Tokens per Sec:     3806, Lr: 0.000593\r\n",
      "2024-08-27 14:42:39,620 - INFO - joeynmt.training - Epoch  10, Step:      915, Batch Loss:     3.407029, Batch Acc: 0.356762, Tokens per Sec:     3742, Lr: 0.000591\r\n",
      "2024-08-27 14:42:40,722 - INFO - joeynmt.training - Epoch  10, Step:      920, Batch Loss:     3.329957, Batch Acc: 0.332774, Tokens per Sec:     3788, Lr: 0.000590\r\n",
      "2024-08-27 14:42:41,840 - INFO - joeynmt.training - Epoch  10, Step:      925, Batch Loss:     3.282310, Batch Acc: 0.342992, Tokens per Sec:     3800, Lr: 0.000588\r\n",
      "2024-08-27 14:42:42,953 - INFO - joeynmt.training - Epoch  10, Step:      930, Batch Loss:     3.510405, Batch Acc: 0.335765, Tokens per Sec:     3697, Lr: 0.000587\r\n",
      "2024-08-27 14:42:44,038 - INFO - joeynmt.training - Epoch  10, Step:      935, Batch Loss:     3.498026, Batch Acc: 0.337230, Tokens per Sec:     3627, Lr: 0.000585\r\n",
      "2024-08-27 14:42:45,120 - INFO - joeynmt.training - Epoch  10, Step:      940, Batch Loss:     3.495090, Batch Acc: 0.344270, Tokens per Sec:     3950, Lr: 0.000583\r\n",
      "2024-08-27 14:42:46,204 - INFO - joeynmt.training - Epoch  10, Step:      945, Batch Loss:     3.359413, Batch Acc: 0.357002, Tokens per Sec:     3756, Lr: 0.000582\r\n",
      "2024-08-27 14:42:47,361 - INFO - joeynmt.training - Epoch  10, Step:      950, Batch Loss:     3.457932, Batch Acc: 0.342035, Tokens per Sec:     3579, Lr: 0.000580\r\n",
      "2024-08-27 14:42:48,194 - INFO - joeynmt.training - Epoch  10, total training loss: 315.43, num. of seqs: 8065, num. of tokens: 80463, 21.2405[sec]\r\n",
      "2024-08-27 14:42:48,194 - INFO - joeynmt.training - EPOCH 11\r\n",
      "2024-08-27 14:42:48,663 - INFO - joeynmt.training - Epoch  11, Step:      955, Batch Loss:     3.099026, Batch Acc: 0.372595, Tokens per Sec:     3688, Lr: 0.000579\r\n",
      "2024-08-27 14:42:49,798 - INFO - joeynmt.training - Epoch  11, Step:      960, Batch Loss:     2.942713, Batch Acc: 0.377772, Tokens per Sec:     3541, Lr: 0.000577\r\n",
      "2024-08-27 14:42:50,932 - INFO - joeynmt.training - Epoch  11, Step:      965, Batch Loss:     3.415873, Batch Acc: 0.384335, Tokens per Sec:     3627, Lr: 0.000576\r\n",
      "2024-08-27 14:42:52,051 - INFO - joeynmt.training - Epoch  11, Step:      970, Batch Loss:     3.116822, Batch Acc: 0.388582, Tokens per Sec:     3730, Lr: 0.000574\r\n",
      "2024-08-27 14:42:53,170 - INFO - joeynmt.training - Epoch  11, Step:      975, Batch Loss:     3.080947, Batch Acc: 0.367319, Tokens per Sec:     3930, Lr: 0.000573\r\n",
      "2024-08-27 14:42:54,372 - INFO - joeynmt.training - Epoch  11, Step:      980, Batch Loss:     2.933091, Batch Acc: 0.388337, Tokens per Sec:     3269, Lr: 0.000571\r\n",
      "2024-08-27 14:42:55,504 - INFO - joeynmt.training - Epoch  11, Step:      985, Batch Loss:     3.057718, Batch Acc: 0.371057, Tokens per Sec:     3669, Lr: 0.000570\r\n",
      "2024-08-27 14:42:56,644 - INFO - joeynmt.training - Epoch  11, Step:      990, Batch Loss:     3.014960, Batch Acc: 0.371654, Tokens per Sec:     3673, Lr: 0.000569\r\n",
      "2024-08-27 14:42:57,780 - INFO - joeynmt.training - Epoch  11, Step:      995, Batch Loss:     3.077970, Batch Acc: 0.368647, Tokens per Sec:     3690, Lr: 0.000567\r\n",
      "2024-08-27 14:42:58,875 - INFO - joeynmt.training - Epoch  11, Step:     1000, Batch Loss:     3.098430, Batch Acc: 0.377408, Tokens per Sec:     3795, Lr: 0.000566\r\n",
      "2024-08-27 14:42:58,876 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=1042\r\n",
      "2024-08-27 14:42:58,876 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:23<00:00, 61.50it/s]\r\n",
      "2024-08-27 14:43:22,617 - INFO - joeynmt.prediction - Generation took 23.7392[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 43145.00 examples/s]\r\n",
      "2024-08-27 14:43:22,983 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 14:43:22,983 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   1.20, loss:   4.45, ppl:  85.48, acc:   0.25, 0.1508[sec]\r\n",
      "2024-08-27 14:43:22,984 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\r\n",
      "2024-08-27 14:43:23,316 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/1000.ckpt.\r\n",
      "2024-08-27 14:43:23,317 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 14:43:23,467 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 14:43:23,468 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 14:43:23,468 - INFO - joeynmt.training - \tHypothesis: tu es sûr qu'elles aimèrent\r\n",
      "2024-08-27 14:43:23,765 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 14:43:23,914 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 14:43:23,914 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 14:43:23,914 - INFO - joeynmt.training - \tHypothesis: trois maisons\r\n",
      "2024-08-27 14:43:24,211 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 14:43:24,360 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 14:43:24,360 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 14:43:24,360 - INFO - joeynmt.training - \tHypothesis: les gens sont les livres\r\n",
      "2024-08-27 14:43:24,654 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 14:43:24,818 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 14:43:24,819 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 14:43:24,819 - INFO - joeynmt.training - \tHypothesis: une petites tables\r\n",
      "2024-08-27 14:43:26,280 - INFO - joeynmt.training - Epoch  11, Step:     1005, Batch Loss:     3.232439, Batch Acc: 0.368916, Tokens per Sec:     3797, Lr: 0.000564\r\n",
      "2024-08-27 14:43:27,402 - INFO - joeynmt.training - Epoch  11, Step:     1010, Batch Loss:     3.228381, Batch Acc: 0.359934, Tokens per Sec:     3791, Lr: 0.000563\r\n",
      "2024-08-27 14:43:28,525 - INFO - joeynmt.training - Epoch  11, Step:     1015, Batch Loss:     3.027882, Batch Acc: 0.377511, Tokens per Sec:     3637, Lr: 0.000561\r\n",
      "2024-08-27 14:43:29,652 - INFO - joeynmt.training - Epoch  11, Step:     1020, Batch Loss:     3.256492, Batch Acc: 0.377310, Tokens per Sec:     3793, Lr: 0.000560\r\n",
      "2024-08-27 14:43:30,803 - INFO - joeynmt.training - Epoch  11, Step:     1025, Batch Loss:     3.138286, Batch Acc: 0.379623, Tokens per Sec:     3458, Lr: 0.000559\r\n",
      "2024-08-27 14:43:31,961 - INFO - joeynmt.training - Epoch  11, Step:     1030, Batch Loss:     3.177274, Batch Acc: 0.361752, Tokens per Sec:     3669, Lr: 0.000557\r\n",
      "2024-08-27 14:43:33,128 - INFO - joeynmt.training - Epoch  11, Step:     1035, Batch Loss:     3.089594, Batch Acc: 0.361871, Tokens per Sec:     3664, Lr: 0.000556\r\n",
      "2024-08-27 14:43:34,235 - INFO - joeynmt.training - Epoch  11, Step:     1040, Batch Loss:     2.916787, Batch Acc: 0.374671, Tokens per Sec:     3775, Lr: 0.000555\r\n",
      "2024-08-27 14:43:35,344 - INFO - joeynmt.training - Epoch  11, Step:     1045, Batch Loss:     3.066448, Batch Acc: 0.365948, Tokens per Sec:     3977, Lr: 0.000553\r\n",
      "2024-08-27 14:43:36,235 - INFO - joeynmt.training - Epoch  11, total training loss: 299.66, num. of seqs: 8065, num. of tokens: 80463, 21.7653[sec]\r\n",
      "2024-08-27 14:43:36,235 - INFO - joeynmt.training - EPOCH 12\r\n",
      "2024-08-27 14:43:36,464 - INFO - joeynmt.training - Epoch  12, Step:     1050, Batch Loss:     2.973256, Batch Acc: 0.412240, Tokens per Sec:     3847, Lr: 0.000552\r\n",
      "2024-08-27 14:43:37,625 - INFO - joeynmt.training - Epoch  12, Step:     1055, Batch Loss:     2.796183, Batch Acc: 0.413492, Tokens per Sec:     3756, Lr: 0.000551\r\n",
      "2024-08-27 14:43:38,757 - INFO - joeynmt.training - Epoch  12, Step:     1060, Batch Loss:     2.716036, Batch Acc: 0.413785, Tokens per Sec:     3681, Lr: 0.000549\r\n",
      "2024-08-27 14:43:39,862 - INFO - joeynmt.training - Epoch  12, Step:     1065, Batch Loss:     2.942599, Batch Acc: 0.421601, Tokens per Sec:     3821, Lr: 0.000548\r\n",
      "2024-08-27 14:43:40,941 - INFO - joeynmt.training - Epoch  12, Step:     1070, Batch Loss:     2.865866, Batch Acc: 0.412071, Tokens per Sec:     4102, Lr: 0.000547\r\n",
      "2024-08-27 14:43:42,036 - INFO - joeynmt.training - Epoch  12, Step:     1075, Batch Loss:     2.880430, Batch Acc: 0.407502, Tokens per Sec:     3922, Lr: 0.000546\r\n",
      "2024-08-27 14:43:43,121 - INFO - joeynmt.training - Epoch  12, Step:     1080, Batch Loss:     2.945370, Batch Acc: 0.408694, Tokens per Sec:     3906, Lr: 0.000544\r\n",
      "2024-08-27 14:43:44,225 - INFO - joeynmt.training - Epoch  12, Step:     1085, Batch Loss:     2.857931, Batch Acc: 0.403741, Tokens per Sec:     3635, Lr: 0.000543\r\n",
      "2024-08-27 14:43:45,358 - INFO - joeynmt.training - Epoch  12, Step:     1090, Batch Loss:     2.846276, Batch Acc: 0.396186, Tokens per Sec:     3750, Lr: 0.000542\r\n",
      "2024-08-27 14:43:46,481 - INFO - joeynmt.training - Epoch  12, Step:     1095, Batch Loss:     2.939728, Batch Acc: 0.406198, Tokens per Sec:     3740, Lr: 0.000541\r\n",
      "2024-08-27 14:43:47,621 - INFO - joeynmt.training - Epoch  12, Step:     1100, Batch Loss:     2.946935, Batch Acc: 0.404706, Tokens per Sec:     3728, Lr: 0.000539\r\n",
      "2024-08-27 14:43:48,734 - INFO - joeynmt.training - Epoch  12, Step:     1105, Batch Loss:     2.915342, Batch Acc: 0.400923, Tokens per Sec:     3702, Lr: 0.000538\r\n",
      "2024-08-27 14:43:49,854 - INFO - joeynmt.training - Epoch  12, Step:     1110, Batch Loss:     2.879514, Batch Acc: 0.405578, Tokens per Sec:     3781, Lr: 0.000537\r\n",
      "2024-08-27 14:43:50,959 - INFO - joeynmt.training - Epoch  12, Step:     1115, Batch Loss:     2.860743, Batch Acc: 0.391006, Tokens per Sec:     3826, Lr: 0.000536\r\n",
      "2024-08-27 14:43:52,068 - INFO - joeynmt.training - Epoch  12, Step:     1120, Batch Loss:     2.965893, Batch Acc: 0.381456, Tokens per Sec:     3758, Lr: 0.000535\r\n",
      "2024-08-27 14:43:53,193 - INFO - joeynmt.training - Epoch  12, Step:     1125, Batch Loss:     2.849554, Batch Acc: 0.399953, Tokens per Sec:     3779, Lr: 0.000533\r\n",
      "2024-08-27 14:43:54,303 - INFO - joeynmt.training - Epoch  12, Step:     1130, Batch Loss:     3.112272, Batch Acc: 0.396005, Tokens per Sec:     3834, Lr: 0.000532\r\n",
      "2024-08-27 14:43:55,370 - INFO - joeynmt.training - Epoch  12, Step:     1135, Batch Loss:     3.028677, Batch Acc: 0.392866, Tokens per Sec:     3944, Lr: 0.000531\r\n",
      "2024-08-27 14:43:56,570 - INFO - joeynmt.training - Epoch  12, Step:     1140, Batch Loss:     3.073964, Batch Acc: 0.390603, Tokens per Sec:     3621, Lr: 0.000530\r\n",
      "2024-08-27 14:43:57,521 - INFO - joeynmt.training - Epoch  12, total training loss: 277.72, num. of seqs: 8065, num. of tokens: 80463, 21.2688[sec]\r\n",
      "2024-08-27 14:43:57,521 - INFO - joeynmt.training - EPOCH 13\r\n",
      "2024-08-27 14:43:57,736 - INFO - joeynmt.training - Epoch  13, Step:     1145, Batch Loss:     2.785706, Batch Acc: 0.424129, Tokens per Sec:     3816, Lr: 0.000529\r\n",
      "2024-08-27 14:43:58,872 - INFO - joeynmt.training - Epoch  13, Step:     1150, Batch Loss:     2.663780, Batch Acc: 0.431932, Tokens per Sec:     3858, Lr: 0.000528\r\n",
      "2024-08-27 14:44:00,036 - INFO - joeynmt.training - Epoch  13, Step:     1155, Batch Loss:     2.676390, Batch Acc: 0.443060, Tokens per Sec:     3865, Lr: 0.000526\r\n",
      "2024-08-27 14:44:01,172 - INFO - joeynmt.training - Epoch  13, Step:     1160, Batch Loss:     2.776470, Batch Acc: 0.448409, Tokens per Sec:     3655, Lr: 0.000525\r\n",
      "2024-08-27 14:44:02,307 - INFO - joeynmt.training - Epoch  13, Step:     1165, Batch Loss:     2.843917, Batch Acc: 0.443552, Tokens per Sec:     3730, Lr: 0.000524\r\n",
      "2024-08-27 14:44:03,438 - INFO - joeynmt.training - Epoch  13, Step:     1170, Batch Loss:     2.615380, Batch Acc: 0.438363, Tokens per Sec:     3460, Lr: 0.000523\r\n",
      "2024-08-27 14:44:04,551 - INFO - joeynmt.training - Epoch  13, Step:     1175, Batch Loss:     2.506051, Batch Acc: 0.436092, Tokens per Sec:     3794, Lr: 0.000522\r\n",
      "2024-08-27 14:44:05,669 - INFO - joeynmt.training - Epoch  13, Step:     1180, Batch Loss:     2.725738, Batch Acc: 0.425034, Tokens per Sec:     3938, Lr: 0.000521\r\n",
      "2024-08-27 14:44:06,825 - INFO - joeynmt.training - Epoch  13, Step:     1185, Batch Loss:     2.686759, Batch Acc: 0.438808, Tokens per Sec:     3516, Lr: 0.000520\r\n",
      "2024-08-27 14:44:07,945 - INFO - joeynmt.training - Epoch  13, Step:     1190, Batch Loss:     2.593812, Batch Acc: 0.431269, Tokens per Sec:     3737, Lr: 0.000519\r\n",
      "2024-08-27 14:44:09,069 - INFO - joeynmt.training - Epoch  13, Step:     1195, Batch Loss:     2.722558, Batch Acc: 0.426107, Tokens per Sec:     3819, Lr: 0.000517\r\n",
      "2024-08-27 14:44:10,148 - INFO - joeynmt.training - Epoch  13, Step:     1200, Batch Loss:     2.620880, Batch Acc: 0.423469, Tokens per Sec:     3817, Lr: 0.000516\r\n",
      "2024-08-27 14:44:11,212 - INFO - joeynmt.training - Epoch  13, Step:     1205, Batch Loss:     2.711275, Batch Acc: 0.440333, Tokens per Sec:     3840, Lr: 0.000515\r\n",
      "2024-08-27 14:44:12,319 - INFO - joeynmt.training - Epoch  13, Step:     1210, Batch Loss:     2.924713, Batch Acc: 0.424096, Tokens per Sec:     3751, Lr: 0.000514\r\n",
      "2024-08-27 14:44:13,463 - INFO - joeynmt.training - Epoch  13, Step:     1215, Batch Loss:     2.594653, Batch Acc: 0.416725, Tokens per Sec:     3746, Lr: 0.000513\r\n",
      "2024-08-27 14:44:14,623 - INFO - joeynmt.training - Epoch  13, Step:     1220, Batch Loss:     2.939261, Batch Acc: 0.425985, Tokens per Sec:     3811, Lr: 0.000512\r\n",
      "2024-08-27 14:44:15,736 - INFO - joeynmt.training - Epoch  13, Step:     1225, Batch Loss:     2.852969, Batch Acc: 0.414123, Tokens per Sec:     3859, Lr: 0.000511\r\n",
      "2024-08-27 14:44:16,877 - INFO - joeynmt.training - Epoch  13, Step:     1230, Batch Loss:     2.702803, Batch Acc: 0.422630, Tokens per Sec:     3773, Lr: 0.000510\r\n",
      "2024-08-27 14:44:17,986 - INFO - joeynmt.training - Epoch  13, Step:     1235, Batch Loss:     2.814941, Batch Acc: 0.404905, Tokens per Sec:     3751, Lr: 0.000509\r\n",
      "2024-08-27 14:44:18,985 - INFO - joeynmt.training - Epoch  13, total training loss: 260.14, num. of seqs: 8065, num. of tokens: 80463, 21.4456[sec]\r\n",
      "2024-08-27 14:44:18,986 - INFO - joeynmt.training - EPOCH 14\r\n",
      "2024-08-27 14:44:19,215 - INFO - joeynmt.training - Epoch  14, Step:     1240, Batch Loss:     2.558596, Batch Acc: 0.446701, Tokens per Sec:     3508, Lr: 0.000508\r\n",
      "2024-08-27 14:44:20,330 - INFO - joeynmt.training - Epoch  14, Step:     1245, Batch Loss:     2.526910, Batch Acc: 0.467469, Tokens per Sec:     3875, Lr: 0.000507\r\n",
      "2024-08-27 14:44:21,435 - INFO - joeynmt.training - Epoch  14, Step:     1250, Batch Loss:     2.438948, Batch Acc: 0.469984, Tokens per Sec:     3877, Lr: 0.000506\r\n",
      "2024-08-27 14:44:22,551 - INFO - joeynmt.training - Epoch  14, Step:     1255, Batch Loss:     2.456394, Batch Acc: 0.458657, Tokens per Sec:     3805, Lr: 0.000505\r\n",
      "2024-08-27 14:44:23,667 - INFO - joeynmt.training - Epoch  14, Step:     1260, Batch Loss:     2.476883, Batch Acc: 0.468787, Tokens per Sec:     3809, Lr: 0.000504\r\n",
      "2024-08-27 14:44:24,773 - INFO - joeynmt.training - Epoch  14, Step:     1265, Batch Loss:     2.366005, Batch Acc: 0.461153, Tokens per Sec:     3971, Lr: 0.000503\r\n",
      "2024-08-27 14:44:25,862 - INFO - joeynmt.training - Epoch  14, Step:     1270, Batch Loss:     2.497173, Batch Acc: 0.460622, Tokens per Sec:     3931, Lr: 0.000502\r\n",
      "2024-08-27 14:44:26,994 - INFO - joeynmt.training - Epoch  14, Step:     1275, Batch Loss:     2.451076, Batch Acc: 0.446289, Tokens per Sec:     3835, Lr: 0.000501\r\n",
      "2024-08-27 14:44:28,236 - INFO - joeynmt.training - Epoch  14, Step:     1280, Batch Loss:     2.542224, Batch Acc: 0.460123, Tokens per Sec:     3413, Lr: 0.000500\r\n",
      "2024-08-27 14:44:29,387 - INFO - joeynmt.training - Epoch  14, Step:     1285, Batch Loss:     2.648322, Batch Acc: 0.466926, Tokens per Sec:     3798, Lr: 0.000499\r\n",
      "2024-08-27 14:44:30,488 - INFO - joeynmt.training - Epoch  14, Step:     1290, Batch Loss:     2.637093, Batch Acc: 0.468199, Tokens per Sec:     3914, Lr: 0.000498\r\n",
      "2024-08-27 14:44:31,590 - INFO - joeynmt.training - Epoch  14, Step:     1295, Batch Loss:     2.593322, Batch Acc: 0.458204, Tokens per Sec:     3815, Lr: 0.000497\r\n",
      "2024-08-27 14:44:32,707 - INFO - joeynmt.training - Epoch  14, Step:     1300, Batch Loss:     2.564687, Batch Acc: 0.453868, Tokens per Sec:     3727, Lr: 0.000496\r\n",
      "2024-08-27 14:44:33,836 - INFO - joeynmt.training - Epoch  14, Step:     1305, Batch Loss:     2.663891, Batch Acc: 0.444471, Tokens per Sec:     3730, Lr: 0.000495\r\n",
      "2024-08-27 14:44:34,976 - INFO - joeynmt.training - Epoch  14, Step:     1310, Batch Loss:     2.608240, Batch Acc: 0.459610, Tokens per Sec:     3466, Lr: 0.000494\r\n",
      "2024-08-27 14:44:36,087 - INFO - joeynmt.training - Epoch  14, Step:     1315, Batch Loss:     2.452118, Batch Acc: 0.457557, Tokens per Sec:     3725, Lr: 0.000493\r\n",
      "2024-08-27 14:44:37,240 - INFO - joeynmt.training - Epoch  14, Step:     1320, Batch Loss:     2.846279, Batch Acc: 0.429507, Tokens per Sec:     3710, Lr: 0.000492\r\n",
      "2024-08-27 14:44:38,346 - INFO - joeynmt.training - Epoch  14, Step:     1325, Batch Loss:     2.710746, Batch Acc: 0.447229, Tokens per Sec:     3757, Lr: 0.000491\r\n",
      "2024-08-27 14:44:39,441 - INFO - joeynmt.training - Epoch  14, Step:     1330, Batch Loss:     2.516834, Batch Acc: 0.446758, Tokens per Sec:     3817, Lr: 0.000491\r\n",
      "2024-08-27 14:44:40,315 - INFO - joeynmt.training - Epoch  14, total training loss: 243.08, num. of seqs: 8065, num. of tokens: 80463, 21.3107[sec]\r\n",
      "2024-08-27 14:44:40,315 - INFO - joeynmt.training - EPOCH 15\r\n",
      "2024-08-27 14:44:40,538 - INFO - joeynmt.training - Epoch  15, Step:     1335, Batch Loss:     2.408541, Batch Acc: 0.493377, Tokens per Sec:     4124, Lr: 0.000490\r\n",
      "2024-08-27 14:44:41,651 - INFO - joeynmt.training - Epoch  15, Step:     1340, Batch Loss:     2.202730, Batch Acc: 0.506198, Tokens per Sec:     3552, Lr: 0.000489\r\n",
      "2024-08-27 14:44:42,789 - INFO - joeynmt.training - Epoch  15, Step:     1345, Batch Loss:     2.251499, Batch Acc: 0.499882, Tokens per Sec:     3718, Lr: 0.000488\r\n",
      "2024-08-27 14:44:43,935 - INFO - joeynmt.training - Epoch  15, Step:     1350, Batch Loss:     2.377892, Batch Acc: 0.487459, Tokens per Sec:     3689, Lr: 0.000487\r\n",
      "2024-08-27 14:44:45,049 - INFO - joeynmt.training - Epoch  15, Step:     1355, Batch Loss:     2.458094, Batch Acc: 0.508985, Tokens per Sec:     3849, Lr: 0.000486\r\n",
      "2024-08-27 14:44:46,148 - INFO - joeynmt.training - Epoch  15, Step:     1360, Batch Loss:     2.319612, Batch Acc: 0.489221, Tokens per Sec:     3716, Lr: 0.000485\r\n",
      "2024-08-27 14:44:47,297 - INFO - joeynmt.training - Epoch  15, Step:     1365, Batch Loss:     2.519186, Batch Acc: 0.488491, Tokens per Sec:     3747, Lr: 0.000484\r\n",
      "2024-08-27 14:44:48,414 - INFO - joeynmt.training - Epoch  15, Step:     1370, Batch Loss:     2.450869, Batch Acc: 0.480344, Tokens per Sec:     3850, Lr: 0.000483\r\n",
      "2024-08-27 14:44:49,520 - INFO - joeynmt.training - Epoch  15, Step:     1375, Batch Loss:     2.324664, Batch Acc: 0.487360, Tokens per Sec:     3867, Lr: 0.000482\r\n",
      "2024-08-27 14:44:50,626 - INFO - joeynmt.training - Epoch  15, Step:     1380, Batch Loss:     2.508452, Batch Acc: 0.471203, Tokens per Sec:     3894, Lr: 0.000482\r\n",
      "2024-08-27 14:44:51,737 - INFO - joeynmt.training - Epoch  15, Step:     1385, Batch Loss:     2.500158, Batch Acc: 0.475676, Tokens per Sec:     3667, Lr: 0.000481\r\n",
      "2024-08-27 14:44:52,846 - INFO - joeynmt.training - Epoch  15, Step:     1390, Batch Loss:     2.453701, Batch Acc: 0.477798, Tokens per Sec:     3983, Lr: 0.000480\r\n",
      "2024-08-27 14:44:53,922 - INFO - joeynmt.training - Epoch  15, Step:     1395, Batch Loss:     2.379809, Batch Acc: 0.473596, Tokens per Sec:     3893, Lr: 0.000479\r\n",
      "2024-08-27 14:44:54,993 - INFO - joeynmt.training - Epoch  15, Step:     1400, Batch Loss:     2.305842, Batch Acc: 0.483680, Tokens per Sec:     4095, Lr: 0.000478\r\n",
      "2024-08-27 14:44:56,147 - INFO - joeynmt.training - Epoch  15, Step:     1405, Batch Loss:     2.448858, Batch Acc: 0.481464, Tokens per Sec:     3696, Lr: 0.000477\r\n",
      "2024-08-27 14:44:57,340 - INFO - joeynmt.training - Epoch  15, Step:     1410, Batch Loss:     2.350243, Batch Acc: 0.483160, Tokens per Sec:     3385, Lr: 0.000476\r\n",
      "2024-08-27 14:44:58,517 - INFO - joeynmt.training - Epoch  15, Step:     1415, Batch Loss:     2.682549, Batch Acc: 0.459659, Tokens per Sec:     3341, Lr: 0.000476\r\n",
      "2024-08-27 14:44:59,719 - INFO - joeynmt.training - Epoch  15, Step:     1420, Batch Loss:     2.583717, Batch Acc: 0.460517, Tokens per Sec:     3699, Lr: 0.000475\r\n",
      "2024-08-27 14:45:00,818 - INFO - joeynmt.training - Epoch  15, Step:     1425, Batch Loss:     2.447176, Batch Acc: 0.461198, Tokens per Sec:     3906, Lr: 0.000474\r\n",
      "2024-08-27 14:45:01,748 - INFO - joeynmt.training - Epoch  15, total training loss: 227.62, num. of seqs: 8065, num. of tokens: 80463, 21.4162[sec]\r\n",
      "2024-08-27 14:45:01,749 - INFO - joeynmt.training - EPOCH 16\r\n",
      "2024-08-27 14:45:01,974 - INFO - joeynmt.training - Epoch  16, Step:     1430, Batch Loss:     2.090499, Batch Acc: 0.540659, Tokens per Sec:     4121, Lr: 0.000473\r\n",
      "2024-08-27 14:45:03,103 - INFO - joeynmt.training - Epoch  16, Step:     1435, Batch Loss:     2.148496, Batch Acc: 0.524094, Tokens per Sec:     3862, Lr: 0.000472\r\n",
      "2024-08-27 14:45:04,235 - INFO - joeynmt.training - Epoch  16, Step:     1440, Batch Loss:     2.119503, Batch Acc: 0.535623, Tokens per Sec:     3809, Lr: 0.000471\r\n",
      "2024-08-27 14:45:05,360 - INFO - joeynmt.training - Epoch  16, Step:     1445, Batch Loss:     2.163839, Batch Acc: 0.527121, Tokens per Sec:     3723, Lr: 0.000471\r\n",
      "2024-08-27 14:45:06,470 - INFO - joeynmt.training - Epoch  16, Step:     1450, Batch Loss:     2.115495, Batch Acc: 0.514709, Tokens per Sec:     3828, Lr: 0.000470\r\n",
      "2024-08-27 14:45:07,597 - INFO - joeynmt.training - Epoch  16, Step:     1455, Batch Loss:     2.164791, Batch Acc: 0.527853, Tokens per Sec:     3588, Lr: 0.000469\r\n",
      "2024-08-27 14:45:08,692 - INFO - joeynmt.training - Epoch  16, Step:     1460, Batch Loss:     2.282297, Batch Acc: 0.518689, Tokens per Sec:     3765, Lr: 0.000468\r\n",
      "2024-08-27 14:45:09,793 - INFO - joeynmt.training - Epoch  16, Step:     1465, Batch Loss:     2.239802, Batch Acc: 0.517156, Tokens per Sec:     3680, Lr: 0.000467\r\n",
      "2024-08-27 14:45:10,949 - INFO - joeynmt.training - Epoch  16, Step:     1470, Batch Loss:     2.205263, Batch Acc: 0.526328, Tokens per Sec:     3649, Lr: 0.000467\r\n",
      "2024-08-27 14:45:12,106 - INFO - joeynmt.training - Epoch  16, Step:     1475, Batch Loss:     2.377580, Batch Acc: 0.490444, Tokens per Sec:     3623, Lr: 0.000466\r\n",
      "2024-08-27 14:45:13,239 - INFO - joeynmt.training - Epoch  16, Step:     1480, Batch Loss:     2.267774, Batch Acc: 0.495967, Tokens per Sec:     3832, Lr: 0.000465\r\n",
      "2024-08-27 14:45:14,369 - INFO - joeynmt.training - Epoch  16, Step:     1485, Batch Loss:     2.482503, Batch Acc: 0.483871, Tokens per Sec:     3870, Lr: 0.000464\r\n",
      "2024-08-27 14:45:15,499 - INFO - joeynmt.training - Epoch  16, Step:     1490, Batch Loss:     2.123514, Batch Acc: 0.533585, Tokens per Sec:     3756, Lr: 0.000463\r\n",
      "2024-08-27 14:45:16,627 - INFO - joeynmt.training - Epoch  16, Step:     1495, Batch Loss:     2.114550, Batch Acc: 0.513559, Tokens per Sec:     3730, Lr: 0.000463\r\n",
      "2024-08-27 14:45:17,776 - INFO - joeynmt.training - Epoch  16, Step:     1500, Batch Loss:     2.136676, Batch Acc: 0.501675, Tokens per Sec:     3641, Lr: 0.000462\r\n",
      "2024-08-27 14:45:17,777 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=1542\r\n",
      "2024-08-27 14:45:17,777 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:30<00:00, 48.28it/s]\r\n",
      "2024-08-27 14:45:48,019 - INFO - joeynmt.prediction - Generation took 30.2404[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44338.72 examples/s]\r\n",
      "2024-08-27 14:45:48,393 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 14:45:48,393 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   1.96, loss:   4.73, ppl: 113.00, acc:   0.26, 0.1572[sec]\r\n",
      "2024-08-27 14:45:48,394 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\r\n",
      "2024-08-27 14:45:48,710 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/1500.ckpt.\r\n",
      "2024-08-27 14:45:48,711 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 14:45:48,857 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 14:45:48,857 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 14:45:48,857 - INFO - joeynmt.training - \tHypothesis: tu es sûr qu’elle aima\r\n",
      "2024-08-27 14:45:49,148 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 14:45:49,293 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 14:45:49,293 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 14:45:49,293 - INFO - joeynmt.training - \tHypothesis: trois éléphants\r\n",
      "2024-08-27 14:45:49,585 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 14:45:49,730 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 14:45:49,730 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 14:45:49,730 - INFO - joeynmt.training - \tHypothesis: une petite chapelle et jolie\r\n",
      "2024-08-27 14:45:50,021 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 14:45:50,186 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 14:45:50,186 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 14:45:50,186 - INFO - joeynmt.training - \tHypothesis: une petite chapelle et un\r\n",
      "2024-08-27 14:45:51,565 - INFO - joeynmt.training - Epoch  16, Step:     1505, Batch Loss:     2.359806, Batch Acc: 0.512177, Tokens per Sec:     3635, Lr: 0.000461\r\n",
      "2024-08-27 14:45:52,662 - INFO - joeynmt.training - Epoch  16, Step:     1510, Batch Loss:     2.289347, Batch Acc: 0.501452, Tokens per Sec:     3768, Lr: 0.000460\r\n",
      "2024-08-27 14:45:53,811 - INFO - joeynmt.training - Epoch  16, Step:     1515, Batch Loss:     2.333122, Batch Acc: 0.486770, Tokens per Sec:     3817, Lr: 0.000460\r\n",
      "2024-08-27 14:45:54,964 - INFO - joeynmt.training - Epoch  16, Step:     1520, Batch Loss:     2.430550, Batch Acc: 0.503378, Tokens per Sec:     3599, Lr: 0.000459\r\n",
      "2024-08-27 14:45:56,097 - INFO - joeynmt.training - Epoch  16, Step:     1525, Batch Loss:     2.175981, Batch Acc: 0.492694, Tokens per Sec:     3444, Lr: 0.000458\r\n",
      "2024-08-27 14:45:56,098 - INFO - joeynmt.training - Epoch  16, total training loss: 213.90, num. of seqs: 8065, num. of tokens: 80463, 21.6260[sec]\r\n",
      "2024-08-27 14:45:56,098 - INFO - joeynmt.training - EPOCH 17\r\n",
      "2024-08-27 14:45:57,275 - INFO - joeynmt.training - Epoch  17, Step:     1530, Batch Loss:     1.873204, Batch Acc: 0.550783, Tokens per Sec:     3374, Lr: 0.000457\r\n",
      "2024-08-27 14:45:58,398 - INFO - joeynmt.training - Epoch  17, Step:     1535, Batch Loss:     1.993887, Batch Acc: 0.555902, Tokens per Sec:     3714, Lr: 0.000457\r\n",
      "2024-08-27 14:45:59,499 - INFO - joeynmt.training - Epoch  17, Step:     1540, Batch Loss:     2.075719, Batch Acc: 0.552383, Tokens per Sec:     3756, Lr: 0.000456\r\n",
      "2024-08-27 14:46:00,597 - INFO - joeynmt.training - Epoch  17, Step:     1545, Batch Loss:     2.094880, Batch Acc: 0.553988, Tokens per Sec:     3749, Lr: 0.000455\r\n",
      "2024-08-27 14:46:01,783 - INFO - joeynmt.training - Epoch  17, Step:     1550, Batch Loss:     2.141002, Batch Acc: 0.554429, Tokens per Sec:     3743, Lr: 0.000454\r\n",
      "2024-08-27 14:46:02,894 - INFO - joeynmt.training - Epoch  17, Step:     1555, Batch Loss:     2.071877, Batch Acc: 0.532708, Tokens per Sec:     3553, Lr: 0.000454\r\n",
      "2024-08-27 14:46:03,982 - INFO - joeynmt.training - Epoch  17, Step:     1560, Batch Loss:     2.084601, Batch Acc: 0.541687, Tokens per Sec:     3838, Lr: 0.000453\r\n",
      "2024-08-27 14:46:05,082 - INFO - joeynmt.training - Epoch  17, Step:     1565, Batch Loss:     2.169175, Batch Acc: 0.526802, Tokens per Sec:     4040, Lr: 0.000452\r\n",
      "2024-08-27 14:46:06,160 - INFO - joeynmt.training - Epoch  17, Step:     1570, Batch Loss:     2.059780, Batch Acc: 0.544261, Tokens per Sec:     4101, Lr: 0.000451\r\n",
      "2024-08-27 14:46:07,290 - INFO - joeynmt.training - Epoch  17, Step:     1575, Batch Loss:     2.078231, Batch Acc: 0.530382, Tokens per Sec:     3760, Lr: 0.000451\r\n",
      "2024-08-27 14:46:08,438 - INFO - joeynmt.training - Epoch  17, Step:     1580, Batch Loss:     2.120411, Batch Acc: 0.527002, Tokens per Sec:     3807, Lr: 0.000450\r\n",
      "2024-08-27 14:46:09,601 - INFO - joeynmt.training - Epoch  17, Step:     1585, Batch Loss:     2.030001, Batch Acc: 0.535462, Tokens per Sec:     3773, Lr: 0.000449\r\n",
      "2024-08-27 14:46:10,770 - INFO - joeynmt.training - Epoch  17, Step:     1590, Batch Loss:     2.100764, Batch Acc: 0.522367, Tokens per Sec:     3561, Lr: 0.000449\r\n",
      "2024-08-27 14:46:11,883 - INFO - joeynmt.training - Epoch  17, Step:     1595, Batch Loss:     2.178667, Batch Acc: 0.527146, Tokens per Sec:     3876, Lr: 0.000448\r\n",
      "2024-08-27 14:46:12,994 - INFO - joeynmt.training - Epoch  17, Step:     1600, Batch Loss:     2.276144, Batch Acc: 0.527364, Tokens per Sec:     3750, Lr: 0.000447\r\n",
      "2024-08-27 14:46:14,121 - INFO - joeynmt.training - Epoch  17, Step:     1605, Batch Loss:     2.241941, Batch Acc: 0.521396, Tokens per Sec:     3715, Lr: 0.000447\r\n",
      "2024-08-27 14:46:15,233 - INFO - joeynmt.training - Epoch  17, Step:     1610, Batch Loss:     2.172776, Batch Acc: 0.527259, Tokens per Sec:     3616, Lr: 0.000446\r\n",
      "2024-08-27 14:46:16,364 - INFO - joeynmt.training - Epoch  17, Step:     1615, Batch Loss:     2.152435, Batch Acc: 0.522902, Tokens per Sec:     3803, Lr: 0.000445\r\n",
      "2024-08-27 14:46:17,524 - INFO - joeynmt.training - Epoch  17, Step:     1620, Batch Loss:     2.110027, Batch Acc: 0.535581, Tokens per Sec:     3458, Lr: 0.000444\r\n",
      "2024-08-27 14:46:17,681 - INFO - joeynmt.training - Epoch  17, total training loss: 197.86, num. of seqs: 8065, num. of tokens: 80463, 21.5650[sec]\r\n",
      "2024-08-27 14:46:17,681 - INFO - joeynmt.training - EPOCH 18\r\n",
      "2024-08-27 14:46:18,773 - INFO - joeynmt.training - Epoch  18, Step:     1625, Batch Loss:     1.818664, Batch Acc: 0.586771, Tokens per Sec:     3655, Lr: 0.000444\r\n",
      "2024-08-27 14:46:19,874 - INFO - joeynmt.training - Epoch  18, Step:     1630, Batch Loss:     1.998280, Batch Acc: 0.571060, Tokens per Sec:     3875, Lr: 0.000443\r\n",
      "2024-08-27 14:46:20,952 - INFO - joeynmt.training - Epoch  18, Step:     1635, Batch Loss:     1.878373, Batch Acc: 0.568604, Tokens per Sec:     3804, Lr: 0.000442\r\n",
      "2024-08-27 14:46:22,060 - INFO - joeynmt.training - Epoch  18, Step:     1640, Batch Loss:     1.835105, Batch Acc: 0.583277, Tokens per Sec:     3975, Lr: 0.000442\r\n",
      "2024-08-27 14:46:23,204 - INFO - joeynmt.training - Epoch  18, Step:     1645, Batch Loss:     1.776425, Batch Acc: 0.579755, Tokens per Sec:     3706, Lr: 0.000441\r\n",
      "2024-08-27 14:46:24,348 - INFO - joeynmt.training - Epoch  18, Step:     1650, Batch Loss:     2.012224, Batch Acc: 0.563987, Tokens per Sec:     3679, Lr: 0.000440\r\n",
      "2024-08-27 14:46:25,474 - INFO - joeynmt.training - Epoch  18, Step:     1655, Batch Loss:     1.854962, Batch Acc: 0.572960, Tokens per Sec:     3647, Lr: 0.000440\r\n",
      "2024-08-27 14:46:26,607 - INFO - joeynmt.training - Epoch  18, Step:     1660, Batch Loss:     1.938444, Batch Acc: 0.575688, Tokens per Sec:     3823, Lr: 0.000439\r\n",
      "2024-08-27 14:46:27,759 - INFO - joeynmt.training - Epoch  18, Step:     1665, Batch Loss:     1.966313, Batch Acc: 0.579960, Tokens per Sec:     3475, Lr: 0.000438\r\n",
      "2024-08-27 14:46:28,880 - INFO - joeynmt.training - Epoch  18, Step:     1670, Batch Loss:     1.782699, Batch Acc: 0.571429, Tokens per Sec:     3581, Lr: 0.000438\r\n",
      "2024-08-27 14:46:30,008 - INFO - joeynmt.training - Epoch  18, Step:     1675, Batch Loss:     1.916014, Batch Acc: 0.569323, Tokens per Sec:     3551, Lr: 0.000437\r\n",
      "2024-08-27 14:46:31,116 - INFO - joeynmt.training - Epoch  18, Step:     1680, Batch Loss:     1.919207, Batch Acc: 0.549917, Tokens per Sec:     3808, Lr: 0.000436\r\n",
      "2024-08-27 14:46:32,259 - INFO - joeynmt.training - Epoch  18, Step:     1685, Batch Loss:     2.049368, Batch Acc: 0.554872, Tokens per Sec:     3701, Lr: 0.000436\r\n",
      "2024-08-27 14:46:33,384 - INFO - joeynmt.training - Epoch  18, Step:     1690, Batch Loss:     2.004932, Batch Acc: 0.556480, Tokens per Sec:     3742, Lr: 0.000435\r\n",
      "2024-08-27 14:46:34,458 - INFO - joeynmt.training - Epoch  18, Step:     1695, Batch Loss:     2.082884, Batch Acc: 0.539172, Tokens per Sec:     3938, Lr: 0.000435\r\n",
      "2024-08-27 14:46:35,544 - INFO - joeynmt.training - Epoch  18, Step:     1700, Batch Loss:     2.018291, Batch Acc: 0.560369, Tokens per Sec:     3997, Lr: 0.000434\r\n",
      "2024-08-27 14:46:36,648 - INFO - joeynmt.training - Epoch  18, Step:     1705, Batch Loss:     1.927544, Batch Acc: 0.554419, Tokens per Sec:     3724, Lr: 0.000433\r\n",
      "2024-08-27 14:46:37,827 - INFO - joeynmt.training - Epoch  18, Step:     1710, Batch Loss:     1.982954, Batch Acc: 0.543827, Tokens per Sec:     3698, Lr: 0.000433\r\n",
      "2024-08-27 14:46:38,961 - INFO - joeynmt.training - Epoch  18, Step:     1715, Batch Loss:     1.882229, Batch Acc: 0.550418, Tokens per Sec:     3796, Lr: 0.000432\r\n",
      "2024-08-27 14:46:39,227 - INFO - joeynmt.training - Epoch  18, total training loss: 186.54, num. of seqs: 8065, num. of tokens: 80463, 21.5272[sec]\r\n",
      "2024-08-27 14:46:39,227 - INFO - joeynmt.training - EPOCH 19\r\n",
      "2024-08-27 14:46:40,119 - INFO - joeynmt.training - Epoch  19, Step:     1720, Batch Loss:     1.695764, Batch Acc: 0.635271, Tokens per Sec:     3938, Lr: 0.000431\r\n",
      "2024-08-27 14:46:41,217 - INFO - joeynmt.training - Epoch  19, Step:     1725, Batch Loss:     1.877412, Batch Acc: 0.607308, Tokens per Sec:     3740, Lr: 0.000431\r\n",
      "2024-08-27 14:46:42,328 - INFO - joeynmt.training - Epoch  19, Step:     1730, Batch Loss:     1.699785, Batch Acc: 0.614760, Tokens per Sec:     3881, Lr: 0.000430\r\n",
      "2024-08-27 14:46:43,435 - INFO - joeynmt.training - Epoch  19, Step:     1735, Batch Loss:     1.858627, Batch Acc: 0.602222, Tokens per Sec:     3661, Lr: 0.000429\r\n",
      "2024-08-27 14:46:44,547 - INFO - joeynmt.training - Epoch  19, Step:     1740, Batch Loss:     1.787077, Batch Acc: 0.590593, Tokens per Sec:     3884, Lr: 0.000429\r\n",
      "2024-08-27 14:46:45,648 - INFO - joeynmt.training - Epoch  19, Step:     1745, Batch Loss:     1.755813, Batch Acc: 0.612126, Tokens per Sec:     3734, Lr: 0.000428\r\n",
      "2024-08-27 14:46:46,822 - INFO - joeynmt.training - Epoch  19, Step:     1750, Batch Loss:     1.770093, Batch Acc: 0.595602, Tokens per Sec:     3680, Lr: 0.000428\r\n",
      "2024-08-27 14:46:47,915 - INFO - joeynmt.training - Epoch  19, Step:     1755, Batch Loss:     1.873140, Batch Acc: 0.604993, Tokens per Sec:     3742, Lr: 0.000427\r\n",
      "2024-08-27 14:46:49,001 - INFO - joeynmt.training - Epoch  19, Step:     1760, Batch Loss:     1.730038, Batch Acc: 0.586925, Tokens per Sec:     3847, Lr: 0.000426\r\n",
      "2024-08-27 14:46:50,083 - INFO - joeynmt.training - Epoch  19, Step:     1765, Batch Loss:     1.739348, Batch Acc: 0.577032, Tokens per Sec:     3914, Lr: 0.000426\r\n",
      "2024-08-27 14:46:51,196 - INFO - joeynmt.training - Epoch  19, Step:     1770, Batch Loss:     1.876363, Batch Acc: 0.589837, Tokens per Sec:     3698, Lr: 0.000425\r\n",
      "2024-08-27 14:46:52,335 - INFO - joeynmt.training - Epoch  19, Step:     1775, Batch Loss:     1.824815, Batch Acc: 0.587921, Tokens per Sec:     3782, Lr: 0.000425\r\n",
      "2024-08-27 14:46:53,468 - INFO - joeynmt.training - Epoch  19, Step:     1780, Batch Loss:     1.903527, Batch Acc: 0.596618, Tokens per Sec:     3552, Lr: 0.000424\r\n",
      "2024-08-27 14:46:54,589 - INFO - joeynmt.training - Epoch  19, Step:     1785, Batch Loss:     1.760154, Batch Acc: 0.577046, Tokens per Sec:     3632, Lr: 0.000423\r\n",
      "2024-08-27 14:46:55,705 - INFO - joeynmt.training - Epoch  19, Step:     1790, Batch Loss:     1.899532, Batch Acc: 0.578671, Tokens per Sec:     3926, Lr: 0.000423\r\n",
      "2024-08-27 14:46:56,883 - INFO - joeynmt.training - Epoch  19, Step:     1795, Batch Loss:     1.748300, Batch Acc: 0.583979, Tokens per Sec:     3510, Lr: 0.000422\r\n",
      "2024-08-27 14:46:58,014 - INFO - joeynmt.training - Epoch  19, Step:     1800, Batch Loss:     1.943155, Batch Acc: 0.570002, Tokens per Sec:     3811, Lr: 0.000422\r\n",
      "2024-08-27 14:46:59,136 - INFO - joeynmt.training - Epoch  19, Step:     1805, Batch Loss:     1.948558, Batch Acc: 0.576011, Tokens per Sec:     3726, Lr: 0.000421\r\n",
      "2024-08-27 14:47:00,259 - INFO - joeynmt.training - Epoch  19, Step:     1810, Batch Loss:     1.929288, Batch Acc: 0.561383, Tokens per Sec:     3735, Lr: 0.000420\r\n",
      "2024-08-27 14:47:00,722 - INFO - joeynmt.training - Epoch  19, total training loss: 173.71, num. of seqs: 8065, num. of tokens: 80463, 21.4772[sec]\r\n",
      "2024-08-27 14:47:00,723 - INFO - joeynmt.training - EPOCH 20\r\n",
      "2024-08-27 14:47:01,395 - INFO - joeynmt.training - Epoch  20, Step:     1815, Batch Loss:     1.620969, Batch Acc: 0.627603, Tokens per Sec:     3668, Lr: 0.000420\r\n",
      "2024-08-27 14:47:02,469 - INFO - joeynmt.training - Epoch  20, Step:     1820, Batch Loss:     1.734451, Batch Acc: 0.629950, Tokens per Sec:     3881, Lr: 0.000419\r\n",
      "2024-08-27 14:47:03,597 - INFO - joeynmt.training - Epoch  20, Step:     1825, Batch Loss:     1.811207, Batch Acc: 0.615792, Tokens per Sec:     3687, Lr: 0.000419\r\n",
      "2024-08-27 14:47:04,696 - INFO - joeynmt.training - Epoch  20, Step:     1830, Batch Loss:     1.571298, Batch Acc: 0.625210, Tokens per Sec:     3783, Lr: 0.000418\r\n",
      "2024-08-27 14:47:05,814 - INFO - joeynmt.training - Epoch  20, Step:     1835, Batch Loss:     1.618824, Batch Acc: 0.629142, Tokens per Sec:     3538, Lr: 0.000418\r\n",
      "2024-08-27 14:47:06,999 - INFO - joeynmt.training - Epoch  20, Step:     1840, Batch Loss:     1.725574, Batch Acc: 0.624238, Tokens per Sec:     3462, Lr: 0.000417\r\n",
      "2024-08-27 14:47:08,153 - INFO - joeynmt.training - Epoch  20, Step:     1845, Batch Loss:     1.630357, Batch Acc: 0.624713, Tokens per Sec:     3784, Lr: 0.000416\r\n",
      "2024-08-27 14:47:09,274 - INFO - joeynmt.training - Epoch  20, Step:     1850, Batch Loss:     1.580144, Batch Acc: 0.606582, Tokens per Sec:     3634, Lr: 0.000416\r\n",
      "2024-08-27 14:47:10,388 - INFO - joeynmt.training - Epoch  20, Step:     1855, Batch Loss:     1.587450, Batch Acc: 0.629093, Tokens per Sec:     3786, Lr: 0.000415\r\n",
      "2024-08-27 14:47:11,529 - INFO - joeynmt.training - Epoch  20, Step:     1860, Batch Loss:     1.685535, Batch Acc: 0.618664, Tokens per Sec:     3805, Lr: 0.000415\r\n",
      "2024-08-27 14:47:12,668 - INFO - joeynmt.training - Epoch  20, Step:     1865, Batch Loss:     1.690358, Batch Acc: 0.620162, Tokens per Sec:     3907, Lr: 0.000414\r\n",
      "2024-08-27 14:47:13,780 - INFO - joeynmt.training - Epoch  20, Step:     1870, Batch Loss:     1.723447, Batch Acc: 0.615030, Tokens per Sec:     3710, Lr: 0.000414\r\n",
      "2024-08-27 14:47:14,888 - INFO - joeynmt.training - Epoch  20, Step:     1875, Batch Loss:     1.563244, Batch Acc: 0.609710, Tokens per Sec:     3834, Lr: 0.000413\r\n",
      "2024-08-27 14:47:15,977 - INFO - joeynmt.training - Epoch  20, Step:     1880, Batch Loss:     1.764583, Batch Acc: 0.606032, Tokens per Sec:     3897, Lr: 0.000413\r\n",
      "2024-08-27 14:47:17,083 - INFO - joeynmt.training - Epoch  20, Step:     1885, Batch Loss:     1.717105, Batch Acc: 0.608194, Tokens per Sec:     3842, Lr: 0.000412\r\n",
      "2024-08-27 14:47:18,170 - INFO - joeynmt.training - Epoch  20, Step:     1890, Batch Loss:     1.719058, Batch Acc: 0.608036, Tokens per Sec:     3942, Lr: 0.000411\r\n",
      "2024-08-27 14:47:19,265 - INFO - joeynmt.training - Epoch  20, Step:     1895, Batch Loss:     1.787419, Batch Acc: 0.600609, Tokens per Sec:     3899, Lr: 0.000411\r\n",
      "2024-08-27 14:47:20,414 - INFO - joeynmt.training - Epoch  20, Step:     1900, Batch Loss:     1.657584, Batch Acc: 0.603600, Tokens per Sec:     3871, Lr: 0.000410\r\n",
      "2024-08-27 14:47:21,573 - INFO - joeynmt.training - Epoch  20, Step:     1905, Batch Loss:     1.552013, Batch Acc: 0.611736, Tokens per Sec:     3532, Lr: 0.000410\r\n",
      "2024-08-27 14:47:22,194 - INFO - joeynmt.training - Epoch  20, total training loss: 159.89, num. of seqs: 8065, num. of tokens: 80463, 21.4537[sec]\r\n",
      "2024-08-27 14:47:22,195 - INFO - joeynmt.training - EPOCH 21\r\n",
      "2024-08-27 14:47:22,866 - INFO - joeynmt.training - Epoch  21, Step:     1910, Batch Loss:     1.481203, Batch Acc: 0.649881, Tokens per Sec:     3779, Lr: 0.000409\r\n",
      "2024-08-27 14:47:23,970 - INFO - joeynmt.training - Epoch  21, Step:     1915, Batch Loss:     1.468321, Batch Acc: 0.654502, Tokens per Sec:     3826, Lr: 0.000409\r\n",
      "2024-08-27 14:47:25,081 - INFO - joeynmt.training - Epoch  21, Step:     1920, Batch Loss:     1.422604, Batch Acc: 0.653133, Tokens per Sec:     3836, Lr: 0.000408\r\n",
      "2024-08-27 14:47:26,198 - INFO - joeynmt.training - Epoch  21, Step:     1925, Batch Loss:     1.553470, Batch Acc: 0.659799, Tokens per Sec:     3828, Lr: 0.000408\r\n",
      "2024-08-27 14:47:27,345 - INFO - joeynmt.training - Epoch  21, Step:     1930, Batch Loss:     1.489310, Batch Acc: 0.652702, Tokens per Sec:     3665, Lr: 0.000407\r\n",
      "2024-08-27 14:47:28,479 - INFO - joeynmt.training - Epoch  21, Step:     1935, Batch Loss:     1.686189, Batch Acc: 0.654863, Tokens per Sec:     3738, Lr: 0.000407\r\n",
      "2024-08-27 14:47:29,608 - INFO - joeynmt.training - Epoch  21, Step:     1940, Batch Loss:     1.633667, Batch Acc: 0.636428, Tokens per Sec:     3743, Lr: 0.000406\r\n",
      "2024-08-27 14:47:30,698 - INFO - joeynmt.training - Epoch  21, Step:     1945, Batch Loss:     1.506384, Batch Acc: 0.644402, Tokens per Sec:     3818, Lr: 0.000406\r\n",
      "2024-08-27 14:47:31,798 - INFO - joeynmt.training - Epoch  21, Step:     1950, Batch Loss:     1.605454, Batch Acc: 0.630758, Tokens per Sec:     3793, Lr: 0.000405\r\n",
      "2024-08-27 14:47:32,888 - INFO - joeynmt.training - Epoch  21, Step:     1955, Batch Loss:     1.563557, Batch Acc: 0.651007, Tokens per Sec:     3829, Lr: 0.000405\r\n",
      "2024-08-27 14:47:34,002 - INFO - joeynmt.training - Epoch  21, Step:     1960, Batch Loss:     1.529209, Batch Acc: 0.648114, Tokens per Sec:     3858, Lr: 0.000404\r\n",
      "2024-08-27 14:47:35,269 - INFO - joeynmt.training - Epoch  21, Step:     1965, Batch Loss:     1.668565, Batch Acc: 0.634078, Tokens per Sec:     3360, Lr: 0.000404\r\n",
      "2024-08-27 14:47:36,429 - INFO - joeynmt.training - Epoch  21, Step:     1970, Batch Loss:     1.578971, Batch Acc: 0.630405, Tokens per Sec:     3790, Lr: 0.000403\r\n",
      "2024-08-27 14:47:37,569 - INFO - joeynmt.training - Epoch  21, Step:     1975, Batch Loss:     1.617378, Batch Acc: 0.629335, Tokens per Sec:     3645, Lr: 0.000403\r\n",
      "2024-08-27 14:47:38,719 - INFO - joeynmt.training - Epoch  21, Step:     1980, Batch Loss:     1.582585, Batch Acc: 0.636934, Tokens per Sec:     3745, Lr: 0.000402\r\n",
      "2024-08-27 14:47:39,834 - INFO - joeynmt.training - Epoch  21, Step:     1985, Batch Loss:     1.589557, Batch Acc: 0.629055, Tokens per Sec:     3816, Lr: 0.000402\r\n",
      "2024-08-27 14:47:40,963 - INFO - joeynmt.training - Epoch  21, Step:     1990, Batch Loss:     1.546915, Batch Acc: 0.620564, Tokens per Sec:     3646, Lr: 0.000401\r\n",
      "2024-08-27 14:47:42,108 - INFO - joeynmt.training - Epoch  21, Step:     1995, Batch Loss:     1.703361, Batch Acc: 0.627068, Tokens per Sec:     3805, Lr: 0.000401\r\n",
      "2024-08-27 14:47:43,214 - INFO - joeynmt.training - Epoch  21, Step:     2000, Batch Loss:     1.647858, Batch Acc: 0.625090, Tokens per Sec:     3749, Lr: 0.000400\r\n",
      "2024-08-27 14:47:43,215 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=2042\r\n",
      "2024-08-27 14:47:43,215 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:23<00:00, 61.49it/s]\r\n",
      "2024-08-27 14:48:06,962 - INFO - joeynmt.prediction - Generation took 23.7455[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 43890.51 examples/s]\r\n",
      "2024-08-27 14:48:07,335 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 14:48:07,335 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   2.38, loss:   5.07, ppl: 159.15, acc:   0.28, 0.1568[sec]\r\n",
      "2024-08-27 14:48:07,336 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\r\n",
      "2024-08-27 14:48:07,651 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/2000.ckpt.\r\n",
      "2024-08-27 14:48:07,653 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/500.ckpt\r\n",
      "2024-08-27 14:48:07,677 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 14:48:07,826 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 14:48:07,826 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 14:48:07,826 - INFO - joeynmt.training - \tHypothesis: il fait semblant\r\n",
      "2024-08-27 14:48:08,121 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 14:48:08,268 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 14:48:08,268 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 14:48:08,269 - INFO - joeynmt.training - \tHypothesis: trois tables\r\n",
      "2024-08-27 14:48:08,562 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 14:48:08,709 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 14:48:08,709 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 14:48:08,709 - INFO - joeynmt.training - \tHypothesis: un autre animal jaune\r\n",
      "2024-08-27 14:48:09,001 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 14:48:09,149 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 14:48:09,149 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 14:48:09,149 - INFO - joeynmt.training - \tHypothesis: une voiture de personnes\r\n",
      "2024-08-27 14:48:09,933 - INFO - joeynmt.training - Epoch  21, total training loss: 148.79, num. of seqs: 8065, num. of tokens: 80463, 21.4914[sec]\r\n",
      "2024-08-27 14:48:09,934 - INFO - joeynmt.training - EPOCH 22\r\n",
      "2024-08-27 14:48:10,608 - INFO - joeynmt.training - Epoch  22, Step:     2005, Batch Loss:     1.387342, Batch Acc: 0.680168, Tokens per Sec:     3904, Lr: 0.000400\r\n",
      "2024-08-27 14:48:11,728 - INFO - joeynmt.training - Epoch  22, Step:     2010, Batch Loss:     1.432975, Batch Acc: 0.695974, Tokens per Sec:     3863, Lr: 0.000399\r\n",
      "2024-08-27 14:48:12,820 - INFO - joeynmt.training - Epoch  22, Step:     2015, Batch Loss:     1.354234, Batch Acc: 0.685194, Tokens per Sec:     3825, Lr: 0.000399\r\n",
      "2024-08-27 14:48:13,920 - INFO - joeynmt.training - Epoch  22, Step:     2020, Batch Loss:     1.341608, Batch Acc: 0.684105, Tokens per Sec:     3635, Lr: 0.000398\r\n",
      "2024-08-27 14:48:15,037 - INFO - joeynmt.training - Epoch  22, Step:     2025, Batch Loss:     1.510849, Batch Acc: 0.674353, Tokens per Sec:     3806, Lr: 0.000398\r\n",
      "2024-08-27 14:48:16,148 - INFO - joeynmt.training - Epoch  22, Step:     2030, Batch Loss:     1.370833, Batch Acc: 0.663406, Tokens per Sec:     3867, Lr: 0.000397\r\n",
      "2024-08-27 14:48:17,287 - INFO - joeynmt.training - Epoch  22, Step:     2035, Batch Loss:     1.497357, Batch Acc: 0.674970, Tokens per Sec:     3704, Lr: 0.000397\r\n",
      "2024-08-27 14:48:18,420 - INFO - joeynmt.training - Epoch  22, Step:     2040, Batch Loss:     1.407692, Batch Acc: 0.676602, Tokens per Sec:     3761, Lr: 0.000396\r\n",
      "2024-08-27 14:48:19,567 - INFO - joeynmt.training - Epoch  22, Step:     2045, Batch Loss:     1.483083, Batch Acc: 0.667599, Tokens per Sec:     3745, Lr: 0.000396\r\n",
      "2024-08-27 14:48:20,702 - INFO - joeynmt.training - Epoch  22, Step:     2050, Batch Loss:     1.522121, Batch Acc: 0.661686, Tokens per Sec:     3774, Lr: 0.000395\r\n",
      "2024-08-27 14:48:21,824 - INFO - joeynmt.training - Epoch  22, Step:     2055, Batch Loss:     1.524405, Batch Acc: 0.663114, Tokens per Sec:     3849, Lr: 0.000395\r\n",
      "2024-08-27 14:48:22,972 - INFO - joeynmt.training - Epoch  22, Step:     2060, Batch Loss:     1.538399, Batch Acc: 0.643973, Tokens per Sec:     3738, Lr: 0.000394\r\n",
      "2024-08-27 14:48:24,087 - INFO - joeynmt.training - Epoch  22, Step:     2065, Batch Loss:     1.578238, Batch Acc: 0.653900, Tokens per Sec:     3822, Lr: 0.000394\r\n",
      "2024-08-27 14:48:25,213 - INFO - joeynmt.training - Epoch  22, Step:     2070, Batch Loss:     1.520384, Batch Acc: 0.653959, Tokens per Sec:     3637, Lr: 0.000393\r\n",
      "2024-08-27 14:48:26,324 - INFO - joeynmt.training - Epoch  22, Step:     2075, Batch Loss:     1.552636, Batch Acc: 0.644455, Tokens per Sec:     3742, Lr: 0.000393\r\n",
      "2024-08-27 14:48:27,464 - INFO - joeynmt.training - Epoch  22, Step:     2080, Batch Loss:     1.533395, Batch Acc: 0.642839, Tokens per Sec:     3513, Lr: 0.000392\r\n",
      "2024-08-27 14:48:28,564 - INFO - joeynmt.training - Epoch  22, Step:     2085, Batch Loss:     1.487338, Batch Acc: 0.637286, Tokens per Sec:     3767, Lr: 0.000392\r\n",
      "2024-08-27 14:48:29,664 - INFO - joeynmt.training - Epoch  22, Step:     2090, Batch Loss:     1.438355, Batch Acc: 0.667233, Tokens per Sec:     3749, Lr: 0.000391\r\n",
      "2024-08-27 14:48:30,760 - INFO - joeynmt.training - Epoch  22, Step:     2095, Batch Loss:     1.520688, Batch Acc: 0.673564, Tokens per Sec:     3750, Lr: 0.000391\r\n",
      "2024-08-27 14:48:31,358 - INFO - joeynmt.training - Epoch  22, total training loss: 138.84, num. of seqs: 8065, num. of tokens: 80463, 21.4063[sec]\r\n",
      "2024-08-27 14:48:31,358 - INFO - joeynmt.training - EPOCH 23\r\n",
      "2024-08-27 14:48:32,014 - INFO - joeynmt.training - Epoch  23, Step:     2100, Batch Loss:     1.240952, Batch Acc: 0.710876, Tokens per Sec:     3923, Lr: 0.000390\r\n",
      "2024-08-27 14:48:33,140 - INFO - joeynmt.training - Epoch  23, Step:     2105, Batch Loss:     1.273015, Batch Acc: 0.695694, Tokens per Sec:     3716, Lr: 0.000390\r\n",
      "2024-08-27 14:48:34,265 - INFO - joeynmt.training - Epoch  23, Step:     2110, Batch Loss:     1.249346, Batch Acc: 0.699172, Tokens per Sec:     3757, Lr: 0.000389\r\n",
      "2024-08-27 14:48:35,369 - INFO - joeynmt.training - Epoch  23, Step:     2115, Batch Loss:     1.289387, Batch Acc: 0.699321, Tokens per Sec:     3740, Lr: 0.000389\r\n",
      "2024-08-27 14:48:36,476 - INFO - joeynmt.training - Epoch  23, Step:     2120, Batch Loss:     1.334200, Batch Acc: 0.689076, Tokens per Sec:     3872, Lr: 0.000389\r\n",
      "2024-08-27 14:48:37,704 - INFO - joeynmt.training - Epoch  23, Step:     2125, Batch Loss:     1.322030, Batch Acc: 0.684676, Tokens per Sec:     3317, Lr: 0.000388\r\n",
      "2024-08-27 14:48:38,855 - INFO - joeynmt.training - Epoch  23, Step:     2130, Batch Loss:     1.312681, Batch Acc: 0.687787, Tokens per Sec:     3596, Lr: 0.000388\r\n",
      "2024-08-27 14:48:39,981 - INFO - joeynmt.training - Epoch  23, Step:     2135, Batch Loss:     1.425737, Batch Acc: 0.685063, Tokens per Sec:     3782, Lr: 0.000387\r\n",
      "2024-08-27 14:48:41,159 - INFO - joeynmt.training - Epoch  23, Step:     2140, Batch Loss:     1.342228, Batch Acc: 0.683530, Tokens per Sec:     3744, Lr: 0.000387\r\n",
      "2024-08-27 14:48:42,261 - INFO - joeynmt.training - Epoch  23, Step:     2145, Batch Loss:     1.332834, Batch Acc: 0.693610, Tokens per Sec:     4024, Lr: 0.000386\r\n",
      "2024-08-27 14:48:43,444 - INFO - joeynmt.training - Epoch  23, Step:     2150, Batch Loss:     1.317279, Batch Acc: 0.685067, Tokens per Sec:     3585, Lr: 0.000386\r\n",
      "2024-08-27 14:48:44,543 - INFO - joeynmt.training - Epoch  23, Step:     2155, Batch Loss:     1.314769, Batch Acc: 0.694564, Tokens per Sec:     3601, Lr: 0.000385\r\n",
      "2024-08-27 14:48:45,661 - INFO - joeynmt.training - Epoch  23, Step:     2160, Batch Loss:     1.400387, Batch Acc: 0.689655, Tokens per Sec:     3737, Lr: 0.000385\r\n",
      "2024-08-27 14:48:46,844 - INFO - joeynmt.training - Epoch  23, Step:     2165, Batch Loss:     1.354142, Batch Acc: 0.679579, Tokens per Sec:     3540, Lr: 0.000384\r\n",
      "2024-08-27 14:48:47,970 - INFO - joeynmt.training - Epoch  23, Step:     2170, Batch Loss:     1.416994, Batch Acc: 0.670548, Tokens per Sec:     3891, Lr: 0.000384\r\n",
      "2024-08-27 14:48:49,087 - INFO - joeynmt.training - Epoch  23, Step:     2175, Batch Loss:     1.358348, Batch Acc: 0.673250, Tokens per Sec:     3676, Lr: 0.000384\r\n",
      "2024-08-27 14:48:50,178 - INFO - joeynmt.training - Epoch  23, Step:     2180, Batch Loss:     1.388774, Batch Acc: 0.686470, Tokens per Sec:     3952, Lr: 0.000383\r\n",
      "2024-08-27 14:48:51,316 - INFO - joeynmt.training - Epoch  23, Step:     2185, Batch Loss:     1.431327, Batch Acc: 0.664286, Tokens per Sec:     3692, Lr: 0.000383\r\n",
      "2024-08-27 14:48:52,434 - INFO - joeynmt.training - Epoch  23, Step:     2190, Batch Loss:     1.443145, Batch Acc: 0.674676, Tokens per Sec:     3799, Lr: 0.000382\r\n",
      "2024-08-27 14:48:53,024 - INFO - joeynmt.training - Epoch  23, total training loss: 129.07, num. of seqs: 8065, num. of tokens: 80463, 21.6471[sec]\r\n",
      "2024-08-27 14:48:53,024 - INFO - joeynmt.training - EPOCH 24\r\n",
      "2024-08-27 14:48:53,698 - INFO - joeynmt.training - Epoch  24, Step:     2195, Batch Loss:     1.296881, Batch Acc: 0.709882, Tokens per Sec:     3540, Lr: 0.000382\r\n",
      "2024-08-27 14:48:54,796 - INFO - joeynmt.training - Epoch  24, Step:     2200, Batch Loss:     1.195109, Batch Acc: 0.725601, Tokens per Sec:     3715, Lr: 0.000381\r\n",
      "2024-08-27 14:48:55,912 - INFO - joeynmt.training - Epoch  24, Step:     2205, Batch Loss:     1.234142, Batch Acc: 0.723505, Tokens per Sec:     3961, Lr: 0.000381\r\n",
      "2024-08-27 14:48:57,099 - INFO - joeynmt.training - Epoch  24, Step:     2210, Batch Loss:     1.163467, Batch Acc: 0.717052, Tokens per Sec:     3612, Lr: 0.000381\r\n",
      "2024-08-27 14:48:58,250 - INFO - joeynmt.training - Epoch  24, Step:     2215, Batch Loss:     1.176635, Batch Acc: 0.711993, Tokens per Sec:     3901, Lr: 0.000380\r\n",
      "2024-08-27 14:48:59,357 - INFO - joeynmt.training - Epoch  24, Step:     2220, Batch Loss:     1.283961, Batch Acc: 0.714178, Tokens per Sec:     3603, Lr: 0.000380\r\n",
      "2024-08-27 14:49:00,434 - INFO - joeynmt.training - Epoch  24, Step:     2225, Batch Loss:     1.141831, Batch Acc: 0.729440, Tokens per Sec:     3819, Lr: 0.000379\r\n",
      "2024-08-27 14:49:01,536 - INFO - joeynmt.training - Epoch  24, Step:     2230, Batch Loss:     1.304417, Batch Acc: 0.709885, Tokens per Sec:     3803, Lr: 0.000379\r\n",
      "2024-08-27 14:49:02,658 - INFO - joeynmt.training - Epoch  24, Step:     2235, Batch Loss:     1.288464, Batch Acc: 0.711034, Tokens per Sec:     3605, Lr: 0.000378\r\n",
      "2024-08-27 14:49:03,779 - INFO - joeynmt.training - Epoch  24, Step:     2240, Batch Loss:     1.311784, Batch Acc: 0.702251, Tokens per Sec:     3686, Lr: 0.000378\r\n",
      "2024-08-27 14:49:04,894 - INFO - joeynmt.training - Epoch  24, Step:     2245, Batch Loss:     1.236848, Batch Acc: 0.699814, Tokens per Sec:     3854, Lr: 0.000378\r\n",
      "2024-08-27 14:49:06,035 - INFO - joeynmt.training - Epoch  24, Step:     2250, Batch Loss:     1.303606, Batch Acc: 0.699055, Tokens per Sec:     3901, Lr: 0.000377\r\n",
      "2024-08-27 14:49:07,167 - INFO - joeynmt.training - Epoch  24, Step:     2255, Batch Loss:     1.292513, Batch Acc: 0.700119, Tokens per Sec:     3698, Lr: 0.000377\r\n",
      "2024-08-27 14:49:08,298 - INFO - joeynmt.training - Epoch  24, Step:     2260, Batch Loss:     1.311208, Batch Acc: 0.704691, Tokens per Sec:     3584, Lr: 0.000376\r\n",
      "2024-08-27 14:49:09,581 - INFO - joeynmt.training - Epoch  24, Step:     2265, Batch Loss:     1.396946, Batch Acc: 0.685566, Tokens per Sec:     3302, Lr: 0.000376\r\n",
      "2024-08-27 14:49:10,677 - INFO - joeynmt.training - Epoch  24, Step:     2270, Batch Loss:     1.333314, Batch Acc: 0.688974, Tokens per Sec:     3791, Lr: 0.000375\r\n",
      "2024-08-27 14:49:11,795 - INFO - joeynmt.training - Epoch  24, Step:     2275, Batch Loss:     1.283082, Batch Acc: 0.679396, Tokens per Sec:     3916, Lr: 0.000375\r\n",
      "2024-08-27 14:49:12,890 - INFO - joeynmt.training - Epoch  24, Step:     2280, Batch Loss:     1.280320, Batch Acc: 0.702458, Tokens per Sec:     3827, Lr: 0.000375\r\n",
      "2024-08-27 14:49:14,010 - INFO - joeynmt.training - Epoch  24, Step:     2285, Batch Loss:     1.363470, Batch Acc: 0.686702, Tokens per Sec:     3571, Lr: 0.000374\r\n",
      "2024-08-27 14:49:14,695 - INFO - joeynmt.training - Epoch  24, total training loss: 121.98, num. of seqs: 8065, num. of tokens: 80463, 21.6518[sec]\r\n",
      "2024-08-27 14:49:14,695 - INFO - joeynmt.training - EPOCH 25\r\n",
      "2024-08-27 14:49:15,162 - INFO - joeynmt.training - Epoch  25, Step:     2290, Batch Loss:     1.129850, Batch Acc: 0.741399, Tokens per Sec:     3765, Lr: 0.000374\r\n",
      "2024-08-27 14:49:16,305 - INFO - joeynmt.training - Epoch  25, Step:     2295, Batch Loss:     1.141722, Batch Acc: 0.746382, Tokens per Sec:     3934, Lr: 0.000373\r\n",
      "2024-08-27 14:49:17,467 - INFO - joeynmt.training - Epoch  25, Step:     2300, Batch Loss:     1.113824, Batch Acc: 0.743657, Tokens per Sec:     3598, Lr: 0.000373\r\n",
      "2024-08-27 14:49:18,576 - INFO - joeynmt.training - Epoch  25, Step:     2305, Batch Loss:     1.132364, Batch Acc: 0.747666, Tokens per Sec:     3960, Lr: 0.000373\r\n",
      "2024-08-27 14:49:19,692 - INFO - joeynmt.training - Epoch  25, Step:     2310, Batch Loss:     1.229682, Batch Acc: 0.737910, Tokens per Sec:     3802, Lr: 0.000372\r\n",
      "2024-08-27 14:49:20,790 - INFO - joeynmt.training - Epoch  25, Step:     2315, Batch Loss:     1.059476, Batch Acc: 0.745459, Tokens per Sec:     3763, Lr: 0.000372\r\n",
      "2024-08-27 14:49:21,924 - INFO - joeynmt.training - Epoch  25, Step:     2320, Batch Loss:     1.098433, Batch Acc: 0.725526, Tokens per Sec:     3814, Lr: 0.000371\r\n",
      "2024-08-27 14:49:23,032 - INFO - joeynmt.training - Epoch  25, Step:     2325, Batch Loss:     1.213349, Batch Acc: 0.729006, Tokens per Sec:     3743, Lr: 0.000371\r\n",
      "2024-08-27 14:49:24,122 - INFO - joeynmt.training - Epoch  25, Step:     2330, Batch Loss:     1.262039, Batch Acc: 0.717918, Tokens per Sec:     3791, Lr: 0.000371\r\n",
      "2024-08-27 14:49:25,238 - INFO - joeynmt.training - Epoch  25, Step:     2335, Batch Loss:     1.167538, Batch Acc: 0.726196, Tokens per Sec:     3861, Lr: 0.000370\r\n",
      "2024-08-27 14:49:26,341 - INFO - joeynmt.training - Epoch  25, Step:     2340, Batch Loss:     1.223969, Batch Acc: 0.732659, Tokens per Sec:     3858, Lr: 0.000370\r\n",
      "2024-08-27 14:49:27,481 - INFO - joeynmt.training - Epoch  25, Step:     2345, Batch Loss:     1.145490, Batch Acc: 0.722798, Tokens per Sec:     3726, Lr: 0.000369\r\n",
      "2024-08-27 14:49:28,592 - INFO - joeynmt.training - Epoch  25, Step:     2350, Batch Loss:     1.237245, Batch Acc: 0.727474, Tokens per Sec:     3670, Lr: 0.000369\r\n",
      "2024-08-27 14:49:29,682 - INFO - joeynmt.training - Epoch  25, Step:     2355, Batch Loss:     1.147111, Batch Acc: 0.727095, Tokens per Sec:     3759, Lr: 0.000369\r\n",
      "2024-08-27 14:49:30,808 - INFO - joeynmt.training - Epoch  25, Step:     2360, Batch Loss:     1.216606, Batch Acc: 0.716097, Tokens per Sec:     3715, Lr: 0.000368\r\n",
      "2024-08-27 14:49:31,958 - INFO - joeynmt.training - Epoch  25, Step:     2365, Batch Loss:     1.254496, Batch Acc: 0.718510, Tokens per Sec:     3736, Lr: 0.000368\r\n",
      "2024-08-27 14:49:33,080 - INFO - joeynmt.training - Epoch  25, Step:     2370, Batch Loss:     1.269731, Batch Acc: 0.701588, Tokens per Sec:     3763, Lr: 0.000367\r\n",
      "2024-08-27 14:49:34,203 - INFO - joeynmt.training - Epoch  25, Step:     2375, Batch Loss:     1.257787, Batch Acc: 0.725542, Tokens per Sec:     3700, Lr: 0.000367\r\n",
      "2024-08-27 14:49:35,338 - INFO - joeynmt.training - Epoch  25, Step:     2380, Batch Loss:     1.228943, Batch Acc: 0.709442, Tokens per Sec:     3742, Lr: 0.000367\r\n",
      "2024-08-27 14:49:36,048 - INFO - joeynmt.training - Epoch  25, total training loss: 112.50, num. of seqs: 8065, num. of tokens: 80463, 21.3356[sec]\r\n",
      "2024-08-27 14:49:36,049 - INFO - joeynmt.training - EPOCH 26\r\n",
      "2024-08-27 14:49:36,501 - INFO - joeynmt.training - Epoch  26, Step:     2385, Batch Loss:     1.029756, Batch Acc: 0.757454, Tokens per Sec:     3894, Lr: 0.000366\r\n",
      "2024-08-27 14:49:37,660 - INFO - joeynmt.training - Epoch  26, Step:     2390, Batch Loss:     0.973125, Batch Acc: 0.767595, Tokens per Sec:     3531, Lr: 0.000366\r\n",
      "2024-08-27 14:49:38,779 - INFO - joeynmt.training - Epoch  26, Step:     2395, Batch Loss:     1.069878, Batch Acc: 0.751503, Tokens per Sec:     3717, Lr: 0.000366\r\n",
      "2024-08-27 14:49:39,984 - INFO - joeynmt.training - Epoch  26, Step:     2400, Batch Loss:     1.041969, Batch Acc: 0.755135, Tokens per Sec:     3641, Lr: 0.000365\r\n",
      "2024-08-27 14:49:41,106 - INFO - joeynmt.training - Epoch  26, Step:     2405, Batch Loss:     1.101948, Batch Acc: 0.741776, Tokens per Sec:     3794, Lr: 0.000365\r\n",
      "2024-08-27 14:49:42,217 - INFO - joeynmt.training - Epoch  26, Step:     2410, Batch Loss:     1.015252, Batch Acc: 0.755239, Tokens per Sec:     4042, Lr: 0.000364\r\n",
      "2024-08-27 14:49:43,333 - INFO - joeynmt.training - Epoch  26, Step:     2415, Batch Loss:     1.000047, Batch Acc: 0.748442, Tokens per Sec:     3885, Lr: 0.000364\r\n",
      "2024-08-27 14:49:44,473 - INFO - joeynmt.training - Epoch  26, Step:     2420, Batch Loss:     1.129150, Batch Acc: 0.753559, Tokens per Sec:     3760, Lr: 0.000364\r\n",
      "2024-08-27 14:49:45,597 - INFO - joeynmt.training - Epoch  26, Step:     2425, Batch Loss:     1.106711, Batch Acc: 0.749045, Tokens per Sec:     3964, Lr: 0.000363\r\n",
      "2024-08-27 14:49:46,721 - INFO - joeynmt.training - Epoch  26, Step:     2430, Batch Loss:     1.192472, Batch Acc: 0.736625, Tokens per Sec:     3660, Lr: 0.000363\r\n",
      "2024-08-27 14:49:47,833 - INFO - joeynmt.training - Epoch  26, Step:     2435, Batch Loss:     1.143330, Batch Acc: 0.742600, Tokens per Sec:     3802, Lr: 0.000363\r\n",
      "2024-08-27 14:49:48,949 - INFO - joeynmt.training - Epoch  26, Step:     2440, Batch Loss:     1.128193, Batch Acc: 0.743975, Tokens per Sec:     3941, Lr: 0.000362\r\n",
      "2024-08-27 14:49:50,052 - INFO - joeynmt.training - Epoch  26, Step:     2445, Batch Loss:     1.094790, Batch Acc: 0.737438, Tokens per Sec:     3684, Lr: 0.000362\r\n",
      "2024-08-27 14:49:51,162 - INFO - joeynmt.training - Epoch  26, Step:     2450, Batch Loss:     1.122995, Batch Acc: 0.735091, Tokens per Sec:     3660, Lr: 0.000361\r\n",
      "2024-08-27 14:49:52,260 - INFO - joeynmt.training - Epoch  26, Step:     2455, Batch Loss:     1.153893, Batch Acc: 0.738492, Tokens per Sec:     3663, Lr: 0.000361\r\n",
      "2024-08-27 14:49:53,402 - INFO - joeynmt.training - Epoch  26, Step:     2460, Batch Loss:     1.068470, Batch Acc: 0.731918, Tokens per Sec:     3548, Lr: 0.000361\r\n",
      "2024-08-27 14:49:54,569 - INFO - joeynmt.training - Epoch  26, Step:     2465, Batch Loss:     1.137574, Batch Acc: 0.743359, Tokens per Sec:     3809, Lr: 0.000360\r\n",
      "2024-08-27 14:49:55,662 - INFO - joeynmt.training - Epoch  26, Step:     2470, Batch Loss:     1.108664, Batch Acc: 0.736918, Tokens per Sec:     3795, Lr: 0.000360\r\n",
      "2024-08-27 14:49:56,824 - INFO - joeynmt.training - Epoch  26, Step:     2475, Batch Loss:     1.225098, Batch Acc: 0.732898, Tokens per Sec:     3563, Lr: 0.000360\r\n",
      "2024-08-27 14:49:57,537 - INFO - joeynmt.training - Epoch  26, total training loss: 105.31, num. of seqs: 8065, num. of tokens: 80463, 21.4702[sec]\r\n",
      "2024-08-27 14:49:57,538 - INFO - joeynmt.training - EPOCH 27\r\n",
      "2024-08-27 14:49:57,985 - INFO - joeynmt.training - Epoch  27, Step:     2480, Batch Loss:     0.965585, Batch Acc: 0.759904, Tokens per Sec:     3764, Lr: 0.000359\r\n",
      "2024-08-27 14:49:59,106 - INFO - joeynmt.training - Epoch  27, Step:     2485, Batch Loss:     0.984309, Batch Acc: 0.767782, Tokens per Sec:     4028, Lr: 0.000359\r\n",
      "2024-08-27 14:50:00,241 - INFO - joeynmt.training - Epoch  27, Step:     2490, Batch Loss:     0.982412, Batch Acc: 0.782700, Tokens per Sec:     3779, Lr: 0.000358\r\n",
      "2024-08-27 14:50:01,332 - INFO - joeynmt.training - Epoch  27, Step:     2495, Batch Loss:     1.007558, Batch Acc: 0.772384, Tokens per Sec:     3650, Lr: 0.000358\r\n",
      "2024-08-27 14:50:02,449 - INFO - joeynmt.training - Epoch  27, Step:     2500, Batch Loss:     0.990883, Batch Acc: 0.759701, Tokens per Sec:     3601, Lr: 0.000358\r\n",
      "2024-08-27 14:50:02,450 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=2542\r\n",
      "2024-08-27 14:50:02,450 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:23<00:00, 61.56it/s]\r\n",
      "2024-08-27 14:50:26,167 - INFO - joeynmt.prediction - Generation took 23.7155[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44410.21 examples/s]\r\n",
      "2024-08-27 14:50:26,544 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 14:50:26,544 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   2.84, loss:   5.41, ppl: 223.85, acc:   0.28, 0.1612[sec]\r\n",
      "2024-08-27 14:50:26,545 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\r\n",
      "2024-08-27 14:50:26,885 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/2500.ckpt.\r\n",
      "2024-08-27 14:50:26,886 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/1000.ckpt\r\n",
      "2024-08-27 14:50:26,911 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 14:50:27,059 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 14:50:27,059 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 14:50:27,059 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 14:50:27,352 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 14:50:27,499 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 14:50:27,499 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 14:50:27,499 - INFO - joeynmt.training - \tHypothesis: trois maisons en bois\r\n",
      "2024-08-27 14:50:27,793 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 14:50:27,941 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 14:50:27,942 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 14:50:27,942 - INFO - joeynmt.training - \tHypothesis: mais cet amendement de cœur\r\n",
      "2024-08-27 14:50:28,236 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 14:50:28,383 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 14:50:28,383 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 14:50:28,383 - INFO - joeynmt.training - \tHypothesis: une trentaine d’étudiants\r\n",
      "2024-08-27 14:50:29,773 - INFO - joeynmt.training - Epoch  27, Step:     2505, Batch Loss:     0.977300, Batch Acc: 0.769323, Tokens per Sec:     3814, Lr: 0.000357\r\n",
      "2024-08-27 14:50:30,896 - INFO - joeynmt.training - Epoch  27, Step:     2510, Batch Loss:     1.062131, Batch Acc: 0.757129, Tokens per Sec:     3750, Lr: 0.000357\r\n",
      "2024-08-27 14:50:32,015 - INFO - joeynmt.training - Epoch  27, Step:     2515, Batch Loss:     0.954679, Batch Acc: 0.773748, Tokens per Sec:     3818, Lr: 0.000357\r\n",
      "2024-08-27 14:50:33,123 - INFO - joeynmt.training - Epoch  27, Step:     2520, Batch Loss:     0.951018, Batch Acc: 0.769697, Tokens per Sec:     3876, Lr: 0.000356\r\n",
      "2024-08-27 14:50:34,221 - INFO - joeynmt.training - Epoch  27, Step:     2525, Batch Loss:     1.029040, Batch Acc: 0.765647, Tokens per Sec:     3875, Lr: 0.000356\r\n",
      "2024-08-27 14:50:35,346 - INFO - joeynmt.training - Epoch  27, Step:     2530, Batch Loss:     1.067617, Batch Acc: 0.740196, Tokens per Sec:     3811, Lr: 0.000356\r\n",
      "2024-08-27 14:50:36,474 - INFO - joeynmt.training - Epoch  27, Step:     2535, Batch Loss:     1.129753, Batch Acc: 0.745873, Tokens per Sec:     3816, Lr: 0.000355\r\n",
      "2024-08-27 14:50:37,639 - INFO - joeynmt.training - Epoch  27, Step:     2540, Batch Loss:     1.051691, Batch Acc: 0.749006, Tokens per Sec:     3672, Lr: 0.000355\r\n",
      "2024-08-27 14:50:38,769 - INFO - joeynmt.training - Epoch  27, Step:     2545, Batch Loss:     1.142515, Batch Acc: 0.758216, Tokens per Sec:     3772, Lr: 0.000355\r\n",
      "2024-08-27 14:50:39,890 - INFO - joeynmt.training - Epoch  27, Step:     2550, Batch Loss:     1.044522, Batch Acc: 0.756703, Tokens per Sec:     3562, Lr: 0.000354\r\n",
      "2024-08-27 14:50:41,021 - INFO - joeynmt.training - Epoch  27, Step:     2555, Batch Loss:     1.165417, Batch Acc: 0.748925, Tokens per Sec:     3909, Lr: 0.000354\r\n",
      "2024-08-27 14:50:42,188 - INFO - joeynmt.training - Epoch  27, Step:     2560, Batch Loss:     1.056322, Batch Acc: 0.747020, Tokens per Sec:     3594, Lr: 0.000354\r\n",
      "2024-08-27 14:50:43,362 - INFO - joeynmt.training - Epoch  27, Step:     2565, Batch Loss:     1.162220, Batch Acc: 0.747475, Tokens per Sec:     3544, Lr: 0.000353\r\n",
      "2024-08-27 14:50:44,478 - INFO - joeynmt.training - Epoch  27, Step:     2570, Batch Loss:     1.161761, Batch Acc: 0.749418, Tokens per Sec:     3851, Lr: 0.000353\r\n",
      "2024-08-27 14:50:45,214 - INFO - joeynmt.training - Epoch  27, total training loss: 99.26, num. of seqs: 8065, num. of tokens: 80463, 21.4270[sec]\r\n",
      "2024-08-27 14:50:45,214 - INFO - joeynmt.training - EPOCH 28\r\n",
      "2024-08-27 14:50:45,671 - INFO - joeynmt.training - Epoch  28, Step:     2575, Batch Loss:     0.900076, Batch Acc: 0.797726, Tokens per Sec:     3688, Lr: 0.000353\r\n",
      "2024-08-27 14:50:46,841 - INFO - joeynmt.training - Epoch  28, Step:     2580, Batch Loss:     0.894230, Batch Acc: 0.786391, Tokens per Sec:     3974, Lr: 0.000352\r\n",
      "2024-08-27 14:50:47,955 - INFO - joeynmt.training - Epoch  28, Step:     2585, Batch Loss:     0.917733, Batch Acc: 0.782471, Tokens per Sec:     3679, Lr: 0.000352\r\n",
      "2024-08-27 14:50:49,084 - INFO - joeynmt.training - Epoch  28, Step:     2590, Batch Loss:     0.969079, Batch Acc: 0.790159, Tokens per Sec:     3676, Lr: 0.000351\r\n",
      "2024-08-27 14:50:50,201 - INFO - joeynmt.training - Epoch  28, Step:     2595, Batch Loss:     0.998583, Batch Acc: 0.782588, Tokens per Sec:     3745, Lr: 0.000351\r\n",
      "2024-08-27 14:50:51,315 - INFO - joeynmt.training - Epoch  28, Step:     2600, Batch Loss:     1.048617, Batch Acc: 0.767840, Tokens per Sec:     3677, Lr: 0.000351\r\n",
      "2024-08-27 14:50:52,415 - INFO - joeynmt.training - Epoch  28, Step:     2605, Batch Loss:     0.980773, Batch Acc: 0.782397, Tokens per Sec:     3740, Lr: 0.000350\r\n",
      "2024-08-27 14:50:53,518 - INFO - joeynmt.training - Epoch  28, Step:     2610, Batch Loss:     0.969248, Batch Acc: 0.785634, Tokens per Sec:     4014, Lr: 0.000350\r\n",
      "2024-08-27 14:50:54,631 - INFO - joeynmt.training - Epoch  28, Step:     2615, Batch Loss:     0.955204, Batch Acc: 0.771000, Tokens per Sec:     3833, Lr: 0.000350\r\n",
      "2024-08-27 14:50:55,739 - INFO - joeynmt.training - Epoch  28, Step:     2620, Batch Loss:     0.956502, Batch Acc: 0.777169, Tokens per Sec:     3955, Lr: 0.000349\r\n",
      "2024-08-27 14:50:56,910 - INFO - joeynmt.training - Epoch  28, Step:     2625, Batch Loss:     1.065695, Batch Acc: 0.767857, Tokens per Sec:     3542, Lr: 0.000349\r\n",
      "2024-08-27 14:50:58,028 - INFO - joeynmt.training - Epoch  28, Step:     2630, Batch Loss:     0.955476, Batch Acc: 0.773223, Tokens per Sec:     3778, Lr: 0.000349\r\n",
      "2024-08-27 14:50:59,127 - INFO - joeynmt.training - Epoch  28, Step:     2635, Batch Loss:     0.992061, Batch Acc: 0.780241, Tokens per Sec:     3860, Lr: 0.000348\r\n",
      "2024-08-27 14:51:00,230 - INFO - joeynmt.training - Epoch  28, Step:     2640, Batch Loss:     1.017312, Batch Acc: 0.765382, Tokens per Sec:     3791, Lr: 0.000348\r\n",
      "2024-08-27 14:51:01,375 - INFO - joeynmt.training - Epoch  28, Step:     2645, Batch Loss:     0.977119, Batch Acc: 0.757654, Tokens per Sec:     3710, Lr: 0.000348\r\n",
      "2024-08-27 14:51:02,497 - INFO - joeynmt.training - Epoch  28, Step:     2650, Batch Loss:     0.993589, Batch Acc: 0.758789, Tokens per Sec:     4008, Lr: 0.000347\r\n",
      "2024-08-27 14:51:03,623 - INFO - joeynmt.training - Epoch  28, Step:     2655, Batch Loss:     1.082575, Batch Acc: 0.751201, Tokens per Sec:     3702, Lr: 0.000347\r\n",
      "2024-08-27 14:51:04,745 - INFO - joeynmt.training - Epoch  28, Step:     2660, Batch Loss:     0.999118, Batch Acc: 0.760298, Tokens per Sec:     3831, Lr: 0.000347\r\n",
      "2024-08-27 14:51:05,856 - INFO - joeynmt.training - Epoch  28, Step:     2665, Batch Loss:     1.040028, Batch Acc: 0.748877, Tokens per Sec:     3609, Lr: 0.000347\r\n",
      "2024-08-27 14:51:06,531 - INFO - joeynmt.training - Epoch  28, total training loss: 93.65, num. of seqs: 8065, num. of tokens: 80463, 21.2985[sec]\r\n",
      "2024-08-27 14:51:06,531 - INFO - joeynmt.training - EPOCH 29\r\n",
      "2024-08-27 14:51:07,022 - INFO - joeynmt.training - Epoch  29, Step:     2670, Batch Loss:     0.869283, Batch Acc: 0.786339, Tokens per Sec:     3755, Lr: 0.000346\r\n",
      "2024-08-27 14:51:08,129 - INFO - joeynmt.training - Epoch  29, Step:     2675, Batch Loss:     0.843341, Batch Acc: 0.801445, Tokens per Sec:     3628, Lr: 0.000346\r\n",
      "2024-08-27 14:51:09,235 - INFO - joeynmt.training - Epoch  29, Step:     2680, Batch Loss:     0.812815, Batch Acc: 0.795315, Tokens per Sec:     3824, Lr: 0.000346\r\n",
      "2024-08-27 14:51:10,356 - INFO - joeynmt.training - Epoch  29, Step:     2685, Batch Loss:     0.853087, Batch Acc: 0.805065, Tokens per Sec:     3735, Lr: 0.000345\r\n",
      "2024-08-27 14:51:11,464 - INFO - joeynmt.training - Epoch  29, Step:     2690, Batch Loss:     0.900468, Batch Acc: 0.795884, Tokens per Sec:     3732, Lr: 0.000345\r\n",
      "2024-08-27 14:51:12,581 - INFO - joeynmt.training - Epoch  29, Step:     2695, Batch Loss:     0.843792, Batch Acc: 0.797359, Tokens per Sec:     3730, Lr: 0.000345\r\n",
      "2024-08-27 14:51:13,843 - INFO - joeynmt.training - Epoch  29, Step:     2700, Batch Loss:     0.959441, Batch Acc: 0.779135, Tokens per Sec:     3246, Lr: 0.000344\r\n",
      "2024-08-27 14:51:15,036 - INFO - joeynmt.training - Epoch  29, Step:     2705, Batch Loss:     0.940040, Batch Acc: 0.793119, Tokens per Sec:     3655, Lr: 0.000344\r\n",
      "2024-08-27 14:51:16,179 - INFO - joeynmt.training - Epoch  29, Step:     2710, Batch Loss:     0.930027, Batch Acc: 0.780955, Tokens per Sec:     3797, Lr: 0.000344\r\n",
      "2024-08-27 14:51:17,339 - INFO - joeynmt.training - Epoch  29, Step:     2715, Batch Loss:     0.888956, Batch Acc: 0.783037, Tokens per Sec:     3681, Lr: 0.000343\r\n",
      "2024-08-27 14:51:18,453 - INFO - joeynmt.training - Epoch  29, Step:     2720, Batch Loss:     0.917321, Batch Acc: 0.788928, Tokens per Sec:     3813, Lr: 0.000343\r\n",
      "2024-08-27 14:51:19,576 - INFO - joeynmt.training - Epoch  29, Step:     2725, Batch Loss:     0.906721, Batch Acc: 0.785272, Tokens per Sec:     3594, Lr: 0.000343\r\n",
      "2024-08-27 14:51:20,705 - INFO - joeynmt.training - Epoch  29, Step:     2730, Batch Loss:     0.939501, Batch Acc: 0.774462, Tokens per Sec:     4042, Lr: 0.000342\r\n",
      "2024-08-27 14:51:21,827 - INFO - joeynmt.training - Epoch  29, Step:     2735, Batch Loss:     1.018773, Batch Acc: 0.774268, Tokens per Sec:     3868, Lr: 0.000342\r\n",
      "2024-08-27 14:51:22,940 - INFO - joeynmt.training - Epoch  29, Step:     2740, Batch Loss:     0.942410, Batch Acc: 0.773274, Tokens per Sec:     3660, Lr: 0.000342\r\n",
      "2024-08-27 14:51:24,062 - INFO - joeynmt.training - Epoch  29, Step:     2745, Batch Loss:     0.991946, Batch Acc: 0.769693, Tokens per Sec:     3712, Lr: 0.000341\r\n",
      "2024-08-27 14:51:25,192 - INFO - joeynmt.training - Epoch  29, Step:     2750, Batch Loss:     0.996781, Batch Acc: 0.767312, Tokens per Sec:     3658, Lr: 0.000341\r\n",
      "2024-08-27 14:51:26,342 - INFO - joeynmt.training - Epoch  29, Step:     2755, Batch Loss:     0.930467, Batch Acc: 0.778076, Tokens per Sec:     3890, Lr: 0.000341\r\n",
      "2024-08-27 14:51:27,482 - INFO - joeynmt.training - Epoch  29, Step:     2760, Batch Loss:     0.916075, Batch Acc: 0.758890, Tokens per Sec:     3703, Lr: 0.000341\r\n",
      "2024-08-27 14:51:28,196 - INFO - joeynmt.training - Epoch  29, total training loss: 88.38, num. of seqs: 8065, num. of tokens: 80463, 21.6475[sec]\r\n",
      "2024-08-27 14:51:28,196 - INFO - joeynmt.training - EPOCH 30\r\n",
      "2024-08-27 14:51:28,649 - INFO - joeynmt.training - Epoch  30, Step:     2765, Batch Loss:     0.794309, Batch Acc: 0.807543, Tokens per Sec:     4015, Lr: 0.000340\r\n",
      "2024-08-27 14:51:29,770 - INFO - joeynmt.training - Epoch  30, Step:     2770, Batch Loss:     0.797658, Batch Acc: 0.824385, Tokens per Sec:     3991, Lr: 0.000340\r\n",
      "2024-08-27 14:51:30,884 - INFO - joeynmt.training - Epoch  30, Step:     2775, Batch Loss:     0.863650, Batch Acc: 0.806048, Tokens per Sec:     3595, Lr: 0.000340\r\n",
      "2024-08-27 14:51:31,989 - INFO - joeynmt.training - Epoch  30, Step:     2780, Batch Loss:     0.881634, Batch Acc: 0.803353, Tokens per Sec:     3780, Lr: 0.000339\r\n",
      "2024-08-27 14:51:33,099 - INFO - joeynmt.training - Epoch  30, Step:     2785, Batch Loss:     0.819533, Batch Acc: 0.800870, Tokens per Sec:     3940, Lr: 0.000339\r\n",
      "2024-08-27 14:51:34,202 - INFO - joeynmt.training - Epoch  30, Step:     2790, Batch Loss:     0.868645, Batch Acc: 0.804215, Tokens per Sec:     3874, Lr: 0.000339\r\n",
      "2024-08-27 14:51:35,299 - INFO - joeynmt.training - Epoch  30, Step:     2795, Batch Loss:     0.860325, Batch Acc: 0.805195, Tokens per Sec:     3863, Lr: 0.000338\r\n",
      "2024-08-27 14:51:36,394 - INFO - joeynmt.training - Epoch  30, Step:     2800, Batch Loss:     0.883624, Batch Acc: 0.809690, Tokens per Sec:     3924, Lr: 0.000338\r\n",
      "2024-08-27 14:51:37,513 - INFO - joeynmt.training - Epoch  30, Step:     2805, Batch Loss:     0.860416, Batch Acc: 0.800378, Tokens per Sec:     3789, Lr: 0.000338\r\n",
      "2024-08-27 14:51:38,647 - INFO - joeynmt.training - Epoch  30, Step:     2810, Batch Loss:     0.838863, Batch Acc: 0.785322, Tokens per Sec:     3690, Lr: 0.000337\r\n",
      "2024-08-27 14:51:39,769 - INFO - joeynmt.training - Epoch  30, Step:     2815, Batch Loss:     0.922055, Batch Acc: 0.790671, Tokens per Sec:     3883, Lr: 0.000337\r\n",
      "2024-08-27 14:51:40,888 - INFO - joeynmt.training - Epoch  30, Step:     2820, Batch Loss:     0.905071, Batch Acc: 0.795640, Tokens per Sec:     3611, Lr: 0.000337\r\n",
      "2024-08-27 14:51:42,030 - INFO - joeynmt.training - Epoch  30, Step:     2825, Batch Loss:     0.997024, Batch Acc: 0.784547, Tokens per Sec:     3536, Lr: 0.000337\r\n",
      "2024-08-27 14:51:43,162 - INFO - joeynmt.training - Epoch  30, Step:     2830, Batch Loss:     0.872317, Batch Acc: 0.788899, Tokens per Sec:     3889, Lr: 0.000336\r\n",
      "2024-08-27 14:51:44,285 - INFO - joeynmt.training - Epoch  30, Step:     2835, Batch Loss:     0.943987, Batch Acc: 0.780163, Tokens per Sec:     3609, Lr: 0.000336\r\n",
      "2024-08-27 14:51:45,531 - INFO - joeynmt.training - Epoch  30, Step:     2840, Batch Loss:     0.961852, Batch Acc: 0.778661, Tokens per Sec:     3538, Lr: 0.000336\r\n",
      "2024-08-27 14:51:46,647 - INFO - joeynmt.training - Epoch  30, Step:     2845, Batch Loss:     0.959542, Batch Acc: 0.776487, Tokens per Sec:     3859, Lr: 0.000335\r\n",
      "2024-08-27 14:51:47,772 - INFO - joeynmt.training - Epoch  30, Step:     2850, Batch Loss:     0.901445, Batch Acc: 0.793580, Tokens per Sec:     3798, Lr: 0.000335\r\n",
      "2024-08-27 14:51:48,886 - INFO - joeynmt.training - Epoch  30, Step:     2855, Batch Loss:     0.913352, Batch Acc: 0.778875, Tokens per Sec:     3546, Lr: 0.000335\r\n",
      "2024-08-27 14:51:49,615 - INFO - joeynmt.training - Epoch  30, total training loss: 83.45, num. of seqs: 8065, num. of tokens: 80463, 21.4004[sec]\r\n",
      "2024-08-27 14:51:49,615 - INFO - joeynmt.training - EPOCH 31\r\n",
      "2024-08-27 14:51:50,075 - INFO - joeynmt.training - Epoch  31, Step:     2860, Batch Loss:     0.838753, Batch Acc: 0.807193, Tokens per Sec:     3718, Lr: 0.000334\r\n",
      "2024-08-27 14:51:51,192 - INFO - joeynmt.training - Epoch  31, Step:     2865, Batch Loss:     0.769552, Batch Acc: 0.826379, Tokens per Sec:     3865, Lr: 0.000334\r\n",
      "2024-08-27 14:51:52,311 - INFO - joeynmt.training - Epoch  31, Step:     2870, Batch Loss:     0.764648, Batch Acc: 0.816957, Tokens per Sec:     3650, Lr: 0.000334\r\n",
      "2024-08-27 14:51:53,444 - INFO - joeynmt.training - Epoch  31, Step:     2875, Batch Loss:     0.822691, Batch Acc: 0.823502, Tokens per Sec:     3743, Lr: 0.000334\r\n",
      "2024-08-27 14:51:54,578 - INFO - joeynmt.training - Epoch  31, Step:     2880, Batch Loss:     0.804251, Batch Acc: 0.812429, Tokens per Sec:     3877, Lr: 0.000333\r\n",
      "2024-08-27 14:51:55,705 - INFO - joeynmt.training - Epoch  31, Step:     2885, Batch Loss:     0.803842, Batch Acc: 0.803368, Tokens per Sec:     3743, Lr: 0.000333\r\n",
      "2024-08-27 14:51:56,881 - INFO - joeynmt.training - Epoch  31, Step:     2890, Batch Loss:     0.794565, Batch Acc: 0.804129, Tokens per Sec:     3544, Lr: 0.000333\r\n",
      "2024-08-27 14:51:58,006 - INFO - joeynmt.training - Epoch  31, Step:     2895, Batch Loss:     0.848626, Batch Acc: 0.803405, Tokens per Sec:     3813, Lr: 0.000332\r\n",
      "2024-08-27 14:51:59,148 - INFO - joeynmt.training - Epoch  31, Step:     2900, Batch Loss:     0.764589, Batch Acc: 0.818182, Tokens per Sec:     3662, Lr: 0.000332\r\n",
      "2024-08-27 14:52:00,264 - INFO - joeynmt.training - Epoch  31, Step:     2905, Batch Loss:     0.839156, Batch Acc: 0.812872, Tokens per Sec:     3763, Lr: 0.000332\r\n",
      "2024-08-27 14:52:01,421 - INFO - joeynmt.training - Epoch  31, Step:     2910, Batch Loss:     0.806189, Batch Acc: 0.812223, Tokens per Sec:     3707, Lr: 0.000332\r\n",
      "2024-08-27 14:52:02,567 - INFO - joeynmt.training - Epoch  31, Step:     2915, Batch Loss:     0.862922, Batch Acc: 0.802638, Tokens per Sec:     3774, Lr: 0.000331\r\n",
      "2024-08-27 14:52:03,710 - INFO - joeynmt.training - Epoch  31, Step:     2920, Batch Loss:     0.838016, Batch Acc: 0.802824, Tokens per Sec:     3658, Lr: 0.000331\r\n",
      "2024-08-27 14:52:04,826 - INFO - joeynmt.training - Epoch  31, Step:     2925, Batch Loss:     0.877664, Batch Acc: 0.795302, Tokens per Sec:     3741, Lr: 0.000331\r\n",
      "2024-08-27 14:52:05,963 - INFO - joeynmt.training - Epoch  31, Step:     2930, Batch Loss:     0.881188, Batch Acc: 0.791520, Tokens per Sec:     3758, Lr: 0.000330\r\n",
      "2024-08-27 14:52:07,116 - INFO - joeynmt.training - Epoch  31, Step:     2935, Batch Loss:     0.820919, Batch Acc: 0.803643, Tokens per Sec:     3478, Lr: 0.000330\r\n",
      "2024-08-27 14:52:08,233 - INFO - joeynmt.training - Epoch  31, Step:     2940, Batch Loss:     0.820653, Batch Acc: 0.801181, Tokens per Sec:     3640, Lr: 0.000330\r\n",
      "2024-08-27 14:52:09,349 - INFO - joeynmt.training - Epoch  31, Step:     2945, Batch Loss:     0.858257, Batch Acc: 0.797819, Tokens per Sec:     3947, Lr: 0.000330\r\n",
      "2024-08-27 14:52:10,476 - INFO - joeynmt.training - Epoch  31, Step:     2950, Batch Loss:     0.906719, Batch Acc: 0.799220, Tokens per Sec:     3872, Lr: 0.000329\r\n",
      "2024-08-27 14:52:11,191 - INFO - joeynmt.training - Epoch  31, total training loss: 78.45, num. of seqs: 8065, num. of tokens: 80463, 21.5583[sec]\r\n",
      "2024-08-27 14:52:11,191 - INFO - joeynmt.training - EPOCH 32\r\n",
      "2024-08-27 14:52:11,649 - INFO - joeynmt.training - Epoch  32, Step:     2955, Batch Loss:     0.724963, Batch Acc: 0.831197, Tokens per Sec:     4133, Lr: 0.000329\r\n",
      "2024-08-27 14:52:12,744 - INFO - joeynmt.training - Epoch  32, Step:     2960, Batch Loss:     0.733703, Batch Acc: 0.838386, Tokens per Sec:     3827, Lr: 0.000329\r\n",
      "2024-08-27 14:52:13,877 - INFO - joeynmt.training - Epoch  32, Step:     2965, Batch Loss:     0.714607, Batch Acc: 0.829064, Tokens per Sec:     3689, Lr: 0.000329\r\n",
      "2024-08-27 14:52:15,029 - INFO - joeynmt.training - Epoch  32, Step:     2970, Batch Loss:     0.801993, Batch Acc: 0.836270, Tokens per Sec:     3551, Lr: 0.000328\r\n",
      "2024-08-27 14:52:16,262 - INFO - joeynmt.training - Epoch  32, Step:     2975, Batch Loss:     0.851646, Batch Acc: 0.827979, Tokens per Sec:     3561, Lr: 0.000328\r\n",
      "2024-08-27 14:52:17,446 - INFO - joeynmt.training - Epoch  32, Step:     2980, Batch Loss:     0.724724, Batch Acc: 0.820289, Tokens per Sec:     3679, Lr: 0.000328\r\n",
      "2024-08-27 14:52:18,566 - INFO - joeynmt.training - Epoch  32, Step:     2985, Batch Loss:     0.773506, Batch Acc: 0.810913, Tokens per Sec:     3800, Lr: 0.000327\r\n",
      "2024-08-27 14:52:19,678 - INFO - joeynmt.training - Epoch  32, Step:     2990, Batch Loss:     0.710626, Batch Acc: 0.823132, Tokens per Sec:     3868, Lr: 0.000327\r\n",
      "2024-08-27 14:52:20,791 - INFO - joeynmt.training - Epoch  32, Step:     2995, Batch Loss:     0.834689, Batch Acc: 0.816272, Tokens per Sec:     3681, Lr: 0.000327\r\n",
      "2024-08-27 14:52:21,913 - INFO - joeynmt.training - Epoch  32, Step:     3000, Batch Loss:     0.811242, Batch Acc: 0.810461, Tokens per Sec:     3853, Lr: 0.000327\r\n",
      "2024-08-27 14:52:21,914 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=3042\r\n",
      "2024-08-27 14:52:21,914 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 64.45it/s]\r\n",
      "2024-08-27 14:52:44,570 - INFO - joeynmt.prediction - Generation took 22.6537[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44385.61 examples/s]\r\n",
      "2024-08-27 14:52:44,941 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 14:52:44,941 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   3.22, loss:   5.68, ppl: 292.78, acc:   0.28, 0.1575[sec]\r\n",
      "2024-08-27 14:52:44,942 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\r\n",
      "2024-08-27 14:52:45,258 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/3000.ckpt.\r\n",
      "2024-08-27 14:52:45,259 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/1500.ckpt\r\n",
      "2024-08-27 14:52:45,283 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 14:52:45,431 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 14:52:45,432 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 14:52:45,432 - INFO - joeynmt.training - \tHypothesis: il faudrait faudrait\r\n",
      "2024-08-27 14:52:45,724 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 14:52:45,869 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 14:52:45,870 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 14:52:45,870 - INFO - joeynmt.training - \tHypothesis: trois tables\r\n",
      "2024-08-27 14:52:46,163 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 14:52:46,310 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 14:52:46,310 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 14:52:46,310 - INFO - joeynmt.training - \tHypothesis: mais cela n’y avait plus vu\r\n",
      "2024-08-27 14:52:46,606 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 14:52:46,782 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 14:52:46,782 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 14:52:46,782 - INFO - joeynmt.training - \tHypothesis: une heure et demie\r\n",
      "2024-08-27 14:52:48,310 - INFO - joeynmt.training - Epoch  32, Step:     3005, Batch Loss:     0.831647, Batch Acc: 0.810701, Tokens per Sec:     3498, Lr: 0.000326\r\n",
      "2024-08-27 14:52:49,417 - INFO - joeynmt.training - Epoch  32, Step:     3010, Batch Loss:     0.777397, Batch Acc: 0.811708, Tokens per Sec:     3924, Lr: 0.000326\r\n",
      "2024-08-27 14:52:50,495 - INFO - joeynmt.training - Epoch  32, Step:     3015, Batch Loss:     0.813989, Batch Acc: 0.814881, Tokens per Sec:     3655, Lr: 0.000326\r\n",
      "2024-08-27 14:52:51,630 - INFO - joeynmt.training - Epoch  32, Step:     3020, Batch Loss:     0.788912, Batch Acc: 0.805204, Tokens per Sec:     3761, Lr: 0.000326\r\n",
      "2024-08-27 14:52:52,743 - INFO - joeynmt.training - Epoch  32, Step:     3025, Batch Loss:     0.816000, Batch Acc: 0.802563, Tokens per Sec:     3647, Lr: 0.000325\r\n",
      "2024-08-27 14:52:53,862 - INFO - joeynmt.training - Epoch  32, Step:     3030, Batch Loss:     0.824194, Batch Acc: 0.803201, Tokens per Sec:     3576, Lr: 0.000325\r\n",
      "2024-08-27 14:52:54,980 - INFO - joeynmt.training - Epoch  32, Step:     3035, Batch Loss:     0.864400, Batch Acc: 0.798834, Tokens per Sec:     3841, Lr: 0.000325\r\n",
      "2024-08-27 14:52:56,126 - INFO - joeynmt.training - Epoch  32, Step:     3040, Batch Loss:     0.891644, Batch Acc: 0.794173, Tokens per Sec:     3686, Lr: 0.000324\r\n",
      "2024-08-27 14:52:57,297 - INFO - joeynmt.training - Epoch  32, Step:     3045, Batch Loss:     0.823437, Batch Acc: 0.815342, Tokens per Sec:     3420, Lr: 0.000324\r\n",
      "2024-08-27 14:52:58,122 - INFO - joeynmt.training - Epoch  32, total training loss: 74.67, num. of seqs: 8065, num. of tokens: 80463, 21.7070[sec]\r\n",
      "2024-08-27 14:52:58,122 - INFO - joeynmt.training - EPOCH 33\r\n",
      "2024-08-27 14:52:58,569 - INFO - joeynmt.training - Epoch  33, Step:     3050, Batch Loss:     0.727268, Batch Acc: 0.837853, Tokens per Sec:     3915, Lr: 0.000324\r\n",
      "2024-08-27 14:52:59,665 - INFO - joeynmt.training - Epoch  33, Step:     3055, Batch Loss:     0.680302, Batch Acc: 0.828544, Tokens per Sec:     3811, Lr: 0.000324\r\n",
      "2024-08-27 14:53:00,775 - INFO - joeynmt.training - Epoch  33, Step:     3060, Batch Loss:     0.760619, Batch Acc: 0.840594, Tokens per Sec:     3705, Lr: 0.000323\r\n",
      "2024-08-27 14:53:01,877 - INFO - joeynmt.training - Epoch  33, Step:     3065, Batch Loss:     0.707194, Batch Acc: 0.841506, Tokens per Sec:     3669, Lr: 0.000323\r\n",
      "2024-08-27 14:53:03,007 - INFO - joeynmt.training - Epoch  33, Step:     3070, Batch Loss:     0.703265, Batch Acc: 0.837079, Tokens per Sec:     3781, Lr: 0.000323\r\n",
      "2024-08-27 14:53:04,121 - INFO - joeynmt.training - Epoch  33, Step:     3075, Batch Loss:     0.769070, Batch Acc: 0.832253, Tokens per Sec:     3744, Lr: 0.000323\r\n",
      "2024-08-27 14:53:05,247 - INFO - joeynmt.training - Epoch  33, Step:     3080, Batch Loss:     0.752493, Batch Acc: 0.825690, Tokens per Sec:     3603, Lr: 0.000322\r\n",
      "2024-08-27 14:53:06,387 - INFO - joeynmt.training - Epoch  33, Step:     3085, Batch Loss:     0.754501, Batch Acc: 0.828256, Tokens per Sec:     3662, Lr: 0.000322\r\n",
      "2024-08-27 14:53:07,545 - INFO - joeynmt.training - Epoch  33, Step:     3090, Batch Loss:     0.753495, Batch Acc: 0.825305, Tokens per Sec:     3750, Lr: 0.000322\r\n",
      "2024-08-27 14:53:08,676 - INFO - joeynmt.training - Epoch  33, Step:     3095, Batch Loss:     0.747255, Batch Acc: 0.829210, Tokens per Sec:     3681, Lr: 0.000322\r\n",
      "2024-08-27 14:53:09,814 - INFO - joeynmt.training - Epoch  33, Step:     3100, Batch Loss:     0.746750, Batch Acc: 0.815843, Tokens per Sec:     3863, Lr: 0.000321\r\n",
      "2024-08-27 14:53:10,944 - INFO - joeynmt.training - Epoch  33, Step:     3105, Batch Loss:     0.764596, Batch Acc: 0.818615, Tokens per Sec:     3721, Lr: 0.000321\r\n",
      "2024-08-27 14:53:12,080 - INFO - joeynmt.training - Epoch  33, Step:     3110, Batch Loss:     0.795909, Batch Acc: 0.819687, Tokens per Sec:     3829, Lr: 0.000321\r\n",
      "2024-08-27 14:53:13,217 - INFO - joeynmt.training - Epoch  33, Step:     3115, Batch Loss:     0.762311, Batch Acc: 0.812751, Tokens per Sec:     3727, Lr: 0.000321\r\n",
      "2024-08-27 14:53:14,350 - INFO - joeynmt.training - Epoch  33, Step:     3120, Batch Loss:     0.789532, Batch Acc: 0.824652, Tokens per Sec:     3614, Lr: 0.000320\r\n",
      "2024-08-27 14:53:15,522 - INFO - joeynmt.training - Epoch  33, Step:     3125, Batch Loss:     0.738741, Batch Acc: 0.821799, Tokens per Sec:     3540, Lr: 0.000320\r\n",
      "2024-08-27 14:53:16,662 - INFO - joeynmt.training - Epoch  33, Step:     3130, Batch Loss:     0.777699, Batch Acc: 0.823269, Tokens per Sec:     3763, Lr: 0.000320\r\n",
      "2024-08-27 14:53:17,787 - INFO - joeynmt.training - Epoch  33, Step:     3135, Batch Loss:     0.817672, Batch Acc: 0.811876, Tokens per Sec:     3567, Lr: 0.000319\r\n",
      "2024-08-27 14:53:19,146 - INFO - joeynmt.training - Epoch  33, Step:     3140, Batch Loss:     0.849070, Batch Acc: 0.811001, Tokens per Sec:     3039, Lr: 0.000319\r\n",
      "2024-08-27 14:53:20,043 - INFO - joeynmt.training - Epoch  33, total training loss: 72.29, num. of seqs: 8065, num. of tokens: 80463, 21.9025[sec]\r\n",
      "2024-08-27 14:53:20,043 - INFO - joeynmt.training - EPOCH 34\r\n",
      "2024-08-27 14:53:20,267 - INFO - joeynmt.training - Epoch  34, Step:     3145, Batch Loss:     0.667097, Batch Acc: 0.839237, Tokens per Sec:     3334, Lr: 0.000319\r\n",
      "2024-08-27 14:53:21,420 - INFO - joeynmt.training - Epoch  34, Step:     3150, Batch Loss:     0.708512, Batch Acc: 0.841795, Tokens per Sec:     3674, Lr: 0.000319\r\n",
      "2024-08-27 14:53:22,518 - INFO - joeynmt.training - Epoch  34, Step:     3155, Batch Loss:     0.695412, Batch Acc: 0.833013, Tokens per Sec:     3793, Lr: 0.000318\r\n",
      "2024-08-27 14:53:23,634 - INFO - joeynmt.training - Epoch  34, Step:     3160, Batch Loss:     0.672138, Batch Acc: 0.832705, Tokens per Sec:     3808, Lr: 0.000318\r\n",
      "2024-08-27 14:53:24,718 - INFO - joeynmt.training - Epoch  34, Step:     3165, Batch Loss:     0.687908, Batch Acc: 0.841831, Tokens per Sec:     3890, Lr: 0.000318\r\n",
      "2024-08-27 14:53:25,814 - INFO - joeynmt.training - Epoch  34, Step:     3170, Batch Loss:     0.776260, Batch Acc: 0.839133, Tokens per Sec:     3961, Lr: 0.000318\r\n",
      "2024-08-27 14:53:26,943 - INFO - joeynmt.training - Epoch  34, Step:     3175, Batch Loss:     0.693458, Batch Acc: 0.842017, Tokens per Sec:     3711, Lr: 0.000317\r\n",
      "2024-08-27 14:53:28,054 - INFO - joeynmt.training - Epoch  34, Step:     3180, Batch Loss:     0.724239, Batch Acc: 0.840651, Tokens per Sec:     3651, Lr: 0.000317\r\n",
      "2024-08-27 14:53:29,181 - INFO - joeynmt.training - Epoch  34, Step:     3185, Batch Loss:     0.775789, Batch Acc: 0.829919, Tokens per Sec:     3726, Lr: 0.000317\r\n",
      "2024-08-27 14:53:30,292 - INFO - joeynmt.training - Epoch  34, Step:     3190, Batch Loss:     0.716606, Batch Acc: 0.822997, Tokens per Sec:     3877, Lr: 0.000317\r\n",
      "2024-08-27 14:53:31,385 - INFO - joeynmt.training - Epoch  34, Step:     3195, Batch Loss:     0.690017, Batch Acc: 0.832230, Tokens per Sec:     3732, Lr: 0.000316\r\n",
      "2024-08-27 14:53:32,481 - INFO - joeynmt.training - Epoch  34, Step:     3200, Batch Loss:     0.730108, Batch Acc: 0.827415, Tokens per Sec:     3857, Lr: 0.000316\r\n",
      "2024-08-27 14:53:33,589 - INFO - joeynmt.training - Epoch  34, Step:     3205, Batch Loss:     0.785017, Batch Acc: 0.825371, Tokens per Sec:     3841, Lr: 0.000316\r\n",
      "2024-08-27 14:53:34,672 - INFO - joeynmt.training - Epoch  34, Step:     3210, Batch Loss:     0.723514, Batch Acc: 0.824895, Tokens per Sec:     3941, Lr: 0.000316\r\n",
      "2024-08-27 14:53:35,824 - INFO - joeynmt.training - Epoch  34, Step:     3215, Batch Loss:     0.741672, Batch Acc: 0.824541, Tokens per Sec:     3737, Lr: 0.000315\r\n",
      "2024-08-27 14:53:36,996 - INFO - joeynmt.training - Epoch  34, Step:     3220, Batch Loss:     0.738344, Batch Acc: 0.826190, Tokens per Sec:     3605, Lr: 0.000315\r\n",
      "2024-08-27 14:53:38,157 - INFO - joeynmt.training - Epoch  34, Step:     3225, Batch Loss:     0.729622, Batch Acc: 0.823529, Tokens per Sec:     3766, Lr: 0.000315\r\n",
      "2024-08-27 14:53:39,292 - INFO - joeynmt.training - Epoch  34, Step:     3230, Batch Loss:     0.746289, Batch Acc: 0.828397, Tokens per Sec:     3750, Lr: 0.000315\r\n",
      "2024-08-27 14:53:40,407 - INFO - joeynmt.training - Epoch  34, Step:     3235, Batch Loss:     0.771438, Batch Acc: 0.822763, Tokens per Sec:     3723, Lr: 0.000315\r\n",
      "2024-08-27 14:53:41,414 - INFO - joeynmt.training - Epoch  34, total training loss: 68.28, num. of seqs: 8065, num. of tokens: 80463, 21.3531[sec]\r\n",
      "2024-08-27 14:53:41,414 - INFO - joeynmt.training - EPOCH 35\r\n",
      "2024-08-27 14:53:41,649 - INFO - joeynmt.training - Epoch  35, Step:     3240, Batch Loss:     0.686520, Batch Acc: 0.837983, Tokens per Sec:     4050, Lr: 0.000314\r\n",
      "2024-08-27 14:53:42,769 - INFO - joeynmt.training - Epoch  35, Step:     3245, Batch Loss:     0.611736, Batch Acc: 0.852297, Tokens per Sec:     3714, Lr: 0.000314\r\n",
      "2024-08-27 14:53:43,903 - INFO - joeynmt.training - Epoch  35, Step:     3250, Batch Loss:     0.632950, Batch Acc: 0.850203, Tokens per Sec:     3687, Lr: 0.000314\r\n",
      "2024-08-27 14:53:45,032 - INFO - joeynmt.training - Epoch  35, Step:     3255, Batch Loss:     0.621725, Batch Acc: 0.849226, Tokens per Sec:     3949, Lr: 0.000314\r\n",
      "2024-08-27 14:53:46,162 - INFO - joeynmt.training - Epoch  35, Step:     3260, Batch Loss:     0.668080, Batch Acc: 0.852197, Tokens per Sec:     3710, Lr: 0.000313\r\n",
      "2024-08-27 14:53:47,353 - INFO - joeynmt.training - Epoch  35, Step:     3265, Batch Loss:     0.694338, Batch Acc: 0.840957, Tokens per Sec:     3619, Lr: 0.000313\r\n",
      "2024-08-27 14:53:48,496 - INFO - joeynmt.training - Epoch  35, Step:     3270, Batch Loss:     0.662134, Batch Acc: 0.840915, Tokens per Sec:     3598, Lr: 0.000313\r\n",
      "2024-08-27 14:53:49,745 - INFO - joeynmt.training - Epoch  35, Step:     3275, Batch Loss:     0.676782, Batch Acc: 0.843305, Tokens per Sec:     3092, Lr: 0.000313\r\n",
      "2024-08-27 14:53:50,915 - INFO - joeynmt.training - Epoch  35, Step:     3280, Batch Loss:     0.688037, Batch Acc: 0.843239, Tokens per Sec:     3611, Lr: 0.000312\r\n",
      "2024-08-27 14:53:52,063 - INFO - joeynmt.training - Epoch  35, Step:     3285, Batch Loss:     0.728234, Batch Acc: 0.829045, Tokens per Sec:     3627, Lr: 0.000312\r\n",
      "2024-08-27 14:53:53,226 - INFO - joeynmt.training - Epoch  35, Step:     3290, Batch Loss:     0.676612, Batch Acc: 0.824499, Tokens per Sec:     3603, Lr: 0.000312\r\n",
      "2024-08-27 14:53:54,371 - INFO - joeynmt.training - Epoch  35, Step:     3295, Batch Loss:     0.695048, Batch Acc: 0.841939, Tokens per Sec:     3589, Lr: 0.000312\r\n",
      "2024-08-27 14:53:55,490 - INFO - joeynmt.training - Epoch  35, Step:     3300, Batch Loss:     0.739519, Batch Acc: 0.835052, Tokens per Sec:     3730, Lr: 0.000311\r\n",
      "2024-08-27 14:53:56,650 - INFO - joeynmt.training - Epoch  35, Step:     3305, Batch Loss:     0.613663, Batch Acc: 0.837632, Tokens per Sec:     3746, Lr: 0.000311\r\n",
      "2024-08-27 14:53:57,802 - INFO - joeynmt.training - Epoch  35, Step:     3310, Batch Loss:     0.728124, Batch Acc: 0.832268, Tokens per Sec:     3808, Lr: 0.000311\r\n",
      "2024-08-27 14:53:58,939 - INFO - joeynmt.training - Epoch  35, Step:     3315, Batch Loss:     0.699888, Batch Acc: 0.834715, Tokens per Sec:     3714, Lr: 0.000311\r\n",
      "2024-08-27 14:54:00,075 - INFO - joeynmt.training - Epoch  35, Step:     3320, Batch Loss:     0.720628, Batch Acc: 0.837345, Tokens per Sec:     3624, Lr: 0.000310\r\n",
      "2024-08-27 14:54:01,169 - INFO - joeynmt.training - Epoch  35, Step:     3325, Batch Loss:     0.713808, Batch Acc: 0.835610, Tokens per Sec:     3682, Lr: 0.000310\r\n",
      "2024-08-27 14:54:02,281 - INFO - joeynmt.training - Epoch  35, Step:     3330, Batch Loss:     0.795786, Batch Acc: 0.828876, Tokens per Sec:     3801, Lr: 0.000310\r\n",
      "2024-08-27 14:54:03,394 - INFO - joeynmt.training - Epoch  35, Step:     3335, Batch Loss:     0.661562, Batch Acc: 0.831469, Tokens per Sec:     3698, Lr: 0.000310\r\n",
      "2024-08-27 14:54:03,394 - INFO - joeynmt.training - Epoch  35, total training loss: 65.67, num. of seqs: 8065, num. of tokens: 80463, 21.9620[sec]\r\n",
      "2024-08-27 14:54:03,395 - INFO - joeynmt.training - EPOCH 36\r\n",
      "2024-08-27 14:54:04,487 - INFO - joeynmt.training - Epoch  36, Step:     3340, Batch Loss:     0.635379, Batch Acc: 0.855052, Tokens per Sec:     3829, Lr: 0.000310\r\n",
      "2024-08-27 14:54:05,591 - INFO - joeynmt.training - Epoch  36, Step:     3345, Batch Loss:     0.609475, Batch Acc: 0.853454, Tokens per Sec:     3896, Lr: 0.000309\r\n",
      "2024-08-27 14:54:06,697 - INFO - joeynmt.training - Epoch  36, Step:     3350, Batch Loss:     0.651267, Batch Acc: 0.854162, Tokens per Sec:     3772, Lr: 0.000309\r\n",
      "2024-08-27 14:54:07,801 - INFO - joeynmt.training - Epoch  36, Step:     3355, Batch Loss:     0.637453, Batch Acc: 0.855120, Tokens per Sec:     3840, Lr: 0.000309\r\n",
      "2024-08-27 14:54:08,912 - INFO - joeynmt.training - Epoch  36, Step:     3360, Batch Loss:     0.599363, Batch Acc: 0.852856, Tokens per Sec:     4022, Lr: 0.000309\r\n",
      "2024-08-27 14:54:10,023 - INFO - joeynmt.training - Epoch  36, Step:     3365, Batch Loss:     0.622093, Batch Acc: 0.852251, Tokens per Sec:     3763, Lr: 0.000308\r\n",
      "2024-08-27 14:54:11,148 - INFO - joeynmt.training - Epoch  36, Step:     3370, Batch Loss:     0.585188, Batch Acc: 0.856906, Tokens per Sec:     3761, Lr: 0.000308\r\n",
      "2024-08-27 14:54:12,249 - INFO - joeynmt.training - Epoch  36, Step:     3375, Batch Loss:     0.702279, Batch Acc: 0.849694, Tokens per Sec:     3710, Lr: 0.000308\r\n",
      "2024-08-27 14:54:13,365 - INFO - joeynmt.training - Epoch  36, Step:     3380, Batch Loss:     0.672739, Batch Acc: 0.842595, Tokens per Sec:     3859, Lr: 0.000308\r\n",
      "2024-08-27 14:54:14,468 - INFO - joeynmt.training - Epoch  36, Step:     3385, Batch Loss:     0.675907, Batch Acc: 0.855690, Tokens per Sec:     3745, Lr: 0.000307\r\n",
      "2024-08-27 14:54:15,581 - INFO - joeynmt.training - Epoch  36, Step:     3390, Batch Loss:     0.653770, Batch Acc: 0.846452, Tokens per Sec:     3713, Lr: 0.000307\r\n",
      "2024-08-27 14:54:16,688 - INFO - joeynmt.training - Epoch  36, Step:     3395, Batch Loss:     0.652682, Batch Acc: 0.848321, Tokens per Sec:     3687, Lr: 0.000307\r\n",
      "2024-08-27 14:54:17,814 - INFO - joeynmt.training - Epoch  36, Step:     3400, Batch Loss:     0.669563, Batch Acc: 0.831390, Tokens per Sec:     3659, Lr: 0.000307\r\n",
      "2024-08-27 14:54:18,934 - INFO - joeynmt.training - Epoch  36, Step:     3405, Batch Loss:     0.676456, Batch Acc: 0.838903, Tokens per Sec:     3879, Lr: 0.000307\r\n",
      "2024-08-27 14:54:20,052 - INFO - joeynmt.training - Epoch  36, Step:     3410, Batch Loss:     0.687475, Batch Acc: 0.841589, Tokens per Sec:     3649, Lr: 0.000306\r\n",
      "2024-08-27 14:54:21,236 - INFO - joeynmt.training - Epoch  36, Step:     3415, Batch Loss:     0.651179, Batch Acc: 0.844875, Tokens per Sec:     3356, Lr: 0.000306\r\n",
      "2024-08-27 14:54:22,364 - INFO - joeynmt.training - Epoch  36, Step:     3420, Batch Loss:     0.724514, Batch Acc: 0.834171, Tokens per Sec:     3708, Lr: 0.000306\r\n",
      "2024-08-27 14:54:23,474 - INFO - joeynmt.training - Epoch  36, Step:     3425, Batch Loss:     0.767645, Batch Acc: 0.834791, Tokens per Sec:     3815, Lr: 0.000306\r\n",
      "2024-08-27 14:54:24,571 - INFO - joeynmt.training - Epoch  36, Step:     3430, Batch Loss:     0.727863, Batch Acc: 0.831054, Tokens per Sec:     3935, Lr: 0.000305\r\n",
      "2024-08-27 14:54:24,781 - INFO - joeynmt.training - Epoch  36, total training loss: 62.73, num. of seqs: 8065, num. of tokens: 80463, 21.3692[sec]\r\n",
      "2024-08-27 14:54:24,781 - INFO - joeynmt.training - EPOCH 37\r\n",
      "2024-08-27 14:54:25,683 - INFO - joeynmt.training - Epoch  37, Step:     3435, Batch Loss:     0.575865, Batch Acc: 0.858242, Tokens per Sec:     4050, Lr: 0.000305\r\n",
      "2024-08-27 14:54:26,828 - INFO - joeynmt.training - Epoch  37, Step:     3440, Batch Loss:     0.595053, Batch Acc: 0.862077, Tokens per Sec:     3695, Lr: 0.000305\r\n",
      "2024-08-27 14:54:27,985 - INFO - joeynmt.training - Epoch  37, Step:     3445, Batch Loss:     0.580497, Batch Acc: 0.858283, Tokens per Sec:     3575, Lr: 0.000305\r\n",
      "2024-08-27 14:54:29,146 - INFO - joeynmt.training - Epoch  37, Step:     3450, Batch Loss:     0.556136, Batch Acc: 0.866261, Tokens per Sec:     3685, Lr: 0.000305\r\n",
      "2024-08-27 14:54:30,292 - INFO - joeynmt.training - Epoch  37, Step:     3455, Batch Loss:     0.593071, Batch Acc: 0.854152, Tokens per Sec:     3628, Lr: 0.000304\r\n",
      "2024-08-27 14:54:31,438 - INFO - joeynmt.training - Epoch  37, Step:     3460, Batch Loss:     0.632686, Batch Acc: 0.856023, Tokens per Sec:     3678, Lr: 0.000304\r\n",
      "2024-08-27 14:54:32,576 - INFO - joeynmt.training - Epoch  37, Step:     3465, Batch Loss:     0.654210, Batch Acc: 0.851132, Tokens per Sec:     3844, Lr: 0.000304\r\n",
      "2024-08-27 14:54:33,701 - INFO - joeynmt.training - Epoch  37, Step:     3470, Batch Loss:     0.594687, Batch Acc: 0.849521, Tokens per Sec:     3618, Lr: 0.000304\r\n",
      "2024-08-27 14:54:34,822 - INFO - joeynmt.training - Epoch  37, Step:     3475, Batch Loss:     0.638679, Batch Acc: 0.845074, Tokens per Sec:     3686, Lr: 0.000303\r\n",
      "2024-08-27 14:54:35,922 - INFO - joeynmt.training - Epoch  37, Step:     3480, Batch Loss:     0.640715, Batch Acc: 0.845056, Tokens per Sec:     3569, Lr: 0.000303\r\n",
      "2024-08-27 14:54:37,077 - INFO - joeynmt.training - Epoch  37, Step:     3485, Batch Loss:     0.649900, Batch Acc: 0.846277, Tokens per Sec:     3784, Lr: 0.000303\r\n",
      "2024-08-27 14:54:38,196 - INFO - joeynmt.training - Epoch  37, Step:     3490, Batch Loss:     0.587734, Batch Acc: 0.859090, Tokens per Sec:     3868, Lr: 0.000303\r\n",
      "2024-08-27 14:54:39,351 - INFO - joeynmt.training - Epoch  37, Step:     3495, Batch Loss:     0.609357, Batch Acc: 0.849655, Tokens per Sec:     3770, Lr: 0.000303\r\n",
      "2024-08-27 14:54:40,477 - INFO - joeynmt.training - Epoch  37, Step:     3500, Batch Loss:     0.628131, Batch Acc: 0.845282, Tokens per Sec:     3608, Lr: 0.000302\r\n",
      "2024-08-27 14:54:40,478 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=3542\r\n",
      "2024-08-27 14:54:40,478 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:23<00:00, 62.83it/s]\r\n",
      "2024-08-27 14:55:03,720 - INFO - joeynmt.prediction - Generation took 23.2396[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44438.35 examples/s]\r\n",
      "2024-08-27 14:55:04,089 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 14:55:04,089 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   3.40, loss:   5.88, ppl: 356.37, acc:   0.27, 0.1572[sec]\r\n",
      "2024-08-27 14:55:04,090 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\r\n",
      "2024-08-27 14:55:04,407 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/3500.ckpt.\r\n",
      "2024-08-27 14:55:04,408 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/2000.ckpt\r\n",
      "2024-08-27 14:55:04,432 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 14:55:04,581 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 14:55:04,581 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 14:55:04,581 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 14:55:04,874 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 14:55:05,018 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 14:55:05,019 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 14:55:05,019 - INFO - joeynmt.training - \tHypothesis: trois maisons en bois\r\n",
      "2024-08-27 14:55:05,311 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 14:55:05,457 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 14:55:05,457 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 14:55:05,457 - INFO - joeynmt.training - \tHypothesis: nous abordons un autre sujet\r\n",
      "2024-08-27 14:55:05,751 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 14:55:05,898 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 14:55:05,898 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 14:55:05,898 - INFO - joeynmt.training - \tHypothesis: une heure et demie\r\n",
      "2024-08-27 14:55:07,302 - INFO - joeynmt.training - Epoch  37, Step:     3505, Batch Loss:     0.698551, Batch Acc: 0.846041, Tokens per Sec:     3676, Lr: 0.000302\r\n",
      "2024-08-27 14:55:08,405 - INFO - joeynmt.training - Epoch  37, Step:     3510, Batch Loss:     0.600625, Batch Acc: 0.847026, Tokens per Sec:     3920, Lr: 0.000302\r\n",
      "2024-08-27 14:55:09,489 - INFO - joeynmt.training - Epoch  37, Step:     3515, Batch Loss:     0.630985, Batch Acc: 0.842919, Tokens per Sec:     3822, Lr: 0.000302\r\n",
      "2024-08-27 14:55:10,591 - INFO - joeynmt.training - Epoch  37, Step:     3520, Batch Loss:     0.651012, Batch Acc: 0.837987, Tokens per Sec:     3770, Lr: 0.000302\r\n",
      "2024-08-27 14:55:11,686 - INFO - joeynmt.training - Epoch  37, Step:     3525, Batch Loss:     0.722209, Batch Acc: 0.834353, Tokens per Sec:     3737, Lr: 0.000301\r\n",
      "2024-08-27 14:55:12,055 - INFO - joeynmt.training - Epoch  37, total training loss: 59.44, num. of seqs: 8065, num. of tokens: 80463, 21.5415[sec]\r\n",
      "2024-08-27 14:55:12,055 - INFO - joeynmt.training - EPOCH 38\r\n",
      "2024-08-27 14:55:12,925 - INFO - joeynmt.training - Epoch  38, Step:     3530, Batch Loss:     0.498993, Batch Acc: 0.875627, Tokens per Sec:     4143, Lr: 0.000301\r\n",
      "2024-08-27 14:55:14,004 - INFO - joeynmt.training - Epoch  38, Step:     3535, Batch Loss:     0.622133, Batch Acc: 0.860515, Tokens per Sec:     3929, Lr: 0.000301\r\n",
      "2024-08-27 14:55:15,088 - INFO - joeynmt.training - Epoch  38, Step:     3540, Batch Loss:     0.596362, Batch Acc: 0.868774, Tokens per Sec:     3854, Lr: 0.000301\r\n",
      "2024-08-27 14:55:16,173 - INFO - joeynmt.training - Epoch  38, Step:     3545, Batch Loss:     0.574643, Batch Acc: 0.866616, Tokens per Sec:     3619, Lr: 0.000300\r\n",
      "2024-08-27 14:55:17,306 - INFO - joeynmt.training - Epoch  38, Step:     3550, Batch Loss:     0.623408, Batch Acc: 0.860408, Tokens per Sec:     3939, Lr: 0.000300\r\n",
      "2024-08-27 14:55:18,423 - INFO - joeynmt.training - Epoch  38, Step:     3555, Batch Loss:     0.544153, Batch Acc: 0.856302, Tokens per Sec:     3802, Lr: 0.000300\r\n",
      "2024-08-27 14:55:19,517 - INFO - joeynmt.training - Epoch  38, Step:     3560, Batch Loss:     0.629673, Batch Acc: 0.868421, Tokens per Sec:     3825, Lr: 0.000300\r\n",
      "2024-08-27 14:55:20,627 - INFO - joeynmt.training - Epoch  38, Step:     3565, Batch Loss:     0.608165, Batch Acc: 0.857956, Tokens per Sec:     3641, Lr: 0.000300\r\n",
      "2024-08-27 14:55:21,747 - INFO - joeynmt.training - Epoch  38, Step:     3570, Batch Loss:     0.662785, Batch Acc: 0.852914, Tokens per Sec:     3895, Lr: 0.000299\r\n",
      "2024-08-27 14:55:22,842 - INFO - joeynmt.training - Epoch  38, Step:     3575, Batch Loss:     0.605540, Batch Acc: 0.855746, Tokens per Sec:     3831, Lr: 0.000299\r\n",
      "2024-08-27 14:55:24,125 - INFO - joeynmt.training - Epoch  38, Step:     3580, Batch Loss:     0.640536, Batch Acc: 0.861926, Tokens per Sec:     3192, Lr: 0.000299\r\n",
      "2024-08-27 14:55:25,189 - INFO - joeynmt.training - Epoch  38, Step:     3585, Batch Loss:     0.593227, Batch Acc: 0.853255, Tokens per Sec:     3699, Lr: 0.000299\r\n",
      "2024-08-27 14:55:26,264 - INFO - joeynmt.training - Epoch  38, Step:     3590, Batch Loss:     0.650956, Batch Acc: 0.855633, Tokens per Sec:     3875, Lr: 0.000299\r\n",
      "2024-08-27 14:55:27,400 - INFO - joeynmt.training - Epoch  38, Step:     3595, Batch Loss:     0.680983, Batch Acc: 0.851653, Tokens per Sec:     3780, Lr: 0.000298\r\n",
      "2024-08-27 14:55:28,516 - INFO - joeynmt.training - Epoch  38, Step:     3600, Batch Loss:     0.612492, Batch Acc: 0.843925, Tokens per Sec:     3839, Lr: 0.000298\r\n",
      "2024-08-27 14:55:29,617 - INFO - joeynmt.training - Epoch  38, Step:     3605, Batch Loss:     0.616615, Batch Acc: 0.845388, Tokens per Sec:     3744, Lr: 0.000298\r\n",
      "2024-08-27 14:55:30,703 - INFO - joeynmt.training - Epoch  38, Step:     3610, Batch Loss:     0.595117, Batch Acc: 0.848024, Tokens per Sec:     3942, Lr: 0.000298\r\n",
      "2024-08-27 14:55:31,817 - INFO - joeynmt.training - Epoch  38, Step:     3615, Batch Loss:     0.621870, Batch Acc: 0.853602, Tokens per Sec:     3877, Lr: 0.000298\r\n",
      "2024-08-27 14:55:32,906 - INFO - joeynmt.training - Epoch  38, Step:     3620, Batch Loss:     0.607782, Batch Acc: 0.840217, Tokens per Sec:     3892, Lr: 0.000297\r\n",
      "2024-08-27 14:55:33,278 - INFO - joeynmt.training - Epoch  38, total training loss: 57.36, num. of seqs: 8065, num. of tokens: 80463, 21.2057[sec]\r\n",
      "2024-08-27 14:55:33,279 - INFO - joeynmt.training - EPOCH 39\r\n",
      "2024-08-27 14:55:34,176 - INFO - joeynmt.training - Epoch  39, Step:     3625, Batch Loss:     0.493540, Batch Acc: 0.882803, Tokens per Sec:     3801, Lr: 0.000297\r\n",
      "2024-08-27 14:55:35,332 - INFO - joeynmt.training - Epoch  39, Step:     3630, Batch Loss:     0.534965, Batch Acc: 0.872769, Tokens per Sec:     3735, Lr: 0.000297\r\n",
      "2024-08-27 14:55:36,490 - INFO - joeynmt.training - Epoch  39, Step:     3635, Batch Loss:     0.571890, Batch Acc: 0.870039, Tokens per Sec:     3571, Lr: 0.000297\r\n",
      "2024-08-27 14:55:37,644 - INFO - joeynmt.training - Epoch  39, Step:     3640, Batch Loss:     0.505501, Batch Acc: 0.874678, Tokens per Sec:     3701, Lr: 0.000296\r\n",
      "2024-08-27 14:55:38,770 - INFO - joeynmt.training - Epoch  39, Step:     3645, Batch Loss:     0.592541, Batch Acc: 0.867310, Tokens per Sec:     3682, Lr: 0.000296\r\n",
      "2024-08-27 14:55:39,868 - INFO - joeynmt.training - Epoch  39, Step:     3650, Batch Loss:     0.551534, Batch Acc: 0.862291, Tokens per Sec:     3542, Lr: 0.000296\r\n",
      "2024-08-27 14:55:40,983 - INFO - joeynmt.training - Epoch  39, Step:     3655, Batch Loss:     0.511054, Batch Acc: 0.868384, Tokens per Sec:     3831, Lr: 0.000296\r\n",
      "2024-08-27 14:55:42,146 - INFO - joeynmt.training - Epoch  39, Step:     3660, Batch Loss:     0.573569, Batch Acc: 0.867114, Tokens per Sec:     3847, Lr: 0.000296\r\n",
      "2024-08-27 14:55:43,279 - INFO - joeynmt.training - Epoch  39, Step:     3665, Batch Loss:     0.595618, Batch Acc: 0.859731, Tokens per Sec:     3804, Lr: 0.000295\r\n",
      "2024-08-27 14:55:44,421 - INFO - joeynmt.training - Epoch  39, Step:     3670, Batch Loss:     0.589544, Batch Acc: 0.863824, Tokens per Sec:     3617, Lr: 0.000295\r\n",
      "2024-08-27 14:55:45,569 - INFO - joeynmt.training - Epoch  39, Step:     3675, Batch Loss:     0.573282, Batch Acc: 0.863726, Tokens per Sec:     3969, Lr: 0.000295\r\n",
      "2024-08-27 14:55:46,726 - INFO - joeynmt.training - Epoch  39, Step:     3680, Batch Loss:     0.599073, Batch Acc: 0.867413, Tokens per Sec:     3634, Lr: 0.000295\r\n",
      "2024-08-27 14:55:47,859 - INFO - joeynmt.training - Epoch  39, Step:     3685, Batch Loss:     0.621244, Batch Acc: 0.860861, Tokens per Sec:     3733, Lr: 0.000295\r\n",
      "2024-08-27 14:55:48,981 - INFO - joeynmt.training - Epoch  39, Step:     3690, Batch Loss:     0.571942, Batch Acc: 0.863793, Tokens per Sec:     3889, Lr: 0.000294\r\n",
      "2024-08-27 14:55:50,124 - INFO - joeynmt.training - Epoch  39, Step:     3695, Batch Loss:     0.620563, Batch Acc: 0.858508, Tokens per Sec:     3851, Lr: 0.000294\r\n",
      "2024-08-27 14:55:51,263 - INFO - joeynmt.training - Epoch  39, Step:     3700, Batch Loss:     0.618520, Batch Acc: 0.857809, Tokens per Sec:     3768, Lr: 0.000294\r\n",
      "2024-08-27 14:55:52,365 - INFO - joeynmt.training - Epoch  39, Step:     3705, Batch Loss:     0.600563, Batch Acc: 0.859687, Tokens per Sec:     3773, Lr: 0.000294\r\n",
      "2024-08-27 14:55:53,482 - INFO - joeynmt.training - Epoch  39, Step:     3710, Batch Loss:     0.623905, Batch Acc: 0.853593, Tokens per Sec:     3640, Lr: 0.000294\r\n",
      "2024-08-27 14:55:54,649 - INFO - joeynmt.training - Epoch  39, Step:     3715, Batch Loss:     0.636134, Batch Acc: 0.850122, Tokens per Sec:     3524, Lr: 0.000293\r\n",
      "2024-08-27 14:55:54,935 - INFO - joeynmt.training - Epoch  39, total training loss: 54.31, num. of seqs: 8065, num. of tokens: 80463, 21.6385[sec]\r\n",
      "2024-08-27 14:55:54,936 - INFO - joeynmt.training - EPOCH 40\r\n",
      "2024-08-27 14:55:55,853 - INFO - joeynmt.training - Epoch  40, Step:     3720, Batch Loss:     0.511430, Batch Acc: 0.883060, Tokens per Sec:     4010, Lr: 0.000293\r\n",
      "2024-08-27 14:55:57,044 - INFO - joeynmt.training - Epoch  40, Step:     3725, Batch Loss:     0.538435, Batch Acc: 0.874097, Tokens per Sec:     3603, Lr: 0.000293\r\n",
      "2024-08-27 14:55:58,145 - INFO - joeynmt.training - Epoch  40, Step:     3730, Batch Loss:     0.557339, Batch Acc: 0.883665, Tokens per Sec:     3749, Lr: 0.000293\r\n",
      "2024-08-27 14:55:59,277 - INFO - joeynmt.training - Epoch  40, Step:     3735, Batch Loss:     0.519918, Batch Acc: 0.874067, Tokens per Sec:     3790, Lr: 0.000293\r\n",
      "2024-08-27 14:56:00,381 - INFO - joeynmt.training - Epoch  40, Step:     3740, Batch Loss:     0.521976, Batch Acc: 0.886770, Tokens per Sec:     3753, Lr: 0.000293\r\n",
      "2024-08-27 14:56:01,468 - INFO - joeynmt.training - Epoch  40, Step:     3745, Batch Loss:     0.522834, Batch Acc: 0.875000, Tokens per Sec:     3855, Lr: 0.000292\r\n",
      "2024-08-27 14:56:02,567 - INFO - joeynmt.training - Epoch  40, Step:     3750, Batch Loss:     0.539993, Batch Acc: 0.878708, Tokens per Sec:     3806, Lr: 0.000292\r\n",
      "2024-08-27 14:56:03,655 - INFO - joeynmt.training - Epoch  40, Step:     3755, Batch Loss:     0.623848, Batch Acc: 0.863246, Tokens per Sec:     3851, Lr: 0.000292\r\n",
      "2024-08-27 14:56:04,738 - INFO - joeynmt.training - Epoch  40, Step:     3760, Batch Loss:     0.553553, Batch Acc: 0.878818, Tokens per Sec:     3754, Lr: 0.000292\r\n",
      "2024-08-27 14:56:05,870 - INFO - joeynmt.training - Epoch  40, Step:     3765, Batch Loss:     0.589440, Batch Acc: 0.865791, Tokens per Sec:     3765, Lr: 0.000292\r\n",
      "2024-08-27 14:56:06,995 - INFO - joeynmt.training - Epoch  40, Step:     3770, Batch Loss:     0.569174, Batch Acc: 0.864642, Tokens per Sec:     3660, Lr: 0.000291\r\n",
      "2024-08-27 14:56:08,084 - INFO - joeynmt.training - Epoch  40, Step:     3775, Batch Loss:     0.582405, Batch Acc: 0.868083, Tokens per Sec:     3719, Lr: 0.000291\r\n",
      "2024-08-27 14:56:09,176 - INFO - joeynmt.training - Epoch  40, Step:     3780, Batch Loss:     0.572292, Batch Acc: 0.861886, Tokens per Sec:     3808, Lr: 0.000291\r\n",
      "2024-08-27 14:56:10,268 - INFO - joeynmt.training - Epoch  40, Step:     3785, Batch Loss:     0.555601, Batch Acc: 0.866761, Tokens per Sec:     3864, Lr: 0.000291\r\n",
      "2024-08-27 14:56:11,357 - INFO - joeynmt.training - Epoch  40, Step:     3790, Batch Loss:     0.560661, Batch Acc: 0.873133, Tokens per Sec:     3754, Lr: 0.000291\r\n",
      "2024-08-27 14:56:12,449 - INFO - joeynmt.training - Epoch  40, Step:     3795, Batch Loss:     0.578493, Batch Acc: 0.858026, Tokens per Sec:     3854, Lr: 0.000290\r\n",
      "2024-08-27 14:56:13,544 - INFO - joeynmt.training - Epoch  40, Step:     3800, Batch Loss:     0.600062, Batch Acc: 0.866969, Tokens per Sec:     4026, Lr: 0.000290\r\n",
      "2024-08-27 14:56:14,643 - INFO - joeynmt.training - Epoch  40, Step:     3805, Batch Loss:     0.644013, Batch Acc: 0.859331, Tokens per Sec:     3921, Lr: 0.000290\r\n",
      "2024-08-27 14:56:15,733 - INFO - joeynmt.training - Epoch  40, Step:     3810, Batch Loss:     0.584649, Batch Acc: 0.862956, Tokens per Sec:     4109, Lr: 0.000290\r\n",
      "2024-08-27 14:56:16,007 - INFO - joeynmt.training - Epoch  40, total training loss: 52.24, num. of seqs: 8065, num. of tokens: 80463, 21.0543[sec]\r\n",
      "2024-08-27 14:56:16,007 - INFO - joeynmt.training - EPOCH 41\r\n",
      "2024-08-27 14:56:16,913 - INFO - joeynmt.training - Epoch  41, Step:     3815, Batch Loss:     0.473673, Batch Acc: 0.887045, Tokens per Sec:     3810, Lr: 0.000290\r\n",
      "2024-08-27 14:56:18,004 - INFO - joeynmt.training - Epoch  41, Step:     3820, Batch Loss:     0.542269, Batch Acc: 0.873494, Tokens per Sec:     3806, Lr: 0.000289\r\n",
      "2024-08-27 14:56:19,078 - INFO - joeynmt.training - Epoch  41, Step:     3825, Batch Loss:     0.463879, Batch Acc: 0.882532, Tokens per Sec:     3988, Lr: 0.000289\r\n",
      "2024-08-27 14:56:20,166 - INFO - joeynmt.training - Epoch  41, Step:     3830, Batch Loss:     0.543286, Batch Acc: 0.888051, Tokens per Sec:     3901, Lr: 0.000289\r\n",
      "2024-08-27 14:56:21,279 - INFO - joeynmt.training - Epoch  41, Step:     3835, Batch Loss:     0.510644, Batch Acc: 0.868929, Tokens per Sec:     4054, Lr: 0.000289\r\n",
      "2024-08-27 14:56:22,378 - INFO - joeynmt.training - Epoch  41, Step:     3840, Batch Loss:     0.456239, Batch Acc: 0.888757, Tokens per Sec:     3840, Lr: 0.000289\r\n",
      "2024-08-27 14:56:23,497 - INFO - joeynmt.training - Epoch  41, Step:     3845, Batch Loss:     0.535901, Batch Acc: 0.877287, Tokens per Sec:     3662, Lr: 0.000288\r\n",
      "2024-08-27 14:56:24,605 - INFO - joeynmt.training - Epoch  41, Step:     3850, Batch Loss:     0.507330, Batch Acc: 0.876725, Tokens per Sec:     3996, Lr: 0.000288\r\n",
      "2024-08-27 14:56:25,753 - INFO - joeynmt.training - Epoch  41, Step:     3855, Batch Loss:     0.510280, Batch Acc: 0.875410, Tokens per Sec:     3456, Lr: 0.000288\r\n",
      "2024-08-27 14:56:26,933 - INFO - joeynmt.training - Epoch  41, Step:     3860, Batch Loss:     0.531106, Batch Acc: 0.875916, Tokens per Sec:     3702, Lr: 0.000288\r\n",
      "2024-08-27 14:56:28,032 - INFO - joeynmt.training - Epoch  41, Step:     3865, Batch Loss:     0.498166, Batch Acc: 0.871639, Tokens per Sec:     3896, Lr: 0.000288\r\n",
      "2024-08-27 14:56:29,132 - INFO - joeynmt.training - Epoch  41, Step:     3870, Batch Loss:     0.565014, Batch Acc: 0.864847, Tokens per Sec:     4063, Lr: 0.000288\r\n",
      "2024-08-27 14:56:30,214 - INFO - joeynmt.training - Epoch  41, Step:     3875, Batch Loss:     0.496691, Batch Acc: 0.870123, Tokens per Sec:     3747, Lr: 0.000287\r\n",
      "2024-08-27 14:56:31,294 - INFO - joeynmt.training - Epoch  41, Step:     3880, Batch Loss:     0.499186, Batch Acc: 0.873492, Tokens per Sec:     3763, Lr: 0.000287\r\n",
      "2024-08-27 14:56:32,374 - INFO - joeynmt.training - Epoch  41, Step:     3885, Batch Loss:     0.527607, Batch Acc: 0.871174, Tokens per Sec:     3906, Lr: 0.000287\r\n",
      "2024-08-27 14:56:33,477 - INFO - joeynmt.training - Epoch  41, Step:     3890, Batch Loss:     0.525094, Batch Acc: 0.874409, Tokens per Sec:     4032, Lr: 0.000287\r\n",
      "2024-08-27 14:56:34,591 - INFO - joeynmt.training - Epoch  41, Step:     3895, Batch Loss:     0.578100, Batch Acc: 0.854300, Tokens per Sec:     3791, Lr: 0.000287\r\n",
      "2024-08-27 14:56:35,699 - INFO - joeynmt.training - Epoch  41, Step:     3900, Batch Loss:     0.615034, Batch Acc: 0.867969, Tokens per Sec:     3789, Lr: 0.000286\r\n",
      "2024-08-27 14:56:36,829 - INFO - joeynmt.training - Epoch  41, Step:     3905, Batch Loss:     0.502367, Batch Acc: 0.871854, Tokens per Sec:     3835, Lr: 0.000286\r\n",
      "2024-08-27 14:56:36,977 - INFO - joeynmt.training - Epoch  41, total training loss: 49.53, num. of seqs: 8065, num. of tokens: 80463, 20.9530[sec]\r\n",
      "2024-08-27 14:56:36,977 - INFO - joeynmt.training - EPOCH 42\r\n",
      "2024-08-27 14:56:38,124 - INFO - joeynmt.training - Epoch  42, Step:     3910, Batch Loss:     0.546665, Batch Acc: 0.883480, Tokens per Sec:     3972, Lr: 0.000286\r\n",
      "2024-08-27 14:56:39,252 - INFO - joeynmt.training - Epoch  42, Step:     3915, Batch Loss:     0.497127, Batch Acc: 0.891609, Tokens per Sec:     3733, Lr: 0.000286\r\n",
      "2024-08-27 14:56:40,362 - INFO - joeynmt.training - Epoch  42, Step:     3920, Batch Loss:     0.455880, Batch Acc: 0.893258, Tokens per Sec:     4012, Lr: 0.000286\r\n",
      "2024-08-27 14:56:41,459 - INFO - joeynmt.training - Epoch  42, Step:     3925, Batch Loss:     0.488665, Batch Acc: 0.885551, Tokens per Sec:     3823, Lr: 0.000286\r\n",
      "2024-08-27 14:56:42,575 - INFO - joeynmt.training - Epoch  42, Step:     3930, Batch Loss:     0.472087, Batch Acc: 0.885721, Tokens per Sec:     3964, Lr: 0.000285\r\n",
      "2024-08-27 14:56:43,689 - INFO - joeynmt.training - Epoch  42, Step:     3935, Batch Loss:     0.498862, Batch Acc: 0.888128, Tokens per Sec:     3933, Lr: 0.000285\r\n",
      "2024-08-27 14:56:44,786 - INFO - joeynmt.training - Epoch  42, Step:     3940, Batch Loss:     0.467222, Batch Acc: 0.878760, Tokens per Sec:     3973, Lr: 0.000285\r\n",
      "2024-08-27 14:56:45,875 - INFO - joeynmt.training - Epoch  42, Step:     3945, Batch Loss:     0.477554, Batch Acc: 0.883520, Tokens per Sec:     3937, Lr: 0.000285\r\n",
      "2024-08-27 14:56:47,005 - INFO - joeynmt.training - Epoch  42, Step:     3950, Batch Loss:     0.508753, Batch Acc: 0.875957, Tokens per Sec:     3582, Lr: 0.000285\r\n",
      "2024-08-27 14:56:48,109 - INFO - joeynmt.training - Epoch  42, Step:     3955, Batch Loss:     0.523162, Batch Acc: 0.876100, Tokens per Sec:     3812, Lr: 0.000284\r\n",
      "2024-08-27 14:56:49,213 - INFO - joeynmt.training - Epoch  42, Step:     3960, Batch Loss:     0.482439, Batch Acc: 0.873498, Tokens per Sec:     3697, Lr: 0.000284\r\n",
      "2024-08-27 14:56:50,327 - INFO - joeynmt.training - Epoch  42, Step:     3965, Batch Loss:     0.527195, Batch Acc: 0.880792, Tokens per Sec:     3994, Lr: 0.000284\r\n",
      "2024-08-27 14:56:51,423 - INFO - joeynmt.training - Epoch  42, Step:     3970, Batch Loss:     0.477763, Batch Acc: 0.870882, Tokens per Sec:     3768, Lr: 0.000284\r\n",
      "2024-08-27 14:56:52,511 - INFO - joeynmt.training - Epoch  42, Step:     3975, Batch Loss:     0.517366, Batch Acc: 0.878795, Tokens per Sec:     3817, Lr: 0.000284\r\n",
      "2024-08-27 14:56:53,603 - INFO - joeynmt.training - Epoch  42, Step:     3980, Batch Loss:     0.531145, Batch Acc: 0.870990, Tokens per Sec:     4000, Lr: 0.000284\r\n",
      "2024-08-27 14:56:54,683 - INFO - joeynmt.training - Epoch  42, Step:     3985, Batch Loss:     0.548154, Batch Acc: 0.870257, Tokens per Sec:     3785, Lr: 0.000283\r\n",
      "2024-08-27 14:56:55,774 - INFO - joeynmt.training - Epoch  42, Step:     3990, Batch Loss:     0.576607, Batch Acc: 0.869587, Tokens per Sec:     3664, Lr: 0.000283\r\n",
      "2024-08-27 14:56:56,966 - INFO - joeynmt.training - Epoch  42, Step:     3995, Batch Loss:     0.534134, Batch Acc: 0.873028, Tokens per Sec:     3354, Lr: 0.000283\r\n",
      "2024-08-27 14:56:58,176 - INFO - joeynmt.training - Epoch  42, Step:     4000, Batch Loss:     0.545252, Batch Acc: 0.873250, Tokens per Sec:     3424, Lr: 0.000283\r\n",
      "2024-08-27 14:56:58,177 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=4042\r\n",
      "2024-08-27 14:56:58,177 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:23<00:00, 61.00it/s]\r\n",
      "2024-08-27 14:57:22,115 - INFO - joeynmt.prediction - Generation took 23.9364[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44074.87 examples/s]\r\n",
      "2024-08-27 14:57:22,496 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 14:57:22,496 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   3.83, loss:   6.07, ppl: 431.39, acc:   0.28, 0.1652[sec]\r\n",
      "2024-08-27 14:57:22,497 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\r\n",
      "2024-08-27 14:57:22,821 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/4000.ckpt.\r\n",
      "2024-08-27 14:57:22,822 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/2500.ckpt\r\n",
      "2024-08-27 14:57:22,846 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 14:57:22,996 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 14:57:22,996 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 14:57:22,997 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 14:57:23,294 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 14:57:23,441 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 14:57:23,441 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 14:57:23,441 - INFO - joeynmt.training - \tHypothesis: trois tables blanches\r\n",
      "2024-08-27 14:57:23,736 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 14:57:23,883 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 14:57:23,883 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 14:57:23,883 - INFO - joeynmt.training - \tHypothesis: cependant cependant l’autre\r\n",
      "2024-08-27 14:57:24,175 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 14:57:24,323 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 14:57:24,323 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 14:57:24,323 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 14:57:24,617 - INFO - joeynmt.training - Epoch  42, total training loss: 48.27, num. of seqs: 8065, num. of tokens: 80463, 21.1820[sec]\r\n",
      "2024-08-27 14:57:24,617 - INFO - joeynmt.training - EPOCH 43\r\n",
      "2024-08-27 14:57:25,727 - INFO - joeynmt.training - Epoch  43, Step:     4005, Batch Loss:     0.510162, Batch Acc: 0.889370, Tokens per Sec:     3548, Lr: 0.000283\r\n",
      "2024-08-27 14:57:26,881 - INFO - joeynmt.training - Epoch  43, Step:     4010, Batch Loss:     0.453630, Batch Acc: 0.890684, Tokens per Sec:     3649, Lr: 0.000282\r\n",
      "2024-08-27 14:57:28,044 - INFO - joeynmt.training - Epoch  43, Step:     4015, Batch Loss:     0.525843, Batch Acc: 0.887250, Tokens per Sec:     3791, Lr: 0.000282\r\n",
      "2024-08-27 14:57:29,234 - INFO - joeynmt.training - Epoch  43, Step:     4020, Batch Loss:     0.504005, Batch Acc: 0.886834, Tokens per Sec:     3457, Lr: 0.000282\r\n",
      "2024-08-27 14:57:30,361 - INFO - joeynmt.training - Epoch  43, Step:     4025, Batch Loss:     0.500219, Batch Acc: 0.898117, Tokens per Sec:     3676, Lr: 0.000282\r\n",
      "2024-08-27 14:57:31,498 - INFO - joeynmt.training - Epoch  43, Step:     4030, Batch Loss:     0.481091, Batch Acc: 0.884763, Tokens per Sec:     3889, Lr: 0.000282\r\n",
      "2024-08-27 14:57:32,596 - INFO - joeynmt.training - Epoch  43, Step:     4035, Batch Loss:     0.479769, Batch Acc: 0.887860, Tokens per Sec:     3737, Lr: 0.000282\r\n",
      "2024-08-27 14:57:33,711 - INFO - joeynmt.training - Epoch  43, Step:     4040, Batch Loss:     0.489816, Batch Acc: 0.890073, Tokens per Sec:     3958, Lr: 0.000281\r\n",
      "2024-08-27 14:57:34,825 - INFO - joeynmt.training - Epoch  43, Step:     4045, Batch Loss:     0.461323, Batch Acc: 0.886668, Tokens per Sec:     3821, Lr: 0.000281\r\n",
      "2024-08-27 14:57:35,970 - INFO - joeynmt.training - Epoch  43, Step:     4050, Batch Loss:     0.471292, Batch Acc: 0.887605, Tokens per Sec:     3631, Lr: 0.000281\r\n",
      "2024-08-27 14:57:37,109 - INFO - joeynmt.training - Epoch  43, Step:     4055, Batch Loss:     0.467958, Batch Acc: 0.883907, Tokens per Sec:     3626, Lr: 0.000281\r\n",
      "2024-08-27 14:57:38,209 - INFO - joeynmt.training - Epoch  43, Step:     4060, Batch Loss:     0.480884, Batch Acc: 0.883721, Tokens per Sec:     3872, Lr: 0.000281\r\n",
      "2024-08-27 14:57:39,357 - INFO - joeynmt.training - Epoch  43, Step:     4065, Batch Loss:     0.548849, Batch Acc: 0.870551, Tokens per Sec:     3576, Lr: 0.000281\r\n",
      "2024-08-27 14:57:40,446 - INFO - joeynmt.training - Epoch  43, Step:     4070, Batch Loss:     0.539694, Batch Acc: 0.876156, Tokens per Sec:     3777, Lr: 0.000280\r\n",
      "2024-08-27 14:57:41,546 - INFO - joeynmt.training - Epoch  43, Step:     4075, Batch Loss:     0.529343, Batch Acc: 0.878443, Tokens per Sec:     3832, Lr: 0.000280\r\n",
      "2024-08-27 14:57:42,648 - INFO - joeynmt.training - Epoch  43, Step:     4080, Batch Loss:     0.489120, Batch Acc: 0.875867, Tokens per Sec:     3794, Lr: 0.000280\r\n",
      "2024-08-27 14:57:43,756 - INFO - joeynmt.training - Epoch  43, Step:     4085, Batch Loss:     0.496963, Batch Acc: 0.877356, Tokens per Sec:     3977, Lr: 0.000280\r\n",
      "2024-08-27 14:57:44,831 - INFO - joeynmt.training - Epoch  43, Step:     4090, Batch Loss:     0.528949, Batch Acc: 0.880446, Tokens per Sec:     3762, Lr: 0.000280\r\n",
      "2024-08-27 14:57:45,923 - INFO - joeynmt.training - Epoch  43, Step:     4095, Batch Loss:     0.500716, Batch Acc: 0.869850, Tokens per Sec:     3913, Lr: 0.000280\r\n",
      "2024-08-27 14:57:46,080 - INFO - joeynmt.training - Epoch  43, total training loss: 46.37, num. of seqs: 8065, num. of tokens: 80463, 21.4451[sec]\r\n",
      "2024-08-27 14:57:46,080 - INFO - joeynmt.training - EPOCH 44\r\n",
      "2024-08-27 14:57:47,202 - INFO - joeynmt.training - Epoch  44, Step:     4100, Batch Loss:     0.460257, Batch Acc: 0.901643, Tokens per Sec:     3705, Lr: 0.000279\r\n",
      "2024-08-27 14:57:48,318 - INFO - joeynmt.training - Epoch  44, Step:     4105, Batch Loss:     0.477614, Batch Acc: 0.887673, Tokens per Sec:     4012, Lr: 0.000279\r\n",
      "2024-08-27 14:57:49,411 - INFO - joeynmt.training - Epoch  44, Step:     4110, Batch Loss:     0.446892, Batch Acc: 0.897860, Tokens per Sec:     3766, Lr: 0.000279\r\n",
      "2024-08-27 14:57:50,502 - INFO - joeynmt.training - Epoch  44, Step:     4115, Batch Loss:     0.451925, Batch Acc: 0.900731, Tokens per Sec:     3891, Lr: 0.000279\r\n",
      "2024-08-27 14:57:51,605 - INFO - joeynmt.training - Epoch  44, Step:     4120, Batch Loss:     0.467957, Batch Acc: 0.880968, Tokens per Sec:     4121, Lr: 0.000279\r\n",
      "2024-08-27 14:57:52,703 - INFO - joeynmt.training - Epoch  44, Step:     4125, Batch Loss:     0.451886, Batch Acc: 0.892613, Tokens per Sec:     3726, Lr: 0.000279\r\n",
      "2024-08-27 14:57:53,823 - INFO - joeynmt.training - Epoch  44, Step:     4130, Batch Loss:     0.492964, Batch Acc: 0.886693, Tokens per Sec:     3889, Lr: 0.000278\r\n",
      "2024-08-27 14:57:54,970 - INFO - joeynmt.training - Epoch  44, Step:     4135, Batch Loss:     0.468791, Batch Acc: 0.888393, Tokens per Sec:     3713, Lr: 0.000278\r\n",
      "2024-08-27 14:57:56,089 - INFO - joeynmt.training - Epoch  44, Step:     4140, Batch Loss:     0.462422, Batch Acc: 0.886150, Tokens per Sec:     3810, Lr: 0.000278\r\n",
      "2024-08-27 14:57:57,231 - INFO - joeynmt.training - Epoch  44, Step:     4145, Batch Loss:     0.490121, Batch Acc: 0.888708, Tokens per Sec:     3770, Lr: 0.000278\r\n",
      "2024-08-27 14:57:58,334 - INFO - joeynmt.training - Epoch  44, Step:     4150, Batch Loss:     0.426535, Batch Acc: 0.884484, Tokens per Sec:     3730, Lr: 0.000278\r\n",
      "2024-08-27 14:57:59,484 - INFO - joeynmt.training - Epoch  44, Step:     4155, Batch Loss:     0.414651, Batch Acc: 0.890086, Tokens per Sec:     3636, Lr: 0.000278\r\n",
      "2024-08-27 14:58:00,712 - INFO - joeynmt.training - Epoch  44, Step:     4160, Batch Loss:     0.462139, Batch Acc: 0.889917, Tokens per Sec:     3432, Lr: 0.000277\r\n",
      "2024-08-27 14:58:01,804 - INFO - joeynmt.training - Epoch  44, Step:     4165, Batch Loss:     0.500004, Batch Acc: 0.881448, Tokens per Sec:     3695, Lr: 0.000277\r\n",
      "2024-08-27 14:58:02,904 - INFO - joeynmt.training - Epoch  44, Step:     4170, Batch Loss:     0.504091, Batch Acc: 0.886008, Tokens per Sec:     3720, Lr: 0.000277\r\n",
      "2024-08-27 14:58:04,000 - INFO - joeynmt.training - Epoch  44, Step:     4175, Batch Loss:     0.518374, Batch Acc: 0.879667, Tokens per Sec:     3839, Lr: 0.000277\r\n",
      "2024-08-27 14:58:05,104 - INFO - joeynmt.training - Epoch  44, Step:     4180, Batch Loss:     0.525711, Batch Acc: 0.879634, Tokens per Sec:     3863, Lr: 0.000277\r\n",
      "2024-08-27 14:58:06,191 - INFO - joeynmt.training - Epoch  44, Step:     4185, Batch Loss:     0.457975, Batch Acc: 0.887470, Tokens per Sec:     3893, Lr: 0.000277\r\n",
      "2024-08-27 14:58:07,310 - INFO - joeynmt.training - Epoch  44, Step:     4190, Batch Loss:     0.502218, Batch Acc: 0.874666, Tokens per Sec:     3682, Lr: 0.000276\r\n",
      "2024-08-27 14:58:07,407 - INFO - joeynmt.training - Epoch  44, total training loss: 44.69, num. of seqs: 8065, num. of tokens: 80463, 21.3085[sec]\r\n",
      "2024-08-27 14:58:07,407 - INFO - joeynmt.training - EPOCH 45\r\n",
      "2024-08-27 14:58:08,501 - INFO - joeynmt.training - Epoch  45, Step:     4195, Batch Loss:     0.401593, Batch Acc: 0.904024, Tokens per Sec:     3967, Lr: 0.000276\r\n",
      "2024-08-27 14:58:09,584 - INFO - joeynmt.training - Epoch  45, Step:     4200, Batch Loss:     0.420265, Batch Acc: 0.894101, Tokens per Sec:     3899, Lr: 0.000276\r\n",
      "2024-08-27 14:58:10,704 - INFO - joeynmt.training - Epoch  45, Step:     4205, Batch Loss:     0.425647, Batch Acc: 0.900134, Tokens per Sec:     3999, Lr: 0.000276\r\n",
      "2024-08-27 14:58:11,808 - INFO - joeynmt.training - Epoch  45, Step:     4210, Batch Loss:     0.443989, Batch Acc: 0.895252, Tokens per Sec:     3705, Lr: 0.000276\r\n",
      "2024-08-27 14:58:12,905 - INFO - joeynmt.training - Epoch  45, Step:     4215, Batch Loss:     0.485574, Batch Acc: 0.904151, Tokens per Sec:     3911, Lr: 0.000276\r\n",
      "2024-08-27 14:58:14,000 - INFO - joeynmt.training - Epoch  45, Step:     4220, Batch Loss:     0.452826, Batch Acc: 0.894803, Tokens per Sec:     3657, Lr: 0.000275\r\n",
      "2024-08-27 14:58:15,122 - INFO - joeynmt.training - Epoch  45, Step:     4225, Batch Loss:     0.455721, Batch Acc: 0.891930, Tokens per Sec:     3813, Lr: 0.000275\r\n",
      "2024-08-27 14:58:16,240 - INFO - joeynmt.training - Epoch  45, Step:     4230, Batch Loss:     0.422690, Batch Acc: 0.901947, Tokens per Sec:     3723, Lr: 0.000275\r\n",
      "2024-08-27 14:58:17,374 - INFO - joeynmt.training - Epoch  45, Step:     4235, Batch Loss:     0.452091, Batch Acc: 0.888730, Tokens per Sec:     3704, Lr: 0.000275\r\n",
      "2024-08-27 14:58:18,477 - INFO - joeynmt.training - Epoch  45, Step:     4240, Batch Loss:     0.473180, Batch Acc: 0.888721, Tokens per Sec:     3611, Lr: 0.000275\r\n",
      "2024-08-27 14:58:19,585 - INFO - joeynmt.training - Epoch  45, Step:     4245, Batch Loss:     0.440194, Batch Acc: 0.893401, Tokens per Sec:     3916, Lr: 0.000275\r\n",
      "2024-08-27 14:58:20,666 - INFO - joeynmt.training - Epoch  45, Step:     4250, Batch Loss:     0.421821, Batch Acc: 0.890942, Tokens per Sec:     4055, Lr: 0.000274\r\n",
      "2024-08-27 14:58:21,754 - INFO - joeynmt.training - Epoch  45, Step:     4255, Batch Loss:     0.480434, Batch Acc: 0.884983, Tokens per Sec:     3846, Lr: 0.000274\r\n",
      "2024-08-27 14:58:22,844 - INFO - joeynmt.training - Epoch  45, Step:     4260, Batch Loss:     0.488785, Batch Acc: 0.880652, Tokens per Sec:     4001, Lr: 0.000274\r\n",
      "2024-08-27 14:58:23,920 - INFO - joeynmt.training - Epoch  45, Step:     4265, Batch Loss:     0.445045, Batch Acc: 0.893435, Tokens per Sec:     3796, Lr: 0.000274\r\n",
      "2024-08-27 14:58:25,001 - INFO - joeynmt.training - Epoch  45, Step:     4270, Batch Loss:     0.516600, Batch Acc: 0.882755, Tokens per Sec:     3923, Lr: 0.000274\r\n",
      "2024-08-27 14:58:26,122 - INFO - joeynmt.training - Epoch  45, Step:     4275, Batch Loss:     0.453315, Batch Acc: 0.889808, Tokens per Sec:     3672, Lr: 0.000274\r\n",
      "2024-08-27 14:58:27,233 - INFO - joeynmt.training - Epoch  45, Step:     4280, Batch Loss:     0.486173, Batch Acc: 0.882636, Tokens per Sec:     3744, Lr: 0.000273\r\n",
      "2024-08-27 14:58:28,296 - INFO - joeynmt.training - Epoch  45, Step:     4285, Batch Loss:     0.472764, Batch Acc: 0.880646, Tokens per Sec:     4023, Lr: 0.000273\r\n",
      "2024-08-27 14:58:28,397 - INFO - joeynmt.training - Epoch  45, total training loss: 43.36, num. of seqs: 8065, num. of tokens: 80463, 20.9723[sec]\r\n",
      "2024-08-27 14:58:28,397 - INFO - joeynmt.training - EPOCH 46\r\n",
      "2024-08-27 14:58:29,508 - INFO - joeynmt.training - Epoch  46, Step:     4290, Batch Loss:     0.424518, Batch Acc: 0.904785, Tokens per Sec:     3777, Lr: 0.000273\r\n",
      "2024-08-27 14:58:30,700 - INFO - joeynmt.training - Epoch  46, Step:     4295, Batch Loss:     0.429437, Batch Acc: 0.907757, Tokens per Sec:     3602, Lr: 0.000273\r\n",
      "2024-08-27 14:58:31,863 - INFO - joeynmt.training - Epoch  46, Step:     4300, Batch Loss:     0.368517, Batch Acc: 0.913024, Tokens per Sec:     3782, Lr: 0.000273\r\n",
      "2024-08-27 14:58:32,929 - INFO - joeynmt.training - Epoch  46, Step:     4305, Batch Loss:     0.392682, Batch Acc: 0.903480, Tokens per Sec:     4043, Lr: 0.000273\r\n",
      "2024-08-27 14:58:34,023 - INFO - joeynmt.training - Epoch  46, Step:     4310, Batch Loss:     0.379722, Batch Acc: 0.898365, Tokens per Sec:     3864, Lr: 0.000272\r\n",
      "2024-08-27 14:58:35,113 - INFO - joeynmt.training - Epoch  46, Step:     4315, Batch Loss:     0.435942, Batch Acc: 0.902108, Tokens per Sec:     3788, Lr: 0.000272\r\n",
      "2024-08-27 14:58:36,206 - INFO - joeynmt.training - Epoch  46, Step:     4320, Batch Loss:     0.472197, Batch Acc: 0.886379, Tokens per Sec:     3972, Lr: 0.000272\r\n",
      "2024-08-27 14:58:37,313 - INFO - joeynmt.training - Epoch  46, Step:     4325, Batch Loss:     0.379294, Batch Acc: 0.904586, Tokens per Sec:     3666, Lr: 0.000272\r\n",
      "2024-08-27 14:58:38,396 - INFO - joeynmt.training - Epoch  46, Step:     4330, Batch Loss:     0.407754, Batch Acc: 0.906104, Tokens per Sec:     3769, Lr: 0.000272\r\n",
      "2024-08-27 14:58:39,491 - INFO - joeynmt.training - Epoch  46, Step:     4335, Batch Loss:     0.437702, Batch Acc: 0.896430, Tokens per Sec:     3892, Lr: 0.000272\r\n",
      "2024-08-27 14:58:40,569 - INFO - joeynmt.training - Epoch  46, Step:     4340, Batch Loss:     0.468040, Batch Acc: 0.893540, Tokens per Sec:     3836, Lr: 0.000272\r\n",
      "2024-08-27 14:58:41,668 - INFO - joeynmt.training - Epoch  46, Step:     4345, Batch Loss:     0.437940, Batch Acc: 0.895145, Tokens per Sec:     3993, Lr: 0.000271\r\n",
      "2024-08-27 14:58:42,765 - INFO - joeynmt.training - Epoch  46, Step:     4350, Batch Loss:     0.470368, Batch Acc: 0.895973, Tokens per Sec:     3807, Lr: 0.000271\r\n",
      "2024-08-27 14:58:43,864 - INFO - joeynmt.training - Epoch  46, Step:     4355, Batch Loss:     0.420051, Batch Acc: 0.897066, Tokens per Sec:     3786, Lr: 0.000271\r\n",
      "2024-08-27 14:58:44,968 - INFO - joeynmt.training - Epoch  46, Step:     4360, Batch Loss:     0.473563, Batch Acc: 0.887414, Tokens per Sec:     3822, Lr: 0.000271\r\n",
      "2024-08-27 14:58:46,058 - INFO - joeynmt.training - Epoch  46, Step:     4365, Batch Loss:     0.472820, Batch Acc: 0.896869, Tokens per Sec:     3696, Lr: 0.000271\r\n",
      "2024-08-27 14:58:47,186 - INFO - joeynmt.training - Epoch  46, Step:     4370, Batch Loss:     0.493355, Batch Acc: 0.891991, Tokens per Sec:     3875, Lr: 0.000271\r\n",
      "2024-08-27 14:58:48,301 - INFO - joeynmt.training - Epoch  46, Step:     4375, Batch Loss:     0.459859, Batch Acc: 0.894569, Tokens per Sec:     3934, Lr: 0.000270\r\n",
      "2024-08-27 14:58:49,390 - INFO - joeynmt.training - Epoch  46, Step:     4380, Batch Loss:     0.550470, Batch Acc: 0.884422, Tokens per Sec:     3841, Lr: 0.000270\r\n",
      "2024-08-27 14:58:49,441 - INFO - joeynmt.training - Epoch  46, total training loss: 41.22, num. of seqs: 8065, num. of tokens: 80463, 21.0267[sec]\r\n",
      "2024-08-27 14:58:49,442 - INFO - joeynmt.training - EPOCH 47\r\n",
      "2024-08-27 14:58:50,536 - INFO - joeynmt.training - Epoch  47, Step:     4385, Batch Loss:     0.364726, Batch Acc: 0.905293, Tokens per Sec:     3865, Lr: 0.000270\r\n",
      "2024-08-27 14:58:51,624 - INFO - joeynmt.training - Epoch  47, Step:     4390, Batch Loss:     0.400884, Batch Acc: 0.908403, Tokens per Sec:     3764, Lr: 0.000270\r\n",
      "2024-08-27 14:58:52,720 - INFO - joeynmt.training - Epoch  47, Step:     4395, Batch Loss:     0.374019, Batch Acc: 0.910299, Tokens per Sec:     3849, Lr: 0.000270\r\n",
      "2024-08-27 14:58:53,875 - INFO - joeynmt.training - Epoch  47, Step:     4400, Batch Loss:     0.438066, Batch Acc: 0.899782, Tokens per Sec:     3976, Lr: 0.000270\r\n",
      "2024-08-27 14:58:54,988 - INFO - joeynmt.training - Epoch  47, Step:     4405, Batch Loss:     0.405637, Batch Acc: 0.902296, Tokens per Sec:     3835, Lr: 0.000270\r\n",
      "2024-08-27 14:58:56,125 - INFO - joeynmt.training - Epoch  47, Step:     4410, Batch Loss:     0.377618, Batch Acc: 0.896907, Tokens per Sec:     3673, Lr: 0.000269\r\n",
      "2024-08-27 14:58:57,284 - INFO - joeynmt.training - Epoch  47, Step:     4415, Batch Loss:     0.396379, Batch Acc: 0.907579, Tokens per Sec:     3530, Lr: 0.000269\r\n",
      "2024-08-27 14:58:58,412 - INFO - joeynmt.training - Epoch  47, Step:     4420, Batch Loss:     0.428805, Batch Acc: 0.892284, Tokens per Sec:     4036, Lr: 0.000269\r\n",
      "2024-08-27 14:58:59,540 - INFO - joeynmt.training - Epoch  47, Step:     4425, Batch Loss:     0.442127, Batch Acc: 0.902093, Tokens per Sec:     3689, Lr: 0.000269\r\n",
      "2024-08-27 14:59:00,666 - INFO - joeynmt.training - Epoch  47, Step:     4430, Batch Loss:     0.421870, Batch Acc: 0.903566, Tokens per Sec:     3786, Lr: 0.000269\r\n",
      "2024-08-27 14:59:01,820 - INFO - joeynmt.training - Epoch  47, Step:     4435, Batch Loss:     0.367814, Batch Acc: 0.912506, Tokens per Sec:     3508, Lr: 0.000269\r\n",
      "2024-08-27 14:59:03,006 - INFO - joeynmt.training - Epoch  47, Step:     4440, Batch Loss:     0.423133, Batch Acc: 0.897332, Tokens per Sec:     3543, Lr: 0.000268\r\n",
      "2024-08-27 14:59:04,164 - INFO - joeynmt.training - Epoch  47, Step:     4445, Batch Loss:     0.417746, Batch Acc: 0.898711, Tokens per Sec:     3752, Lr: 0.000268\r\n",
      "2024-08-27 14:59:05,306 - INFO - joeynmt.training - Epoch  47, Step:     4450, Batch Loss:     0.431512, Batch Acc: 0.900800, Tokens per Sec:     3614, Lr: 0.000268\r\n",
      "2024-08-27 14:59:06,425 - INFO - joeynmt.training - Epoch  47, Step:     4455, Batch Loss:     0.392781, Batch Acc: 0.903288, Tokens per Sec:     3698, Lr: 0.000268\r\n",
      "2024-08-27 14:59:07,579 - INFO - joeynmt.training - Epoch  47, Step:     4460, Batch Loss:     0.452799, Batch Acc: 0.890137, Tokens per Sec:     3552, Lr: 0.000268\r\n",
      "2024-08-27 14:59:08,705 - INFO - joeynmt.training - Epoch  47, Step:     4465, Batch Loss:     0.442935, Batch Acc: 0.895273, Tokens per Sec:     3666, Lr: 0.000268\r\n",
      "2024-08-27 14:59:09,837 - INFO - joeynmt.training - Epoch  47, Step:     4470, Batch Loss:     0.433590, Batch Acc: 0.886465, Tokens per Sec:     3768, Lr: 0.000268\r\n",
      "2024-08-27 14:59:10,956 - INFO - joeynmt.training - Epoch  47, Step:     4475, Batch Loss:     0.456451, Batch Acc: 0.891099, Tokens per Sec:     3596, Lr: 0.000267\r\n",
      "2024-08-27 14:59:11,119 - INFO - joeynmt.training - Epoch  47, total training loss: 39.95, num. of seqs: 8065, num. of tokens: 80463, 21.6601[sec]\r\n",
      "2024-08-27 14:59:11,119 - INFO - joeynmt.training - EPOCH 48\r\n",
      "2024-08-27 14:59:12,230 - INFO - joeynmt.training - Epoch  48, Step:     4480, Batch Loss:     0.418927, Batch Acc: 0.913065, Tokens per Sec:     3721, Lr: 0.000267\r\n",
      "2024-08-27 14:59:13,352 - INFO - joeynmt.training - Epoch  48, Step:     4485, Batch Loss:     0.396734, Batch Acc: 0.909503, Tokens per Sec:     3735, Lr: 0.000267\r\n",
      "2024-08-27 14:59:14,474 - INFO - joeynmt.training - Epoch  48, Step:     4490, Batch Loss:     0.429330, Batch Acc: 0.906750, Tokens per Sec:     3847, Lr: 0.000267\r\n",
      "2024-08-27 14:59:15,575 - INFO - joeynmt.training - Epoch  48, Step:     4495, Batch Loss:     0.430652, Batch Acc: 0.907751, Tokens per Sec:     3821, Lr: 0.000267\r\n",
      "2024-08-27 14:59:16,714 - INFO - joeynmt.training - Epoch  48, Step:     4500, Batch Loss:     0.397884, Batch Acc: 0.908008, Tokens per Sec:     3762, Lr: 0.000267\r\n",
      "2024-08-27 14:59:16,715 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=4542\r\n",
      "2024-08-27 14:59:16,715 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 63.55it/s]\r\n",
      "2024-08-27 14:59:39,691 - INFO - joeynmt.prediction - Generation took 22.9732[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 45322.68 examples/s]\r\n",
      "2024-08-27 14:59:40,067 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 14:59:40,067 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   3.78, loss:   6.25, ppl: 518.65, acc:   0.28, 0.1621[sec]\r\n",
      "2024-08-27 14:59:40,385 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/4500.ckpt.\r\n",
      "2024-08-27 14:59:40,386 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/3000.ckpt\r\n",
      "2024-08-27 14:59:40,410 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 14:59:40,559 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 14:59:40,560 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 14:59:40,560 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 14:59:40,853 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 14:59:40,999 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 14:59:40,999 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 14:59:40,999 - INFO - joeynmt.training - \tHypothesis: trois montée de l'enclos\r\n",
      "2024-08-27 14:59:41,300 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 14:59:41,448 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 14:59:41,448 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 14:59:41,448 - INFO - joeynmt.training - \tHypothesis: cependant à cet amendement l’autre\r\n",
      "2024-08-27 14:59:41,743 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 14:59:41,890 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 14:59:41,890 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 14:59:41,890 - INFO - joeynmt.training - \tHypothesis: une minute cinquante\r\n",
      "2024-08-27 14:59:43,309 - INFO - joeynmt.training - Epoch  48, Step:     4505, Batch Loss:     0.393559, Batch Acc: 0.906581, Tokens per Sec:     3608, Lr: 0.000267\r\n",
      "2024-08-27 14:59:44,448 - INFO - joeynmt.training - Epoch  48, Step:     4510, Batch Loss:     0.392589, Batch Acc: 0.906887, Tokens per Sec:     3877, Lr: 0.000266\r\n",
      "2024-08-27 14:59:45,546 - INFO - joeynmt.training - Epoch  48, Step:     4515, Batch Loss:     0.408646, Batch Acc: 0.904820, Tokens per Sec:     3725, Lr: 0.000266\r\n",
      "2024-08-27 14:59:46,647 - INFO - joeynmt.training - Epoch  48, Step:     4520, Batch Loss:     0.426561, Batch Acc: 0.905021, Tokens per Sec:     3839, Lr: 0.000266\r\n",
      "2024-08-27 14:59:47,776 - INFO - joeynmt.training - Epoch  48, Step:     4525, Batch Loss:     0.409171, Batch Acc: 0.901223, Tokens per Sec:     3768, Lr: 0.000266\r\n",
      "2024-08-27 14:59:48,890 - INFO - joeynmt.training - Epoch  48, Step:     4530, Batch Loss:     0.395723, Batch Acc: 0.902008, Tokens per Sec:     3711, Lr: 0.000266\r\n",
      "2024-08-27 14:59:50,010 - INFO - joeynmt.training - Epoch  48, Step:     4535, Batch Loss:     0.461017, Batch Acc: 0.895189, Tokens per Sec:     3847, Lr: 0.000266\r\n",
      "2024-08-27 14:59:51,107 - INFO - joeynmt.training - Epoch  48, Step:     4540, Batch Loss:     0.396444, Batch Acc: 0.901223, Tokens per Sec:     3951, Lr: 0.000265\r\n",
      "2024-08-27 14:59:52,234 - INFO - joeynmt.training - Epoch  48, Step:     4545, Batch Loss:     0.445986, Batch Acc: 0.896018, Tokens per Sec:     3612, Lr: 0.000265\r\n",
      "2024-08-27 14:59:53,353 - INFO - joeynmt.training - Epoch  48, Step:     4550, Batch Loss:     0.379723, Batch Acc: 0.903796, Tokens per Sec:     3746, Lr: 0.000265\r\n",
      "2024-08-27 14:59:54,475 - INFO - joeynmt.training - Epoch  48, Step:     4555, Batch Loss:     0.422639, Batch Acc: 0.891790, Tokens per Sec:     3791, Lr: 0.000265\r\n",
      "2024-08-27 14:59:55,578 - INFO - joeynmt.training - Epoch  48, Step:     4560, Batch Loss:     0.451168, Batch Acc: 0.896215, Tokens per Sec:     3717, Lr: 0.000265\r\n",
      "2024-08-27 14:59:56,723 - INFO - joeynmt.training - Epoch  48, Step:     4565, Batch Loss:     0.459170, Batch Acc: 0.899279, Tokens per Sec:     3756, Lr: 0.000265\r\n",
      "2024-08-27 14:59:57,864 - INFO - joeynmt.training - Epoch  48, Step:     4570, Batch Loss:     0.423937, Batch Acc: 0.893566, Tokens per Sec:     3667, Lr: 0.000265\r\n",
      "2024-08-27 14:59:58,019 - INFO - joeynmt.training - Epoch  48, total training loss: 38.83, num. of seqs: 8065, num. of tokens: 80463, 21.4119[sec]\r\n",
      "2024-08-27 14:59:58,019 - INFO - joeynmt.training - EPOCH 49\r\n",
      "2024-08-27 14:59:59,150 - INFO - joeynmt.training - Epoch  49, Step:     4575, Batch Loss:     0.327435, Batch Acc: 0.917627, Tokens per Sec:     3696, Lr: 0.000264\r\n",
      "2024-08-27 15:00:00,278 - INFO - joeynmt.training - Epoch  49, Step:     4580, Batch Loss:     0.365352, Batch Acc: 0.916838, Tokens per Sec:     3872, Lr: 0.000264\r\n",
      "2024-08-27 15:00:01,408 - INFO - joeynmt.training - Epoch  49, Step:     4585, Batch Loss:     0.362225, Batch Acc: 0.915751, Tokens per Sec:     3869, Lr: 0.000264\r\n",
      "2024-08-27 15:00:02,532 - INFO - joeynmt.training - Epoch  49, Step:     4590, Batch Loss:     0.369045, Batch Acc: 0.910096, Tokens per Sec:     3703, Lr: 0.000264\r\n",
      "2024-08-27 15:00:03,650 - INFO - joeynmt.training - Epoch  49, Step:     4595, Batch Loss:     0.401698, Batch Acc: 0.909050, Tokens per Sec:     3997, Lr: 0.000264\r\n",
      "2024-08-27 15:00:04,875 - INFO - joeynmt.training - Epoch  49, Step:     4600, Batch Loss:     0.376576, Batch Acc: 0.917044, Tokens per Sec:     3426, Lr: 0.000264\r\n",
      "2024-08-27 15:00:05,989 - INFO - joeynmt.training - Epoch  49, Step:     4605, Batch Loss:     0.406548, Batch Acc: 0.906763, Tokens per Sec:     3611, Lr: 0.000264\r\n",
      "2024-08-27 15:00:07,122 - INFO - joeynmt.training - Epoch  49, Step:     4610, Batch Loss:     0.452414, Batch Acc: 0.904059, Tokens per Sec:     3829, Lr: 0.000263\r\n",
      "2024-08-27 15:00:08,224 - INFO - joeynmt.training - Epoch  49, Step:     4615, Batch Loss:     0.410084, Batch Acc: 0.907501, Tokens per Sec:     3791, Lr: 0.000263\r\n",
      "2024-08-27 15:00:09,336 - INFO - joeynmt.training - Epoch  49, Step:     4620, Batch Loss:     0.431806, Batch Acc: 0.901647, Tokens per Sec:     3934, Lr: 0.000263\r\n",
      "2024-08-27 15:00:10,444 - INFO - joeynmt.training - Epoch  49, Step:     4625, Batch Loss:     0.405037, Batch Acc: 0.902866, Tokens per Sec:     3969, Lr: 0.000263\r\n",
      "2024-08-27 15:00:11,532 - INFO - joeynmt.training - Epoch  49, Step:     4630, Batch Loss:     0.371270, Batch Acc: 0.905576, Tokens per Sec:     3711, Lr: 0.000263\r\n",
      "2024-08-27 15:00:12,618 - INFO - joeynmt.training - Epoch  49, Step:     4635, Batch Loss:     0.401582, Batch Acc: 0.906221, Tokens per Sec:     3909, Lr: 0.000263\r\n",
      "2024-08-27 15:00:13,734 - INFO - joeynmt.training - Epoch  49, Step:     4640, Batch Loss:     0.395246, Batch Acc: 0.902283, Tokens per Sec:     3930, Lr: 0.000263\r\n",
      "2024-08-27 15:00:14,871 - INFO - joeynmt.training - Epoch  49, Step:     4645, Batch Loss:     0.359469, Batch Acc: 0.901568, Tokens per Sec:     3818, Lr: 0.000262\r\n",
      "2024-08-27 15:00:15,988 - INFO - joeynmt.training - Epoch  49, Step:     4650, Batch Loss:     0.412962, Batch Acc: 0.907827, Tokens per Sec:     3674, Lr: 0.000262\r\n",
      "2024-08-27 15:00:17,144 - INFO - joeynmt.training - Epoch  49, Step:     4655, Batch Loss:     0.372956, Batch Acc: 0.902439, Tokens per Sec:     3690, Lr: 0.000262\r\n",
      "2024-08-27 15:00:18,213 - INFO - joeynmt.training - Epoch  49, Step:     4660, Batch Loss:     0.404144, Batch Acc: 0.903746, Tokens per Sec:     3773, Lr: 0.000262\r\n",
      "2024-08-27 15:00:19,232 - INFO - joeynmt.training - Epoch  49, total training loss: 37.09, num. of seqs: 8065, num. of tokens: 80463, 21.1952[sec]\r\n",
      "2024-08-27 15:00:19,232 - INFO - joeynmt.training - EPOCH 50\r\n",
      "2024-08-27 15:00:19,455 - INFO - joeynmt.training - Epoch  50, Step:     4665, Batch Loss:     0.355466, Batch Acc: 0.918546, Tokens per Sec:     3644, Lr: 0.000262\r\n",
      "2024-08-27 15:00:20,539 - INFO - joeynmt.training - Epoch  50, Step:     4670, Batch Loss:     0.346525, Batch Acc: 0.915226, Tokens per Sec:     3897, Lr: 0.000262\r\n",
      "2024-08-27 15:00:21,626 - INFO - joeynmt.training - Epoch  50, Step:     4675, Batch Loss:     0.355040, Batch Acc: 0.913311, Tokens per Sec:     4047, Lr: 0.000262\r\n",
      "2024-08-27 15:00:22,708 - INFO - joeynmt.training - Epoch  50, Step:     4680, Batch Loss:     0.394499, Batch Acc: 0.916727, Tokens per Sec:     3812, Lr: 0.000261\r\n",
      "2024-08-27 15:00:23,789 - INFO - joeynmt.training - Epoch  50, Step:     4685, Batch Loss:     0.373200, Batch Acc: 0.912865, Tokens per Sec:     3834, Lr: 0.000261\r\n",
      "2024-08-27 15:00:24,876 - INFO - joeynmt.training - Epoch  50, Step:     4690, Batch Loss:     0.331736, Batch Acc: 0.915780, Tokens per Sec:     3977, Lr: 0.000261\r\n",
      "2024-08-27 15:00:25,973 - INFO - joeynmt.training - Epoch  50, Step:     4695, Batch Loss:     0.380709, Batch Acc: 0.909507, Tokens per Sec:     3792, Lr: 0.000261\r\n",
      "2024-08-27 15:00:27,104 - INFO - joeynmt.training - Epoch  50, Step:     4700, Batch Loss:     0.344245, Batch Acc: 0.908751, Tokens per Sec:     3792, Lr: 0.000261\r\n",
      "2024-08-27 15:00:28,211 - INFO - joeynmt.training - Epoch  50, Step:     4705, Batch Loss:     0.348141, Batch Acc: 0.912365, Tokens per Sec:     3937, Lr: 0.000261\r\n",
      "2024-08-27 15:00:29,288 - INFO - joeynmt.training - Epoch  50, Step:     4710, Batch Loss:     0.364515, Batch Acc: 0.911061, Tokens per Sec:     3729, Lr: 0.000261\r\n",
      "2024-08-27 15:00:30,424 - INFO - joeynmt.training - Epoch  50, Step:     4715, Batch Loss:     0.405723, Batch Acc: 0.906190, Tokens per Sec:     3701, Lr: 0.000261\r\n",
      "2024-08-27 15:00:31,545 - INFO - joeynmt.training - Epoch  50, Step:     4720, Batch Loss:     0.408003, Batch Acc: 0.907302, Tokens per Sec:     3767, Lr: 0.000260\r\n",
      "2024-08-27 15:00:32,664 - INFO - joeynmt.training - Epoch  50, Step:     4725, Batch Loss:     0.354493, Batch Acc: 0.913137, Tokens per Sec:     3757, Lr: 0.000260\r\n",
      "2024-08-27 15:00:33,775 - INFO - joeynmt.training - Epoch  50, Step:     4730, Batch Loss:     0.369945, Batch Acc: 0.909447, Tokens per Sec:     3909, Lr: 0.000260\r\n",
      "2024-08-27 15:00:34,881 - INFO - joeynmt.training - Epoch  50, Step:     4735, Batch Loss:     0.412044, Batch Acc: 0.898594, Tokens per Sec:     3924, Lr: 0.000260\r\n",
      "2024-08-27 15:00:36,178 - INFO - joeynmt.training - Epoch  50, Step:     4740, Batch Loss:     0.444314, Batch Acc: 0.903958, Tokens per Sec:     3197, Lr: 0.000260\r\n",
      "2024-08-27 15:00:37,303 - INFO - joeynmt.training - Epoch  50, Step:     4745, Batch Loss:     0.402466, Batch Acc: 0.907817, Tokens per Sec:     3744, Lr: 0.000260\r\n",
      "2024-08-27 15:00:38,402 - INFO - joeynmt.training - Epoch  50, Step:     4750, Batch Loss:     0.428145, Batch Acc: 0.908776, Tokens per Sec:     3945, Lr: 0.000260\r\n",
      "2024-08-27 15:00:39,513 - INFO - joeynmt.training - Epoch  50, Step:     4755, Batch Loss:     0.439703, Batch Acc: 0.900415, Tokens per Sec:     3907, Lr: 0.000259\r\n",
      "2024-08-27 15:00:40,426 - INFO - joeynmt.training - Epoch  50, total training loss: 36.13, num. of seqs: 8065, num. of tokens: 80463, 21.1754[sec]\r\n",
      "2024-08-27 15:00:40,426 - INFO - joeynmt.training - EPOCH 51\r\n",
      "2024-08-27 15:00:40,656 - INFO - joeynmt.training - Epoch  51, Step:     4760, Batch Loss:     0.355958, Batch Acc: 0.911483, Tokens per Sec:     3708, Lr: 0.000259\r\n",
      "2024-08-27 15:00:41,774 - INFO - joeynmt.training - Epoch  51, Step:     4765, Batch Loss:     0.336833, Batch Acc: 0.924235, Tokens per Sec:     3803, Lr: 0.000259\r\n",
      "2024-08-27 15:00:42,899 - INFO - joeynmt.training - Epoch  51, Step:     4770, Batch Loss:     0.368998, Batch Acc: 0.918077, Tokens per Sec:     3942, Lr: 0.000259\r\n",
      "2024-08-27 15:00:44,004 - INFO - joeynmt.training - Epoch  51, Step:     4775, Batch Loss:     0.323924, Batch Acc: 0.923971, Tokens per Sec:     3740, Lr: 0.000259\r\n",
      "2024-08-27 15:00:45,112 - INFO - joeynmt.training - Epoch  51, Step:     4780, Batch Loss:     0.372261, Batch Acc: 0.915233, Tokens per Sec:     3623, Lr: 0.000259\r\n",
      "2024-08-27 15:00:46,214 - INFO - joeynmt.training - Epoch  51, Step:     4785, Batch Loss:     0.344691, Batch Acc: 0.916082, Tokens per Sec:     3885, Lr: 0.000259\r\n",
      "2024-08-27 15:00:47,373 - INFO - joeynmt.training - Epoch  51, Step:     4790, Batch Loss:     0.393616, Batch Acc: 0.916972, Tokens per Sec:     3774, Lr: 0.000258\r\n",
      "2024-08-27 15:00:48,498 - INFO - joeynmt.training - Epoch  51, Step:     4795, Batch Loss:     0.334975, Batch Acc: 0.917606, Tokens per Sec:     3552, Lr: 0.000258\r\n",
      "2024-08-27 15:00:49,588 - INFO - joeynmt.training - Epoch  51, Step:     4800, Batch Loss:     0.357347, Batch Acc: 0.924262, Tokens per Sec:     3576, Lr: 0.000258\r\n",
      "2024-08-27 15:00:50,693 - INFO - joeynmt.training - Epoch  51, Step:     4805, Batch Loss:     0.359199, Batch Acc: 0.914340, Tokens per Sec:     3827, Lr: 0.000258\r\n",
      "2024-08-27 15:00:51,806 - INFO - joeynmt.training - Epoch  51, Step:     4810, Batch Loss:     0.385475, Batch Acc: 0.916208, Tokens per Sec:     3593, Lr: 0.000258\r\n",
      "2024-08-27 15:00:52,924 - INFO - joeynmt.training - Epoch  51, Step:     4815, Batch Loss:     0.347619, Batch Acc: 0.911687, Tokens per Sec:     3730, Lr: 0.000258\r\n",
      "2024-08-27 15:00:54,063 - INFO - joeynmt.training - Epoch  51, Step:     4820, Batch Loss:     0.334845, Batch Acc: 0.913583, Tokens per Sec:     3823, Lr: 0.000258\r\n",
      "2024-08-27 15:00:55,176 - INFO - joeynmt.training - Epoch  51, Step:     4825, Batch Loss:     0.414313, Batch Acc: 0.909091, Tokens per Sec:     3866, Lr: 0.000258\r\n",
      "2024-08-27 15:00:56,289 - INFO - joeynmt.training - Epoch  51, Step:     4830, Batch Loss:     0.362302, Batch Acc: 0.907305, Tokens per Sec:     3890, Lr: 0.000257\r\n",
      "2024-08-27 15:00:57,425 - INFO - joeynmt.training - Epoch  51, Step:     4835, Batch Loss:     0.339194, Batch Acc: 0.915094, Tokens per Sec:     3640, Lr: 0.000257\r\n",
      "2024-08-27 15:00:58,539 - INFO - joeynmt.training - Epoch  51, Step:     4840, Batch Loss:     0.385012, Batch Acc: 0.909154, Tokens per Sec:     3858, Lr: 0.000257\r\n",
      "2024-08-27 15:00:59,632 - INFO - joeynmt.training - Epoch  51, Step:     4845, Batch Loss:     0.389391, Batch Acc: 0.915629, Tokens per Sec:     3969, Lr: 0.000257\r\n",
      "2024-08-27 15:01:00,742 - INFO - joeynmt.training - Epoch  51, Step:     4850, Batch Loss:     0.421717, Batch Acc: 0.906026, Tokens per Sec:     3771, Lr: 0.000257\r\n",
      "2024-08-27 15:01:01,776 - INFO - joeynmt.training - Epoch  51, total training loss: 34.85, num. of seqs: 8065, num. of tokens: 80463, 21.3319[sec]\r\n",
      "2024-08-27 15:01:01,776 - INFO - joeynmt.training - EPOCH 52\r\n",
      "2024-08-27 15:01:02,003 - INFO - joeynmt.training - Epoch  52, Step:     4855, Batch Loss:     0.337003, Batch Acc: 0.929172, Tokens per Sec:     3741, Lr: 0.000257\r\n",
      "2024-08-27 15:01:03,087 - INFO - joeynmt.training - Epoch  52, Step:     4860, Batch Loss:     0.320087, Batch Acc: 0.930309, Tokens per Sec:     3910, Lr: 0.000257\r\n",
      "2024-08-27 15:01:04,169 - INFO - joeynmt.training - Epoch  52, Step:     4865, Batch Loss:     0.308962, Batch Acc: 0.925827, Tokens per Sec:     3801, Lr: 0.000256\r\n",
      "2024-08-27 15:01:05,246 - INFO - joeynmt.training - Epoch  52, Step:     4870, Batch Loss:     0.339329, Batch Acc: 0.923793, Tokens per Sec:     3696, Lr: 0.000256\r\n",
      "2024-08-27 15:01:06,337 - INFO - joeynmt.training - Epoch  52, Step:     4875, Batch Loss:     0.325775, Batch Acc: 0.926495, Tokens per Sec:     3816, Lr: 0.000256\r\n",
      "2024-08-27 15:01:07,624 - INFO - joeynmt.training - Epoch  52, Step:     4880, Batch Loss:     0.293771, Batch Acc: 0.928555, Tokens per Sec:     3341, Lr: 0.000256\r\n",
      "2024-08-27 15:01:08,727 - INFO - joeynmt.training - Epoch  52, Step:     4885, Batch Loss:     0.318202, Batch Acc: 0.917514, Tokens per Sec:     3838, Lr: 0.000256\r\n",
      "2024-08-27 15:01:09,812 - INFO - joeynmt.training - Epoch  52, Step:     4890, Batch Loss:     0.339632, Batch Acc: 0.929237, Tokens per Sec:     3858, Lr: 0.000256\r\n",
      "2024-08-27 15:01:10,921 - INFO - joeynmt.training - Epoch  52, Step:     4895, Batch Loss:     0.349478, Batch Acc: 0.922044, Tokens per Sec:     3764, Lr: 0.000256\r\n",
      "2024-08-27 15:01:12,017 - INFO - joeynmt.training - Epoch  52, Step:     4900, Batch Loss:     0.358110, Batch Acc: 0.915363, Tokens per Sec:     3851, Lr: 0.000256\r\n",
      "2024-08-27 15:01:13,120 - INFO - joeynmt.training - Epoch  52, Step:     4905, Batch Loss:     0.393144, Batch Acc: 0.912394, Tokens per Sec:     4070, Lr: 0.000255\r\n",
      "2024-08-27 15:01:14,210 - INFO - joeynmt.training - Epoch  52, Step:     4910, Batch Loss:     0.360505, Batch Acc: 0.916939, Tokens per Sec:     3935, Lr: 0.000255\r\n",
      "2024-08-27 15:01:15,317 - INFO - joeynmt.training - Epoch  52, Step:     4915, Batch Loss:     0.325325, Batch Acc: 0.920143, Tokens per Sec:     3789, Lr: 0.000255\r\n",
      "2024-08-27 15:01:16,414 - INFO - joeynmt.training - Epoch  52, Step:     4920, Batch Loss:     0.347268, Batch Acc: 0.915934, Tokens per Sec:     3733, Lr: 0.000255\r\n",
      "2024-08-27 15:01:17,534 - INFO - joeynmt.training - Epoch  52, Step:     4925, Batch Loss:     0.368308, Batch Acc: 0.917992, Tokens per Sec:     3934, Lr: 0.000255\r\n",
      "2024-08-27 15:01:18,630 - INFO - joeynmt.training - Epoch  52, Step:     4930, Batch Loss:     0.367469, Batch Acc: 0.912032, Tokens per Sec:     3925, Lr: 0.000255\r\n",
      "2024-08-27 15:01:19,724 - INFO - joeynmt.training - Epoch  52, Step:     4935, Batch Loss:     0.338939, Batch Acc: 0.921697, Tokens per Sec:     3922, Lr: 0.000255\r\n",
      "2024-08-27 15:01:20,825 - INFO - joeynmt.training - Epoch  52, Step:     4940, Batch Loss:     0.365746, Batch Acc: 0.916804, Tokens per Sec:     3868, Lr: 0.000255\r\n",
      "2024-08-27 15:01:21,905 - INFO - joeynmt.training - Epoch  52, Step:     4945, Batch Loss:     0.330855, Batch Acc: 0.912259, Tokens per Sec:     3748, Lr: 0.000254\r\n",
      "2024-08-27 15:01:22,870 - INFO - joeynmt.training - Epoch  52, total training loss: 33.16, num. of seqs: 8065, num. of tokens: 80463, 21.0764[sec]\r\n",
      "2024-08-27 15:01:22,871 - INFO - joeynmt.training - EPOCH 53\r\n",
      "2024-08-27 15:01:23,098 - INFO - joeynmt.training - Epoch  53, Step:     4950, Batch Loss:     0.311767, Batch Acc: 0.928000, Tokens per Sec:     3928, Lr: 0.000254\r\n",
      "2024-08-27 15:01:24,194 - INFO - joeynmt.training - Epoch  53, Step:     4955, Batch Loss:     0.322767, Batch Acc: 0.931930, Tokens per Sec:     3901, Lr: 0.000254\r\n",
      "2024-08-27 15:01:25,301 - INFO - joeynmt.training - Epoch  53, Step:     4960, Batch Loss:     0.342152, Batch Acc: 0.929040, Tokens per Sec:     3861, Lr: 0.000254\r\n",
      "2024-08-27 15:01:26,396 - INFO - joeynmt.training - Epoch  53, Step:     4965, Batch Loss:     0.326257, Batch Acc: 0.929528, Tokens per Sec:     3891, Lr: 0.000254\r\n",
      "2024-08-27 15:01:27,511 - INFO - joeynmt.training - Epoch  53, Step:     4970, Batch Loss:     0.334630, Batch Acc: 0.928225, Tokens per Sec:     3888, Lr: 0.000254\r\n",
      "2024-08-27 15:01:28,613 - INFO - joeynmt.training - Epoch  53, Step:     4975, Batch Loss:     0.325127, Batch Acc: 0.926996, Tokens per Sec:     3992, Lr: 0.000254\r\n",
      "2024-08-27 15:01:29,682 - INFO - joeynmt.training - Epoch  53, Step:     4980, Batch Loss:     0.350310, Batch Acc: 0.924269, Tokens per Sec:     3810, Lr: 0.000253\r\n",
      "2024-08-27 15:01:30,750 - INFO - joeynmt.training - Epoch  53, Step:     4985, Batch Loss:     0.354013, Batch Acc: 0.920678, Tokens per Sec:     3756, Lr: 0.000253\r\n",
      "2024-08-27 15:01:31,833 - INFO - joeynmt.training - Epoch  53, Step:     4990, Batch Loss:     0.323626, Batch Acc: 0.923769, Tokens per Sec:     3902, Lr: 0.000253\r\n",
      "2024-08-27 15:01:32,937 - INFO - joeynmt.training - Epoch  53, Step:     4995, Batch Loss:     0.349343, Batch Acc: 0.919109, Tokens per Sec:     3987, Lr: 0.000253\r\n",
      "2024-08-27 15:01:34,015 - INFO - joeynmt.training - Epoch  53, Step:     5000, Batch Loss:     0.347016, Batch Acc: 0.924420, Tokens per Sec:     3881, Lr: 0.000253\r\n",
      "2024-08-27 15:01:34,016 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=5042\r\n",
      "2024-08-27 15:01:34,017 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 63.65it/s]\r\n",
      "2024-08-27 15:01:56,958 - INFO - joeynmt.prediction - Generation took 22.9393[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44736.08 examples/s]\r\n",
      "2024-08-27 15:01:57,343 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:01:57,343 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   4.02, loss:   6.37, ppl: 584.71, acc:   0.28, 0.1652[sec]\r\n",
      "2024-08-27 15:01:57,344 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\r\n",
      "2024-08-27 15:01:57,661 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/5000.ckpt.\r\n",
      "2024-08-27 15:01:57,662 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/3500.ckpt\r\n",
      "2024-08-27 15:01:57,686 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:01:57,834 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:01:57,834 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:01:57,834 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 15:01:58,396 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:01:58,543 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:01:58,543 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:01:58,544 - INFO - joeynmt.training - \tHypothesis: trois éléphants\r\n",
      "2024-08-27 15:01:58,839 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:01:58,993 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:01:58,993 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:01:58,993 - INFO - joeynmt.training - \tHypothesis: cependant l’autre part\r\n",
      "2024-08-27 15:01:59,288 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:01:59,435 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:01:59,436 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:01:59,436 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 15:02:00,904 - INFO - joeynmt.training - Epoch  53, Step:     5005, Batch Loss:     0.322544, Batch Acc: 0.922108, Tokens per Sec:     3664, Lr: 0.000253\r\n",
      "2024-08-27 15:02:02,047 - INFO - joeynmt.training - Epoch  53, Step:     5010, Batch Loss:     0.352777, Batch Acc: 0.926541, Tokens per Sec:     3635, Lr: 0.000253\r\n",
      "2024-08-27 15:02:03,178 - INFO - joeynmt.training - Epoch  53, Step:     5015, Batch Loss:     0.370408, Batch Acc: 0.915466, Tokens per Sec:     3750, Lr: 0.000253\r\n",
      "2024-08-27 15:02:04,298 - INFO - joeynmt.training - Epoch  53, Step:     5020, Batch Loss:     0.341688, Batch Acc: 0.918597, Tokens per Sec:     3896, Lr: 0.000252\r\n",
      "2024-08-27 15:02:05,403 - INFO - joeynmt.training - Epoch  53, Step:     5025, Batch Loss:     0.334106, Batch Acc: 0.923928, Tokens per Sec:     3843, Lr: 0.000252\r\n",
      "2024-08-27 15:02:06,498 - INFO - joeynmt.training - Epoch  53, Step:     5030, Batch Loss:     0.369583, Batch Acc: 0.913200, Tokens per Sec:     3801, Lr: 0.000252\r\n",
      "2024-08-27 15:02:07,622 - INFO - joeynmt.training - Epoch  53, Step:     5035, Batch Loss:     0.331872, Batch Acc: 0.912452, Tokens per Sec:     3927, Lr: 0.000252\r\n",
      "2024-08-27 15:02:08,726 - INFO - joeynmt.training - Epoch  53, Step:     5040, Batch Loss:     0.391214, Batch Acc: 0.915222, Tokens per Sec:     3814, Lr: 0.000252\r\n",
      "2024-08-27 15:02:09,621 - INFO - joeynmt.training - Epoch  53, total training loss: 32.13, num. of seqs: 8065, num. of tokens: 80463, 21.0146[sec]\r\n",
      "2024-08-27 15:02:09,621 - INFO - joeynmt.training - EPOCH 54\r\n",
      "2024-08-27 15:02:10,114 - INFO - joeynmt.training - Epoch  54, Step:     5045, Batch Loss:     0.324468, Batch Acc: 0.924043, Tokens per Sec:     3587, Lr: 0.000252\r\n",
      "2024-08-27 15:02:11,188 - INFO - joeynmt.training - Epoch  54, Step:     5050, Batch Loss:     0.281780, Batch Acc: 0.928911, Tokens per Sec:     3917, Lr: 0.000252\r\n",
      "2024-08-27 15:02:12,276 - INFO - joeynmt.training - Epoch  54, Step:     5055, Batch Loss:     0.309431, Batch Acc: 0.928269, Tokens per Sec:     3912, Lr: 0.000252\r\n",
      "2024-08-27 15:02:13,372 - INFO - joeynmt.training - Epoch  54, Step:     5060, Batch Loss:     0.336245, Batch Acc: 0.930862, Tokens per Sec:     4015, Lr: 0.000251\r\n",
      "2024-08-27 15:02:14,454 - INFO - joeynmt.training - Epoch  54, Step:     5065, Batch Loss:     0.323319, Batch Acc: 0.928162, Tokens per Sec:     3708, Lr: 0.000251\r\n",
      "2024-08-27 15:02:15,551 - INFO - joeynmt.training - Epoch  54, Step:     5070, Batch Loss:     0.350297, Batch Acc: 0.928860, Tokens per Sec:     3832, Lr: 0.000251\r\n",
      "2024-08-27 15:02:16,651 - INFO - joeynmt.training - Epoch  54, Step:     5075, Batch Loss:     0.309708, Batch Acc: 0.924190, Tokens per Sec:     3961, Lr: 0.000251\r\n",
      "2024-08-27 15:02:17,768 - INFO - joeynmt.training - Epoch  54, Step:     5080, Batch Loss:     0.317677, Batch Acc: 0.918182, Tokens per Sec:     3843, Lr: 0.000251\r\n",
      "2024-08-27 15:02:18,850 - INFO - joeynmt.training - Epoch  54, Step:     5085, Batch Loss:     0.301705, Batch Acc: 0.921310, Tokens per Sec:     3784, Lr: 0.000251\r\n",
      "2024-08-27 15:02:19,941 - INFO - joeynmt.training - Epoch  54, Step:     5090, Batch Loss:     0.364472, Batch Acc: 0.920410, Tokens per Sec:     3940, Lr: 0.000251\r\n",
      "2024-08-27 15:02:21,018 - INFO - joeynmt.training - Epoch  54, Step:     5095, Batch Loss:     0.327942, Batch Acc: 0.923910, Tokens per Sec:     3858, Lr: 0.000251\r\n",
      "2024-08-27 15:02:22,122 - INFO - joeynmt.training - Epoch  54, Step:     5100, Batch Loss:     0.296860, Batch Acc: 0.927118, Tokens per Sec:     3830, Lr: 0.000250\r\n",
      "2024-08-27 15:02:23,202 - INFO - joeynmt.training - Epoch  54, Step:     5105, Batch Loss:     0.379540, Batch Acc: 0.925117, Tokens per Sec:     3775, Lr: 0.000250\r\n",
      "2024-08-27 15:02:24,291 - INFO - joeynmt.training - Epoch  54, Step:     5110, Batch Loss:     0.337366, Batch Acc: 0.918310, Tokens per Sec:     3914, Lr: 0.000250\r\n",
      "2024-08-27 15:02:25,369 - INFO - joeynmt.training - Epoch  54, Step:     5115, Batch Loss:     0.336446, Batch Acc: 0.924631, Tokens per Sec:     3904, Lr: 0.000250\r\n",
      "2024-08-27 15:02:26,468 - INFO - joeynmt.training - Epoch  54, Step:     5120, Batch Loss:     0.350627, Batch Acc: 0.918457, Tokens per Sec:     3941, Lr: 0.000250\r\n",
      "2024-08-27 15:02:27,595 - INFO - joeynmt.training - Epoch  54, Step:     5125, Batch Loss:     0.325682, Batch Acc: 0.917234, Tokens per Sec:     3918, Lr: 0.000250\r\n",
      "2024-08-27 15:02:28,727 - INFO - joeynmt.training - Epoch  54, Step:     5130, Batch Loss:     0.381693, Batch Acc: 0.908756, Tokens per Sec:     3604, Lr: 0.000250\r\n",
      "2024-08-27 15:02:29,818 - INFO - joeynmt.training - Epoch  54, Step:     5135, Batch Loss:     0.372392, Batch Acc: 0.915865, Tokens per Sec:     3816, Lr: 0.000250\r\n",
      "2024-08-27 15:02:30,575 - INFO - joeynmt.training - Epoch  54, total training loss: 31.70, num. of seqs: 8065, num. of tokens: 80463, 20.9360[sec]\r\n",
      "2024-08-27 15:02:30,575 - INFO - joeynmt.training - EPOCH 55\r\n",
      "2024-08-27 15:02:31,009 - INFO - joeynmt.training - Epoch  55, Step:     5140, Batch Loss:     0.286640, Batch Acc: 0.943258, Tokens per Sec:     3809, Lr: 0.000250\r\n",
      "2024-08-27 15:02:32,083 - INFO - joeynmt.training - Epoch  55, Step:     5145, Batch Loss:     0.299286, Batch Acc: 0.936700, Tokens per Sec:     3860, Lr: 0.000249\r\n",
      "2024-08-27 15:02:33,172 - INFO - joeynmt.training - Epoch  55, Step:     5150, Batch Loss:     0.294268, Batch Acc: 0.929375, Tokens per Sec:     3836, Lr: 0.000249\r\n",
      "2024-08-27 15:02:34,281 - INFO - joeynmt.training - Epoch  55, Step:     5155, Batch Loss:     0.277527, Batch Acc: 0.937253, Tokens per Sec:     3652, Lr: 0.000249\r\n",
      "2024-08-27 15:02:35,364 - INFO - joeynmt.training - Epoch  55, Step:     5160, Batch Loss:     0.295402, Batch Acc: 0.933284, Tokens per Sec:     3778, Lr: 0.000249\r\n",
      "2024-08-27 15:02:36,441 - INFO - joeynmt.training - Epoch  55, Step:     5165, Batch Loss:     0.350566, Batch Acc: 0.923436, Tokens per Sec:     3983, Lr: 0.000249\r\n",
      "2024-08-27 15:02:37,570 - INFO - joeynmt.training - Epoch  55, Step:     5170, Batch Loss:     0.291438, Batch Acc: 0.925829, Tokens per Sec:     3717, Lr: 0.000249\r\n",
      "2024-08-27 15:02:38,665 - INFO - joeynmt.training - Epoch  55, Step:     5175, Batch Loss:     0.343914, Batch Acc: 0.931523, Tokens per Sec:     3870, Lr: 0.000249\r\n",
      "2024-08-27 15:02:39,781 - INFO - joeynmt.training - Epoch  55, Step:     5180, Batch Loss:     0.309811, Batch Acc: 0.926155, Tokens per Sec:     3763, Lr: 0.000249\r\n",
      "2024-08-27 15:02:40,973 - INFO - joeynmt.training - Epoch  55, Step:     5185, Batch Loss:     0.302583, Batch Acc: 0.927896, Tokens per Sec:     3551, Lr: 0.000248\r\n",
      "2024-08-27 15:02:42,070 - INFO - joeynmt.training - Epoch  55, Step:     5190, Batch Loss:     0.314929, Batch Acc: 0.926670, Tokens per Sec:     3906, Lr: 0.000248\r\n",
      "2024-08-27 15:02:43,164 - INFO - joeynmt.training - Epoch  55, Step:     5195, Batch Loss:     0.323627, Batch Acc: 0.923676, Tokens per Sec:     3989, Lr: 0.000248\r\n",
      "2024-08-27 15:02:44,257 - INFO - joeynmt.training - Epoch  55, Step:     5200, Batch Loss:     0.325521, Batch Acc: 0.923168, Tokens per Sec:     3860, Lr: 0.000248\r\n",
      "2024-08-27 15:02:45,340 - INFO - joeynmt.training - Epoch  55, Step:     5205, Batch Loss:     0.309010, Batch Acc: 0.925908, Tokens per Sec:     3740, Lr: 0.000248\r\n",
      "2024-08-27 15:02:46,419 - INFO - joeynmt.training - Epoch  55, Step:     5210, Batch Loss:     0.349594, Batch Acc: 0.929544, Tokens per Sec:     3952, Lr: 0.000248\r\n",
      "2024-08-27 15:02:47,548 - INFO - joeynmt.training - Epoch  55, Step:     5215, Batch Loss:     0.336428, Batch Acc: 0.928939, Tokens per Sec:     3617, Lr: 0.000248\r\n",
      "2024-08-27 15:02:48,655 - INFO - joeynmt.training - Epoch  55, Step:     5220, Batch Loss:     0.304817, Batch Acc: 0.922250, Tokens per Sec:     3953, Lr: 0.000248\r\n",
      "2024-08-27 15:02:49,754 - INFO - joeynmt.training - Epoch  55, Step:     5225, Batch Loss:     0.354070, Batch Acc: 0.922912, Tokens per Sec:     3826, Lr: 0.000247\r\n",
      "2024-08-27 15:02:50,846 - INFO - joeynmt.training - Epoch  55, Step:     5230, Batch Loss:     0.345244, Batch Acc: 0.921183, Tokens per Sec:     3871, Lr: 0.000247\r\n",
      "2024-08-27 15:02:51,714 - INFO - joeynmt.training - Epoch  55, total training loss: 30.74, num. of seqs: 8065, num. of tokens: 80463, 21.1218[sec]\r\n",
      "2024-08-27 15:02:51,714 - INFO - joeynmt.training - EPOCH 56\r\n",
      "2024-08-27 15:02:51,933 - INFO - joeynmt.training - Epoch  56, Step:     5235, Batch Loss:     0.292735, Batch Acc: 0.935335, Tokens per Sec:     4014, Lr: 0.000247\r\n",
      "2024-08-27 15:02:53,014 - INFO - joeynmt.training - Epoch  56, Step:     5240, Batch Loss:     0.286946, Batch Acc: 0.934694, Tokens per Sec:     4082, Lr: 0.000247\r\n",
      "2024-08-27 15:02:54,096 - INFO - joeynmt.training - Epoch  56, Step:     5245, Batch Loss:     0.256916, Batch Acc: 0.942512, Tokens per Sec:     3828, Lr: 0.000247\r\n",
      "2024-08-27 15:02:55,181 - INFO - joeynmt.training - Epoch  56, Step:     5250, Batch Loss:     0.294511, Batch Acc: 0.928068, Tokens per Sec:     3795, Lr: 0.000247\r\n",
      "2024-08-27 15:02:56,309 - INFO - joeynmt.training - Epoch  56, Step:     5255, Batch Loss:     0.315970, Batch Acc: 0.930679, Tokens per Sec:     3698, Lr: 0.000247\r\n",
      "2024-08-27 15:02:57,448 - INFO - joeynmt.training - Epoch  56, Step:     5260, Batch Loss:     0.335591, Batch Acc: 0.930784, Tokens per Sec:     3631, Lr: 0.000247\r\n",
      "2024-08-27 15:02:58,541 - INFO - joeynmt.training - Epoch  56, Step:     5265, Batch Loss:     0.329081, Batch Acc: 0.932864, Tokens per Sec:     3899, Lr: 0.000247\r\n",
      "2024-08-27 15:02:59,632 - INFO - joeynmt.training - Epoch  56, Step:     5270, Batch Loss:     0.334811, Batch Acc: 0.925706, Tokens per Sec:     4024, Lr: 0.000246\r\n",
      "2024-08-27 15:03:00,695 - INFO - joeynmt.training - Epoch  56, Step:     5275, Batch Loss:     0.260598, Batch Acc: 0.931162, Tokens per Sec:     3816, Lr: 0.000246\r\n",
      "2024-08-27 15:03:01,820 - INFO - joeynmt.training - Epoch  56, Step:     5280, Batch Loss:     0.337731, Batch Acc: 0.927169, Tokens per Sec:     3579, Lr: 0.000246\r\n",
      "2024-08-27 15:03:02,943 - INFO - joeynmt.training - Epoch  56, Step:     5285, Batch Loss:     0.306999, Batch Acc: 0.934011, Tokens per Sec:     3768, Lr: 0.000246\r\n",
      "2024-08-27 15:03:04,035 - INFO - joeynmt.training - Epoch  56, Step:     5290, Batch Loss:     0.300155, Batch Acc: 0.927624, Tokens per Sec:     4075, Lr: 0.000246\r\n",
      "2024-08-27 15:03:05,116 - INFO - joeynmt.training - Epoch  56, Step:     5295, Batch Loss:     0.352557, Batch Acc: 0.928623, Tokens per Sec:     3840, Lr: 0.000246\r\n",
      "2024-08-27 15:03:06,176 - INFO - joeynmt.training - Epoch  56, Step:     5300, Batch Loss:     0.334069, Batch Acc: 0.923668, Tokens per Sec:     3933, Lr: 0.000246\r\n",
      "2024-08-27 15:03:07,300 - INFO - joeynmt.training - Epoch  56, Step:     5305, Batch Loss:     0.346861, Batch Acc: 0.923490, Tokens per Sec:     3817, Lr: 0.000246\r\n",
      "2024-08-27 15:03:08,402 - INFO - joeynmt.training - Epoch  56, Step:     5310, Batch Loss:     0.367213, Batch Acc: 0.926209, Tokens per Sec:     3926, Lr: 0.000245\r\n",
      "2024-08-27 15:03:09,511 - INFO - joeynmt.training - Epoch  56, Step:     5315, Batch Loss:     0.314773, Batch Acc: 0.928731, Tokens per Sec:     3646, Lr: 0.000245\r\n",
      "2024-08-27 15:03:10,645 - INFO - joeynmt.training - Epoch  56, Step:     5320, Batch Loss:     0.308051, Batch Acc: 0.929291, Tokens per Sec:     3682, Lr: 0.000245\r\n",
      "2024-08-27 15:03:11,873 - INFO - joeynmt.training - Epoch  56, Step:     5325, Batch Loss:     0.339287, Batch Acc: 0.922572, Tokens per Sec:     3472, Lr: 0.000245\r\n",
      "2024-08-27 15:03:12,994 - INFO - joeynmt.training - Epoch  56, total training loss: 29.80, num. of seqs: 8065, num. of tokens: 80463, 21.2630[sec]\r\n",
      "2024-08-27 15:03:12,995 - INFO - joeynmt.training - EPOCH 57\r\n",
      "2024-08-27 15:03:13,224 - INFO - joeynmt.training - Epoch  57, Step:     5330, Batch Loss:     0.273301, Batch Acc: 0.946073, Tokens per Sec:     3792, Lr: 0.000245\r\n",
      "2024-08-27 15:03:14,339 - INFO - joeynmt.training - Epoch  57, Step:     5335, Batch Loss:     0.285147, Batch Acc: 0.936275, Tokens per Sec:     3844, Lr: 0.000245\r\n",
      "2024-08-27 15:03:15,470 - INFO - joeynmt.training - Epoch  57, Step:     5340, Batch Loss:     0.281228, Batch Acc: 0.942518, Tokens per Sec:     3647, Lr: 0.000245\r\n",
      "2024-08-27 15:03:16,573 - INFO - joeynmt.training - Epoch  57, Step:     5345, Batch Loss:     0.302686, Batch Acc: 0.932987, Tokens per Sec:     3668, Lr: 0.000245\r\n",
      "2024-08-27 15:03:17,695 - INFO - joeynmt.training - Epoch  57, Step:     5350, Batch Loss:     0.311509, Batch Acc: 0.940925, Tokens per Sec:     3760, Lr: 0.000245\r\n",
      "2024-08-27 15:03:18,796 - INFO - joeynmt.training - Epoch  57, Step:     5355, Batch Loss:     0.288906, Batch Acc: 0.937008, Tokens per Sec:     3695, Lr: 0.000244\r\n",
      "2024-08-27 15:03:19,893 - INFO - joeynmt.training - Epoch  57, Step:     5360, Batch Loss:     0.321986, Batch Acc: 0.928106, Tokens per Sec:     3918, Lr: 0.000244\r\n",
      "2024-08-27 15:03:21,014 - INFO - joeynmt.training - Epoch  57, Step:     5365, Batch Loss:     0.307919, Batch Acc: 0.932568, Tokens per Sec:     3735, Lr: 0.000244\r\n",
      "2024-08-27 15:03:22,149 - INFO - joeynmt.training - Epoch  57, Step:     5370, Batch Loss:     0.321961, Batch Acc: 0.929740, Tokens per Sec:     3827, Lr: 0.000244\r\n",
      "2024-08-27 15:03:23,286 - INFO - joeynmt.training - Epoch  57, Step:     5375, Batch Loss:     0.327244, Batch Acc: 0.929587, Tokens per Sec:     3775, Lr: 0.000244\r\n",
      "2024-08-27 15:03:24,437 - INFO - joeynmt.training - Epoch  57, Step:     5380, Batch Loss:     0.275178, Batch Acc: 0.927916, Tokens per Sec:     3884, Lr: 0.000244\r\n",
      "2024-08-27 15:03:25,580 - INFO - joeynmt.training - Epoch  57, Step:     5385, Batch Loss:     0.293048, Batch Acc: 0.934648, Tokens per Sec:     3817, Lr: 0.000244\r\n",
      "2024-08-27 15:03:26,715 - INFO - joeynmt.training - Epoch  57, Step:     5390, Batch Loss:     0.299785, Batch Acc: 0.926906, Tokens per Sec:     3668, Lr: 0.000244\r\n",
      "2024-08-27 15:03:27,862 - INFO - joeynmt.training - Epoch  57, Step:     5395, Batch Loss:     0.316377, Batch Acc: 0.924895, Tokens per Sec:     3726, Lr: 0.000244\r\n",
      "2024-08-27 15:03:28,984 - INFO - joeynmt.training - Epoch  57, Step:     5400, Batch Loss:     0.322963, Batch Acc: 0.930985, Tokens per Sec:     3696, Lr: 0.000243\r\n",
      "2024-08-27 15:03:30,099 - INFO - joeynmt.training - Epoch  57, Step:     5405, Batch Loss:     0.337450, Batch Acc: 0.928466, Tokens per Sec:     3666, Lr: 0.000243\r\n",
      "2024-08-27 15:03:31,194 - INFO - joeynmt.training - Epoch  57, Step:     5410, Batch Loss:     0.345095, Batch Acc: 0.923585, Tokens per Sec:     3873, Lr: 0.000243\r\n",
      "2024-08-27 15:03:32,303 - INFO - joeynmt.training - Epoch  57, Step:     5415, Batch Loss:     0.317049, Batch Acc: 0.924789, Tokens per Sec:     3853, Lr: 0.000243\r\n",
      "2024-08-27 15:03:33,431 - INFO - joeynmt.training - Epoch  57, Step:     5420, Batch Loss:     0.310969, Batch Acc: 0.929130, Tokens per Sec:     3517, Lr: 0.000243\r\n",
      "2024-08-27 15:03:34,478 - INFO - joeynmt.training - Epoch  57, total training loss: 28.95, num. of seqs: 8065, num. of tokens: 80463, 21.4658[sec]\r\n",
      "2024-08-27 15:03:34,479 - INFO - joeynmt.training - EPOCH 58\r\n",
      "2024-08-27 15:03:34,702 - INFO - joeynmt.training - Epoch  58, Step:     5425, Batch Loss:     0.292333, Batch Acc: 0.937073, Tokens per Sec:     3346, Lr: 0.000243\r\n",
      "2024-08-27 15:03:35,807 - INFO - joeynmt.training - Epoch  58, Step:     5430, Batch Loss:     0.297273, Batch Acc: 0.934055, Tokens per Sec:     3761, Lr: 0.000243\r\n",
      "2024-08-27 15:03:36,918 - INFO - joeynmt.training - Epoch  58, Step:     5435, Batch Loss:     0.259790, Batch Acc: 0.945647, Tokens per Sec:     3745, Lr: 0.000243\r\n",
      "2024-08-27 15:03:37,995 - INFO - joeynmt.training - Epoch  58, Step:     5440, Batch Loss:     0.248794, Batch Acc: 0.941231, Tokens per Sec:     4015, Lr: 0.000243\r\n",
      "2024-08-27 15:03:39,072 - INFO - joeynmt.training - Epoch  58, Step:     5445, Batch Loss:     0.278098, Batch Acc: 0.937933, Tokens per Sec:     4027, Lr: 0.000242\r\n",
      "2024-08-27 15:03:40,179 - INFO - joeynmt.training - Epoch  58, Step:     5450, Batch Loss:     0.276305, Batch Acc: 0.942012, Tokens per Sec:     3759, Lr: 0.000242\r\n",
      "2024-08-27 15:03:41,252 - INFO - joeynmt.training - Epoch  58, Step:     5455, Batch Loss:     0.271078, Batch Acc: 0.942085, Tokens per Sec:     3865, Lr: 0.000242\r\n",
      "2024-08-27 15:03:42,357 - INFO - joeynmt.training - Epoch  58, Step:     5460, Batch Loss:     0.285749, Batch Acc: 0.941281, Tokens per Sec:     3577, Lr: 0.000242\r\n",
      "2024-08-27 15:03:43,544 - INFO - joeynmt.training - Epoch  58, Step:     5465, Batch Loss:     0.282557, Batch Acc: 0.930164, Tokens per Sec:     3706, Lr: 0.000242\r\n",
      "2024-08-27 15:03:44,641 - INFO - joeynmt.training - Epoch  58, Step:     5470, Batch Loss:     0.307062, Batch Acc: 0.932043, Tokens per Sec:     3864, Lr: 0.000242\r\n",
      "2024-08-27 15:03:45,717 - INFO - joeynmt.training - Epoch  58, Step:     5475, Batch Loss:     0.292023, Batch Acc: 0.936180, Tokens per Sec:     3920, Lr: 0.000242\r\n",
      "2024-08-27 15:03:46,818 - INFO - joeynmt.training - Epoch  58, Step:     5480, Batch Loss:     0.303254, Batch Acc: 0.938869, Tokens per Sec:     3775, Lr: 0.000242\r\n",
      "2024-08-27 15:03:47,925 - INFO - joeynmt.training - Epoch  58, Step:     5485, Batch Loss:     0.306480, Batch Acc: 0.927134, Tokens per Sec:     3908, Lr: 0.000242\r\n",
      "2024-08-27 15:03:49,005 - INFO - joeynmt.training - Epoch  58, Step:     5490, Batch Loss:     0.319934, Batch Acc: 0.934566, Tokens per Sec:     3910, Lr: 0.000241\r\n",
      "2024-08-27 15:03:50,088 - INFO - joeynmt.training - Epoch  58, Step:     5495, Batch Loss:     0.295007, Batch Acc: 0.930877, Tokens per Sec:     3835, Lr: 0.000241\r\n",
      "2024-08-27 15:03:51,171 - INFO - joeynmt.training - Epoch  58, Step:     5500, Batch Loss:     0.313228, Batch Acc: 0.935038, Tokens per Sec:     3939, Lr: 0.000241\r\n",
      "2024-08-27 15:03:51,172 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=5542\r\n",
      "2024-08-27 15:03:51,173 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 63.88it/s]\r\n",
      "2024-08-27 15:04:14,030 - INFO - joeynmt.prediction - Generation took 22.8556[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 31980.25 examples/s]\r\n",
      "2024-08-27 15:04:14,446 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:04:14,446 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   4.10, loss:   6.52, ppl: 679.93, acc:   0.28, 0.1704[sec]\r\n",
      "2024-08-27 15:04:14,447 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\r\n",
      "2024-08-27 15:04:14,765 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/5500.ckpt.\r\n",
      "2024-08-27 15:04:14,766 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/4500.ckpt\r\n",
      "2024-08-27 15:04:14,790 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:04:14,939 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:04:14,940 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:04:14,940 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 15:04:15,232 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:04:15,378 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:04:15,378 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:04:15,379 - INFO - joeynmt.training - \tHypothesis: trois montée de l'enclos\r\n",
      "2024-08-27 15:04:15,671 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:04:15,818 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:04:15,818 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:04:15,818 - INFO - joeynmt.training - \tHypothesis: cependant l’ancienne lentille et l’autre\r\n",
      "2024-08-27 15:04:16,110 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:04:16,257 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:04:16,257 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:04:16,258 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 15:04:17,695 - INFO - joeynmt.training - Epoch  58, Step:     5505, Batch Loss:     0.319996, Batch Acc: 0.929901, Tokens per Sec:     3642, Lr: 0.000241\r\n",
      "2024-08-27 15:04:18,791 - INFO - joeynmt.training - Epoch  58, Step:     5510, Batch Loss:     0.291589, Batch Acc: 0.925130, Tokens per Sec:     3866, Lr: 0.000241\r\n",
      "2024-08-27 15:04:19,905 - INFO - joeynmt.training - Epoch  58, Step:     5515, Batch Loss:     0.277911, Batch Acc: 0.935190, Tokens per Sec:     3741, Lr: 0.000241\r\n",
      "2024-08-27 15:04:20,998 - INFO - joeynmt.training - Epoch  58, Step:     5520, Batch Loss:     0.390384, Batch Acc: 0.927504, Tokens per Sec:     3676, Lr: 0.000241\r\n",
      "2024-08-27 15:04:20,998 - INFO - joeynmt.training - Epoch  58, total training loss: 28.08, num. of seqs: 8065, num. of tokens: 80463, 21.1148[sec]\r\n",
      "2024-08-27 15:04:20,999 - INFO - joeynmt.training - EPOCH 59\r\n",
      "2024-08-27 15:04:22,077 - INFO - joeynmt.training - Epoch  59, Step:     5525, Batch Loss:     0.277356, Batch Acc: 0.932761, Tokens per Sec:     3792, Lr: 0.000241\r\n",
      "2024-08-27 15:04:23,164 - INFO - joeynmt.training - Epoch  59, Step:     5530, Batch Loss:     0.232917, Batch Acc: 0.946977, Tokens per Sec:     3714, Lr: 0.000241\r\n",
      "2024-08-27 15:04:24,272 - INFO - joeynmt.training - Epoch  59, Step:     5535, Batch Loss:     0.295368, Batch Acc: 0.935001, Tokens per Sec:     3863, Lr: 0.000240\r\n",
      "2024-08-27 15:04:25,361 - INFO - joeynmt.training - Epoch  59, Step:     5540, Batch Loss:     0.286539, Batch Acc: 0.938870, Tokens per Sec:     3771, Lr: 0.000240\r\n",
      "2024-08-27 15:04:26,468 - INFO - joeynmt.training - Epoch  59, Step:     5545, Batch Loss:     0.298075, Batch Acc: 0.939379, Tokens per Sec:     3581, Lr: 0.000240\r\n",
      "2024-08-27 15:04:27,605 - INFO - joeynmt.training - Epoch  59, Step:     5550, Batch Loss:     0.280789, Batch Acc: 0.938425, Tokens per Sec:     3685, Lr: 0.000240\r\n",
      "2024-08-27 15:04:28,710 - INFO - joeynmt.training - Epoch  59, Step:     5555, Batch Loss:     0.324409, Batch Acc: 0.934211, Tokens per Sec:     4199, Lr: 0.000240\r\n",
      "2024-08-27 15:04:29,790 - INFO - joeynmt.training - Epoch  59, Step:     5560, Batch Loss:     0.271440, Batch Acc: 0.939798, Tokens per Sec:     3756, Lr: 0.000240\r\n",
      "2024-08-27 15:04:30,881 - INFO - joeynmt.training - Epoch  59, Step:     5565, Batch Loss:     0.286137, Batch Acc: 0.937844, Tokens per Sec:     3835, Lr: 0.000240\r\n",
      "2024-08-27 15:04:31,970 - INFO - joeynmt.training - Epoch  59, Step:     5570, Batch Loss:     0.252204, Batch Acc: 0.931823, Tokens per Sec:     3978, Lr: 0.000240\r\n",
      "2024-08-27 15:04:33,072 - INFO - joeynmt.training - Epoch  59, Step:     5575, Batch Loss:     0.284155, Batch Acc: 0.935476, Tokens per Sec:     3811, Lr: 0.000240\r\n",
      "2024-08-27 15:04:34,152 - INFO - joeynmt.training - Epoch  59, Step:     5580, Batch Loss:     0.294565, Batch Acc: 0.936615, Tokens per Sec:     3861, Lr: 0.000239\r\n",
      "2024-08-27 15:04:35,239 - INFO - joeynmt.training - Epoch  59, Step:     5585, Batch Loss:     0.315201, Batch Acc: 0.931772, Tokens per Sec:     4046, Lr: 0.000239\r\n",
      "2024-08-27 15:04:36,336 - INFO - joeynmt.training - Epoch  59, Step:     5590, Batch Loss:     0.333569, Batch Acc: 0.936097, Tokens per Sec:     3982, Lr: 0.000239\r\n",
      "2024-08-27 15:04:37,461 - INFO - joeynmt.training - Epoch  59, Step:     5595, Batch Loss:     0.277715, Batch Acc: 0.929584, Tokens per Sec:     3701, Lr: 0.000239\r\n",
      "2024-08-27 15:04:38,537 - INFO - joeynmt.training - Epoch  59, Step:     5600, Batch Loss:     0.297903, Batch Acc: 0.928090, Tokens per Sec:     3869, Lr: 0.000239\r\n",
      "2024-08-27 15:04:39,629 - INFO - joeynmt.training - Epoch  59, Step:     5605, Batch Loss:     0.300066, Batch Acc: 0.936412, Tokens per Sec:     3790, Lr: 0.000239\r\n",
      "2024-08-27 15:04:40,717 - INFO - joeynmt.training - Epoch  59, Step:     5610, Batch Loss:     0.323005, Batch Acc: 0.935143, Tokens per Sec:     3998, Lr: 0.000239\r\n",
      "2024-08-27 15:04:41,796 - INFO - joeynmt.training - Epoch  59, Step:     5615, Batch Loss:     0.291108, Batch Acc: 0.931663, Tokens per Sec:     4072, Lr: 0.000239\r\n",
      "2024-08-27 15:04:41,895 - INFO - joeynmt.training - Epoch  59, total training loss: 27.43, num. of seqs: 8065, num. of tokens: 80463, 20.8804[sec]\r\n",
      "2024-08-27 15:04:41,896 - INFO - joeynmt.training - EPOCH 60\r\n",
      "2024-08-27 15:04:42,994 - INFO - joeynmt.training - Epoch  60, Step:     5620, Batch Loss:     0.278201, Batch Acc: 0.942361, Tokens per Sec:     3947, Lr: 0.000239\r\n",
      "2024-08-27 15:04:44,090 - INFO - joeynmt.training - Epoch  60, Step:     5625, Batch Loss:     0.246335, Batch Acc: 0.947442, Tokens per Sec:     3908, Lr: 0.000239\r\n",
      "2024-08-27 15:04:45,226 - INFO - joeynmt.training - Epoch  60, Step:     5630, Batch Loss:     0.262676, Batch Acc: 0.942594, Tokens per Sec:     3729, Lr: 0.000238\r\n",
      "2024-08-27 15:04:46,447 - INFO - joeynmt.training - Epoch  60, Step:     5635, Batch Loss:     0.263469, Batch Acc: 0.943596, Tokens per Sec:     3487, Lr: 0.000238\r\n",
      "2024-08-27 15:04:47,552 - INFO - joeynmt.training - Epoch  60, Step:     5640, Batch Loss:     0.253427, Batch Acc: 0.942671, Tokens per Sec:     3885, Lr: 0.000238\r\n",
      "2024-08-27 15:04:48,634 - INFO - joeynmt.training - Epoch  60, Step:     5645, Batch Loss:     0.259885, Batch Acc: 0.938751, Tokens per Sec:     3928, Lr: 0.000238\r\n",
      "2024-08-27 15:04:49,749 - INFO - joeynmt.training - Epoch  60, Step:     5650, Batch Loss:     0.240300, Batch Acc: 0.944704, Tokens per Sec:     3846, Lr: 0.000238\r\n",
      "2024-08-27 15:04:50,839 - INFO - joeynmt.training - Epoch  60, Step:     5655, Batch Loss:     0.259035, Batch Acc: 0.940062, Tokens per Sec:     3828, Lr: 0.000238\r\n",
      "2024-08-27 15:04:51,931 - INFO - joeynmt.training - Epoch  60, Step:     5660, Batch Loss:     0.256815, Batch Acc: 0.940189, Tokens per Sec:     3877, Lr: 0.000238\r\n",
      "2024-08-27 15:04:53,013 - INFO - joeynmt.training - Epoch  60, Step:     5665, Batch Loss:     0.278082, Batch Acc: 0.935943, Tokens per Sec:     3897, Lr: 0.000238\r\n",
      "2024-08-27 15:04:54,124 - INFO - joeynmt.training - Epoch  60, Step:     5670, Batch Loss:     0.277903, Batch Acc: 0.934705, Tokens per Sec:     3767, Lr: 0.000238\r\n",
      "2024-08-27 15:04:55,219 - INFO - joeynmt.training - Epoch  60, Step:     5675, Batch Loss:     0.309940, Batch Acc: 0.938629, Tokens per Sec:     3945, Lr: 0.000237\r\n",
      "2024-08-27 15:04:56,348 - INFO - joeynmt.training - Epoch  60, Step:     5680, Batch Loss:     0.259971, Batch Acc: 0.944585, Tokens per Sec:     3856, Lr: 0.000237\r\n",
      "2024-08-27 15:04:57,499 - INFO - joeynmt.training - Epoch  60, Step:     5685, Batch Loss:     0.303351, Batch Acc: 0.935348, Tokens per Sec:     3709, Lr: 0.000237\r\n",
      "2024-08-27 15:04:58,622 - INFO - joeynmt.training - Epoch  60, Step:     5690, Batch Loss:     0.306930, Batch Acc: 0.938450, Tokens per Sec:     3636, Lr: 0.000237\r\n",
      "2024-08-27 15:04:59,744 - INFO - joeynmt.training - Epoch  60, Step:     5695, Batch Loss:     0.294377, Batch Acc: 0.932616, Tokens per Sec:     3810, Lr: 0.000237\r\n",
      "2024-08-27 15:05:00,847 - INFO - joeynmt.training - Epoch  60, Step:     5700, Batch Loss:     0.291454, Batch Acc: 0.933412, Tokens per Sec:     3827, Lr: 0.000237\r\n",
      "2024-08-27 15:05:01,957 - INFO - joeynmt.training - Epoch  60, Step:     5705, Batch Loss:     0.273022, Batch Acc: 0.939733, Tokens per Sec:     3787, Lr: 0.000237\r\n",
      "2024-08-27 15:05:03,060 - INFO - joeynmt.training - Epoch  60, Step:     5710, Batch Loss:     0.266872, Batch Acc: 0.932576, Tokens per Sec:     3671, Lr: 0.000237\r\n",
      "2024-08-27 15:05:03,061 - INFO - joeynmt.training - Epoch  60, total training loss: 26.17, num. of seqs: 8065, num. of tokens: 80463, 21.1480[sec]\r\n",
      "2024-08-27 15:05:03,061 - INFO - joeynmt.training - EPOCH 61\r\n",
      "2024-08-27 15:05:04,177 - INFO - joeynmt.training - Epoch  61, Step:     5715, Batch Loss:     0.235879, Batch Acc: 0.946288, Tokens per Sec:     3765, Lr: 0.000237\r\n",
      "2024-08-27 15:05:05,267 - INFO - joeynmt.training - Epoch  61, Step:     5720, Batch Loss:     0.264638, Batch Acc: 0.947673, Tokens per Sec:     3966, Lr: 0.000237\r\n",
      "2024-08-27 15:05:06,356 - INFO - joeynmt.training - Epoch  61, Step:     5725, Batch Loss:     0.246569, Batch Acc: 0.947174, Tokens per Sec:     3739, Lr: 0.000236\r\n",
      "2024-08-27 15:05:07,529 - INFO - joeynmt.training - Epoch  61, Step:     5730, Batch Loss:     0.228162, Batch Acc: 0.945048, Tokens per Sec:     3928, Lr: 0.000236\r\n",
      "2024-08-27 15:05:08,652 - INFO - joeynmt.training - Epoch  61, Step:     5735, Batch Loss:     0.269978, Batch Acc: 0.945209, Tokens per Sec:     3754, Lr: 0.000236\r\n",
      "2024-08-27 15:05:09,774 - INFO - joeynmt.training - Epoch  61, Step:     5740, Batch Loss:     0.245578, Batch Acc: 0.945714, Tokens per Sec:     3747, Lr: 0.000236\r\n",
      "2024-08-27 15:05:10,905 - INFO - joeynmt.training - Epoch  61, Step:     5745, Batch Loss:     0.277872, Batch Acc: 0.941831, Tokens per Sec:     3816, Lr: 0.000236\r\n",
      "2024-08-27 15:05:12,021 - INFO - joeynmt.training - Epoch  61, Step:     5750, Batch Loss:     0.258403, Batch Acc: 0.943777, Tokens per Sec:     3734, Lr: 0.000236\r\n",
      "2024-08-27 15:05:13,134 - INFO - joeynmt.training - Epoch  61, Step:     5755, Batch Loss:     0.292955, Batch Acc: 0.933936, Tokens per Sec:     3880, Lr: 0.000236\r\n",
      "2024-08-27 15:05:14,253 - INFO - joeynmt.training - Epoch  61, Step:     5760, Batch Loss:     0.266768, Batch Acc: 0.934307, Tokens per Sec:     3675, Lr: 0.000236\r\n",
      "2024-08-27 15:05:15,359 - INFO - joeynmt.training - Epoch  61, Step:     5765, Batch Loss:     0.274373, Batch Acc: 0.942174, Tokens per Sec:     3785, Lr: 0.000236\r\n",
      "2024-08-27 15:05:16,574 - INFO - joeynmt.training - Epoch  61, Step:     5770, Batch Loss:     0.252191, Batch Acc: 0.939839, Tokens per Sec:     3475, Lr: 0.000235\r\n",
      "2024-08-27 15:05:17,770 - INFO - joeynmt.training - Epoch  61, Step:     5775, Batch Loss:     0.282081, Batch Acc: 0.940108, Tokens per Sec:     3411, Lr: 0.000235\r\n",
      "2024-08-27 15:05:18,877 - INFO - joeynmt.training - Epoch  61, Step:     5780, Batch Loss:     0.291541, Batch Acc: 0.944378, Tokens per Sec:     3785, Lr: 0.000235\r\n",
      "2024-08-27 15:05:19,996 - INFO - joeynmt.training - Epoch  61, Step:     5785, Batch Loss:     0.247764, Batch Acc: 0.940617, Tokens per Sec:     3857, Lr: 0.000235\r\n",
      "2024-08-27 15:05:21,110 - INFO - joeynmt.training - Epoch  61, Step:     5790, Batch Loss:     0.283070, Batch Acc: 0.938649, Tokens per Sec:     3906, Lr: 0.000235\r\n",
      "2024-08-27 15:05:22,214 - INFO - joeynmt.training - Epoch  61, Step:     5795, Batch Loss:     0.314266, Batch Acc: 0.933256, Tokens per Sec:     3898, Lr: 0.000235\r\n",
      "2024-08-27 15:05:23,341 - INFO - joeynmt.training - Epoch  61, Step:     5800, Batch Loss:     0.286464, Batch Acc: 0.929128, Tokens per Sec:     3761, Lr: 0.000235\r\n",
      "2024-08-27 15:05:24,412 - INFO - joeynmt.training - Epoch  61, Step:     5805, Batch Loss:     0.318184, Batch Acc: 0.942655, Tokens per Sec:     3826, Lr: 0.000235\r\n",
      "2024-08-27 15:05:24,413 - INFO - joeynmt.training - Epoch  61, total training loss: 25.58, num. of seqs: 8065, num. of tokens: 80463, 21.3345[sec]\r\n",
      "2024-08-27 15:05:24,413 - INFO - joeynmt.training - EPOCH 62\r\n",
      "2024-08-27 15:05:25,490 - INFO - joeynmt.training - Epoch  62, Step:     5810, Batch Loss:     0.240225, Batch Acc: 0.949682, Tokens per Sec:     3816, Lr: 0.000235\r\n",
      "2024-08-27 15:05:26,596 - INFO - joeynmt.training - Epoch  62, Step:     5815, Batch Loss:     0.235494, Batch Acc: 0.948325, Tokens per Sec:     3832, Lr: 0.000235\r\n",
      "2024-08-27 15:05:27,721 - INFO - joeynmt.training - Epoch  62, Step:     5820, Batch Loss:     0.241427, Batch Acc: 0.941478, Tokens per Sec:     3649, Lr: 0.000234\r\n",
      "2024-08-27 15:05:28,832 - INFO - joeynmt.training - Epoch  62, Step:     5825, Batch Loss:     0.254983, Batch Acc: 0.949725, Tokens per Sec:     3923, Lr: 0.000234\r\n",
      "2024-08-27 15:05:29,927 - INFO - joeynmt.training - Epoch  62, Step:     5830, Batch Loss:     0.263583, Batch Acc: 0.945556, Tokens per Sec:     3928, Lr: 0.000234\r\n",
      "2024-08-27 15:05:31,031 - INFO - joeynmt.training - Epoch  62, Step:     5835, Batch Loss:     0.228055, Batch Acc: 0.945302, Tokens per Sec:     3878, Lr: 0.000234\r\n",
      "2024-08-27 15:05:32,102 - INFO - joeynmt.training - Epoch  62, Step:     5840, Batch Loss:     0.243919, Batch Acc: 0.936847, Tokens per Sec:     4022, Lr: 0.000234\r\n",
      "2024-08-27 15:05:33,167 - INFO - joeynmt.training - Epoch  62, Step:     5845, Batch Loss:     0.255636, Batch Acc: 0.943419, Tokens per Sec:     3871, Lr: 0.000234\r\n",
      "2024-08-27 15:05:34,230 - INFO - joeynmt.training - Epoch  62, Step:     5850, Batch Loss:     0.255568, Batch Acc: 0.946680, Tokens per Sec:     3881, Lr: 0.000234\r\n",
      "2024-08-27 15:05:35,328 - INFO - joeynmt.training - Epoch  62, Step:     5855, Batch Loss:     0.250624, Batch Acc: 0.945604, Tokens per Sec:     3887, Lr: 0.000234\r\n",
      "2024-08-27 15:05:36,418 - INFO - joeynmt.training - Epoch  62, Step:     5860, Batch Loss:     0.290871, Batch Acc: 0.938600, Tokens per Sec:     4068, Lr: 0.000234\r\n",
      "2024-08-27 15:05:37,541 - INFO - joeynmt.training - Epoch  62, Step:     5865, Batch Loss:     0.257463, Batch Acc: 0.939842, Tokens per Sec:     3732, Lr: 0.000234\r\n",
      "2024-08-27 15:05:38,634 - INFO - joeynmt.training - Epoch  62, Step:     5870, Batch Loss:     0.271123, Batch Acc: 0.943503, Tokens per Sec:     3889, Lr: 0.000233\r\n",
      "2024-08-27 15:05:39,719 - INFO - joeynmt.training - Epoch  62, Step:     5875, Batch Loss:     0.292692, Batch Acc: 0.941810, Tokens per Sec:     4025, Lr: 0.000233\r\n",
      "2024-08-27 15:05:40,799 - INFO - joeynmt.training - Epoch  62, Step:     5880, Batch Loss:     0.269581, Batch Acc: 0.937470, Tokens per Sec:     3810, Lr: 0.000233\r\n",
      "2024-08-27 15:05:41,879 - INFO - joeynmt.training - Epoch  62, Step:     5885, Batch Loss:     0.232903, Batch Acc: 0.943187, Tokens per Sec:     4011, Lr: 0.000233\r\n",
      "2024-08-27 15:05:42,936 - INFO - joeynmt.training - Epoch  62, Step:     5890, Batch Loss:     0.272901, Batch Acc: 0.939088, Tokens per Sec:     3837, Lr: 0.000233\r\n",
      "2024-08-27 15:05:44,027 - INFO - joeynmt.training - Epoch  62, Step:     5895, Batch Loss:     0.288616, Batch Acc: 0.941860, Tokens per Sec:     3946, Lr: 0.000233\r\n",
      "2024-08-27 15:05:45,117 - INFO - joeynmt.training - Epoch  62, Step:     5900, Batch Loss:     0.249818, Batch Acc: 0.940306, Tokens per Sec:     3905, Lr: 0.000233\r\n",
      "2024-08-27 15:05:45,117 - INFO - joeynmt.training - Epoch  62, total training loss: 24.85, num. of seqs: 8065, num. of tokens: 80463, 20.6879[sec]\r\n",
      "2024-08-27 15:05:45,118 - INFO - joeynmt.training - EPOCH 63\r\n",
      "2024-08-27 15:05:46,182 - INFO - joeynmt.training - Epoch  63, Step:     5905, Batch Loss:     0.251008, Batch Acc: 0.953000, Tokens per Sec:     3770, Lr: 0.000233\r\n",
      "2024-08-27 15:05:47,291 - INFO - joeynmt.training - Epoch  63, Step:     5910, Batch Loss:     0.258560, Batch Acc: 0.949257, Tokens per Sec:     3647, Lr: 0.000233\r\n",
      "2024-08-27 15:05:48,600 - INFO - joeynmt.training - Epoch  63, Step:     5915, Batch Loss:     0.263979, Batch Acc: 0.947826, Tokens per Sec:     3250, Lr: 0.000233\r\n",
      "2024-08-27 15:05:49,691 - INFO - joeynmt.training - Epoch  63, Step:     5920, Batch Loss:     0.247504, Batch Acc: 0.944724, Tokens per Sec:     3835, Lr: 0.000232\r\n",
      "2024-08-27 15:05:50,770 - INFO - joeynmt.training - Epoch  63, Step:     5925, Batch Loss:     0.222243, Batch Acc: 0.950942, Tokens per Sec:     3990, Lr: 0.000232\r\n",
      "2024-08-27 15:05:51,855 - INFO - joeynmt.training - Epoch  63, Step:     5930, Batch Loss:     0.253989, Batch Acc: 0.950696, Tokens per Sec:     3909, Lr: 0.000232\r\n",
      "2024-08-27 15:05:52,952 - INFO - joeynmt.training - Epoch  63, Step:     5935, Batch Loss:     0.243006, Batch Acc: 0.950216, Tokens per Sec:     4010, Lr: 0.000232\r\n",
      "2024-08-27 15:05:54,031 - INFO - joeynmt.training - Epoch  63, Step:     5940, Batch Loss:     0.266447, Batch Acc: 0.950363, Tokens per Sec:     3962, Lr: 0.000232\r\n",
      "2024-08-27 15:05:55,103 - INFO - joeynmt.training - Epoch  63, Step:     5945, Batch Loss:     0.252837, Batch Acc: 0.948600, Tokens per Sec:     3867, Lr: 0.000232\r\n",
      "2024-08-27 15:05:56,176 - INFO - joeynmt.training - Epoch  63, Step:     5950, Batch Loss:     0.281772, Batch Acc: 0.943187, Tokens per Sec:     3876, Lr: 0.000232\r\n",
      "2024-08-27 15:05:57,303 - INFO - joeynmt.training - Epoch  63, Step:     5955, Batch Loss:     0.218756, Batch Acc: 0.941889, Tokens per Sec:     3955, Lr: 0.000232\r\n",
      "2024-08-27 15:05:58,380 - INFO - joeynmt.training - Epoch  63, Step:     5960, Batch Loss:     0.254290, Batch Acc: 0.946251, Tokens per Sec:     4027, Lr: 0.000232\r\n",
      "2024-08-27 15:05:59,462 - INFO - joeynmt.training - Epoch  63, Step:     5965, Batch Loss:     0.249255, Batch Acc: 0.939897, Tokens per Sec:     3787, Lr: 0.000232\r\n",
      "2024-08-27 15:06:00,533 - INFO - joeynmt.training - Epoch  63, Step:     5970, Batch Loss:     0.306598, Batch Acc: 0.934705, Tokens per Sec:     3948, Lr: 0.000232\r\n",
      "2024-08-27 15:06:01,596 - INFO - joeynmt.training - Epoch  63, Step:     5975, Batch Loss:     0.263969, Batch Acc: 0.944747, Tokens per Sec:     3972, Lr: 0.000231\r\n",
      "2024-08-27 15:06:02,669 - INFO - joeynmt.training - Epoch  63, Step:     5980, Batch Loss:     0.260731, Batch Acc: 0.947100, Tokens per Sec:     4017, Lr: 0.000231\r\n",
      "2024-08-27 15:06:03,769 - INFO - joeynmt.training - Epoch  63, Step:     5985, Batch Loss:     0.279235, Batch Acc: 0.934094, Tokens per Sec:     3989, Lr: 0.000231\r\n",
      "2024-08-27 15:06:04,820 - INFO - joeynmt.training - Epoch  63, Step:     5990, Batch Loss:     0.280940, Batch Acc: 0.943196, Tokens per Sec:     3771, Lr: 0.000231\r\n",
      "2024-08-27 15:06:05,903 - INFO - joeynmt.training - Epoch  63, Step:     5995, Batch Loss:     0.287849, Batch Acc: 0.937456, Tokens per Sec:     3917, Lr: 0.000231\r\n",
      "2024-08-27 15:06:06,002 - INFO - joeynmt.training - Epoch  63, total training loss: 24.20, num. of seqs: 8065, num. of tokens: 80463, 20.8681[sec]\r\n",
      "2024-08-27 15:06:06,002 - INFO - joeynmt.training - EPOCH 64\r\n",
      "2024-08-27 15:06:07,126 - INFO - joeynmt.training - Epoch  64, Step:     6000, Batch Loss:     0.254491, Batch Acc: 0.953933, Tokens per Sec:     3779, Lr: 0.000231\r\n",
      "2024-08-27 15:06:07,127 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=6042\r\n",
      "2024-08-27 15:06:07,127 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:23<00:00, 61.98it/s]\r\n",
      "2024-08-27 15:06:30,686 - INFO - joeynmt.prediction - Generation took 23.5570[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 45421.77 examples/s]\r\n",
      "2024-08-27 15:06:31,067 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:06:31,067 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   4.28, loss:   6.64, ppl: 768.66, acc:   0.28, 0.1657[sec]\r\n",
      "2024-08-27 15:06:31,068 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\r\n",
      "2024-08-27 15:06:31,386 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/6000.ckpt.\r\n",
      "2024-08-27 15:06:31,386 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/4000.ckpt\r\n",
      "2024-08-27 15:06:31,410 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:06:31,558 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:06:31,558 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:06:31,558 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 15:06:31,850 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:06:31,997 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:06:31,997 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:06:31,997 - INFO - joeynmt.training - \tHypothesis: trois éléphants\r\n",
      "2024-08-27 15:06:32,296 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:06:32,441 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:06:32,441 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:06:32,442 - INFO - joeynmt.training - \tHypothesis: cependant enjolras à cet amendement\r\n",
      "2024-08-27 15:06:32,732 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:06:32,878 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:06:32,878 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:06:32,878 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 15:06:34,241 - INFO - joeynmt.training - Epoch  64, Step:     6005, Batch Loss:     0.262495, Batch Acc: 0.953204, Tokens per Sec:     4057, Lr: 0.000231\r\n",
      "2024-08-27 15:06:35,307 - INFO - joeynmt.training - Epoch  64, Step:     6010, Batch Loss:     0.223591, Batch Acc: 0.950294, Tokens per Sec:     3984, Lr: 0.000231\r\n",
      "2024-08-27 15:06:36,388 - INFO - joeynmt.training - Epoch  64, Step:     6015, Batch Loss:     0.226090, Batch Acc: 0.948923, Tokens per Sec:     4041, Lr: 0.000231\r\n",
      "2024-08-27 15:06:37,489 - INFO - joeynmt.training - Epoch  64, Step:     6020, Batch Loss:     0.233320, Batch Acc: 0.954303, Tokens per Sec:     3915, Lr: 0.000231\r\n",
      "2024-08-27 15:06:38,558 - INFO - joeynmt.training - Epoch  64, Step:     6025, Batch Loss:     0.247961, Batch Acc: 0.953136, Tokens per Sec:     3955, Lr: 0.000230\r\n",
      "2024-08-27 15:06:39,632 - INFO - joeynmt.training - Epoch  64, Step:     6030, Batch Loss:     0.234966, Batch Acc: 0.951677, Tokens per Sec:     3972, Lr: 0.000230\r\n",
      "2024-08-27 15:06:40,726 - INFO - joeynmt.training - Epoch  64, Step:     6035, Batch Loss:     0.238797, Batch Acc: 0.944175, Tokens per Sec:     3770, Lr: 0.000230\r\n",
      "2024-08-27 15:06:41,820 - INFO - joeynmt.training - Epoch  64, Step:     6040, Batch Loss:     0.244812, Batch Acc: 0.947621, Tokens per Sec:     3997, Lr: 0.000230\r\n",
      "2024-08-27 15:06:42,899 - INFO - joeynmt.training - Epoch  64, Step:     6045, Batch Loss:     0.242045, Batch Acc: 0.944266, Tokens per Sec:     3762, Lr: 0.000230\r\n",
      "2024-08-27 15:06:44,001 - INFO - joeynmt.training - Epoch  64, Step:     6050, Batch Loss:     0.263558, Batch Acc: 0.946551, Tokens per Sec:     3855, Lr: 0.000230\r\n",
      "2024-08-27 15:06:45,089 - INFO - joeynmt.training - Epoch  64, Step:     6055, Batch Loss:     0.263145, Batch Acc: 0.947025, Tokens per Sec:     3940, Lr: 0.000230\r\n",
      "2024-08-27 15:06:46,187 - INFO - joeynmt.training - Epoch  64, Step:     6060, Batch Loss:     0.256867, Batch Acc: 0.946466, Tokens per Sec:     3883, Lr: 0.000230\r\n",
      "2024-08-27 15:06:47,274 - INFO - joeynmt.training - Epoch  64, Step:     6065, Batch Loss:     0.285213, Batch Acc: 0.943522, Tokens per Sec:     3601, Lr: 0.000230\r\n",
      "2024-08-27 15:06:48,341 - INFO - joeynmt.training - Epoch  64, Step:     6070, Batch Loss:     0.228004, Batch Acc: 0.945381, Tokens per Sec:     3950, Lr: 0.000230\r\n",
      "2024-08-27 15:06:49,427 - INFO - joeynmt.training - Epoch  64, Step:     6075, Batch Loss:     0.256963, Batch Acc: 0.944739, Tokens per Sec:     4000, Lr: 0.000230\r\n",
      "2024-08-27 15:06:50,623 - INFO - joeynmt.training - Epoch  64, Step:     6080, Batch Loss:     0.261086, Batch Acc: 0.940931, Tokens per Sec:     3611, Lr: 0.000229\r\n",
      "2024-08-27 15:06:51,751 - INFO - joeynmt.training - Epoch  64, Step:     6085, Batch Loss:     0.268876, Batch Acc: 0.948853, Tokens per Sec:     3714, Lr: 0.000229\r\n",
      "2024-08-27 15:06:52,816 - INFO - joeynmt.training - Epoch  64, Step:     6090, Batch Loss:     0.295983, Batch Acc: 0.938436, Tokens per Sec:     3891, Lr: 0.000229\r\n",
      "2024-08-27 15:06:52,860 - INFO - joeynmt.training - Epoch  64, total training loss: 23.55, num. of seqs: 8065, num. of tokens: 80463, 20.7963[sec]\r\n",
      "2024-08-27 15:06:52,860 - INFO - joeynmt.training - EPOCH 65\r\n",
      "2024-08-27 15:06:53,934 - INFO - joeynmt.training - Epoch  65, Step:     6095, Batch Loss:     0.213671, Batch Acc: 0.952833, Tokens per Sec:     3745, Lr: 0.000229\r\n",
      "2024-08-27 15:06:55,007 - INFO - joeynmt.training - Epoch  65, Step:     6100, Batch Loss:     0.185769, Batch Acc: 0.957321, Tokens per Sec:     4108, Lr: 0.000229\r\n",
      "2024-08-27 15:06:56,102 - INFO - joeynmt.training - Epoch  65, Step:     6105, Batch Loss:     0.194722, Batch Acc: 0.948960, Tokens per Sec:     3869, Lr: 0.000229\r\n",
      "2024-08-27 15:06:57,222 - INFO - joeynmt.training - Epoch  65, Step:     6110, Batch Loss:     0.241434, Batch Acc: 0.957396, Tokens per Sec:     3773, Lr: 0.000229\r\n",
      "2024-08-27 15:06:58,300 - INFO - joeynmt.training - Epoch  65, Step:     6115, Batch Loss:     0.250837, Batch Acc: 0.950602, Tokens per Sec:     3855, Lr: 0.000229\r\n",
      "2024-08-27 15:06:59,361 - INFO - joeynmt.training - Epoch  65, Step:     6120, Batch Loss:     0.229136, Batch Acc: 0.951081, Tokens per Sec:     4144, Lr: 0.000229\r\n",
      "2024-08-27 15:07:00,438 - INFO - joeynmt.training - Epoch  65, Step:     6125, Batch Loss:     0.237409, Batch Acc: 0.953229, Tokens per Sec:     3914, Lr: 0.000229\r\n",
      "2024-08-27 15:07:01,556 - INFO - joeynmt.training - Epoch  65, Step:     6130, Batch Loss:     0.245600, Batch Acc: 0.950356, Tokens per Sec:     3517, Lr: 0.000228\r\n",
      "2024-08-27 15:07:02,637 - INFO - joeynmt.training - Epoch  65, Step:     6135, Batch Loss:     0.231164, Batch Acc: 0.951750, Tokens per Sec:     3913, Lr: 0.000228\r\n",
      "2024-08-27 15:07:03,717 - INFO - joeynmt.training - Epoch  65, Step:     6140, Batch Loss:     0.258198, Batch Acc: 0.950156, Tokens per Sec:     3867, Lr: 0.000228\r\n",
      "2024-08-27 15:07:04,784 - INFO - joeynmt.training - Epoch  65, Step:     6145, Batch Loss:     0.217966, Batch Acc: 0.945140, Tokens per Sec:     3742, Lr: 0.000228\r\n",
      "2024-08-27 15:07:05,879 - INFO - joeynmt.training - Epoch  65, Step:     6150, Batch Loss:     0.243958, Batch Acc: 0.954299, Tokens per Sec:     3879, Lr: 0.000228\r\n",
      "2024-08-27 15:07:06,984 - INFO - joeynmt.training - Epoch  65, Step:     6155, Batch Loss:     0.242233, Batch Acc: 0.942254, Tokens per Sec:     3858, Lr: 0.000228\r\n",
      "2024-08-27 15:07:08,065 - INFO - joeynmt.training - Epoch  65, Step:     6160, Batch Loss:     0.261620, Batch Acc: 0.946106, Tokens per Sec:     4056, Lr: 0.000228\r\n",
      "2024-08-27 15:07:09,182 - INFO - joeynmt.training - Epoch  65, Step:     6165, Batch Loss:     0.243552, Batch Acc: 0.947690, Tokens per Sec:     3953, Lr: 0.000228\r\n",
      "2024-08-27 15:07:10,249 - INFO - joeynmt.training - Epoch  65, Step:     6170, Batch Loss:     0.247386, Batch Acc: 0.941761, Tokens per Sec:     3962, Lr: 0.000228\r\n",
      "2024-08-27 15:07:11,334 - INFO - joeynmt.training - Epoch  65, Step:     6175, Batch Loss:     0.279131, Batch Acc: 0.938542, Tokens per Sec:     3872, Lr: 0.000228\r\n",
      "2024-08-27 15:07:12,427 - INFO - joeynmt.training - Epoch  65, Step:     6180, Batch Loss:     0.291411, Batch Acc: 0.948070, Tokens per Sec:     4055, Lr: 0.000228\r\n",
      "2024-08-27 15:07:13,516 - INFO - joeynmt.training - Epoch  65, Step:     6185, Batch Loss:     0.288395, Batch Acc: 0.942184, Tokens per Sec:     3702, Lr: 0.000227\r\n",
      "2024-08-27 15:07:13,620 - INFO - joeynmt.training - Epoch  65, total training loss: 22.80, num. of seqs: 8065, num. of tokens: 80463, 20.7418[sec]\r\n",
      "2024-08-27 15:07:13,620 - INFO - joeynmt.training - EPOCH 66\r\n",
      "2024-08-27 15:07:14,713 - INFO - joeynmt.training - Epoch  66, Step:     6190, Batch Loss:     0.216332, Batch Acc: 0.953756, Tokens per Sec:     3911, Lr: 0.000227\r\n",
      "2024-08-27 15:07:15,789 - INFO - joeynmt.training - Epoch  66, Step:     6195, Batch Loss:     0.224605, Batch Acc: 0.958070, Tokens per Sec:     3972, Lr: 0.000227\r\n",
      "2024-08-27 15:07:16,888 - INFO - joeynmt.training - Epoch  66, Step:     6200, Batch Loss:     0.208674, Batch Acc: 0.956501, Tokens per Sec:     3768, Lr: 0.000227\r\n",
      "2024-08-27 15:07:17,960 - INFO - joeynmt.training - Epoch  66, Step:     6205, Batch Loss:     0.235564, Batch Acc: 0.954461, Tokens per Sec:     4018, Lr: 0.000227\r\n",
      "2024-08-27 15:07:19,025 - INFO - joeynmt.training - Epoch  66, Step:     6210, Batch Loss:     0.222660, Batch Acc: 0.951784, Tokens per Sec:     3896, Lr: 0.000227\r\n",
      "2024-08-27 15:07:20,085 - INFO - joeynmt.training - Epoch  66, Step:     6215, Batch Loss:     0.255225, Batch Acc: 0.955067, Tokens per Sec:     3867, Lr: 0.000227\r\n",
      "2024-08-27 15:07:21,169 - INFO - joeynmt.training - Epoch  66, Step:     6220, Batch Loss:     0.205100, Batch Acc: 0.952244, Tokens per Sec:     3846, Lr: 0.000227\r\n",
      "2024-08-27 15:07:22,323 - INFO - joeynmt.training - Epoch  66, Step:     6225, Batch Loss:     0.230284, Batch Acc: 0.954409, Tokens per Sec:     3766, Lr: 0.000227\r\n",
      "2024-08-27 15:07:23,430 - INFO - joeynmt.training - Epoch  66, Step:     6230, Batch Loss:     0.241590, Batch Acc: 0.950462, Tokens per Sec:     4012, Lr: 0.000227\r\n",
      "2024-08-27 15:07:24,486 - INFO - joeynmt.training - Epoch  66, Step:     6235, Batch Loss:     0.242640, Batch Acc: 0.949804, Tokens per Sec:     3870, Lr: 0.000227\r\n",
      "2024-08-27 15:07:25,554 - INFO - joeynmt.training - Epoch  66, Step:     6240, Batch Loss:     0.224171, Batch Acc: 0.947282, Tokens per Sec:     4000, Lr: 0.000226\r\n",
      "2024-08-27 15:07:26,620 - INFO - joeynmt.training - Epoch  66, Step:     6245, Batch Loss:     0.235836, Batch Acc: 0.948515, Tokens per Sec:     3920, Lr: 0.000226\r\n",
      "2024-08-27 15:07:27,727 - INFO - joeynmt.training - Epoch  66, Step:     6250, Batch Loss:     0.245092, Batch Acc: 0.949834, Tokens per Sec:     3822, Lr: 0.000226\r\n",
      "2024-08-27 15:07:28,817 - INFO - joeynmt.training - Epoch  66, Step:     6255, Batch Loss:     0.230568, Batch Acc: 0.951386, Tokens per Sec:     3776, Lr: 0.000226\r\n",
      "2024-08-27 15:07:29,887 - INFO - joeynmt.training - Epoch  66, Step:     6260, Batch Loss:     0.207600, Batch Acc: 0.950404, Tokens per Sec:     4056, Lr: 0.000226\r\n",
      "2024-08-27 15:07:30,945 - INFO - joeynmt.training - Epoch  66, Step:     6265, Batch Loss:     0.252772, Batch Acc: 0.949453, Tokens per Sec:     4058, Lr: 0.000226\r\n",
      "2024-08-27 15:07:31,991 - INFO - joeynmt.training - Epoch  66, Step:     6270, Batch Loss:     0.253707, Batch Acc: 0.945348, Tokens per Sec:     3764, Lr: 0.000226\r\n",
      "2024-08-27 15:07:33,086 - INFO - joeynmt.training - Epoch  66, Step:     6275, Batch Loss:     0.237850, Batch Acc: 0.945683, Tokens per Sec:     3852, Lr: 0.000226\r\n",
      "2024-08-27 15:07:34,182 - INFO - joeynmt.training - Epoch  66, Step:     6280, Batch Loss:     0.210879, Batch Acc: 0.952497, Tokens per Sec:     3749, Lr: 0.000226\r\n",
      "2024-08-27 15:07:34,336 - INFO - joeynmt.training - Epoch  66, total training loss: 22.13, num. of seqs: 8065, num. of tokens: 80463, 20.6979[sec]\r\n",
      "2024-08-27 15:07:34,336 - INFO - joeynmt.training - EPOCH 67\r\n",
      "2024-08-27 15:07:35,462 - INFO - joeynmt.training - Epoch  67, Step:     6285, Batch Loss:     0.230127, Batch Acc: 0.956796, Tokens per Sec:     3672, Lr: 0.000226\r\n",
      "2024-08-27 15:07:36,543 - INFO - joeynmt.training - Epoch  67, Step:     6290, Batch Loss:     0.230150, Batch Acc: 0.954655, Tokens per Sec:     3839, Lr: 0.000226\r\n",
      "2024-08-27 15:07:37,692 - INFO - joeynmt.training - Epoch  67, Step:     6295, Batch Loss:     0.207353, Batch Acc: 0.956607, Tokens per Sec:     3531, Lr: 0.000225\r\n",
      "2024-08-27 15:07:38,778 - INFO - joeynmt.training - Epoch  67, Step:     6300, Batch Loss:     0.235706, Batch Acc: 0.951214, Tokens per Sec:     3835, Lr: 0.000225\r\n",
      "2024-08-27 15:07:39,874 - INFO - joeynmt.training - Epoch  67, Step:     6305, Batch Loss:     0.199940, Batch Acc: 0.960056, Tokens per Sec:     3885, Lr: 0.000225\r\n",
      "2024-08-27 15:07:40,952 - INFO - joeynmt.training - Epoch  67, Step:     6310, Batch Loss:     0.232165, Batch Acc: 0.950952, Tokens per Sec:     3898, Lr: 0.000225\r\n",
      "2024-08-27 15:07:42,030 - INFO - joeynmt.training - Epoch  67, Step:     6315, Batch Loss:     0.221650, Batch Acc: 0.954296, Tokens per Sec:     3717, Lr: 0.000225\r\n",
      "2024-08-27 15:07:43,107 - INFO - joeynmt.training - Epoch  67, Step:     6320, Batch Loss:     0.247492, Batch Acc: 0.947851, Tokens per Sec:     3957, Lr: 0.000225\r\n",
      "2024-08-27 15:07:44,207 - INFO - joeynmt.training - Epoch  67, Step:     6325, Batch Loss:     0.239121, Batch Acc: 0.953494, Tokens per Sec:     3813, Lr: 0.000225\r\n",
      "2024-08-27 15:07:45,283 - INFO - joeynmt.training - Epoch  67, Step:     6330, Batch Loss:     0.277196, Batch Acc: 0.948398, Tokens per Sec:     3947, Lr: 0.000225\r\n",
      "2024-08-27 15:07:46,334 - INFO - joeynmt.training - Epoch  67, Step:     6335, Batch Loss:     0.245325, Batch Acc: 0.950742, Tokens per Sec:     4042, Lr: 0.000225\r\n",
      "2024-08-27 15:07:47,447 - INFO - joeynmt.training - Epoch  67, Step:     6340, Batch Loss:     0.224937, Batch Acc: 0.951617, Tokens per Sec:     3641, Lr: 0.000225\r\n",
      "2024-08-27 15:07:48,534 - INFO - joeynmt.training - Epoch  67, Step:     6345, Batch Loss:     0.238488, Batch Acc: 0.952211, Tokens per Sec:     4120, Lr: 0.000225\r\n",
      "2024-08-27 15:07:49,620 - INFO - joeynmt.training - Epoch  67, Step:     6350, Batch Loss:     0.267283, Batch Acc: 0.949203, Tokens per Sec:     3701, Lr: 0.000224\r\n",
      "2024-08-27 15:07:50,697 - INFO - joeynmt.training - Epoch  67, Step:     6355, Batch Loss:     0.221647, Batch Acc: 0.953850, Tokens per Sec:     4006, Lr: 0.000224\r\n",
      "2024-08-27 15:07:51,787 - INFO - joeynmt.training - Epoch  67, Step:     6360, Batch Loss:     0.238668, Batch Acc: 0.948754, Tokens per Sec:     3869, Lr: 0.000224\r\n",
      "2024-08-27 15:07:52,971 - INFO - joeynmt.training - Epoch  67, Step:     6365, Batch Loss:     0.279158, Batch Acc: 0.945874, Tokens per Sec:     3515, Lr: 0.000224\r\n",
      "2024-08-27 15:07:54,089 - INFO - joeynmt.training - Epoch  67, Step:     6370, Batch Loss:     0.217543, Batch Acc: 0.954355, Tokens per Sec:     3842, Lr: 0.000224\r\n",
      "2024-08-27 15:07:55,181 - INFO - joeynmt.training - Epoch  67, Step:     6375, Batch Loss:     0.230381, Batch Acc: 0.954713, Tokens per Sec:     3984, Lr: 0.000224\r\n",
      "2024-08-27 15:07:55,441 - INFO - joeynmt.training - Epoch  67, total training loss: 22.12, num. of seqs: 8065, num. of tokens: 80463, 21.0878[sec]\r\n",
      "2024-08-27 15:07:55,441 - INFO - joeynmt.training - EPOCH 68\r\n",
      "2024-08-27 15:07:56,357 - INFO - joeynmt.training - Epoch  68, Step:     6380, Batch Loss:     0.215908, Batch Acc: 0.958666, Tokens per Sec:     3979, Lr: 0.000224\r\n",
      "2024-08-27 15:07:57,463 - INFO - joeynmt.training - Epoch  68, Step:     6385, Batch Loss:     0.199774, Batch Acc: 0.958146, Tokens per Sec:     4020, Lr: 0.000224\r\n",
      "2024-08-27 15:07:58,558 - INFO - joeynmt.training - Epoch  68, Step:     6390, Batch Loss:     0.229008, Batch Acc: 0.955990, Tokens per Sec:     3884, Lr: 0.000224\r\n",
      "2024-08-27 15:07:59,637 - INFO - joeynmt.training - Epoch  68, Step:     6395, Batch Loss:     0.214809, Batch Acc: 0.956987, Tokens per Sec:     3818, Lr: 0.000224\r\n",
      "2024-08-27 15:08:00,703 - INFO - joeynmt.training - Epoch  68, Step:     6400, Batch Loss:     0.219488, Batch Acc: 0.957212, Tokens per Sec:     3903, Lr: 0.000224\r\n",
      "2024-08-27 15:08:01,782 - INFO - joeynmt.training - Epoch  68, Step:     6405, Batch Loss:     0.222203, Batch Acc: 0.958275, Tokens per Sec:     3956, Lr: 0.000224\r\n",
      "2024-08-27 15:08:02,882 - INFO - joeynmt.training - Epoch  68, Step:     6410, Batch Loss:     0.223946, Batch Acc: 0.955611, Tokens per Sec:     4038, Lr: 0.000223\r\n",
      "2024-08-27 15:08:03,966 - INFO - joeynmt.training - Epoch  68, Step:     6415, Batch Loss:     0.239207, Batch Acc: 0.950932, Tokens per Sec:     3914, Lr: 0.000223\r\n",
      "2024-08-27 15:08:05,031 - INFO - joeynmt.training - Epoch  68, Step:     6420, Batch Loss:     0.250666, Batch Acc: 0.954448, Tokens per Sec:     3958, Lr: 0.000223\r\n",
      "2024-08-27 15:08:06,120 - INFO - joeynmt.training - Epoch  68, Step:     6425, Batch Loss:     0.212644, Batch Acc: 0.951729, Tokens per Sec:     3958, Lr: 0.000223\r\n",
      "2024-08-27 15:08:07,241 - INFO - joeynmt.training - Epoch  68, Step:     6430, Batch Loss:     0.229681, Batch Acc: 0.948096, Tokens per Sec:     3939, Lr: 0.000223\r\n",
      "2024-08-27 15:08:08,324 - INFO - joeynmt.training - Epoch  68, Step:     6435, Batch Loss:     0.265214, Batch Acc: 0.947696, Tokens per Sec:     4011, Lr: 0.000223\r\n",
      "2024-08-27 15:08:09,372 - INFO - joeynmt.training - Epoch  68, Step:     6440, Batch Loss:     0.212370, Batch Acc: 0.953077, Tokens per Sec:     3724, Lr: 0.000223\r\n",
      "2024-08-27 15:08:10,442 - INFO - joeynmt.training - Epoch  68, Step:     6445, Batch Loss:     0.228454, Batch Acc: 0.956680, Tokens per Sec:     3863, Lr: 0.000223\r\n",
      "2024-08-27 15:08:11,517 - INFO - joeynmt.training - Epoch  68, Step:     6450, Batch Loss:     0.254996, Batch Acc: 0.956715, Tokens per Sec:     3762, Lr: 0.000223\r\n",
      "2024-08-27 15:08:12,585 - INFO - joeynmt.training - Epoch  68, Step:     6455, Batch Loss:     0.226955, Batch Acc: 0.955659, Tokens per Sec:     3824, Lr: 0.000223\r\n",
      "2024-08-27 15:08:13,654 - INFO - joeynmt.training - Epoch  68, Step:     6460, Batch Loss:     0.209913, Batch Acc: 0.955224, Tokens per Sec:     3890, Lr: 0.000223\r\n",
      "2024-08-27 15:08:14,716 - INFO - joeynmt.training - Epoch  68, Step:     6465, Batch Loss:     0.224412, Batch Acc: 0.948967, Tokens per Sec:     3877, Lr: 0.000222\r\n",
      "2024-08-27 15:08:15,778 - INFO - joeynmt.training - Epoch  68, Step:     6470, Batch Loss:     0.236057, Batch Acc: 0.949541, Tokens per Sec:     4107, Lr: 0.000222\r\n",
      "2024-08-27 15:08:15,993 - INFO - joeynmt.training - Epoch  68, total training loss: 21.15, num. of seqs: 8065, num. of tokens: 80463, 20.5352[sec]\r\n",
      "2024-08-27 15:08:15,993 - INFO - joeynmt.training - EPOCH 69\r\n",
      "2024-08-27 15:08:16,893 - INFO - joeynmt.training - Epoch  69, Step:     6475, Batch Loss:     0.175524, Batch Acc: 0.958405, Tokens per Sec:     3914, Lr: 0.000222\r\n",
      "2024-08-27 15:08:17,960 - INFO - joeynmt.training - Epoch  69, Step:     6480, Batch Loss:     0.198755, Batch Acc: 0.961008, Tokens per Sec:     3874, Lr: 0.000222\r\n",
      "2024-08-27 15:08:19,036 - INFO - joeynmt.training - Epoch  69, Step:     6485, Batch Loss:     0.224182, Batch Acc: 0.954045, Tokens per Sec:     3884, Lr: 0.000222\r\n",
      "2024-08-27 15:08:20,110 - INFO - joeynmt.training - Epoch  69, Step:     6490, Batch Loss:     0.226016, Batch Acc: 0.957362, Tokens per Sec:     3956, Lr: 0.000222\r\n",
      "2024-08-27 15:08:21,185 - INFO - joeynmt.training - Epoch  69, Step:     6495, Batch Loss:     0.217616, Batch Acc: 0.954853, Tokens per Sec:     4122, Lr: 0.000222\r\n",
      "2024-08-27 15:08:22,269 - INFO - joeynmt.training - Epoch  69, Step:     6500, Batch Loss:     0.201597, Batch Acc: 0.959415, Tokens per Sec:     3912, Lr: 0.000222\r\n",
      "2024-08-27 15:08:22,270 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=6542\r\n",
      "2024-08-27 15:08:22,270 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 63.54it/s]\r\n",
      "2024-08-27 15:08:45,251 - INFO - joeynmt.prediction - Generation took 22.9783[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 42797.54 examples/s]\r\n",
      "2024-08-27 15:08:45,628 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:08:45,628 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   4.73, loss:   6.73, ppl: 834.30, acc:   0.28, 0.1627[sec]\r\n",
      "2024-08-27 15:08:45,629 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\r\n",
      "2024-08-27 15:08:45,943 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/6500.ckpt.\r\n",
      "2024-08-27 15:08:45,944 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/5000.ckpt\r\n",
      "2024-08-27 15:08:45,968 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:08:46,116 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:08:46,116 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:08:46,116 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 15:08:46,405 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:08:46,553 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:08:46,554 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:08:46,554 - INFO - joeynmt.training - \tHypothesis: trois tables blanches\r\n",
      "2024-08-27 15:08:46,862 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:08:47,007 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:08:47,007 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:08:47,007 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 15:08:47,296 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:08:47,444 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:08:47,444 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:08:47,444 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 15:08:48,831 - INFO - joeynmt.training - Epoch  69, Step:     6505, Batch Loss:     0.221341, Batch Acc: 0.954397, Tokens per Sec:     3933, Lr: 0.000222\r\n",
      "2024-08-27 15:08:49,918 - INFO - joeynmt.training - Epoch  69, Step:     6510, Batch Loss:     0.243029, Batch Acc: 0.953087, Tokens per Sec:     3785, Lr: 0.000222\r\n",
      "2024-08-27 15:08:50,991 - INFO - joeynmt.training - Epoch  69, Step:     6515, Batch Loss:     0.215910, Batch Acc: 0.948011, Tokens per Sec:     3894, Lr: 0.000222\r\n",
      "2024-08-27 15:08:52,055 - INFO - joeynmt.training - Epoch  69, Step:     6520, Batch Loss:     0.216943, Batch Acc: 0.948919, Tokens per Sec:     4087, Lr: 0.000222\r\n",
      "2024-08-27 15:08:53,116 - INFO - joeynmt.training - Epoch  69, Step:     6525, Batch Loss:     0.220097, Batch Acc: 0.955387, Tokens per Sec:     3976, Lr: 0.000221\r\n",
      "2024-08-27 15:08:54,187 - INFO - joeynmt.training - Epoch  69, Step:     6530, Batch Loss:     0.194987, Batch Acc: 0.952595, Tokens per Sec:     3940, Lr: 0.000221\r\n",
      "2024-08-27 15:08:55,324 - INFO - joeynmt.training - Epoch  69, Step:     6535, Batch Loss:     0.254418, Batch Acc: 0.951714, Tokens per Sec:     3517, Lr: 0.000221\r\n",
      "2024-08-27 15:08:56,459 - INFO - joeynmt.training - Epoch  69, Step:     6540, Batch Loss:     0.268014, Batch Acc: 0.950395, Tokens per Sec:     3680, Lr: 0.000221\r\n",
      "2024-08-27 15:08:57,595 - INFO - joeynmt.training - Epoch  69, Step:     6545, Batch Loss:     0.258087, Batch Acc: 0.945706, Tokens per Sec:     3764, Lr: 0.000221\r\n",
      "2024-08-27 15:08:58,707 - INFO - joeynmt.training - Epoch  69, Step:     6550, Batch Loss:     0.241626, Batch Acc: 0.947136, Tokens per Sec:     3863, Lr: 0.000221\r\n",
      "2024-08-27 15:08:59,870 - INFO - joeynmt.training - Epoch  69, Step:     6555, Batch Loss:     0.254673, Batch Acc: 0.950244, Tokens per Sec:     3528, Lr: 0.000221\r\n",
      "2024-08-27 15:09:01,014 - INFO - joeynmt.training - Epoch  69, Step:     6560, Batch Loss:     0.232968, Batch Acc: 0.946513, Tokens per Sec:     3501, Lr: 0.000221\r\n",
      "2024-08-27 15:09:02,115 - INFO - joeynmt.training - Epoch  69, Step:     6565, Batch Loss:     0.218358, Batch Acc: 0.953607, Tokens per Sec:     4092, Lr: 0.000221\r\n",
      "2024-08-27 15:09:02,397 - INFO - joeynmt.training - Epoch  69, total training loss: 21.28, num. of seqs: 8065, num. of tokens: 80463, 20.9197[sec]\r\n",
      "2024-08-27 15:09:02,398 - INFO - joeynmt.training - EPOCH 70\r\n",
      "2024-08-27 15:09:03,290 - INFO - joeynmt.training - Epoch  70, Step:     6570, Batch Loss:     0.200682, Batch Acc: 0.958273, Tokens per Sec:     3885, Lr: 0.000221\r\n",
      "2024-08-27 15:09:04,366 - INFO - joeynmt.training - Epoch  70, Step:     6575, Batch Loss:     0.187067, Batch Acc: 0.958090, Tokens per Sec:     3815, Lr: 0.000221\r\n",
      "2024-08-27 15:09:05,441 - INFO - joeynmt.training - Epoch  70, Step:     6580, Batch Loss:     0.214669, Batch Acc: 0.960912, Tokens per Sec:     4004, Lr: 0.000221\r\n",
      "2024-08-27 15:09:06,528 - INFO - joeynmt.training - Epoch  70, Step:     6585, Batch Loss:     0.218857, Batch Acc: 0.960905, Tokens per Sec:     4024, Lr: 0.000220\r\n",
      "2024-08-27 15:09:07,663 - INFO - joeynmt.training - Epoch  70, Step:     6590, Batch Loss:     0.243520, Batch Acc: 0.954473, Tokens per Sec:     3856, Lr: 0.000220\r\n",
      "2024-08-27 15:09:08,739 - INFO - joeynmt.training - Epoch  70, Step:     6595, Batch Loss:     0.229403, Batch Acc: 0.961326, Tokens per Sec:     3870, Lr: 0.000220\r\n",
      "2024-08-27 15:09:09,829 - INFO - joeynmt.training - Epoch  70, Step:     6600, Batch Loss:     0.197558, Batch Acc: 0.962129, Tokens per Sec:     3708, Lr: 0.000220\r\n",
      "2024-08-27 15:09:10,955 - INFO - joeynmt.training - Epoch  70, Step:     6605, Batch Loss:     0.188068, Batch Acc: 0.960560, Tokens per Sec:     3810, Lr: 0.000220\r\n",
      "2024-08-27 15:09:12,067 - INFO - joeynmt.training - Epoch  70, Step:     6610, Batch Loss:     0.224222, Batch Acc: 0.959098, Tokens per Sec:     3870, Lr: 0.000220\r\n",
      "2024-08-27 15:09:13,355 - INFO - joeynmt.training - Epoch  70, Step:     6615, Batch Loss:     0.203290, Batch Acc: 0.951690, Tokens per Sec:     3267, Lr: 0.000220\r\n",
      "2024-08-27 15:09:14,496 - INFO - joeynmt.training - Epoch  70, Step:     6620, Batch Loss:     0.226067, Batch Acc: 0.956342, Tokens per Sec:     3818, Lr: 0.000220\r\n",
      "2024-08-27 15:09:15,580 - INFO - joeynmt.training - Epoch  70, Step:     6625, Batch Loss:     0.234185, Batch Acc: 0.952956, Tokens per Sec:     3748, Lr: 0.000220\r\n",
      "2024-08-27 15:09:16,694 - INFO - joeynmt.training - Epoch  70, Step:     6630, Batch Loss:     0.246262, Batch Acc: 0.951507, Tokens per Sec:     3962, Lr: 0.000220\r\n",
      "2024-08-27 15:09:17,807 - INFO - joeynmt.training - Epoch  70, Step:     6635, Batch Loss:     0.218288, Batch Acc: 0.955388, Tokens per Sec:     3688, Lr: 0.000220\r\n",
      "2024-08-27 15:09:18,894 - INFO - joeynmt.training - Epoch  70, Step:     6640, Batch Loss:     0.205934, Batch Acc: 0.948123, Tokens per Sec:     3728, Lr: 0.000220\r\n",
      "2024-08-27 15:09:19,982 - INFO - joeynmt.training - Epoch  70, Step:     6645, Batch Loss:     0.214052, Batch Acc: 0.954211, Tokens per Sec:     3875, Lr: 0.000219\r\n",
      "2024-08-27 15:09:21,050 - INFO - joeynmt.training - Epoch  70, Step:     6650, Batch Loss:     0.248886, Batch Acc: 0.953189, Tokens per Sec:     3861, Lr: 0.000219\r\n",
      "2024-08-27 15:09:22,137 - INFO - joeynmt.training - Epoch  70, Step:     6655, Batch Loss:     0.231306, Batch Acc: 0.952815, Tokens per Sec:     3941, Lr: 0.000219\r\n",
      "2024-08-27 15:09:23,231 - INFO - joeynmt.training - Epoch  70, Step:     6660, Batch Loss:     0.214242, Batch Acc: 0.950338, Tokens per Sec:     3795, Lr: 0.000219\r\n",
      "2024-08-27 15:09:23,551 - INFO - joeynmt.training - Epoch  70, total training loss: 20.33, num. of seqs: 8065, num. of tokens: 80463, 21.1358[sec]\r\n",
      "2024-08-27 15:09:23,551 - INFO - joeynmt.training - EPOCH 71\r\n",
      "2024-08-27 15:09:24,464 - INFO - joeynmt.training - Epoch  71, Step:     6665, Batch Loss:     0.170868, Batch Acc: 0.961738, Tokens per Sec:     3828, Lr: 0.000219\r\n",
      "2024-08-27 15:09:25,582 - INFO - joeynmt.training - Epoch  71, Step:     6670, Batch Loss:     0.210074, Batch Acc: 0.961055, Tokens per Sec:     3699, Lr: 0.000219\r\n",
      "2024-08-27 15:09:26,862 - INFO - joeynmt.training - Epoch  71, Step:     6675, Batch Loss:     0.202667, Batch Acc: 0.963224, Tokens per Sec:     3551, Lr: 0.000219\r\n",
      "2024-08-27 15:09:27,956 - INFO - joeynmt.training - Epoch  71, Step:     6680, Batch Loss:     0.190017, Batch Acc: 0.963000, Tokens per Sec:     3681, Lr: 0.000219\r\n",
      "2024-08-27 15:09:29,054 - INFO - joeynmt.training - Epoch  71, Step:     6685, Batch Loss:     0.202035, Batch Acc: 0.959889, Tokens per Sec:     3954, Lr: 0.000219\r\n",
      "2024-08-27 15:09:30,159 - INFO - joeynmt.training - Epoch  71, Step:     6690, Batch Loss:     0.199813, Batch Acc: 0.959063, Tokens per Sec:     3981, Lr: 0.000219\r\n",
      "2024-08-27 15:09:31,250 - INFO - joeynmt.training - Epoch  71, Step:     6695, Batch Loss:     0.202559, Batch Acc: 0.959436, Tokens per Sec:     3641, Lr: 0.000219\r\n",
      "2024-08-27 15:09:32,332 - INFO - joeynmt.training - Epoch  71, Step:     6700, Batch Loss:     0.197194, Batch Acc: 0.957542, Tokens per Sec:     3702, Lr: 0.000219\r\n",
      "2024-08-27 15:09:33,416 - INFO - joeynmt.training - Epoch  71, Step:     6705, Batch Loss:     0.203015, Batch Acc: 0.959790, Tokens per Sec:     4041, Lr: 0.000218\r\n",
      "2024-08-27 15:09:34,503 - INFO - joeynmt.training - Epoch  71, Step:     6710, Batch Loss:     0.216621, Batch Acc: 0.960605, Tokens per Sec:     3832, Lr: 0.000218\r\n",
      "2024-08-27 15:09:35,565 - INFO - joeynmt.training - Epoch  71, Step:     6715, Batch Loss:     0.226896, Batch Acc: 0.954687, Tokens per Sec:     3640, Lr: 0.000218\r\n",
      "2024-08-27 15:09:36,637 - INFO - joeynmt.training - Epoch  71, Step:     6720, Batch Loss:     0.208268, Batch Acc: 0.959419, Tokens per Sec:     3727, Lr: 0.000218\r\n",
      "2024-08-27 15:09:37,765 - INFO - joeynmt.training - Epoch  71, Step:     6725, Batch Loss:     0.205620, Batch Acc: 0.955614, Tokens per Sec:     3697, Lr: 0.000218\r\n",
      "2024-08-27 15:09:38,857 - INFO - joeynmt.training - Epoch  71, Step:     6730, Batch Loss:     0.196825, Batch Acc: 0.956584, Tokens per Sec:     3820, Lr: 0.000218\r\n",
      "2024-08-27 15:09:39,951 - INFO - joeynmt.training - Epoch  71, Step:     6735, Batch Loss:     0.213390, Batch Acc: 0.957615, Tokens per Sec:     3820, Lr: 0.000218\r\n",
      "2024-08-27 15:09:41,059 - INFO - joeynmt.training - Epoch  71, Step:     6740, Batch Loss:     0.242039, Batch Acc: 0.955193, Tokens per Sec:     3869, Lr: 0.000218\r\n",
      "2024-08-27 15:09:42,158 - INFO - joeynmt.training - Epoch  71, Step:     6745, Batch Loss:     0.252566, Batch Acc: 0.956057, Tokens per Sec:     3916, Lr: 0.000218\r\n",
      "2024-08-27 15:09:43,238 - INFO - joeynmt.training - Epoch  71, Step:     6750, Batch Loss:     0.203378, Batch Acc: 0.958701, Tokens per Sec:     3993, Lr: 0.000218\r\n",
      "2024-08-27 15:09:44,309 - INFO - joeynmt.training - Epoch  71, Step:     6755, Batch Loss:     0.231880, Batch Acc: 0.957512, Tokens per Sec:     3982, Lr: 0.000218\r\n",
      "2024-08-27 15:09:44,680 - INFO - joeynmt.training - Epoch  71, total training loss: 19.69, num. of seqs: 8065, num. of tokens: 80463, 21.1109[sec]\r\n",
      "2024-08-27 15:09:44,680 - INFO - joeynmt.training - EPOCH 72\r\n",
      "2024-08-27 15:09:45,525 - INFO - joeynmt.training - Epoch  72, Step:     6760, Batch Loss:     0.206319, Batch Acc: 0.962264, Tokens per Sec:     3843, Lr: 0.000218\r\n",
      "2024-08-27 15:09:46,600 - INFO - joeynmt.training - Epoch  72, Step:     6765, Batch Loss:     0.195115, Batch Acc: 0.965967, Tokens per Sec:     3996, Lr: 0.000217\r\n",
      "2024-08-27 15:09:47,722 - INFO - joeynmt.training - Epoch  72, Step:     6770, Batch Loss:     0.192984, Batch Acc: 0.964728, Tokens per Sec:     3740, Lr: 0.000217\r\n",
      "2024-08-27 15:09:48,774 - INFO - joeynmt.training - Epoch  72, Step:     6775, Batch Loss:     0.219750, Batch Acc: 0.954362, Tokens per Sec:     3774, Lr: 0.000217\r\n",
      "2024-08-27 15:09:49,842 - INFO - joeynmt.training - Epoch  72, Step:     6780, Batch Loss:     0.183996, Batch Acc: 0.957767, Tokens per Sec:     3925, Lr: 0.000217\r\n",
      "2024-08-27 15:09:50,942 - INFO - joeynmt.training - Epoch  72, Step:     6785, Batch Loss:     0.190369, Batch Acc: 0.961701, Tokens per Sec:     3872, Lr: 0.000217\r\n",
      "2024-08-27 15:09:52,044 - INFO - joeynmt.training - Epoch  72, Step:     6790, Batch Loss:     0.200769, Batch Acc: 0.960481, Tokens per Sec:     3930, Lr: 0.000217\r\n",
      "2024-08-27 15:09:53,148 - INFO - joeynmt.training - Epoch  72, Step:     6795, Batch Loss:     0.212288, Batch Acc: 0.960886, Tokens per Sec:     3846, Lr: 0.000217\r\n",
      "2024-08-27 15:09:54,242 - INFO - joeynmt.training - Epoch  72, Step:     6800, Batch Loss:     0.187422, Batch Acc: 0.963985, Tokens per Sec:     3911, Lr: 0.000217\r\n",
      "2024-08-27 15:09:55,318 - INFO - joeynmt.training - Epoch  72, Step:     6805, Batch Loss:     0.190281, Batch Acc: 0.961309, Tokens per Sec:     3894, Lr: 0.000217\r\n",
      "2024-08-27 15:09:56,410 - INFO - joeynmt.training - Epoch  72, Step:     6810, Batch Loss:     0.191053, Batch Acc: 0.963807, Tokens per Sec:     3623, Lr: 0.000217\r\n",
      "2024-08-27 15:09:57,631 - INFO - joeynmt.training - Epoch  72, Step:     6815, Batch Loss:     0.207121, Batch Acc: 0.954738, Tokens per Sec:     3475, Lr: 0.000217\r\n",
      "2024-08-27 15:09:58,787 - INFO - joeynmt.training - Epoch  72, Step:     6820, Batch Loss:     0.187337, Batch Acc: 0.956054, Tokens per Sec:     3703, Lr: 0.000217\r\n",
      "2024-08-27 15:09:59,882 - INFO - joeynmt.training - Epoch  72, Step:     6825, Batch Loss:     0.221221, Batch Acc: 0.953807, Tokens per Sec:     3936, Lr: 0.000217\r\n",
      "2024-08-27 15:10:00,975 - INFO - joeynmt.training - Epoch  72, Step:     6830, Batch Loss:     0.209741, Batch Acc: 0.958392, Tokens per Sec:     3915, Lr: 0.000216\r\n",
      "2024-08-27 15:10:02,075 - INFO - joeynmt.training - Epoch  72, Step:     6835, Batch Loss:     0.199078, Batch Acc: 0.952336, Tokens per Sec:     3838, Lr: 0.000216\r\n",
      "2024-08-27 15:10:03,166 - INFO - joeynmt.training - Epoch  72, Step:     6840, Batch Loss:     0.203171, Batch Acc: 0.954885, Tokens per Sec:     3923, Lr: 0.000216\r\n",
      "2024-08-27 15:10:04,241 - INFO - joeynmt.training - Epoch  72, Step:     6845, Batch Loss:     0.198094, Batch Acc: 0.956543, Tokens per Sec:     3895, Lr: 0.000216\r\n",
      "2024-08-27 15:10:05,307 - INFO - joeynmt.training - Epoch  72, Step:     6850, Batch Loss:     0.204100, Batch Acc: 0.958156, Tokens per Sec:     3747, Lr: 0.000216\r\n",
      "2024-08-27 15:10:05,780 - INFO - joeynmt.training - Epoch  72, total training loss: 19.66, num. of seqs: 8065, num. of tokens: 80463, 21.0835[sec]\r\n",
      "2024-08-27 15:10:05,781 - INFO - joeynmt.training - EPOCH 73\r\n",
      "2024-08-27 15:10:06,432 - INFO - joeynmt.training - Epoch  73, Step:     6855, Batch Loss:     0.179204, Batch Acc: 0.968750, Tokens per Sec:     3756, Lr: 0.000216\r\n",
      "2024-08-27 15:10:07,555 - INFO - joeynmt.training - Epoch  73, Step:     6860, Batch Loss:     0.157489, Batch Acc: 0.965211, Tokens per Sec:     3816, Lr: 0.000216\r\n",
      "2024-08-27 15:10:08,645 - INFO - joeynmt.training - Epoch  73, Step:     6865, Batch Loss:     0.187320, Batch Acc: 0.962340, Tokens per Sec:     3877, Lr: 0.000216\r\n",
      "2024-08-27 15:10:09,761 - INFO - joeynmt.training - Epoch  73, Step:     6870, Batch Loss:     0.166689, Batch Acc: 0.964990, Tokens per Sec:     3866, Lr: 0.000216\r\n",
      "2024-08-27 15:10:10,844 - INFO - joeynmt.training - Epoch  73, Step:     6875, Batch Loss:     0.173889, Batch Acc: 0.963095, Tokens per Sec:     3882, Lr: 0.000216\r\n",
      "2024-08-27 15:10:11,934 - INFO - joeynmt.training - Epoch  73, Step:     6880, Batch Loss:     0.179139, Batch Acc: 0.966438, Tokens per Sec:     4019, Lr: 0.000216\r\n",
      "2024-08-27 15:10:13,026 - INFO - joeynmt.training - Epoch  73, Step:     6885, Batch Loss:     0.197529, Batch Acc: 0.957061, Tokens per Sec:     3841, Lr: 0.000216\r\n",
      "2024-08-27 15:10:14,132 - INFO - joeynmt.training - Epoch  73, Step:     6890, Batch Loss:     0.199031, Batch Acc: 0.959814, Tokens per Sec:     3693, Lr: 0.000216\r\n",
      "2024-08-27 15:10:15,232 - INFO - joeynmt.training - Epoch  73, Step:     6895, Batch Loss:     0.196814, Batch Acc: 0.964596, Tokens per Sec:     3983, Lr: 0.000215\r\n",
      "2024-08-27 15:10:16,331 - INFO - joeynmt.training - Epoch  73, Step:     6900, Batch Loss:     0.209522, Batch Acc: 0.960510, Tokens per Sec:     3641, Lr: 0.000215\r\n",
      "2024-08-27 15:10:17,454 - INFO - joeynmt.training - Epoch  73, Step:     6905, Batch Loss:     0.216947, Batch Acc: 0.955326, Tokens per Sec:     3790, Lr: 0.000215\r\n",
      "2024-08-27 15:10:18,521 - INFO - joeynmt.training - Epoch  73, Step:     6910, Batch Loss:     0.203308, Batch Acc: 0.957942, Tokens per Sec:     3995, Lr: 0.000215\r\n",
      "2024-08-27 15:10:19,583 - INFO - joeynmt.training - Epoch  73, Step:     6915, Batch Loss:     0.187377, Batch Acc: 0.957150, Tokens per Sec:     4043, Lr: 0.000215\r\n",
      "2024-08-27 15:10:20,648 - INFO - joeynmt.training - Epoch  73, Step:     6920, Batch Loss:     0.206755, Batch Acc: 0.960457, Tokens per Sec:     3946, Lr: 0.000215\r\n",
      "2024-08-27 15:10:21,726 - INFO - joeynmt.training - Epoch  73, Step:     6925, Batch Loss:     0.185765, Batch Acc: 0.958531, Tokens per Sec:     3712, Lr: 0.000215\r\n",
      "2024-08-27 15:10:22,835 - INFO - joeynmt.training - Epoch  73, Step:     6930, Batch Loss:     0.203646, Batch Acc: 0.962823, Tokens per Sec:     3812, Lr: 0.000215\r\n",
      "2024-08-27 15:10:23,901 - INFO - joeynmt.training - Epoch  73, Step:     6935, Batch Loss:     0.198744, Batch Acc: 0.957869, Tokens per Sec:     3877, Lr: 0.000215\r\n",
      "2024-08-27 15:10:24,982 - INFO - joeynmt.training - Epoch  73, Step:     6940, Batch Loss:     0.208724, Batch Acc: 0.959766, Tokens per Sec:     4117, Lr: 0.000215\r\n",
      "2024-08-27 15:10:26,060 - INFO - joeynmt.training - Epoch  73, Step:     6945, Batch Loss:     0.202750, Batch Acc: 0.955213, Tokens per Sec:     3856, Lr: 0.000215\r\n",
      "2024-08-27 15:10:26,606 - INFO - joeynmt.training - Epoch  73, total training loss: 18.94, num. of seqs: 8065, num. of tokens: 80463, 20.8091[sec]\r\n",
      "2024-08-27 15:10:26,607 - INFO - joeynmt.training - EPOCH 74\r\n",
      "2024-08-27 15:10:27,290 - INFO - joeynmt.training - Epoch  74, Step:     6950, Batch Loss:     0.197518, Batch Acc: 0.965035, Tokens per Sec:     3790, Lr: 0.000215\r\n",
      "2024-08-27 15:10:28,385 - INFO - joeynmt.training - Epoch  74, Step:     6955, Batch Loss:     0.157792, Batch Acc: 0.965209, Tokens per Sec:     3784, Lr: 0.000214\r\n",
      "2024-08-27 15:10:29,624 - INFO - joeynmt.training - Epoch  74, Step:     6960, Batch Loss:     0.213395, Batch Acc: 0.963103, Tokens per Sec:     3414, Lr: 0.000214\r\n",
      "2024-08-27 15:10:30,743 - INFO - joeynmt.training - Epoch  74, Step:     6965, Batch Loss:     0.196264, Batch Acc: 0.962639, Tokens per Sec:     3684, Lr: 0.000214\r\n",
      "2024-08-27 15:10:31,872 - INFO - joeynmt.training - Epoch  74, Step:     6970, Batch Loss:     0.191372, Batch Acc: 0.964072, Tokens per Sec:     3701, Lr: 0.000214\r\n",
      "2024-08-27 15:10:32,971 - INFO - joeynmt.training - Epoch  74, Step:     6975, Batch Loss:     0.200158, Batch Acc: 0.962097, Tokens per Sec:     4013, Lr: 0.000214\r\n",
      "2024-08-27 15:10:34,053 - INFO - joeynmt.training - Epoch  74, Step:     6980, Batch Loss:     0.174221, Batch Acc: 0.964674, Tokens per Sec:     4083, Lr: 0.000214\r\n",
      "2024-08-27 15:10:35,130 - INFO - joeynmt.training - Epoch  74, Step:     6985, Batch Loss:     0.198612, Batch Acc: 0.961065, Tokens per Sec:     4151, Lr: 0.000214\r\n",
      "2024-08-27 15:10:36,216 - INFO - joeynmt.training - Epoch  74, Step:     6990, Batch Loss:     0.225279, Batch Acc: 0.963163, Tokens per Sec:     3926, Lr: 0.000214\r\n",
      "2024-08-27 15:10:37,338 - INFO - joeynmt.training - Epoch  74, Step:     6995, Batch Loss:     0.186401, Batch Acc: 0.960922, Tokens per Sec:     3562, Lr: 0.000214\r\n",
      "2024-08-27 15:10:38,468 - INFO - joeynmt.training - Epoch  74, Step:     7000, Batch Loss:     0.208663, Batch Acc: 0.953832, Tokens per Sec:     3778, Lr: 0.000214\r\n",
      "2024-08-27 15:10:38,469 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=7042\r\n",
      "2024-08-27 15:10:38,469 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 63.98it/s]\r\n",
      "2024-08-27 15:11:01,290 - INFO - joeynmt.prediction - Generation took 22.8187[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44694.92 examples/s]\r\n",
      "2024-08-27 15:11:01,665 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:11:01,666 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   4.78, loss:   6.74, ppl: 849.72, acc:   0.28, 0.1627[sec]\r\n",
      "2024-08-27 15:11:01,666 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\r\n",
      "2024-08-27 15:11:01,980 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/7000.ckpt.\r\n",
      "2024-08-27 15:11:01,981 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/5500.ckpt\r\n",
      "2024-08-27 15:11:02,005 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:11:02,152 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:11:02,152 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:11:02,152 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 15:11:02,440 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:11:02,584 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:11:02,585 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:11:02,585 - INFO - joeynmt.training - \tHypothesis: trois éléphants\r\n",
      "2024-08-27 15:11:02,876 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:11:03,022 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:11:03,022 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:11:03,022 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 15:11:03,313 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:11:03,459 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:11:03,459 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:11:03,459 - INFO - joeynmt.training - \tHypothesis: une minute cinquante\r\n",
      "2024-08-27 15:11:04,844 - INFO - joeynmt.training - Epoch  74, Step:     7005, Batch Loss:     0.198592, Batch Acc: 0.965358, Tokens per Sec:     3968, Lr: 0.000214\r\n",
      "2024-08-27 15:11:05,928 - INFO - joeynmt.training - Epoch  74, Step:     7010, Batch Loss:     0.177859, Batch Acc: 0.960362, Tokens per Sec:     3984, Lr: 0.000214\r\n",
      "2024-08-27 15:11:07,033 - INFO - joeynmt.training - Epoch  74, Step:     7015, Batch Loss:     0.169201, Batch Acc: 0.958989, Tokens per Sec:     3798, Lr: 0.000214\r\n",
      "2024-08-27 15:11:08,092 - INFO - joeynmt.training - Epoch  74, Step:     7020, Batch Loss:     0.221592, Batch Acc: 0.955198, Tokens per Sec:     3878, Lr: 0.000214\r\n",
      "2024-08-27 15:11:09,139 - INFO - joeynmt.training - Epoch  74, Step:     7025, Batch Loss:     0.202231, Batch Acc: 0.959358, Tokens per Sec:     3813, Lr: 0.000213\r\n",
      "2024-08-27 15:11:10,203 - INFO - joeynmt.training - Epoch  74, Step:     7030, Batch Loss:     0.206282, Batch Acc: 0.956867, Tokens per Sec:     3902, Lr: 0.000213\r\n",
      "2024-08-27 15:11:11,270 - INFO - joeynmt.training - Epoch  74, Step:     7035, Batch Loss:     0.216643, Batch Acc: 0.953895, Tokens per Sec:     4130, Lr: 0.000213\r\n",
      "2024-08-27 15:11:12,330 - INFO - joeynmt.training - Epoch  74, Step:     7040, Batch Loss:     0.203975, Batch Acc: 0.955021, Tokens per Sec:     3880, Lr: 0.000213\r\n",
      "2024-08-27 15:11:12,823 - INFO - joeynmt.training - Epoch  74, total training loss: 18.87, num. of seqs: 8065, num. of tokens: 80463, 20.9141[sec]\r\n",
      "2024-08-27 15:11:12,823 - INFO - joeynmt.training - EPOCH 75\r\n",
      "2024-08-27 15:11:13,471 - INFO - joeynmt.training - Epoch  75, Step:     7045, Batch Loss:     0.168117, Batch Acc: 0.965531, Tokens per Sec:     3879, Lr: 0.000213\r\n",
      "2024-08-27 15:11:14,532 - INFO - joeynmt.training - Epoch  75, Step:     7050, Batch Loss:     0.176136, Batch Acc: 0.961697, Tokens per Sec:     3890, Lr: 0.000213\r\n",
      "2024-08-27 15:11:15,617 - INFO - joeynmt.training - Epoch  75, Step:     7055, Batch Loss:     0.199842, Batch Acc: 0.962048, Tokens per Sec:     4180, Lr: 0.000213\r\n",
      "2024-08-27 15:11:16,683 - INFO - joeynmt.training - Epoch  75, Step:     7060, Batch Loss:     0.196136, Batch Acc: 0.963632, Tokens per Sec:     3793, Lr: 0.000213\r\n",
      "2024-08-27 15:11:17,770 - INFO - joeynmt.training - Epoch  75, Step:     7065, Batch Loss:     0.215655, Batch Acc: 0.958561, Tokens per Sec:     3713, Lr: 0.000213\r\n",
      "2024-08-27 15:11:18,829 - INFO - joeynmt.training - Epoch  75, Step:     7070, Batch Loss:     0.182842, Batch Acc: 0.967598, Tokens per Sec:     3819, Lr: 0.000213\r\n",
      "2024-08-27 15:11:19,919 - INFO - joeynmt.training - Epoch  75, Step:     7075, Batch Loss:     0.183533, Batch Acc: 0.966019, Tokens per Sec:     4162, Lr: 0.000213\r\n",
      "2024-08-27 15:11:21,001 - INFO - joeynmt.training - Epoch  75, Step:     7080, Batch Loss:     0.210451, Batch Acc: 0.960675, Tokens per Sec:     3995, Lr: 0.000213\r\n",
      "2024-08-27 15:11:22,079 - INFO - joeynmt.training - Epoch  75, Step:     7085, Batch Loss:     0.196213, Batch Acc: 0.958881, Tokens per Sec:     3817, Lr: 0.000213\r\n",
      "2024-08-27 15:11:23,167 - INFO - joeynmt.training - Epoch  75, Step:     7090, Batch Loss:     0.188968, Batch Acc: 0.959792, Tokens per Sec:     3704, Lr: 0.000212\r\n",
      "2024-08-27 15:11:24,235 - INFO - joeynmt.training - Epoch  75, Step:     7095, Batch Loss:     0.173202, Batch Acc: 0.969201, Tokens per Sec:     3894, Lr: 0.000212\r\n",
      "2024-08-27 15:11:25,308 - INFO - joeynmt.training - Epoch  75, Step:     7100, Batch Loss:     0.166700, Batch Acc: 0.962963, Tokens per Sec:     3777, Lr: 0.000212\r\n",
      "2024-08-27 15:11:26,380 - INFO - joeynmt.training - Epoch  75, Step:     7105, Batch Loss:     0.202069, Batch Acc: 0.959248, Tokens per Sec:     3827, Lr: 0.000212\r\n",
      "2024-08-27 15:11:27,475 - INFO - joeynmt.training - Epoch  75, Step:     7110, Batch Loss:     0.183103, Batch Acc: 0.962450, Tokens per Sec:     3894, Lr: 0.000212\r\n",
      "2024-08-27 15:11:28,529 - INFO - joeynmt.training - Epoch  75, Step:     7115, Batch Loss:     0.226603, Batch Acc: 0.956805, Tokens per Sec:     3934, Lr: 0.000212\r\n",
      "2024-08-27 15:11:29,619 - INFO - joeynmt.training - Epoch  75, Step:     7120, Batch Loss:     0.204946, Batch Acc: 0.957472, Tokens per Sec:     3821, Lr: 0.000212\r\n",
      "2024-08-27 15:11:30,708 - INFO - joeynmt.training - Epoch  75, Step:     7125, Batch Loss:     0.196592, Batch Acc: 0.957829, Tokens per Sec:     3944, Lr: 0.000212\r\n",
      "2024-08-27 15:11:31,875 - INFO - joeynmt.training - Epoch  75, Step:     7130, Batch Loss:     0.176280, Batch Acc: 0.959905, Tokens per Sec:     3593, Lr: 0.000212\r\n",
      "2024-08-27 15:11:32,971 - INFO - joeynmt.training - Epoch  75, Step:     7135, Batch Loss:     0.204049, Batch Acc: 0.960219, Tokens per Sec:     3832, Lr: 0.000212\r\n",
      "2024-08-27 15:11:33,740 - INFO - joeynmt.training - Epoch  75, total training loss: 18.74, num. of seqs: 8065, num. of tokens: 80463, 20.8987[sec]\r\n",
      "2024-08-27 15:11:33,740 - INFO - joeynmt.training - EPOCH 76\r\n",
      "2024-08-27 15:11:34,176 - INFO - joeynmt.training - Epoch  76, Step:     7140, Batch Loss:     0.210743, Batch Acc: 0.966563, Tokens per Sec:     3745, Lr: 0.000212\r\n",
      "2024-08-27 15:11:35,256 - INFO - joeynmt.training - Epoch  76, Step:     7145, Batch Loss:     0.188479, Batch Acc: 0.965724, Tokens per Sec:     3865, Lr: 0.000212\r\n",
      "2024-08-27 15:11:36,344 - INFO - joeynmt.training - Epoch  76, Step:     7150, Batch Loss:     0.160248, Batch Acc: 0.970525, Tokens per Sec:     3869, Lr: 0.000212\r\n",
      "2024-08-27 15:11:37,452 - INFO - joeynmt.training - Epoch  76, Step:     7155, Batch Loss:     0.186690, Batch Acc: 0.965288, Tokens per Sec:     3799, Lr: 0.000211\r\n",
      "2024-08-27 15:11:38,534 - INFO - joeynmt.training - Epoch  76, Step:     7160, Batch Loss:     0.200683, Batch Acc: 0.968150, Tokens per Sec:     3949, Lr: 0.000211\r\n",
      "2024-08-27 15:11:39,614 - INFO - joeynmt.training - Epoch  76, Step:     7165, Batch Loss:     0.175752, Batch Acc: 0.969129, Tokens per Sec:     4051, Lr: 0.000211\r\n",
      "2024-08-27 15:11:40,687 - INFO - joeynmt.training - Epoch  76, Step:     7170, Batch Loss:     0.165381, Batch Acc: 0.966031, Tokens per Sec:     3815, Lr: 0.000211\r\n",
      "2024-08-27 15:11:41,770 - INFO - joeynmt.training - Epoch  76, Step:     7175, Batch Loss:     0.192648, Batch Acc: 0.960645, Tokens per Sec:     3781, Lr: 0.000211\r\n",
      "2024-08-27 15:11:42,845 - INFO - joeynmt.training - Epoch  76, Step:     7180, Batch Loss:     0.178763, Batch Acc: 0.968324, Tokens per Sec:     3821, Lr: 0.000211\r\n",
      "2024-08-27 15:11:43,926 - INFO - joeynmt.training - Epoch  76, Step:     7185, Batch Loss:     0.173042, Batch Acc: 0.965678, Tokens per Sec:     3964, Lr: 0.000211\r\n",
      "2024-08-27 15:11:45,011 - INFO - joeynmt.training - Epoch  76, Step:     7190, Batch Loss:     0.157553, Batch Acc: 0.966040, Tokens per Sec:     3777, Lr: 0.000211\r\n",
      "2024-08-27 15:11:46,082 - INFO - joeynmt.training - Epoch  76, Step:     7195, Batch Loss:     0.182882, Batch Acc: 0.965332, Tokens per Sec:     3827, Lr: 0.000211\r\n",
      "2024-08-27 15:11:47,188 - INFO - joeynmt.training - Epoch  76, Step:     7200, Batch Loss:     0.205039, Batch Acc: 0.961123, Tokens per Sec:     3770, Lr: 0.000211\r\n",
      "2024-08-27 15:11:48,239 - INFO - joeynmt.training - Epoch  76, Step:     7205, Batch Loss:     0.165109, Batch Acc: 0.961628, Tokens per Sec:     4092, Lr: 0.000211\r\n",
      "2024-08-27 15:11:49,316 - INFO - joeynmt.training - Epoch  76, Step:     7210, Batch Loss:     0.189572, Batch Acc: 0.966213, Tokens per Sec:     4096, Lr: 0.000211\r\n",
      "2024-08-27 15:11:50,391 - INFO - joeynmt.training - Epoch  76, Step:     7215, Batch Loss:     0.197250, Batch Acc: 0.966322, Tokens per Sec:     4145, Lr: 0.000211\r\n",
      "2024-08-27 15:11:51,434 - INFO - joeynmt.training - Epoch  76, Step:     7220, Batch Loss:     0.209417, Batch Acc: 0.965312, Tokens per Sec:     3875, Lr: 0.000211\r\n",
      "2024-08-27 15:11:52,499 - INFO - joeynmt.training - Epoch  76, Step:     7225, Batch Loss:     0.197551, Batch Acc: 0.960345, Tokens per Sec:     4028, Lr: 0.000210\r\n",
      "2024-08-27 15:11:53,558 - INFO - joeynmt.training - Epoch  76, Step:     7230, Batch Loss:     0.176659, Batch Acc: 0.959914, Tokens per Sec:     3958, Lr: 0.000210\r\n",
      "2024-08-27 15:11:54,307 - INFO - joeynmt.training - Epoch  76, total training loss: 17.74, num. of seqs: 8065, num. of tokens: 80463, 20.5501[sec]\r\n",
      "2024-08-27 15:11:54,308 - INFO - joeynmt.training - EPOCH 77\r\n",
      "2024-08-27 15:11:54,734 - INFO - joeynmt.training - Epoch  77, Step:     7235, Batch Loss:     0.180752, Batch Acc: 0.972136, Tokens per Sec:     3826, Lr: 0.000210\r\n",
      "2024-08-27 15:11:55,816 - INFO - joeynmt.training - Epoch  77, Step:     7240, Batch Loss:     0.179715, Batch Acc: 0.967506, Tokens per Sec:     3929, Lr: 0.000210\r\n",
      "2024-08-27 15:11:56,978 - INFO - joeynmt.training - Epoch  77, Step:     7245, Batch Loss:     0.186854, Batch Acc: 0.967667, Tokens per Sec:     3701, Lr: 0.000210\r\n",
      "2024-08-27 15:11:58,024 - INFO - joeynmt.training - Epoch  77, Step:     7250, Batch Loss:     0.168174, Batch Acc: 0.971127, Tokens per Sec:     3710, Lr: 0.000210\r\n",
      "2024-08-27 15:11:59,124 - INFO - joeynmt.training - Epoch  77, Step:     7255, Batch Loss:     0.171233, Batch Acc: 0.969072, Tokens per Sec:     3797, Lr: 0.000210\r\n",
      "2024-08-27 15:12:00,219 - INFO - joeynmt.training - Epoch  77, Step:     7260, Batch Loss:     0.211717, Batch Acc: 0.960766, Tokens per Sec:     4006, Lr: 0.000210\r\n",
      "2024-08-27 15:12:01,340 - INFO - joeynmt.training - Epoch  77, Step:     7265, Batch Loss:     0.199252, Batch Acc: 0.966188, Tokens per Sec:     3854, Lr: 0.000210\r\n",
      "2024-08-27 15:12:02,491 - INFO - joeynmt.training - Epoch  77, Step:     7270, Batch Loss:     0.183904, Batch Acc: 0.962703, Tokens per Sec:     3589, Lr: 0.000210\r\n",
      "2024-08-27 15:12:03,594 - INFO - joeynmt.training - Epoch  77, Step:     7275, Batch Loss:     0.156435, Batch Acc: 0.967070, Tokens per Sec:     3746, Lr: 0.000210\r\n",
      "2024-08-27 15:12:04,673 - INFO - joeynmt.training - Epoch  77, Step:     7280, Batch Loss:     0.184250, Batch Acc: 0.965122, Tokens per Sec:     3885, Lr: 0.000210\r\n",
      "2024-08-27 15:12:05,748 - INFO - joeynmt.training - Epoch  77, Step:     7285, Batch Loss:     0.196980, Batch Acc: 0.966245, Tokens per Sec:     3751, Lr: 0.000210\r\n",
      "2024-08-27 15:12:06,859 - INFO - joeynmt.training - Epoch  77, Step:     7290, Batch Loss:     0.193426, Batch Acc: 0.967631, Tokens per Sec:     3921, Lr: 0.000210\r\n",
      "2024-08-27 15:12:07,947 - INFO - joeynmt.training - Epoch  77, Step:     7295, Batch Loss:     0.200841, Batch Acc: 0.965069, Tokens per Sec:     3820, Lr: 0.000209\r\n",
      "2024-08-27 15:12:09,005 - INFO - joeynmt.training - Epoch  77, Step:     7300, Batch Loss:     0.195203, Batch Acc: 0.963260, Tokens per Sec:     4014, Lr: 0.000209\r\n",
      "2024-08-27 15:12:10,067 - INFO - joeynmt.training - Epoch  77, Step:     7305, Batch Loss:     0.195746, Batch Acc: 0.959090, Tokens per Sec:     4100, Lr: 0.000209\r\n",
      "2024-08-27 15:12:11,132 - INFO - joeynmt.training - Epoch  77, Step:     7310, Batch Loss:     0.211569, Batch Acc: 0.961421, Tokens per Sec:     3993, Lr: 0.000209\r\n",
      "2024-08-27 15:12:12,189 - INFO - joeynmt.training - Epoch  77, Step:     7315, Batch Loss:     0.190137, Batch Acc: 0.962007, Tokens per Sec:     3964, Lr: 0.000209\r\n",
      "2024-08-27 15:12:13,251 - INFO - joeynmt.training - Epoch  77, Step:     7320, Batch Loss:     0.171876, Batch Acc: 0.961595, Tokens per Sec:     3849, Lr: 0.000209\r\n",
      "2024-08-27 15:12:14,313 - INFO - joeynmt.training - Epoch  77, Step:     7325, Batch Loss:     0.193926, Batch Acc: 0.960230, Tokens per Sec:     3935, Lr: 0.000209\r\n",
      "2024-08-27 15:12:15,094 - INFO - joeynmt.training - Epoch  77, total training loss: 17.63, num. of seqs: 8065, num. of tokens: 80463, 20.7682[sec]\r\n",
      "2024-08-27 15:12:15,094 - INFO - joeynmt.training - EPOCH 78\r\n",
      "2024-08-27 15:12:15,525 - INFO - joeynmt.training - Epoch  78, Step:     7330, Batch Loss:     0.169805, Batch Acc: 0.972153, Tokens per Sec:     3783, Lr: 0.000209\r\n",
      "2024-08-27 15:12:16,610 - INFO - joeynmt.training - Epoch  78, Step:     7335, Batch Loss:     0.190374, Batch Acc: 0.971182, Tokens per Sec:     3743, Lr: 0.000209\r\n",
      "2024-08-27 15:12:17,718 - INFO - joeynmt.training - Epoch  78, Step:     7340, Batch Loss:     0.164867, Batch Acc: 0.968021, Tokens per Sec:     3869, Lr: 0.000209\r\n",
      "2024-08-27 15:12:18,803 - INFO - joeynmt.training - Epoch  78, Step:     7345, Batch Loss:     0.146029, Batch Acc: 0.970414, Tokens per Sec:     3897, Lr: 0.000209\r\n",
      "2024-08-27 15:12:19,859 - INFO - joeynmt.training - Epoch  78, Step:     7350, Batch Loss:     0.163859, Batch Acc: 0.965863, Tokens per Sec:     3776, Lr: 0.000209\r\n",
      "2024-08-27 15:12:20,924 - INFO - joeynmt.training - Epoch  78, Step:     7355, Batch Loss:     0.197481, Batch Acc: 0.965893, Tokens per Sec:     3965, Lr: 0.000209\r\n",
      "2024-08-27 15:12:21,997 - INFO - joeynmt.training - Epoch  78, Step:     7360, Batch Loss:     0.169602, Batch Acc: 0.964780, Tokens per Sec:     3973, Lr: 0.000209\r\n",
      "2024-08-27 15:12:23,077 - INFO - joeynmt.training - Epoch  78, Step:     7365, Batch Loss:     0.195257, Batch Acc: 0.963063, Tokens per Sec:     3762, Lr: 0.000208\r\n",
      "2024-08-27 15:12:24,149 - INFO - joeynmt.training - Epoch  78, Step:     7370, Batch Loss:     0.180695, Batch Acc: 0.969642, Tokens per Sec:     4089, Lr: 0.000208\r\n",
      "2024-08-27 15:12:25,221 - INFO - joeynmt.training - Epoch  78, Step:     7375, Batch Loss:     0.182454, Batch Acc: 0.963920, Tokens per Sec:     4010, Lr: 0.000208\r\n",
      "2024-08-27 15:12:26,307 - INFO - joeynmt.training - Epoch  78, Step:     7380, Batch Loss:     0.201726, Batch Acc: 0.966323, Tokens per Sec:     4023, Lr: 0.000208\r\n",
      "2024-08-27 15:12:27,414 - INFO - joeynmt.training - Epoch  78, Step:     7385, Batch Loss:     0.219347, Batch Acc: 0.967121, Tokens per Sec:     3848, Lr: 0.000208\r\n",
      "2024-08-27 15:12:28,479 - INFO - joeynmt.training - Epoch  78, Step:     7390, Batch Loss:     0.197335, Batch Acc: 0.961484, Tokens per Sec:     3999, Lr: 0.000208\r\n",
      "2024-08-27 15:12:29,547 - INFO - joeynmt.training - Epoch  78, Step:     7395, Batch Loss:     0.181702, Batch Acc: 0.963502, Tokens per Sec:     3931, Lr: 0.000208\r\n",
      "2024-08-27 15:12:30,599 - INFO - joeynmt.training - Epoch  78, Step:     7400, Batch Loss:     0.177916, Batch Acc: 0.967671, Tokens per Sec:     3911, Lr: 0.000208\r\n",
      "2024-08-27 15:12:31,673 - INFO - joeynmt.training - Epoch  78, Step:     7405, Batch Loss:     0.191339, Batch Acc: 0.962547, Tokens per Sec:     3981, Lr: 0.000208\r\n",
      "2024-08-27 15:12:32,742 - INFO - joeynmt.training - Epoch  78, Step:     7410, Batch Loss:     0.201325, Batch Acc: 0.962150, Tokens per Sec:     3884, Lr: 0.000208\r\n",
      "2024-08-27 15:12:33,891 - INFO - joeynmt.training - Epoch  78, Step:     7415, Batch Loss:     0.187741, Batch Acc: 0.965665, Tokens per Sec:     3651, Lr: 0.000208\r\n",
      "2024-08-27 15:12:34,963 - INFO - joeynmt.training - Epoch  78, Step:     7420, Batch Loss:     0.212582, Batch Acc: 0.960435, Tokens per Sec:     4034, Lr: 0.000208\r\n",
      "2024-08-27 15:12:35,715 - INFO - joeynmt.training - Epoch  78, total training loss: 17.45, num. of seqs: 8065, num. of tokens: 80463, 20.6037[sec]\r\n",
      "2024-08-27 15:12:35,715 - INFO - joeynmt.training - EPOCH 79\r\n",
      "2024-08-27 15:12:36,149 - INFO - joeynmt.training - Epoch  79, Step:     7425, Batch Loss:     0.171162, Batch Acc: 0.975294, Tokens per Sec:     3954, Lr: 0.000208\r\n",
      "2024-08-27 15:12:37,228 - INFO - joeynmt.training - Epoch  79, Step:     7430, Batch Loss:     0.181229, Batch Acc: 0.970945, Tokens per Sec:     3893, Lr: 0.000208\r\n",
      "2024-08-27 15:12:38,307 - INFO - joeynmt.training - Epoch  79, Step:     7435, Batch Loss:     0.154930, Batch Acc: 0.971631, Tokens per Sec:     4055, Lr: 0.000207\r\n",
      "2024-08-27 15:12:39,382 - INFO - joeynmt.training - Epoch  79, Step:     7440, Batch Loss:     0.151054, Batch Acc: 0.970770, Tokens per Sec:     3918, Lr: 0.000207\r\n",
      "2024-08-27 15:12:40,443 - INFO - joeynmt.training - Epoch  79, Step:     7445, Batch Loss:     0.159902, Batch Acc: 0.972470, Tokens per Sec:     3907, Lr: 0.000207\r\n",
      "2024-08-27 15:12:41,553 - INFO - joeynmt.training - Epoch  79, Step:     7450, Batch Loss:     0.166974, Batch Acc: 0.971214, Tokens per Sec:     3725, Lr: 0.000207\r\n",
      "2024-08-27 15:12:42,612 - INFO - joeynmt.training - Epoch  79, Step:     7455, Batch Loss:     0.167560, Batch Acc: 0.967946, Tokens per Sec:     4038, Lr: 0.000207\r\n",
      "2024-08-27 15:12:43,691 - INFO - joeynmt.training - Epoch  79, Step:     7460, Batch Loss:     0.160871, Batch Acc: 0.970333, Tokens per Sec:     3847, Lr: 0.000207\r\n",
      "2024-08-27 15:12:44,837 - INFO - joeynmt.training - Epoch  79, Step:     7465, Batch Loss:     0.172631, Batch Acc: 0.971310, Tokens per Sec:     3774, Lr: 0.000207\r\n",
      "2024-08-27 15:12:45,906 - INFO - joeynmt.training - Epoch  79, Step:     7470, Batch Loss:     0.179451, Batch Acc: 0.962110, Tokens per Sec:     3902, Lr: 0.000207\r\n",
      "2024-08-27 15:12:46,990 - INFO - joeynmt.training - Epoch  79, Step:     7475, Batch Loss:     0.184402, Batch Acc: 0.967209, Tokens per Sec:     3631, Lr: 0.000207\r\n",
      "2024-08-27 15:12:48,052 - INFO - joeynmt.training - Epoch  79, Step:     7480, Batch Loss:     0.169504, Batch Acc: 0.970012, Tokens per Sec:     3991, Lr: 0.000207\r\n",
      "2024-08-27 15:12:49,112 - INFO - joeynmt.training - Epoch  79, Step:     7485, Batch Loss:     0.179539, Batch Acc: 0.971906, Tokens per Sec:     4064, Lr: 0.000207\r\n",
      "2024-08-27 15:12:50,172 - INFO - joeynmt.training - Epoch  79, Step:     7490, Batch Loss:     0.174321, Batch Acc: 0.966147, Tokens per Sec:     3877, Lr: 0.000207\r\n",
      "2024-08-27 15:12:51,229 - INFO - joeynmt.training - Epoch  79, Step:     7495, Batch Loss:     0.176801, Batch Acc: 0.970142, Tokens per Sec:     4122, Lr: 0.000207\r\n",
      "2024-08-27 15:12:52,303 - INFO - joeynmt.training - Epoch  79, Step:     7500, Batch Loss:     0.185635, Batch Acc: 0.964064, Tokens per Sec:     3758, Lr: 0.000207\r\n",
      "2024-08-27 15:12:52,304 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=7542\r\n",
      "2024-08-27 15:12:52,305 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 64.29it/s]\r\n",
      "2024-08-27 15:13:15,018 - INFO - joeynmt.prediction - Generation took 22.7113[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44718.89 examples/s]\r\n",
      "2024-08-27 15:13:15,393 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:13:15,393 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.17, loss:   6.80, ppl: 902.33, acc:   0.28, 0.1642[sec]\r\n",
      "2024-08-27 15:13:15,394 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\r\n",
      "2024-08-27 15:13:15,710 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/7500.ckpt.\r\n",
      "2024-08-27 15:13:15,710 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/6000.ckpt\r\n",
      "2024-08-27 15:13:15,736 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:13:15,886 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:13:15,886 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:13:15,886 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 15:13:16,175 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:13:16,319 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:13:16,319 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:13:16,319 - INFO - joeynmt.training - \tHypothesis: trois tables blanches\r\n",
      "2024-08-27 15:13:16,612 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:13:16,773 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:13:16,773 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:13:16,773 - INFO - joeynmt.training - \tHypothesis: le temps n'est plus tard\r\n",
      "2024-08-27 15:13:17,062 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:13:17,206 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:13:17,206 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:13:17,206 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 15:13:18,576 - INFO - joeynmt.training - Epoch  79, Step:     7505, Batch Loss:     0.191598, Batch Acc: 0.962451, Tokens per Sec:     3758, Lr: 0.000206\r\n",
      "2024-08-27 15:13:19,631 - INFO - joeynmt.training - Epoch  79, Step:     7510, Batch Loss:     0.190710, Batch Acc: 0.961792, Tokens per Sec:     4021, Lr: 0.000206\r\n",
      "2024-08-27 15:13:20,698 - INFO - joeynmt.training - Epoch  79, Step:     7515, Batch Loss:     0.201177, Batch Acc: 0.963680, Tokens per Sec:     3874, Lr: 0.000206\r\n",
      "2024-08-27 15:13:21,594 - INFO - joeynmt.training - Epoch  79, total training loss: 16.79, num. of seqs: 8065, num. of tokens: 80463, 20.6668[sec]\r\n",
      "2024-08-27 15:13:21,594 - INFO - joeynmt.training - EPOCH 80\r\n",
      "2024-08-27 15:13:21,812 - INFO - joeynmt.training - Epoch  80, Step:     7520, Batch Loss:     0.190029, Batch Acc: 0.962830, Tokens per Sec:     3888, Lr: 0.000206\r\n",
      "2024-08-27 15:13:22,889 - INFO - joeynmt.training - Epoch  80, Step:     7525, Batch Loss:     0.191118, Batch Acc: 0.972157, Tokens per Sec:     3973, Lr: 0.000206\r\n",
      "2024-08-27 15:13:23,955 - INFO - joeynmt.training - Epoch  80, Step:     7530, Batch Loss:     0.173329, Batch Acc: 0.972655, Tokens per Sec:     3914, Lr: 0.000206\r\n",
      "2024-08-27 15:13:25,015 - INFO - joeynmt.training - Epoch  80, Step:     7535, Batch Loss:     0.149345, Batch Acc: 0.972243, Tokens per Sec:     3772, Lr: 0.000206\r\n",
      "2024-08-27 15:13:26,100 - INFO - joeynmt.training - Epoch  80, Step:     7540, Batch Loss:     0.163822, Batch Acc: 0.968872, Tokens per Sec:     3792, Lr: 0.000206\r\n",
      "2024-08-27 15:13:27,194 - INFO - joeynmt.training - Epoch  80, Step:     7545, Batch Loss:     0.179228, Batch Acc: 0.968027, Tokens per Sec:     3836, Lr: 0.000206\r\n",
      "2024-08-27 15:13:28,255 - INFO - joeynmt.training - Epoch  80, Step:     7550, Batch Loss:     0.186063, Batch Acc: 0.971209, Tokens per Sec:     3799, Lr: 0.000206\r\n",
      "2024-08-27 15:13:29,347 - INFO - joeynmt.training - Epoch  80, Step:     7555, Batch Loss:     0.194393, Batch Acc: 0.968229, Tokens per Sec:     4066, Lr: 0.000206\r\n",
      "2024-08-27 15:13:30,438 - INFO - joeynmt.training - Epoch  80, Step:     7560, Batch Loss:     0.163389, Batch Acc: 0.969948, Tokens per Sec:     3876, Lr: 0.000206\r\n",
      "2024-08-27 15:13:31,529 - INFO - joeynmt.training - Epoch  80, Step:     7565, Batch Loss:     0.159408, Batch Acc: 0.969871, Tokens per Sec:     3837, Lr: 0.000206\r\n",
      "2024-08-27 15:13:32,612 - INFO - joeynmt.training - Epoch  80, Step:     7570, Batch Loss:     0.184118, Batch Acc: 0.970048, Tokens per Sec:     3823, Lr: 0.000206\r\n",
      "2024-08-27 15:13:33,709 - INFO - joeynmt.training - Epoch  80, Step:     7575, Batch Loss:     0.188861, Batch Acc: 0.963034, Tokens per Sec:     3801, Lr: 0.000206\r\n",
      "2024-08-27 15:13:34,800 - INFO - joeynmt.training - Epoch  80, Step:     7580, Batch Loss:     0.165494, Batch Acc: 0.967074, Tokens per Sec:     3900, Lr: 0.000205\r\n",
      "2024-08-27 15:13:35,906 - INFO - joeynmt.training - Epoch  80, Step:     7585, Batch Loss:     0.174527, Batch Acc: 0.966537, Tokens per Sec:     3730, Lr: 0.000205\r\n",
      "2024-08-27 15:13:37,100 - INFO - joeynmt.training - Epoch  80, Step:     7590, Batch Loss:     0.166563, Batch Acc: 0.964245, Tokens per Sec:     3659, Lr: 0.000205\r\n",
      "2024-08-27 15:13:38,175 - INFO - joeynmt.training - Epoch  80, Step:     7595, Batch Loss:     0.155398, Batch Acc: 0.967534, Tokens per Sec:     3900, Lr: 0.000205\r\n",
      "2024-08-27 15:13:39,259 - INFO - joeynmt.training - Epoch  80, Step:     7600, Batch Loss:     0.173684, Batch Acc: 0.964142, Tokens per Sec:     3885, Lr: 0.000205\r\n",
      "2024-08-27 15:13:40,329 - INFO - joeynmt.training - Epoch  80, Step:     7605, Batch Loss:     0.173277, Batch Acc: 0.966224, Tokens per Sec:     4015, Lr: 0.000205\r\n",
      "2024-08-27 15:13:41,394 - INFO - joeynmt.training - Epoch  80, Step:     7610, Batch Loss:     0.176745, Batch Acc: 0.964595, Tokens per Sec:     3901, Lr: 0.000205\r\n",
      "2024-08-27 15:13:42,474 - INFO - joeynmt.training - Epoch  80, Step:     7615, Batch Loss:     0.163943, Batch Acc: 0.969896, Tokens per Sec:     3818, Lr: 0.000205\r\n",
      "2024-08-27 15:13:42,474 - INFO - joeynmt.training - Epoch  80, total training loss: 16.73, num. of seqs: 8065, num. of tokens: 80463, 20.8625[sec]\r\n",
      "2024-08-27 15:13:42,475 - INFO - joeynmt.training - EPOCH 81\r\n",
      "2024-08-27 15:13:43,559 - INFO - joeynmt.training - Epoch  81, Step:     7620, Batch Loss:     0.159575, Batch Acc: 0.972305, Tokens per Sec:     3706, Lr: 0.000205\r\n",
      "2024-08-27 15:13:44,648 - INFO - joeynmt.training - Epoch  81, Step:     7625, Batch Loss:     0.179833, Batch Acc: 0.968354, Tokens per Sec:     3921, Lr: 0.000205\r\n",
      "2024-08-27 15:13:45,719 - INFO - joeynmt.training - Epoch  81, Step:     7630, Batch Loss:     0.166588, Batch Acc: 0.967727, Tokens per Sec:     3936, Lr: 0.000205\r\n",
      "2024-08-27 15:13:46,830 - INFO - joeynmt.training - Epoch  81, Step:     7635, Batch Loss:     0.169030, Batch Acc: 0.971504, Tokens per Sec:     3763, Lr: 0.000205\r\n",
      "2024-08-27 15:13:47,889 - INFO - joeynmt.training - Epoch  81, Step:     7640, Batch Loss:     0.169319, Batch Acc: 0.970636, Tokens per Sec:     4051, Lr: 0.000205\r\n",
      "2024-08-27 15:13:48,959 - INFO - joeynmt.training - Epoch  81, Step:     7645, Batch Loss:     0.158117, Batch Acc: 0.971140, Tokens per Sec:     3891, Lr: 0.000205\r\n",
      "2024-08-27 15:13:50,032 - INFO - joeynmt.training - Epoch  81, Step:     7650, Batch Loss:     0.165920, Batch Acc: 0.967391, Tokens per Sec:     3687, Lr: 0.000205\r\n",
      "2024-08-27 15:13:51,165 - INFO - joeynmt.training - Epoch  81, Step:     7655, Batch Loss:     0.158863, Batch Acc: 0.968372, Tokens per Sec:     3436, Lr: 0.000204\r\n",
      "2024-08-27 15:13:52,242 - INFO - joeynmt.training - Epoch  81, Step:     7660, Batch Loss:     0.194667, Batch Acc: 0.969606, Tokens per Sec:     4035, Lr: 0.000204\r\n",
      "2024-08-27 15:13:53,322 - INFO - joeynmt.training - Epoch  81, Step:     7665, Batch Loss:     0.171646, Batch Acc: 0.973375, Tokens per Sec:     4105, Lr: 0.000204\r\n",
      "2024-08-27 15:13:54,397 - INFO - joeynmt.training - Epoch  81, Step:     7670, Batch Loss:     0.178002, Batch Acc: 0.967556, Tokens per Sec:     3874, Lr: 0.000204\r\n",
      "2024-08-27 15:13:55,445 - INFO - joeynmt.training - Epoch  81, Step:     7675, Batch Loss:     0.170033, Batch Acc: 0.967590, Tokens per Sec:     3857, Lr: 0.000204\r\n",
      "2024-08-27 15:13:56,577 - INFO - joeynmt.training - Epoch  81, Step:     7680, Batch Loss:     0.176795, Batch Acc: 0.970269, Tokens per Sec:     3747, Lr: 0.000204\r\n",
      "2024-08-27 15:13:57,693 - INFO - joeynmt.training - Epoch  81, Step:     7685, Batch Loss:     0.185544, Batch Acc: 0.968338, Tokens per Sec:     3737, Lr: 0.000204\r\n",
      "2024-08-27 15:13:58,770 - INFO - joeynmt.training - Epoch  81, Step:     7690, Batch Loss:     0.159217, Batch Acc: 0.961202, Tokens per Sec:     3927, Lr: 0.000204\r\n",
      "2024-08-27 15:13:59,859 - INFO - joeynmt.training - Epoch  81, Step:     7695, Batch Loss:     0.191162, Batch Acc: 0.966216, Tokens per Sec:     4082, Lr: 0.000204\r\n",
      "2024-08-27 15:14:00,934 - INFO - joeynmt.training - Epoch  81, Step:     7700, Batch Loss:     0.189251, Batch Acc: 0.964114, Tokens per Sec:     3862, Lr: 0.000204\r\n",
      "2024-08-27 15:14:02,007 - INFO - joeynmt.training - Epoch  81, Step:     7705, Batch Loss:     0.167086, Batch Acc: 0.969613, Tokens per Sec:     4051, Lr: 0.000204\r\n",
      "2024-08-27 15:14:03,086 - INFO - joeynmt.training - Epoch  81, Step:     7710, Batch Loss:     0.175904, Batch Acc: 0.968553, Tokens per Sec:     3834, Lr: 0.000204\r\n",
      "2024-08-27 15:14:03,302 - INFO - joeynmt.training - Epoch  81, total training loss: 16.75, num. of seqs: 8065, num. of tokens: 80463, 20.8119[sec]\r\n",
      "2024-08-27 15:14:03,302 - INFO - joeynmt.training - EPOCH 82\r\n",
      "2024-08-27 15:14:04,157 - INFO - joeynmt.training - Epoch  82, Step:     7715, Batch Loss:     0.144605, Batch Acc: 0.973247, Tokens per Sec:     3819, Lr: 0.000204\r\n",
      "2024-08-27 15:14:05,209 - INFO - joeynmt.training - Epoch  82, Step:     7720, Batch Loss:     0.161743, Batch Acc: 0.974104, Tokens per Sec:     3820, Lr: 0.000204\r\n",
      "2024-08-27 15:14:06,258 - INFO - joeynmt.training - Epoch  82, Step:     7725, Batch Loss:     0.153996, Batch Acc: 0.975227, Tokens per Sec:     3887, Lr: 0.000204\r\n",
      "2024-08-27 15:14:07,470 - INFO - joeynmt.training - Epoch  82, Step:     7730, Batch Loss:     0.174876, Batch Acc: 0.969035, Tokens per Sec:     3626, Lr: 0.000203\r\n",
      "2024-08-27 15:14:08,692 - INFO - joeynmt.training - Epoch  82, Step:     7735, Batch Loss:     0.167346, Batch Acc: 0.967175, Tokens per Sec:     3493, Lr: 0.000203\r\n",
      "2024-08-27 15:14:09,798 - INFO - joeynmt.training - Epoch  82, Step:     7740, Batch Loss:     0.145654, Batch Acc: 0.975069, Tokens per Sec:     3629, Lr: 0.000203\r\n",
      "2024-08-27 15:14:10,890 - INFO - joeynmt.training - Epoch  82, Step:     7745, Batch Loss:     0.154173, Batch Acc: 0.970005, Tokens per Sec:     3972, Lr: 0.000203\r\n",
      "2024-08-27 15:14:11,988 - INFO - joeynmt.training - Epoch  82, Step:     7750, Batch Loss:     0.153151, Batch Acc: 0.971301, Tokens per Sec:     4095, Lr: 0.000203\r\n",
      "2024-08-27 15:14:13,100 - INFO - joeynmt.training - Epoch  82, Step:     7755, Batch Loss:     0.156860, Batch Acc: 0.968495, Tokens per Sec:     3859, Lr: 0.000203\r\n",
      "2024-08-27 15:14:14,200 - INFO - joeynmt.training - Epoch  82, Step:     7760, Batch Loss:     0.181763, Batch Acc: 0.970253, Tokens per Sec:     3912, Lr: 0.000203\r\n",
      "2024-08-27 15:14:15,289 - INFO - joeynmt.training - Epoch  82, Step:     7765, Batch Loss:     0.160563, Batch Acc: 0.969444, Tokens per Sec:     3972, Lr: 0.000203\r\n",
      "2024-08-27 15:14:16,369 - INFO - joeynmt.training - Epoch  82, Step:     7770, Batch Loss:     0.158863, Batch Acc: 0.967828, Tokens per Sec:     3801, Lr: 0.000203\r\n",
      "2024-08-27 15:14:17,475 - INFO - joeynmt.training - Epoch  82, Step:     7775, Batch Loss:     0.165824, Batch Acc: 0.965728, Tokens per Sec:     3696, Lr: 0.000203\r\n",
      "2024-08-27 15:14:18,557 - INFO - joeynmt.training - Epoch  82, Step:     7780, Batch Loss:     0.176039, Batch Acc: 0.965888, Tokens per Sec:     4038, Lr: 0.000203\r\n",
      "2024-08-27 15:14:19,616 - INFO - joeynmt.training - Epoch  82, Step:     7785, Batch Loss:     0.179423, Batch Acc: 0.961432, Tokens per Sec:     3749, Lr: 0.000203\r\n",
      "2024-08-27 15:14:20,692 - INFO - joeynmt.training - Epoch  82, Step:     7790, Batch Loss:     0.176463, Batch Acc: 0.965951, Tokens per Sec:     4071, Lr: 0.000203\r\n",
      "2024-08-27 15:14:21,752 - INFO - joeynmt.training - Epoch  82, Step:     7795, Batch Loss:     0.182226, Batch Acc: 0.969190, Tokens per Sec:     3891, Lr: 0.000203\r\n",
      "2024-08-27 15:14:22,824 - INFO - joeynmt.training - Epoch  82, Step:     7800, Batch Loss:     0.175383, Batch Acc: 0.965792, Tokens per Sec:     3984, Lr: 0.000203\r\n",
      "2024-08-27 15:14:23,897 - INFO - joeynmt.training - Epoch  82, Step:     7805, Batch Loss:     0.178397, Batch Acc: 0.964310, Tokens per Sec:     4049, Lr: 0.000202\r\n",
      "2024-08-27 15:14:24,169 - INFO - joeynmt.training - Epoch  82, total training loss: 16.15, num. of seqs: 8065, num. of tokens: 80463, 20.8497[sec]\r\n",
      "2024-08-27 15:14:24,169 - INFO - joeynmt.training - EPOCH 83\r\n",
      "2024-08-27 15:14:25,021 - INFO - joeynmt.training - Epoch  83, Step:     7810, Batch Loss:     0.138986, Batch Acc: 0.976371, Tokens per Sec:     3890, Lr: 0.000202\r\n",
      "2024-08-27 15:14:26,085 - INFO - joeynmt.training - Epoch  83, Step:     7815, Batch Loss:     0.158846, Batch Acc: 0.973324, Tokens per Sec:     4127, Lr: 0.000202\r\n",
      "2024-08-27 15:14:27,193 - INFO - joeynmt.training - Epoch  83, Step:     7820, Batch Loss:     0.156476, Batch Acc: 0.972509, Tokens per Sec:     3842, Lr: 0.000202\r\n",
      "2024-08-27 15:14:28,269 - INFO - joeynmt.training - Epoch  83, Step:     7825, Batch Loss:     0.176189, Batch Acc: 0.969574, Tokens per Sec:     3885, Lr: 0.000202\r\n",
      "2024-08-27 15:14:29,333 - INFO - joeynmt.training - Epoch  83, Step:     7830, Batch Loss:     0.148812, Batch Acc: 0.974988, Tokens per Sec:     3797, Lr: 0.000202\r\n",
      "2024-08-27 15:14:30,410 - INFO - joeynmt.training - Epoch  83, Step:     7835, Batch Loss:     0.164039, Batch Acc: 0.972015, Tokens per Sec:     4118, Lr: 0.000202\r\n",
      "2024-08-27 15:14:31,475 - INFO - joeynmt.training - Epoch  83, Step:     7840, Batch Loss:     0.152040, Batch Acc: 0.973802, Tokens per Sec:     3978, Lr: 0.000202\r\n",
      "2024-08-27 15:14:32,572 - INFO - joeynmt.training - Epoch  83, Step:     7845, Batch Loss:     0.164317, Batch Acc: 0.969478, Tokens per Sec:     3918, Lr: 0.000202\r\n",
      "2024-08-27 15:14:33,669 - INFO - joeynmt.training - Epoch  83, Step:     7850, Batch Loss:     0.145273, Batch Acc: 0.972209, Tokens per Sec:     3968, Lr: 0.000202\r\n",
      "2024-08-27 15:14:34,776 - INFO - joeynmt.training - Epoch  83, Step:     7855, Batch Loss:     0.155518, Batch Acc: 0.967846, Tokens per Sec:     3935, Lr: 0.000202\r\n",
      "2024-08-27 15:14:35,893 - INFO - joeynmt.training - Epoch  83, Step:     7860, Batch Loss:     0.155155, Batch Acc: 0.970956, Tokens per Sec:     3580, Lr: 0.000202\r\n",
      "2024-08-27 15:14:37,021 - INFO - joeynmt.training - Epoch  83, Step:     7865, Batch Loss:     0.161209, Batch Acc: 0.968987, Tokens per Sec:     3748, Lr: 0.000202\r\n",
      "2024-08-27 15:14:38,105 - INFO - joeynmt.training - Epoch  83, Step:     7870, Batch Loss:     0.169800, Batch Acc: 0.967036, Tokens per Sec:     4003, Lr: 0.000202\r\n",
      "2024-08-27 15:14:39,308 - INFO - joeynmt.training - Epoch  83, Step:     7875, Batch Loss:     0.209670, Batch Acc: 0.968992, Tokens per Sec:     3328, Lr: 0.000202\r\n",
      "2024-08-27 15:14:40,448 - INFO - joeynmt.training - Epoch  83, Step:     7880, Batch Loss:     0.151400, Batch Acc: 0.969955, Tokens per Sec:     3710, Lr: 0.000202\r\n",
      "2024-08-27 15:14:41,562 - INFO - joeynmt.training - Epoch  83, Step:     7885, Batch Loss:     0.173842, Batch Acc: 0.968221, Tokens per Sec:     3873, Lr: 0.000201\r\n",
      "2024-08-27 15:14:42,645 - INFO - joeynmt.training - Epoch  83, Step:     7890, Batch Loss:     0.180126, Batch Acc: 0.965967, Tokens per Sec:     3828, Lr: 0.000201\r\n",
      "2024-08-27 15:14:43,725 - INFO - joeynmt.training - Epoch  83, Step:     7895, Batch Loss:     0.187512, Batch Acc: 0.966126, Tokens per Sec:     3939, Lr: 0.000201\r\n",
      "2024-08-27 15:14:44,798 - INFO - joeynmt.training - Epoch  83, Step:     7900, Batch Loss:     0.188013, Batch Acc: 0.965752, Tokens per Sec:     3837, Lr: 0.000201\r\n",
      "2024-08-27 15:14:45,063 - INFO - joeynmt.training - Epoch  83, total training loss: 15.76, num. of seqs: 8065, num. of tokens: 80463, 20.8768[sec]\r\n",
      "2024-08-27 15:14:45,063 - INFO - joeynmt.training - EPOCH 84\r\n",
      "2024-08-27 15:14:45,938 - INFO - joeynmt.training - Epoch  84, Step:     7905, Batch Loss:     0.159617, Batch Acc: 0.976660, Tokens per Sec:     3790, Lr: 0.000201\r\n",
      "2024-08-27 15:14:47,043 - INFO - joeynmt.training - Epoch  84, Step:     7910, Batch Loss:     0.156895, Batch Acc: 0.972675, Tokens per Sec:     3944, Lr: 0.000201\r\n",
      "2024-08-27 15:14:48,111 - INFO - joeynmt.training - Epoch  84, Step:     7915, Batch Loss:     0.155919, Batch Acc: 0.972121, Tokens per Sec:     4099, Lr: 0.000201\r\n",
      "2024-08-27 15:14:49,188 - INFO - joeynmt.training - Epoch  84, Step:     7920, Batch Loss:     0.151688, Batch Acc: 0.973678, Tokens per Sec:     3883, Lr: 0.000201\r\n",
      "2024-08-27 15:14:50,278 - INFO - joeynmt.training - Epoch  84, Step:     7925, Batch Loss:     0.164720, Batch Acc: 0.971429, Tokens per Sec:     4079, Lr: 0.000201\r\n",
      "2024-08-27 15:14:51,354 - INFO - joeynmt.training - Epoch  84, Step:     7930, Batch Loss:     0.160541, Batch Acc: 0.970442, Tokens per Sec:     3746, Lr: 0.000201\r\n",
      "2024-08-27 15:14:52,438 - INFO - joeynmt.training - Epoch  84, Step:     7935, Batch Loss:     0.170319, Batch Acc: 0.970322, Tokens per Sec:     3979, Lr: 0.000201\r\n",
      "2024-08-27 15:14:53,520 - INFO - joeynmt.training - Epoch  84, Step:     7940, Batch Loss:     0.153277, Batch Acc: 0.971656, Tokens per Sec:     3721, Lr: 0.000201\r\n",
      "2024-08-27 15:14:54,612 - INFO - joeynmt.training - Epoch  84, Step:     7945, Batch Loss:     0.168065, Batch Acc: 0.972801, Tokens per Sec:     4043, Lr: 0.000201\r\n",
      "2024-08-27 15:14:55,702 - INFO - joeynmt.training - Epoch  84, Step:     7950, Batch Loss:     0.156428, Batch Acc: 0.968844, Tokens per Sec:     3948, Lr: 0.000201\r\n",
      "2024-08-27 15:14:56,830 - INFO - joeynmt.training - Epoch  84, Step:     7955, Batch Loss:     0.173424, Batch Acc: 0.971918, Tokens per Sec:     3729, Lr: 0.000201\r\n",
      "2024-08-27 15:14:57,911 - INFO - joeynmt.training - Epoch  84, Step:     7960, Batch Loss:     0.154026, Batch Acc: 0.967742, Tokens per Sec:     3960, Lr: 0.000201\r\n",
      "2024-08-27 15:14:58,994 - INFO - joeynmt.training - Epoch  84, Step:     7965, Batch Loss:     0.158376, Batch Acc: 0.971122, Tokens per Sec:     3870, Lr: 0.000200\r\n",
      "2024-08-27 15:15:00,064 - INFO - joeynmt.training - Epoch  84, Step:     7970, Batch Loss:     0.141202, Batch Acc: 0.970867, Tokens per Sec:     3854, Lr: 0.000200\r\n",
      "2024-08-27 15:15:01,115 - INFO - joeynmt.training - Epoch  84, Step:     7975, Batch Loss:     0.155116, Batch Acc: 0.975686, Tokens per Sec:     3952, Lr: 0.000200\r\n",
      "2024-08-27 15:15:02,195 - INFO - joeynmt.training - Epoch  84, Step:     7980, Batch Loss:     0.156932, Batch Acc: 0.970687, Tokens per Sec:     3857, Lr: 0.000200\r\n",
      "2024-08-27 15:15:03,261 - INFO - joeynmt.training - Epoch  84, Step:     7985, Batch Loss:     0.169170, Batch Acc: 0.967461, Tokens per Sec:     3984, Lr: 0.000200\r\n",
      "2024-08-27 15:15:04,333 - INFO - joeynmt.training - Epoch  84, Step:     7990, Batch Loss:     0.167744, Batch Acc: 0.965887, Tokens per Sec:     3828, Lr: 0.000200\r\n",
      "2024-08-27 15:15:05,400 - INFO - joeynmt.training - Epoch  84, Step:     7995, Batch Loss:     0.158908, Batch Acc: 0.969625, Tokens per Sec:     3952, Lr: 0.000200\r\n",
      "2024-08-27 15:15:05,662 - INFO - joeynmt.training - Epoch  84, total training loss: 15.32, num. of seqs: 8065, num. of tokens: 80463, 20.5819[sec]\r\n",
      "2024-08-27 15:15:05,662 - INFO - joeynmt.training - EPOCH 85\r\n",
      "2024-08-27 15:15:06,514 - INFO - joeynmt.training - Epoch  85, Step:     8000, Batch Loss:     0.152154, Batch Acc: 0.973870, Tokens per Sec:     4019, Lr: 0.000200\r\n",
      "2024-08-27 15:15:06,515 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=8042\r\n",
      "2024-08-27 15:15:06,515 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:21<00:00, 67.54it/s]\r\n",
      "2024-08-27 15:15:28,133 - INFO - joeynmt.prediction - Generation took 21.6159[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 43970.90 examples/s]\r\n",
      "2024-08-27 15:15:28,507 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:15:28,507 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.19, loss:   6.89, ppl: 983.52, acc:   0.28, 0.1621[sec]\r\n",
      "2024-08-27 15:15:28,508 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\r\n",
      "2024-08-27 15:15:28,820 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/8000.ckpt.\r\n",
      "2024-08-27 15:15:28,821 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/6500.ckpt\r\n",
      "2024-08-27 15:15:28,845 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:15:28,993 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:15:28,994 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:15:28,994 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 15:15:29,280 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:15:29,425 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:15:29,425 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:15:29,425 - INFO - joeynmt.training - \tHypothesis: trois tables blanches\r\n",
      "2024-08-27 15:15:29,711 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:15:29,855 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:15:29,855 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:15:29,855 - INFO - joeynmt.training - \tHypothesis: le groupe disparaît avec lui\r\n",
      "2024-08-27 15:15:30,145 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:15:30,291 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:15:30,292 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:15:30,292 - INFO - joeynmt.training - \tHypothesis: une et unième\r\n",
      "2024-08-27 15:15:31,666 - INFO - joeynmt.training - Epoch  85, Step:     8005, Batch Loss:     0.138346, Batch Acc: 0.974624, Tokens per Sec:     4103, Lr: 0.000200\r\n",
      "2024-08-27 15:15:32,732 - INFO - joeynmt.training - Epoch  85, Step:     8010, Batch Loss:     0.147218, Batch Acc: 0.976771, Tokens per Sec:     3999, Lr: 0.000200\r\n",
      "2024-08-27 15:15:33,812 - INFO - joeynmt.training - Epoch  85, Step:     8015, Batch Loss:     0.155663, Batch Acc: 0.978477, Tokens per Sec:     4007, Lr: 0.000200\r\n",
      "2024-08-27 15:15:34,916 - INFO - joeynmt.training - Epoch  85, Step:     8020, Batch Loss:     0.152196, Batch Acc: 0.979288, Tokens per Sec:     3893, Lr: 0.000200\r\n",
      "2024-08-27 15:15:36,001 - INFO - joeynmt.training - Epoch  85, Step:     8025, Batch Loss:     0.163893, Batch Acc: 0.977810, Tokens per Sec:     3865, Lr: 0.000200\r\n",
      "2024-08-27 15:15:37,106 - INFO - joeynmt.training - Epoch  85, Step:     8030, Batch Loss:     0.151144, Batch Acc: 0.970463, Tokens per Sec:     3620, Lr: 0.000200\r\n",
      "2024-08-27 15:15:38,175 - INFO - joeynmt.training - Epoch  85, Step:     8035, Batch Loss:     0.174017, Batch Acc: 0.971292, Tokens per Sec:     3913, Lr: 0.000200\r\n",
      "2024-08-27 15:15:39,248 - INFO - joeynmt.training - Epoch  85, Step:     8040, Batch Loss:     0.183444, Batch Acc: 0.968985, Tokens per Sec:     3847, Lr: 0.000200\r\n",
      "2024-08-27 15:15:40,336 - INFO - joeynmt.training - Epoch  85, Step:     8045, Batch Loss:     0.162964, Batch Acc: 0.967107, Tokens per Sec:     3833, Lr: 0.000199\r\n",
      "2024-08-27 15:15:41,497 - INFO - joeynmt.training - Epoch  85, Step:     8050, Batch Loss:     0.176434, Batch Acc: 0.968423, Tokens per Sec:     3795, Lr: 0.000199\r\n",
      "2024-08-27 15:15:42,562 - INFO - joeynmt.training - Epoch  85, Step:     8055, Batch Loss:     0.152202, Batch Acc: 0.972646, Tokens per Sec:     4119, Lr: 0.000199\r\n",
      "2024-08-27 15:15:43,652 - INFO - joeynmt.training - Epoch  85, Step:     8060, Batch Loss:     0.179380, Batch Acc: 0.968254, Tokens per Sec:     3993, Lr: 0.000199\r\n",
      "2024-08-27 15:15:44,741 - INFO - joeynmt.training - Epoch  85, Step:     8065, Batch Loss:     0.161266, Batch Acc: 0.971635, Tokens per Sec:     3950, Lr: 0.000199\r\n",
      "2024-08-27 15:15:45,809 - INFO - joeynmt.training - Epoch  85, Step:     8070, Batch Loss:     0.202028, Batch Acc: 0.969447, Tokens per Sec:     4202, Lr: 0.000199\r\n",
      "2024-08-27 15:15:46,901 - INFO - joeynmt.training - Epoch  85, Step:     8075, Batch Loss:     0.155322, Batch Acc: 0.972842, Tokens per Sec:     3984, Lr: 0.000199\r\n",
      "2024-08-27 15:15:47,989 - INFO - joeynmt.training - Epoch  85, Step:     8080, Batch Loss:     0.153653, Batch Acc: 0.971172, Tokens per Sec:     3892, Lr: 0.000199\r\n",
      "2024-08-27 15:15:49,058 - INFO - joeynmt.training - Epoch  85, Step:     8085, Batch Loss:     0.195125, Batch Acc: 0.972202, Tokens per Sec:     3770, Lr: 0.000199\r\n",
      "2024-08-27 15:15:50,139 - INFO - joeynmt.training - Epoch  85, Step:     8090, Batch Loss:     0.141763, Batch Acc: 0.971230, Tokens per Sec:     3860, Lr: 0.000199\r\n",
      "2024-08-27 15:15:50,240 - INFO - joeynmt.training - Epoch  85, total training loss: 14.92, num. of seqs: 8065, num. of tokens: 80463, 20.4945[sec]\r\n",
      "2024-08-27 15:15:50,240 - INFO - joeynmt.training - EPOCH 86\r\n",
      "2024-08-27 15:15:51,313 - INFO - joeynmt.training - Epoch  86, Step:     8095, Batch Loss:     0.148821, Batch Acc: 0.976196, Tokens per Sec:     3853, Lr: 0.000199\r\n",
      "2024-08-27 15:15:52,384 - INFO - joeynmt.training - Epoch  86, Step:     8100, Batch Loss:     0.145966, Batch Acc: 0.974184, Tokens per Sec:     3981, Lr: 0.000199\r\n",
      "2024-08-27 15:15:53,442 - INFO - joeynmt.training - Epoch  86, Step:     8105, Batch Loss:     0.155315, Batch Acc: 0.976202, Tokens per Sec:     3895, Lr: 0.000199\r\n",
      "2024-08-27 15:15:54,512 - INFO - joeynmt.training - Epoch  86, Step:     8110, Batch Loss:     0.156129, Batch Acc: 0.972641, Tokens per Sec:     4034, Lr: 0.000199\r\n",
      "2024-08-27 15:15:55,558 - INFO - joeynmt.training - Epoch  86, Step:     8115, Batch Loss:     0.162539, Batch Acc: 0.972182, Tokens per Sec:     3952, Lr: 0.000199\r\n",
      "2024-08-27 15:15:56,675 - INFO - joeynmt.training - Epoch  86, Step:     8120, Batch Loss:     0.150089, Batch Acc: 0.970235, Tokens per Sec:     3882, Lr: 0.000199\r\n",
      "2024-08-27 15:15:57,786 - INFO - joeynmt.training - Epoch  86, Step:     8125, Batch Loss:     0.186342, Batch Acc: 0.968095, Tokens per Sec:     3699, Lr: 0.000198\r\n",
      "2024-08-27 15:15:58,875 - INFO - joeynmt.training - Epoch  86, Step:     8130, Batch Loss:     0.170511, Batch Acc: 0.971335, Tokens per Sec:     3912, Lr: 0.000198\r\n",
      "2024-08-27 15:15:59,977 - INFO - joeynmt.training - Epoch  86, Step:     8135, Batch Loss:     0.170317, Batch Acc: 0.974464, Tokens per Sec:     3767, Lr: 0.000198\r\n",
      "2024-08-27 15:16:01,058 - INFO - joeynmt.training - Epoch  86, Step:     8140, Batch Loss:     0.154236, Batch Acc: 0.971914, Tokens per Sec:     3922, Lr: 0.000198\r\n",
      "2024-08-27 15:16:02,137 - INFO - joeynmt.training - Epoch  86, Step:     8145, Batch Loss:     0.182591, Batch Acc: 0.969204, Tokens per Sec:     3764, Lr: 0.000198\r\n",
      "2024-08-27 15:16:03,216 - INFO - joeynmt.training - Epoch  86, Step:     8150, Batch Loss:     0.179225, Batch Acc: 0.967521, Tokens per Sec:     4055, Lr: 0.000198\r\n",
      "2024-08-27 15:16:04,277 - INFO - joeynmt.training - Epoch  86, Step:     8155, Batch Loss:     0.158987, Batch Acc: 0.972059, Tokens per Sec:     3847, Lr: 0.000198\r\n",
      "2024-08-27 15:16:05,382 - INFO - joeynmt.training - Epoch  86, Step:     8160, Batch Loss:     0.167304, Batch Acc: 0.971390, Tokens per Sec:     3988, Lr: 0.000198\r\n",
      "2024-08-27 15:16:06,450 - INFO - joeynmt.training - Epoch  86, Step:     8165, Batch Loss:     0.165067, Batch Acc: 0.968512, Tokens per Sec:     3809, Lr: 0.000198\r\n",
      "2024-08-27 15:16:07,559 - INFO - joeynmt.training - Epoch  86, Step:     8170, Batch Loss:     0.166440, Batch Acc: 0.970446, Tokens per Sec:     3911, Lr: 0.000198\r\n",
      "2024-08-27 15:16:08,643 - INFO - joeynmt.training - Epoch  86, Step:     8175, Batch Loss:     0.151442, Batch Acc: 0.971847, Tokens per Sec:     4099, Lr: 0.000198\r\n",
      "2024-08-27 15:16:09,700 - INFO - joeynmt.training - Epoch  86, Step:     8180, Batch Loss:     0.184586, Batch Acc: 0.969176, Tokens per Sec:     3744, Lr: 0.000198\r\n",
      "2024-08-27 15:16:10,788 - INFO - joeynmt.training - Epoch  86, Step:     8185, Batch Loss:     0.181989, Batch Acc: 0.966974, Tokens per Sec:     4096, Lr: 0.000198\r\n",
      "2024-08-27 15:16:10,884 - INFO - joeynmt.training - Epoch  86, total training loss: 15.28, num. of seqs: 8065, num. of tokens: 80463, 20.6265[sec]\r\n",
      "2024-08-27 15:16:10,884 - INFO - joeynmt.training - EPOCH 87\r\n",
      "2024-08-27 15:16:11,967 - INFO - joeynmt.training - Epoch  87, Step:     8190, Batch Loss:     0.156281, Batch Acc: 0.973330, Tokens per Sec:     3995, Lr: 0.000198\r\n",
      "2024-08-27 15:16:13,129 - INFO - joeynmt.training - Epoch  87, Step:     8195, Batch Loss:     0.157260, Batch Acc: 0.976407, Tokens per Sec:     3688, Lr: 0.000198\r\n",
      "2024-08-27 15:16:14,217 - INFO - joeynmt.training - Epoch  87, Step:     8200, Batch Loss:     0.161165, Batch Acc: 0.974297, Tokens per Sec:     3791, Lr: 0.000198\r\n",
      "2024-08-27 15:16:15,306 - INFO - joeynmt.training - Epoch  87, Step:     8205, Batch Loss:     0.153314, Batch Acc: 0.971174, Tokens per Sec:     3920, Lr: 0.000197\r\n",
      "2024-08-27 15:16:16,398 - INFO - joeynmt.training - Epoch  87, Step:     8210, Batch Loss:     0.136870, Batch Acc: 0.975047, Tokens per Sec:     3893, Lr: 0.000197\r\n",
      "2024-08-27 15:16:17,506 - INFO - joeynmt.training - Epoch  87, Step:     8215, Batch Loss:     0.136800, Batch Acc: 0.973882, Tokens per Sec:     3840, Lr: 0.000197\r\n",
      "2024-08-27 15:16:18,636 - INFO - joeynmt.training - Epoch  87, Step:     8220, Batch Loss:     0.170299, Batch Acc: 0.970829, Tokens per Sec:     3672, Lr: 0.000197\r\n",
      "2024-08-27 15:16:19,691 - INFO - joeynmt.training - Epoch  87, Step:     8225, Batch Loss:     0.180245, Batch Acc: 0.972807, Tokens per Sec:     4010, Lr: 0.000197\r\n",
      "2024-08-27 15:16:20,775 - INFO - joeynmt.training - Epoch  87, Step:     8230, Batch Loss:     0.149028, Batch Acc: 0.973084, Tokens per Sec:     3809, Lr: 0.000197\r\n",
      "2024-08-27 15:16:21,878 - INFO - joeynmt.training - Epoch  87, Step:     8235, Batch Loss:     0.162923, Batch Acc: 0.971185, Tokens per Sec:     3934, Lr: 0.000197\r\n",
      "2024-08-27 15:16:22,955 - INFO - joeynmt.training - Epoch  87, Step:     8240, Batch Loss:     0.166352, Batch Acc: 0.976987, Tokens per Sec:     3918, Lr: 0.000197\r\n",
      "2024-08-27 15:16:24,006 - INFO - joeynmt.training - Epoch  87, Step:     8245, Batch Loss:     0.155811, Batch Acc: 0.974668, Tokens per Sec:     3872, Lr: 0.000197\r\n",
      "2024-08-27 15:16:25,067 - INFO - joeynmt.training - Epoch  87, Step:     8250, Batch Loss:     0.161529, Batch Acc: 0.968510, Tokens per Sec:     3922, Lr: 0.000197\r\n",
      "2024-08-27 15:16:26,132 - INFO - joeynmt.training - Epoch  87, Step:     8255, Batch Loss:     0.144892, Batch Acc: 0.969449, Tokens per Sec:     3906, Lr: 0.000197\r\n",
      "2024-08-27 15:16:27,227 - INFO - joeynmt.training - Epoch  87, Step:     8260, Batch Loss:     0.155532, Batch Acc: 0.968827, Tokens per Sec:     3722, Lr: 0.000197\r\n",
      "2024-08-27 15:16:28,323 - INFO - joeynmt.training - Epoch  87, Step:     8265, Batch Loss:     0.163295, Batch Acc: 0.972185, Tokens per Sec:     4040, Lr: 0.000197\r\n",
      "2024-08-27 15:16:29,386 - INFO - joeynmt.training - Epoch  87, Step:     8270, Batch Loss:     0.169013, Batch Acc: 0.971687, Tokens per Sec:     3958, Lr: 0.000197\r\n",
      "2024-08-27 15:16:30,439 - INFO - joeynmt.training - Epoch  87, Step:     8275, Batch Loss:     0.160779, Batch Acc: 0.969647, Tokens per Sec:     4038, Lr: 0.000197\r\n",
      "2024-08-27 15:16:31,503 - INFO - joeynmt.training - Epoch  87, Step:     8280, Batch Loss:     0.154881, Batch Acc: 0.969459, Tokens per Sec:     4065, Lr: 0.000197\r\n",
      "2024-08-27 15:16:31,600 - INFO - joeynmt.training - Epoch  87, total training loss: 14.97, num. of seqs: 8065, num. of tokens: 80463, 20.6990[sec]\r\n",
      "2024-08-27 15:16:31,601 - INFO - joeynmt.training - EPOCH 88\r\n",
      "2024-08-27 15:16:32,672 - INFO - joeynmt.training - Epoch  88, Step:     8285, Batch Loss:     0.161590, Batch Acc: 0.970895, Tokens per Sec:     3862, Lr: 0.000197\r\n",
      "2024-08-27 15:16:33,747 - INFO - joeynmt.training - Epoch  88, Step:     8290, Batch Loss:     0.165888, Batch Acc: 0.975822, Tokens per Sec:     3964, Lr: 0.000196\r\n",
      "2024-08-27 15:16:34,809 - INFO - joeynmt.training - Epoch  88, Step:     8295, Batch Loss:     0.144881, Batch Acc: 0.973327, Tokens per Sec:     3887, Lr: 0.000196\r\n",
      "2024-08-27 15:16:35,868 - INFO - joeynmt.training - Epoch  88, Step:     8300, Batch Loss:     0.140649, Batch Acc: 0.977038, Tokens per Sec:     4034, Lr: 0.000196\r\n",
      "2024-08-27 15:16:36,971 - INFO - joeynmt.training - Epoch  88, Step:     8305, Batch Loss:     0.131576, Batch Acc: 0.975373, Tokens per Sec:     3829, Lr: 0.000196\r\n",
      "2024-08-27 15:16:38,065 - INFO - joeynmt.training - Epoch  88, Step:     8310, Batch Loss:     0.149272, Batch Acc: 0.972074, Tokens per Sec:     3933, Lr: 0.000196\r\n",
      "2024-08-27 15:16:39,166 - INFO - joeynmt.training - Epoch  88, Step:     8315, Batch Loss:     0.161347, Batch Acc: 0.976963, Tokens per Sec:     3867, Lr: 0.000196\r\n",
      "2024-08-27 15:16:40,231 - INFO - joeynmt.training - Epoch  88, Step:     8320, Batch Loss:     0.194173, Batch Acc: 0.969689, Tokens per Sec:     3655, Lr: 0.000196\r\n",
      "2024-08-27 15:16:41,321 - INFO - joeynmt.training - Epoch  88, Step:     8325, Batch Loss:     0.169757, Batch Acc: 0.969838, Tokens per Sec:     3957, Lr: 0.000196\r\n",
      "2024-08-27 15:16:42,400 - INFO - joeynmt.training - Epoch  88, Step:     8330, Batch Loss:     0.152288, Batch Acc: 0.978378, Tokens per Sec:     3947, Lr: 0.000196\r\n",
      "2024-08-27 15:16:43,559 - INFO - joeynmt.training - Epoch  88, Step:     8335, Batch Loss:     0.142374, Batch Acc: 0.976296, Tokens per Sec:     3717, Lr: 0.000196\r\n",
      "2024-08-27 15:16:44,711 - INFO - joeynmt.training - Epoch  88, Step:     8340, Batch Loss:     0.170623, Batch Acc: 0.969376, Tokens per Sec:     3684, Lr: 0.000196\r\n",
      "2024-08-27 15:16:45,790 - INFO - joeynmt.training - Epoch  88, Step:     8345, Batch Loss:     0.166387, Batch Acc: 0.968547, Tokens per Sec:     3865, Lr: 0.000196\r\n",
      "2024-08-27 15:16:46,922 - INFO - joeynmt.training - Epoch  88, Step:     8350, Batch Loss:     0.176172, Batch Acc: 0.975154, Tokens per Sec:     3733, Lr: 0.000196\r\n",
      "2024-08-27 15:16:48,001 - INFO - joeynmt.training - Epoch  88, Step:     8355, Batch Loss:     0.162413, Batch Acc: 0.974274, Tokens per Sec:     3932, Lr: 0.000196\r\n",
      "2024-08-27 15:16:49,060 - INFO - joeynmt.training - Epoch  88, Step:     8360, Batch Loss:     0.173643, Batch Acc: 0.968861, Tokens per Sec:     4004, Lr: 0.000196\r\n",
      "2024-08-27 15:16:50,122 - INFO - joeynmt.training - Epoch  88, Step:     8365, Batch Loss:     0.161868, Batch Acc: 0.969349, Tokens per Sec:     3932, Lr: 0.000196\r\n",
      "2024-08-27 15:16:51,181 - INFO - joeynmt.training - Epoch  88, Step:     8370, Batch Loss:     0.151972, Batch Acc: 0.972190, Tokens per Sec:     4113, Lr: 0.000196\r\n",
      "2024-08-27 15:16:52,260 - INFO - joeynmt.training - Epoch  88, Step:     8375, Batch Loss:     0.146463, Batch Acc: 0.972822, Tokens per Sec:     3991, Lr: 0.000195\r\n",
      "2024-08-27 15:16:52,311 - INFO - joeynmt.training - Epoch  88, total training loss: 14.84, num. of seqs: 8065, num. of tokens: 80463, 20.6938[sec]\r\n",
      "2024-08-27 15:16:52,311 - INFO - joeynmt.training - EPOCH 89\r\n",
      "2024-08-27 15:16:53,387 - INFO - joeynmt.training - Epoch  89, Step:     8380, Batch Loss:     0.165559, Batch Acc: 0.973375, Tokens per Sec:     3889, Lr: 0.000195\r\n",
      "2024-08-27 15:16:54,454 - INFO - joeynmt.training - Epoch  89, Step:     8385, Batch Loss:     0.160062, Batch Acc: 0.975786, Tokens per Sec:     4029, Lr: 0.000195\r\n",
      "2024-08-27 15:16:55,527 - INFO - joeynmt.training - Epoch  89, Step:     8390, Batch Loss:     0.153657, Batch Acc: 0.976300, Tokens per Sec:     3855, Lr: 0.000195\r\n",
      "2024-08-27 15:16:56,649 - INFO - joeynmt.training - Epoch  89, Step:     8395, Batch Loss:     0.165827, Batch Acc: 0.974923, Tokens per Sec:     3769, Lr: 0.000195\r\n",
      "2024-08-27 15:16:57,729 - INFO - joeynmt.training - Epoch  89, Step:     8400, Batch Loss:     0.164740, Batch Acc: 0.972786, Tokens per Sec:     3746, Lr: 0.000195\r\n",
      "2024-08-27 15:16:58,804 - INFO - joeynmt.training - Epoch  89, Step:     8405, Batch Loss:     0.149309, Batch Acc: 0.978006, Tokens per Sec:     3893, Lr: 0.000195\r\n",
      "2024-08-27 15:16:59,897 - INFO - joeynmt.training - Epoch  89, Step:     8410, Batch Loss:     0.147348, Batch Acc: 0.972126, Tokens per Sec:     3975, Lr: 0.000195\r\n",
      "2024-08-27 15:17:01,044 - INFO - joeynmt.training - Epoch  89, Step:     8415, Batch Loss:     0.151443, Batch Acc: 0.978112, Tokens per Sec:     3705, Lr: 0.000195\r\n",
      "2024-08-27 15:17:02,121 - INFO - joeynmt.training - Epoch  89, Step:     8420, Batch Loss:     0.154788, Batch Acc: 0.975271, Tokens per Sec:     3946, Lr: 0.000195\r\n",
      "2024-08-27 15:17:03,201 - INFO - joeynmt.training - Epoch  89, Step:     8425, Batch Loss:     0.160231, Batch Acc: 0.969310, Tokens per Sec:     3771, Lr: 0.000195\r\n",
      "2024-08-27 15:17:04,282 - INFO - joeynmt.training - Epoch  89, Step:     8430, Batch Loss:     0.167791, Batch Acc: 0.971059, Tokens per Sec:     3937, Lr: 0.000195\r\n",
      "2024-08-27 15:17:05,347 - INFO - joeynmt.training - Epoch  89, Step:     8435, Batch Loss:     0.139935, Batch Acc: 0.976002, Tokens per Sec:     3916, Lr: 0.000195\r\n",
      "2024-08-27 15:17:06,410 - INFO - joeynmt.training - Epoch  89, Step:     8440, Batch Loss:     0.184085, Batch Acc: 0.972103, Tokens per Sec:     3949, Lr: 0.000195\r\n",
      "2024-08-27 15:17:07,526 - INFO - joeynmt.training - Epoch  89, Step:     8445, Batch Loss:     0.151513, Batch Acc: 0.976705, Tokens per Sec:     3692, Lr: 0.000195\r\n",
      "2024-08-27 15:17:08,593 - INFO - joeynmt.training - Epoch  89, Step:     8450, Batch Loss:     0.167819, Batch Acc: 0.973843, Tokens per Sec:     4052, Lr: 0.000195\r\n",
      "2024-08-27 15:17:09,651 - INFO - joeynmt.training - Epoch  89, Step:     8455, Batch Loss:     0.166550, Batch Acc: 0.972152, Tokens per Sec:     4142, Lr: 0.000195\r\n",
      "2024-08-27 15:17:10,736 - INFO - joeynmt.training - Epoch  89, Step:     8460, Batch Loss:     0.162408, Batch Acc: 0.971462, Tokens per Sec:     3913, Lr: 0.000194\r\n",
      "2024-08-27 15:17:11,821 - INFO - joeynmt.training - Epoch  89, Step:     8465, Batch Loss:     0.180240, Batch Acc: 0.966047, Tokens per Sec:     3722, Lr: 0.000194\r\n",
      "2024-08-27 15:17:12,915 - INFO - joeynmt.training - Epoch  89, Step:     8470, Batch Loss:     0.158840, Batch Acc: 0.973314, Tokens per Sec:     3839, Lr: 0.000194\r\n",
      "2024-08-27 15:17:13,119 - INFO - joeynmt.training - Epoch  89, total training loss: 14.66, num. of seqs: 8065, num. of tokens: 80463, 20.7909[sec]\r\n",
      "2024-08-27 15:17:13,119 - INFO - joeynmt.training - EPOCH 90\r\n",
      "2024-08-27 15:17:13,982 - INFO - joeynmt.training - Epoch  90, Step:     8475, Batch Loss:     0.150027, Batch Acc: 0.970453, Tokens per Sec:     4055, Lr: 0.000194\r\n",
      "2024-08-27 15:17:15,129 - INFO - joeynmt.training - Epoch  90, Step:     8480, Batch Loss:     0.148229, Batch Acc: 0.977893, Tokens per Sec:     3711, Lr: 0.000194\r\n",
      "2024-08-27 15:17:16,210 - INFO - joeynmt.training - Epoch  90, Step:     8485, Batch Loss:     0.123908, Batch Acc: 0.978369, Tokens per Sec:     3893, Lr: 0.000194\r\n",
      "2024-08-27 15:17:17,347 - INFO - joeynmt.training - Epoch  90, Step:     8490, Batch Loss:     0.147431, Batch Acc: 0.978550, Tokens per Sec:     3898, Lr: 0.000194\r\n",
      "2024-08-27 15:17:18,443 - INFO - joeynmt.training - Epoch  90, Step:     8495, Batch Loss:     0.148322, Batch Acc: 0.974085, Tokens per Sec:     3843, Lr: 0.000194\r\n",
      "2024-08-27 15:17:19,529 - INFO - joeynmt.training - Epoch  90, Step:     8500, Batch Loss:     0.132812, Batch Acc: 0.976995, Tokens per Sec:     3843, Lr: 0.000194\r\n",
      "2024-08-27 15:17:19,530 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=8542\r\n",
      "2024-08-27 15:17:19,530 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 65.41it/s]\r\n",
      "2024-08-27 15:17:41,853 - INFO - joeynmt.prediction - Generation took 22.3208[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 45004.64 examples/s]\r\n",
      "2024-08-27 15:17:42,228 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:17:42,228 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.03, loss:   6.87, ppl: 967.09, acc:   0.28, 0.1620[sec]\r\n",
      "2024-08-27 15:17:42,543 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/8500.ckpt.\r\n",
      "2024-08-27 15:17:42,544 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/7000.ckpt\r\n",
      "2024-08-27 15:17:42,569 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:17:42,717 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:17:42,717 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:17:42,718 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 15:17:43,010 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:17:43,155 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:17:43,155 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:17:43,155 - INFO - joeynmt.training - \tHypothesis: trois tables blanches\r\n",
      "2024-08-27 15:17:43,447 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:17:43,592 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:17:43,593 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:17:43,593 - INFO - joeynmt.training - \tHypothesis: le groupe disparaît avec lui\r\n",
      "2024-08-27 15:17:43,883 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:17:44,028 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:17:44,028 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:17:44,028 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 15:17:45,389 - INFO - joeynmt.training - Epoch  90, Step:     8505, Batch Loss:     0.130702, Batch Acc: 0.978873, Tokens per Sec:     3976, Lr: 0.000194\r\n",
      "2024-08-27 15:17:46,465 - INFO - joeynmt.training - Epoch  90, Step:     8510, Batch Loss:     0.138440, Batch Acc: 0.975508, Tokens per Sec:     3798, Lr: 0.000194\r\n",
      "2024-08-27 15:17:47,566 - INFO - joeynmt.training - Epoch  90, Step:     8515, Batch Loss:     0.146598, Batch Acc: 0.976424, Tokens per Sec:     3701, Lr: 0.000194\r\n",
      "2024-08-27 15:17:48,623 - INFO - joeynmt.training - Epoch  90, Step:     8520, Batch Loss:     0.123284, Batch Acc: 0.979344, Tokens per Sec:     3665, Lr: 0.000194\r\n",
      "2024-08-27 15:17:49,710 - INFO - joeynmt.training - Epoch  90, Step:     8525, Batch Loss:     0.153722, Batch Acc: 0.974762, Tokens per Sec:     3869, Lr: 0.000194\r\n",
      "2024-08-27 15:17:50,790 - INFO - joeynmt.training - Epoch  90, Step:     8530, Batch Loss:     0.154751, Batch Acc: 0.974063, Tokens per Sec:     3927, Lr: 0.000194\r\n",
      "2024-08-27 15:17:51,849 - INFO - joeynmt.training - Epoch  90, Step:     8535, Batch Loss:     0.136303, Batch Acc: 0.975651, Tokens per Sec:     3961, Lr: 0.000194\r\n",
      "2024-08-27 15:17:52,928 - INFO - joeynmt.training - Epoch  90, Step:     8540, Batch Loss:     0.138982, Batch Acc: 0.974556, Tokens per Sec:     3972, Lr: 0.000194\r\n",
      "2024-08-27 15:17:53,985 - INFO - joeynmt.training - Epoch  90, Step:     8545, Batch Loss:     0.174325, Batch Acc: 0.971401, Tokens per Sec:     3905, Lr: 0.000194\r\n",
      "2024-08-27 15:17:55,064 - INFO - joeynmt.training - Epoch  90, Step:     8550, Batch Loss:     0.179669, Batch Acc: 0.973286, Tokens per Sec:     3924, Lr: 0.000193\r\n",
      "2024-08-27 15:17:56,210 - INFO - joeynmt.training - Epoch  90, Step:     8555, Batch Loss:     0.134409, Batch Acc: 0.972755, Tokens per Sec:     3779, Lr: 0.000193\r\n",
      "2024-08-27 15:17:57,323 - INFO - joeynmt.training - Epoch  90, Step:     8560, Batch Loss:     0.188912, Batch Acc: 0.970894, Tokens per Sec:     3802, Lr: 0.000193\r\n",
      "2024-08-27 15:17:58,389 - INFO - joeynmt.training - Epoch  90, Step:     8565, Batch Loss:     0.152479, Batch Acc: 0.973822, Tokens per Sec:     3945, Lr: 0.000193\r\n",
      "2024-08-27 15:17:58,770 - INFO - joeynmt.training - Epoch  90, total training loss: 14.18, num. of seqs: 8065, num. of tokens: 80463, 20.8455[sec]\r\n",
      "2024-08-27 15:17:58,770 - INFO - joeynmt.training - EPOCH 91\r\n",
      "2024-08-27 15:17:59,644 - INFO - joeynmt.training - Epoch  91, Step:     8570, Batch Loss:     0.124770, Batch Acc: 0.979515, Tokens per Sec:     3989, Lr: 0.000193\r\n",
      "2024-08-27 15:18:00,705 - INFO - joeynmt.training - Epoch  91, Step:     8575, Batch Loss:     0.129089, Batch Acc: 0.975704, Tokens per Sec:     4151, Lr: 0.000193\r\n",
      "2024-08-27 15:18:01,774 - INFO - joeynmt.training - Epoch  91, Step:     8580, Batch Loss:     0.147445, Batch Acc: 0.976527, Tokens per Sec:     3911, Lr: 0.000193\r\n",
      "2024-08-27 15:18:02,848 - INFO - joeynmt.training - Epoch  91, Step:     8585, Batch Loss:     0.144014, Batch Acc: 0.978800, Tokens per Sec:     3867, Lr: 0.000193\r\n",
      "2024-08-27 15:18:03,911 - INFO - joeynmt.training - Epoch  91, Step:     8590, Batch Loss:     0.155263, Batch Acc: 0.974445, Tokens per Sec:     3941, Lr: 0.000193\r\n",
      "2024-08-27 15:18:04,989 - INFO - joeynmt.training - Epoch  91, Step:     8595, Batch Loss:     0.133085, Batch Acc: 0.976576, Tokens per Sec:     3842, Lr: 0.000193\r\n",
      "2024-08-27 15:18:06,071 - INFO - joeynmt.training - Epoch  91, Step:     8600, Batch Loss:     0.150791, Batch Acc: 0.977262, Tokens per Sec:     3825, Lr: 0.000193\r\n",
      "2024-08-27 15:18:07,206 - INFO - joeynmt.training - Epoch  91, Step:     8605, Batch Loss:     0.152816, Batch Acc: 0.975864, Tokens per Sec:     3799, Lr: 0.000193\r\n",
      "2024-08-27 15:18:08,293 - INFO - joeynmt.training - Epoch  91, Step:     8610, Batch Loss:     0.157431, Batch Acc: 0.975987, Tokens per Sec:     3985, Lr: 0.000193\r\n",
      "2024-08-27 15:18:09,384 - INFO - joeynmt.training - Epoch  91, Step:     8615, Batch Loss:     0.131659, Batch Acc: 0.974900, Tokens per Sec:     3910, Lr: 0.000193\r\n",
      "2024-08-27 15:18:10,463 - INFO - joeynmt.training - Epoch  91, Step:     8620, Batch Loss:     0.161382, Batch Acc: 0.978861, Tokens per Sec:     3727, Lr: 0.000193\r\n",
      "2024-08-27 15:18:11,555 - INFO - joeynmt.training - Epoch  91, Step:     8625, Batch Loss:     0.141646, Batch Acc: 0.970831, Tokens per Sec:     4118, Lr: 0.000193\r\n",
      "2024-08-27 15:18:12,622 - INFO - joeynmt.training - Epoch  91, Step:     8630, Batch Loss:     0.163235, Batch Acc: 0.972062, Tokens per Sec:     3894, Lr: 0.000193\r\n",
      "2024-08-27 15:18:13,687 - INFO - joeynmt.training - Epoch  91, Step:     8635, Batch Loss:     0.140205, Batch Acc: 0.978176, Tokens per Sec:     3829, Lr: 0.000193\r\n",
      "2024-08-27 15:18:14,753 - INFO - joeynmt.training - Epoch  91, Step:     8640, Batch Loss:     0.149198, Batch Acc: 0.971025, Tokens per Sec:     3793, Lr: 0.000192\r\n",
      "2024-08-27 15:18:15,837 - INFO - joeynmt.training - Epoch  91, Step:     8645, Batch Loss:     0.150796, Batch Acc: 0.976546, Tokens per Sec:     4013, Lr: 0.000192\r\n",
      "2024-08-27 15:18:16,968 - INFO - joeynmt.training - Epoch  91, Step:     8650, Batch Loss:     0.150652, Batch Acc: 0.974840, Tokens per Sec:     4009, Lr: 0.000192\r\n",
      "2024-08-27 15:18:18,051 - INFO - joeynmt.training - Epoch  91, Step:     8655, Batch Loss:     0.161543, Batch Acc: 0.976185, Tokens per Sec:     3840, Lr: 0.000192\r\n",
      "2024-08-27 15:18:19,137 - INFO - joeynmt.training - Epoch  91, Step:     8660, Batch Loss:     0.166011, Batch Acc: 0.974038, Tokens per Sec:     3905, Lr: 0.000192\r\n",
      "2024-08-27 15:18:19,397 - INFO - joeynmt.training - Epoch  91, total training loss: 14.08, num. of seqs: 8065, num. of tokens: 80463, 20.6095[sec]\r\n",
      "2024-08-27 15:18:19,398 - INFO - joeynmt.training - EPOCH 92\r\n",
      "2024-08-27 15:18:20,286 - INFO - joeynmt.training - Epoch  92, Step:     8665, Batch Loss:     0.128459, Batch Acc: 0.979378, Tokens per Sec:     3892, Lr: 0.000192\r\n",
      "2024-08-27 15:18:21,369 - INFO - joeynmt.training - Epoch  92, Step:     8670, Batch Loss:     0.119656, Batch Acc: 0.980715, Tokens per Sec:     3930, Lr: 0.000192\r\n",
      "2024-08-27 15:18:22,433 - INFO - joeynmt.training - Epoch  92, Step:     8675, Batch Loss:     0.129978, Batch Acc: 0.978035, Tokens per Sec:     3895, Lr: 0.000192\r\n",
      "2024-08-27 15:18:23,509 - INFO - joeynmt.training - Epoch  92, Step:     8680, Batch Loss:     0.141105, Batch Acc: 0.979637, Tokens per Sec:     4204, Lr: 0.000192\r\n",
      "2024-08-27 15:18:24,585 - INFO - joeynmt.training - Epoch  92, Step:     8685, Batch Loss:     0.146525, Batch Acc: 0.977386, Tokens per Sec:     3903, Lr: 0.000192\r\n",
      "2024-08-27 15:18:25,636 - INFO - joeynmt.training - Epoch  92, Step:     8690, Batch Loss:     0.128169, Batch Acc: 0.978516, Tokens per Sec:     3902, Lr: 0.000192\r\n",
      "2024-08-27 15:18:26,709 - INFO - joeynmt.training - Epoch  92, Step:     8695, Batch Loss:     0.152757, Batch Acc: 0.977267, Tokens per Sec:     3854, Lr: 0.000192\r\n",
      "2024-08-27 15:18:27,795 - INFO - joeynmt.training - Epoch  92, Step:     8700, Batch Loss:     0.139051, Batch Acc: 0.976540, Tokens per Sec:     3575, Lr: 0.000192\r\n",
      "2024-08-27 15:18:28,862 - INFO - joeynmt.training - Epoch  92, Step:     8705, Batch Loss:     0.140423, Batch Acc: 0.972340, Tokens per Sec:     3761, Lr: 0.000192\r\n",
      "2024-08-27 15:18:29,947 - INFO - joeynmt.training - Epoch  92, Step:     8710, Batch Loss:     0.147530, Batch Acc: 0.975232, Tokens per Sec:     3876, Lr: 0.000192\r\n",
      "2024-08-27 15:18:31,016 - INFO - joeynmt.training - Epoch  92, Step:     8715, Batch Loss:     0.144809, Batch Acc: 0.974512, Tokens per Sec:     3928, Lr: 0.000192\r\n",
      "2024-08-27 15:18:32,127 - INFO - joeynmt.training - Epoch  92, Step:     8720, Batch Loss:     0.147206, Batch Acc: 0.973243, Tokens per Sec:     3971, Lr: 0.000192\r\n",
      "2024-08-27 15:18:33,197 - INFO - joeynmt.training - Epoch  92, Step:     8725, Batch Loss:     0.161461, Batch Acc: 0.974901, Tokens per Sec:     3763, Lr: 0.000192\r\n",
      "2024-08-27 15:18:34,267 - INFO - joeynmt.training - Epoch  92, Step:     8730, Batch Loss:     0.152253, Batch Acc: 0.973957, Tokens per Sec:     4059, Lr: 0.000191\r\n",
      "2024-08-27 15:18:35,350 - INFO - joeynmt.training - Epoch  92, Step:     8735, Batch Loss:     0.170200, Batch Acc: 0.972235, Tokens per Sec:     3925, Lr: 0.000191\r\n",
      "2024-08-27 15:18:36,440 - INFO - joeynmt.training - Epoch  92, Step:     8740, Batch Loss:     0.151239, Batch Acc: 0.978333, Tokens per Sec:     3899, Lr: 0.000191\r\n",
      "2024-08-27 15:18:37,553 - INFO - joeynmt.training - Epoch  92, Step:     8745, Batch Loss:     0.152702, Batch Acc: 0.974163, Tokens per Sec:     3654, Lr: 0.000191\r\n",
      "2024-08-27 15:18:38,645 - INFO - joeynmt.training - Epoch  92, Step:     8750, Batch Loss:     0.157534, Batch Acc: 0.972797, Tokens per Sec:     3943, Lr: 0.000191\r\n",
      "2024-08-27 15:18:39,721 - INFO - joeynmt.training - Epoch  92, Step:     8755, Batch Loss:     0.147614, Batch Acc: 0.974628, Tokens per Sec:     3810, Lr: 0.000191\r\n",
      "2024-08-27 15:18:40,196 - INFO - joeynmt.training - Epoch  92, total training loss: 14.01, num. of seqs: 8065, num. of tokens: 80463, 20.7821[sec]\r\n",
      "2024-08-27 15:18:40,197 - INFO - joeynmt.training - EPOCH 93\r\n",
      "2024-08-27 15:18:40,855 - INFO - joeynmt.training - Epoch  93, Step:     8760, Batch Loss:     0.123433, Batch Acc: 0.978972, Tokens per Sec:     3923, Lr: 0.000191\r\n",
      "2024-08-27 15:18:41,957 - INFO - joeynmt.training - Epoch  93, Step:     8765, Batch Loss:     0.151109, Batch Acc: 0.971324, Tokens per Sec:     3705, Lr: 0.000191\r\n",
      "2024-08-27 15:18:43,033 - INFO - joeynmt.training - Epoch  93, Step:     8770, Batch Loss:     0.141150, Batch Acc: 0.976921, Tokens per Sec:     3907, Lr: 0.000191\r\n",
      "2024-08-27 15:18:44,101 - INFO - joeynmt.training - Epoch  93, Step:     8775, Batch Loss:     0.145590, Batch Acc: 0.978561, Tokens per Sec:     3936, Lr: 0.000191\r\n",
      "2024-08-27 15:18:45,158 - INFO - joeynmt.training - Epoch  93, Step:     8780, Batch Loss:     0.137377, Batch Acc: 0.977250, Tokens per Sec:     3868, Lr: 0.000191\r\n",
      "2024-08-27 15:18:46,225 - INFO - joeynmt.training - Epoch  93, Step:     8785, Batch Loss:     0.128226, Batch Acc: 0.980414, Tokens per Sec:     4120, Lr: 0.000191\r\n",
      "2024-08-27 15:18:47,323 - INFO - joeynmt.training - Epoch  93, Step:     8790, Batch Loss:     0.162510, Batch Acc: 0.979271, Tokens per Sec:     3823, Lr: 0.000191\r\n",
      "2024-08-27 15:18:48,371 - INFO - joeynmt.training - Epoch  93, Step:     8795, Batch Loss:     0.150127, Batch Acc: 0.978436, Tokens per Sec:     4031, Lr: 0.000191\r\n",
      "2024-08-27 15:18:49,442 - INFO - joeynmt.training - Epoch  93, Step:     8800, Batch Loss:     0.127406, Batch Acc: 0.977668, Tokens per Sec:     3975, Lr: 0.000191\r\n",
      "2024-08-27 15:18:50,508 - INFO - joeynmt.training - Epoch  93, Step:     8805, Batch Loss:     0.135922, Batch Acc: 0.974823, Tokens per Sec:     3841, Lr: 0.000191\r\n",
      "2024-08-27 15:18:51,595 - INFO - joeynmt.training - Epoch  93, Step:     8810, Batch Loss:     0.150072, Batch Acc: 0.974708, Tokens per Sec:     3783, Lr: 0.000191\r\n",
      "2024-08-27 15:18:52,720 - INFO - joeynmt.training - Epoch  93, Step:     8815, Batch Loss:     0.135423, Batch Acc: 0.974282, Tokens per Sec:     3841, Lr: 0.000191\r\n",
      "2024-08-27 15:18:53,822 - INFO - joeynmt.training - Epoch  93, Step:     8820, Batch Loss:     0.132836, Batch Acc: 0.976899, Tokens per Sec:     3812, Lr: 0.000190\r\n",
      "2024-08-27 15:18:54,909 - INFO - joeynmt.training - Epoch  93, Step:     8825, Batch Loss:     0.148080, Batch Acc: 0.974442, Tokens per Sec:     3999, Lr: 0.000190\r\n",
      "2024-08-27 15:18:55,995 - INFO - joeynmt.training - Epoch  93, Step:     8830, Batch Loss:     0.146893, Batch Acc: 0.976465, Tokens per Sec:     3914, Lr: 0.000190\r\n",
      "2024-08-27 15:18:57,116 - INFO - joeynmt.training - Epoch  93, Step:     8835, Batch Loss:     0.135694, Batch Acc: 0.976665, Tokens per Sec:     3672, Lr: 0.000190\r\n",
      "2024-08-27 15:18:58,189 - INFO - joeynmt.training - Epoch  93, Step:     8840, Batch Loss:     0.146534, Batch Acc: 0.974538, Tokens per Sec:     3882, Lr: 0.000190\r\n",
      "2024-08-27 15:18:59,254 - INFO - joeynmt.training - Epoch  93, Step:     8845, Batch Loss:     0.156043, Batch Acc: 0.974052, Tokens per Sec:     4165, Lr: 0.000190\r\n",
      "2024-08-27 15:19:00,321 - INFO - joeynmt.training - Epoch  93, Step:     8850, Batch Loss:     0.135330, Batch Acc: 0.975475, Tokens per Sec:     3900, Lr: 0.000190\r\n",
      "2024-08-27 15:19:00,883 - INFO - joeynmt.training - Epoch  93, total training loss: 13.73, num. of seqs: 8065, num. of tokens: 80463, 20.6699[sec]\r\n",
      "2024-08-27 15:19:00,883 - INFO - joeynmt.training - EPOCH 94\r\n",
      "2024-08-27 15:19:01,525 - INFO - joeynmt.training - Epoch  94, Step:     8855, Batch Loss:     0.137710, Batch Acc: 0.977692, Tokens per Sec:     4079, Lr: 0.000190\r\n",
      "2024-08-27 15:19:02,613 - INFO - joeynmt.training - Epoch  94, Step:     8860, Batch Loss:     0.144256, Batch Acc: 0.974770, Tokens per Sec:     3900, Lr: 0.000190\r\n",
      "2024-08-27 15:19:03,683 - INFO - joeynmt.training - Epoch  94, Step:     8865, Batch Loss:     0.127456, Batch Acc: 0.977517, Tokens per Sec:     3827, Lr: 0.000190\r\n",
      "2024-08-27 15:19:04,747 - INFO - joeynmt.training - Epoch  94, Step:     8870, Batch Loss:     0.130585, Batch Acc: 0.975209, Tokens per Sec:     3945, Lr: 0.000190\r\n",
      "2024-08-27 15:19:05,831 - INFO - joeynmt.training - Epoch  94, Step:     8875, Batch Loss:     0.126957, Batch Acc: 0.980160, Tokens per Sec:     4047, Lr: 0.000190\r\n",
      "2024-08-27 15:19:06,946 - INFO - joeynmt.training - Epoch  94, Step:     8880, Batch Loss:     0.165121, Batch Acc: 0.976382, Tokens per Sec:     3686, Lr: 0.000190\r\n",
      "2024-08-27 15:19:08,031 - INFO - joeynmt.training - Epoch  94, Step:     8885, Batch Loss:     0.126561, Batch Acc: 0.978462, Tokens per Sec:     3896, Lr: 0.000190\r\n",
      "2024-08-27 15:19:09,117 - INFO - joeynmt.training - Epoch  94, Step:     8890, Batch Loss:     0.134852, Batch Acc: 0.973896, Tokens per Sec:     3778, Lr: 0.000190\r\n",
      "2024-08-27 15:19:10,210 - INFO - joeynmt.training - Epoch  94, Step:     8895, Batch Loss:     0.122225, Batch Acc: 0.977622, Tokens per Sec:     3926, Lr: 0.000190\r\n",
      "2024-08-27 15:19:11,292 - INFO - joeynmt.training - Epoch  94, Step:     8900, Batch Loss:     0.162934, Batch Acc: 0.978788, Tokens per Sec:     3968, Lr: 0.000190\r\n",
      "2024-08-27 15:19:12,357 - INFO - joeynmt.training - Epoch  94, Step:     8905, Batch Loss:     0.123919, Batch Acc: 0.980230, Tokens per Sec:     3755, Lr: 0.000190\r\n",
      "2024-08-27 15:19:13,437 - INFO - joeynmt.training - Epoch  94, Step:     8910, Batch Loss:     0.166708, Batch Acc: 0.978405, Tokens per Sec:     3905, Lr: 0.000190\r\n",
      "2024-08-27 15:19:14,508 - INFO - joeynmt.training - Epoch  94, Step:     8915, Batch Loss:     0.156477, Batch Acc: 0.975210, Tokens per Sec:     3994, Lr: 0.000189\r\n",
      "2024-08-27 15:19:15,599 - INFO - joeynmt.training - Epoch  94, Step:     8920, Batch Loss:     0.132995, Batch Acc: 0.979902, Tokens per Sec:     3925, Lr: 0.000189\r\n",
      "2024-08-27 15:19:16,716 - INFO - joeynmt.training - Epoch  94, Step:     8925, Batch Loss:     0.138316, Batch Acc: 0.978303, Tokens per Sec:     3717, Lr: 0.000189\r\n",
      "2024-08-27 15:19:17,789 - INFO - joeynmt.training - Epoch  94, Step:     8930, Batch Loss:     0.134330, Batch Acc: 0.974386, Tokens per Sec:     3602, Lr: 0.000189\r\n",
      "2024-08-27 15:19:18,860 - INFO - joeynmt.training - Epoch  94, Step:     8935, Batch Loss:     0.145122, Batch Acc: 0.974764, Tokens per Sec:     3964, Lr: 0.000189\r\n",
      "2024-08-27 15:19:19,934 - INFO - joeynmt.training - Epoch  94, Step:     8940, Batch Loss:     0.141967, Batch Acc: 0.974166, Tokens per Sec:     3966, Lr: 0.000189\r\n",
      "2024-08-27 15:19:20,990 - INFO - joeynmt.training - Epoch  94, Step:     8945, Batch Loss:     0.162389, Batch Acc: 0.974576, Tokens per Sec:     3802, Lr: 0.000189\r\n",
      "2024-08-27 15:19:21,701 - INFO - joeynmt.training - Epoch  94, total training loss: 13.66, num. of seqs: 8065, num. of tokens: 80463, 20.8004[sec]\r\n",
      "2024-08-27 15:19:21,701 - INFO - joeynmt.training - EPOCH 95\r\n",
      "2024-08-27 15:19:22,139 - INFO - joeynmt.training - Epoch  95, Step:     8950, Batch Loss:     0.144590, Batch Acc: 0.979058, Tokens per Sec:     3963, Lr: 0.000189\r\n",
      "2024-08-27 15:19:23,219 - INFO - joeynmt.training - Epoch  95, Step:     8955, Batch Loss:     0.137679, Batch Acc: 0.977119, Tokens per Sec:     3971, Lr: 0.000189\r\n",
      "2024-08-27 15:19:24,293 - INFO - joeynmt.training - Epoch  95, Step:     8960, Batch Loss:     0.136682, Batch Acc: 0.976313, Tokens per Sec:     4168, Lr: 0.000189\r\n",
      "2024-08-27 15:19:25,360 - INFO - joeynmt.training - Epoch  95, Step:     8965, Batch Loss:     0.131390, Batch Acc: 0.980076, Tokens per Sec:     3955, Lr: 0.000189\r\n",
      "2024-08-27 15:19:26,426 - INFO - joeynmt.training - Epoch  95, Step:     8970, Batch Loss:     0.157227, Batch Acc: 0.972914, Tokens per Sec:     3878, Lr: 0.000189\r\n",
      "2024-08-27 15:19:27,540 - INFO - joeynmt.training - Epoch  95, Step:     8975, Batch Loss:     0.129937, Batch Acc: 0.978061, Tokens per Sec:     3811, Lr: 0.000189\r\n",
      "2024-08-27 15:19:28,614 - INFO - joeynmt.training - Epoch  95, Step:     8980, Batch Loss:     0.154619, Batch Acc: 0.978665, Tokens per Sec:     4103, Lr: 0.000189\r\n",
      "2024-08-27 15:19:29,685 - INFO - joeynmt.training - Epoch  95, Step:     8985, Batch Loss:     0.125823, Batch Acc: 0.978534, Tokens per Sec:     3873, Lr: 0.000189\r\n",
      "2024-08-27 15:19:30,789 - INFO - joeynmt.training - Epoch  95, Step:     8990, Batch Loss:     0.136173, Batch Acc: 0.978408, Tokens per Sec:     4030, Lr: 0.000189\r\n",
      "2024-08-27 15:19:31,875 - INFO - joeynmt.training - Epoch  95, Step:     8995, Batch Loss:     0.142120, Batch Acc: 0.979597, Tokens per Sec:     3930, Lr: 0.000189\r\n",
      "2024-08-27 15:19:32,945 - INFO - joeynmt.training - Epoch  95, Step:     9000, Batch Loss:     0.131091, Batch Acc: 0.981055, Tokens per Sec:     4145, Lr: 0.000189\r\n",
      "2024-08-27 15:19:32,946 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=9042\r\n",
      "2024-08-27 15:19:32,946 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 65.08it/s]\r\n",
      "2024-08-27 15:19:55,384 - INFO - joeynmt.prediction - Generation took 22.4355[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 45035.19 examples/s]\r\n",
      "2024-08-27 15:19:55,763 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:19:55,763 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.29, loss:   6.91, ppl: 1006.59, acc:   0.28, 0.1660[sec]\r\n",
      "2024-08-27 15:19:55,764 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\r\n",
      "2024-08-27 15:19:56,081 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/9000.ckpt.\r\n",
      "2024-08-27 15:19:56,082 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/8500.ckpt\r\n",
      "2024-08-27 15:19:56,106 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:19:56,255 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:19:56,255 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:19:56,255 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 15:19:56,560 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:19:56,717 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:19:56,717 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:19:56,717 - INFO - joeynmt.training - \tHypothesis: trois maisons en bois\r\n",
      "2024-08-27 15:19:57,017 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:19:57,163 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:19:57,163 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:19:57,164 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 15:19:57,459 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:19:57,606 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:19:57,607 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:19:57,607 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 15:19:59,002 - INFO - joeynmt.training - Epoch  95, Step:     9005, Batch Loss:     0.156051, Batch Acc: 0.979659, Tokens per Sec:     3896, Lr: 0.000189\r\n",
      "2024-08-27 15:20:00,079 - INFO - joeynmt.training - Epoch  95, Step:     9010, Batch Loss:     0.132391, Batch Acc: 0.979749, Tokens per Sec:     3854, Lr: 0.000188\r\n",
      "2024-08-27 15:20:01,153 - INFO - joeynmt.training - Epoch  95, Step:     9015, Batch Loss:     0.143926, Batch Acc: 0.977987, Tokens per Sec:     3765, Lr: 0.000188\r\n",
      "2024-08-27 15:20:02,238 - INFO - joeynmt.training - Epoch  95, Step:     9020, Batch Loss:     0.130762, Batch Acc: 0.975006, Tokens per Sec:     3839, Lr: 0.000188\r\n",
      "2024-08-27 15:20:03,318 - INFO - joeynmt.training - Epoch  95, Step:     9025, Batch Loss:     0.156416, Batch Acc: 0.976287, Tokens per Sec:     3868, Lr: 0.000188\r\n",
      "2024-08-27 15:20:04,420 - INFO - joeynmt.training - Epoch  95, Step:     9030, Batch Loss:     0.149497, Batch Acc: 0.978376, Tokens per Sec:     3947, Lr: 0.000188\r\n",
      "2024-08-27 15:20:05,499 - INFO - joeynmt.training - Epoch  95, Step:     9035, Batch Loss:     0.148605, Batch Acc: 0.975018, Tokens per Sec:     3898, Lr: 0.000188\r\n",
      "2024-08-27 15:20:06,597 - INFO - joeynmt.training - Epoch  95, Step:     9040, Batch Loss:     0.134169, Batch Acc: 0.979253, Tokens per Sec:     3734, Lr: 0.000188\r\n",
      "2024-08-27 15:20:07,219 - INFO - joeynmt.training - Epoch  95, total training loss: 12.91, num. of seqs: 8065, num. of tokens: 80463, 20.5431[sec]\r\n",
      "2024-08-27 15:20:07,220 - INFO - joeynmt.training - EPOCH 96\r\n",
      "2024-08-27 15:20:07,887 - INFO - joeynmt.training - Epoch  96, Step:     9045, Batch Loss:     0.146500, Batch Acc: 0.980330, Tokens per Sec:     3834, Lr: 0.000188\r\n",
      "2024-08-27 15:20:09,003 - INFO - joeynmt.training - Epoch  96, Step:     9050, Batch Loss:     0.136072, Batch Acc: 0.979177, Tokens per Sec:     3746, Lr: 0.000188\r\n",
      "2024-08-27 15:20:10,104 - INFO - joeynmt.training - Epoch  96, Step:     9055, Batch Loss:     0.121508, Batch Acc: 0.977198, Tokens per Sec:     3866, Lr: 0.000188\r\n",
      "2024-08-27 15:20:11,204 - INFO - joeynmt.training - Epoch  96, Step:     9060, Batch Loss:     0.131619, Batch Acc: 0.976225, Tokens per Sec:     3748, Lr: 0.000188\r\n",
      "2024-08-27 15:20:12,275 - INFO - joeynmt.training - Epoch  96, Step:     9065, Batch Loss:     0.120283, Batch Acc: 0.978293, Tokens per Sec:     3791, Lr: 0.000188\r\n",
      "2024-08-27 15:20:13,384 - INFO - joeynmt.training - Epoch  96, Step:     9070, Batch Loss:     0.132457, Batch Acc: 0.982223, Tokens per Sec:     4010, Lr: 0.000188\r\n",
      "2024-08-27 15:20:14,461 - INFO - joeynmt.training - Epoch  96, Step:     9075, Batch Loss:     0.148158, Batch Acc: 0.978785, Tokens per Sec:     3853, Lr: 0.000188\r\n",
      "2024-08-27 15:20:15,550 - INFO - joeynmt.training - Epoch  96, Step:     9080, Batch Loss:     0.142687, Batch Acc: 0.973929, Tokens per Sec:     3945, Lr: 0.000188\r\n",
      "2024-08-27 15:20:16,663 - INFO - joeynmt.training - Epoch  96, Step:     9085, Batch Loss:     0.155941, Batch Acc: 0.975945, Tokens per Sec:     3665, Lr: 0.000188\r\n",
      "2024-08-27 15:20:17,787 - INFO - joeynmt.training - Epoch  96, Step:     9090, Batch Loss:     0.125731, Batch Acc: 0.978616, Tokens per Sec:     3705, Lr: 0.000188\r\n",
      "2024-08-27 15:20:18,864 - INFO - joeynmt.training - Epoch  96, Step:     9095, Batch Loss:     0.141003, Batch Acc: 0.977957, Tokens per Sec:     3793, Lr: 0.000188\r\n",
      "2024-08-27 15:20:19,948 - INFO - joeynmt.training - Epoch  96, Step:     9100, Batch Loss:     0.140360, Batch Acc: 0.977747, Tokens per Sec:     3982, Lr: 0.000188\r\n",
      "2024-08-27 15:20:21,023 - INFO - joeynmt.training - Epoch  96, Step:     9105, Batch Loss:     0.127393, Batch Acc: 0.978678, Tokens per Sec:     3926, Lr: 0.000187\r\n",
      "2024-08-27 15:20:22,112 - INFO - joeynmt.training - Epoch  96, Step:     9110, Batch Loss:     0.142171, Batch Acc: 0.976485, Tokens per Sec:     3869, Lr: 0.000187\r\n",
      "2024-08-27 15:20:23,220 - INFO - joeynmt.training - Epoch  96, Step:     9115, Batch Loss:     0.154644, Batch Acc: 0.976398, Tokens per Sec:     3829, Lr: 0.000187\r\n",
      "2024-08-27 15:20:24,345 - INFO - joeynmt.training - Epoch  96, Step:     9120, Batch Loss:     0.156628, Batch Acc: 0.972696, Tokens per Sec:     3907, Lr: 0.000187\r\n",
      "2024-08-27 15:20:25,431 - INFO - joeynmt.training - Epoch  96, Step:     9125, Batch Loss:     0.141921, Batch Acc: 0.975455, Tokens per Sec:     4057, Lr: 0.000187\r\n",
      "2024-08-27 15:20:26,515 - INFO - joeynmt.training - Epoch  96, Step:     9130, Batch Loss:     0.140949, Batch Acc: 0.979554, Tokens per Sec:     3973, Lr: 0.000187\r\n",
      "2024-08-27 15:20:27,691 - INFO - joeynmt.training - Epoch  96, Step:     9135, Batch Loss:     0.137143, Batch Acc: 0.976826, Tokens per Sec:     3634, Lr: 0.000187\r\n",
      "2024-08-27 15:20:28,176 - INFO - joeynmt.training - Epoch  96, total training loss: 13.21, num. of seqs: 8065, num. of tokens: 80463, 20.9385[sec]\r\n",
      "2024-08-27 15:20:28,176 - INFO - joeynmt.training - EPOCH 97\r\n",
      "2024-08-27 15:20:28,834 - INFO - joeynmt.training - Epoch  97, Step:     9140, Batch Loss:     0.155290, Batch Acc: 0.975717, Tokens per Sec:     3840, Lr: 0.000187\r\n",
      "2024-08-27 15:20:29,954 - INFO - joeynmt.training - Epoch  97, Step:     9145, Batch Loss:     0.140560, Batch Acc: 0.978753, Tokens per Sec:     3868, Lr: 0.000187\r\n",
      "2024-08-27 15:20:31,039 - INFO - joeynmt.training - Epoch  97, Step:     9150, Batch Loss:     0.123049, Batch Acc: 0.981801, Tokens per Sec:     3803, Lr: 0.000187\r\n",
      "2024-08-27 15:20:32,158 - INFO - joeynmt.training - Epoch  97, Step:     9155, Batch Loss:     0.126742, Batch Acc: 0.979688, Tokens per Sec:     3785, Lr: 0.000187\r\n",
      "2024-08-27 15:20:33,241 - INFO - joeynmt.training - Epoch  97, Step:     9160, Batch Loss:     0.130585, Batch Acc: 0.978356, Tokens per Sec:     3800, Lr: 0.000187\r\n",
      "2024-08-27 15:20:34,306 - INFO - joeynmt.training - Epoch  97, Step:     9165, Batch Loss:     0.131798, Batch Acc: 0.979688, Tokens per Sec:     3978, Lr: 0.000187\r\n",
      "2024-08-27 15:20:35,378 - INFO - joeynmt.training - Epoch  97, Step:     9170, Batch Loss:     0.137901, Batch Acc: 0.980689, Tokens per Sec:     4012, Lr: 0.000187\r\n",
      "2024-08-27 15:20:36,446 - INFO - joeynmt.training - Epoch  97, Step:     9175, Batch Loss:     0.148984, Batch Acc: 0.983043, Tokens per Sec:     3812, Lr: 0.000187\r\n",
      "2024-08-27 15:20:37,575 - INFO - joeynmt.training - Epoch  97, Step:     9180, Batch Loss:     0.126839, Batch Acc: 0.980164, Tokens per Sec:     3664, Lr: 0.000187\r\n",
      "2024-08-27 15:20:38,685 - INFO - joeynmt.training - Epoch  97, Step:     9185, Batch Loss:     0.144604, Batch Acc: 0.977506, Tokens per Sec:     3688, Lr: 0.000187\r\n",
      "2024-08-27 15:20:39,779 - INFO - joeynmt.training - Epoch  97, Step:     9190, Batch Loss:     0.139294, Batch Acc: 0.980257, Tokens per Sec:     4122, Lr: 0.000187\r\n",
      "2024-08-27 15:20:40,874 - INFO - joeynmt.training - Epoch  97, Step:     9195, Batch Loss:     0.146933, Batch Acc: 0.977006, Tokens per Sec:     3736, Lr: 0.000187\r\n",
      "2024-08-27 15:20:42,009 - INFO - joeynmt.training - Epoch  97, Step:     9200, Batch Loss:     0.142503, Batch Acc: 0.980498, Tokens per Sec:     3755, Lr: 0.000187\r\n",
      "2024-08-27 15:20:43,121 - INFO - joeynmt.training - Epoch  97, Step:     9205, Batch Loss:     0.149909, Batch Acc: 0.975508, Tokens per Sec:     3671, Lr: 0.000186\r\n",
      "2024-08-27 15:20:44,190 - INFO - joeynmt.training - Epoch  97, Step:     9210, Batch Loss:     0.129704, Batch Acc: 0.979321, Tokens per Sec:     3803, Lr: 0.000186\r\n",
      "2024-08-27 15:20:45,266 - INFO - joeynmt.training - Epoch  97, Step:     9215, Batch Loss:     0.130688, Batch Acc: 0.980122, Tokens per Sec:     4119, Lr: 0.000186\r\n",
      "2024-08-27 15:20:46,348 - INFO - joeynmt.training - Epoch  97, Step:     9220, Batch Loss:     0.160022, Batch Acc: 0.975268, Tokens per Sec:     3962, Lr: 0.000186\r\n",
      "2024-08-27 15:20:47,469 - INFO - joeynmt.training - Epoch  97, Step:     9225, Batch Loss:     0.121523, Batch Acc: 0.976583, Tokens per Sec:     3735, Lr: 0.000186\r\n",
      "2024-08-27 15:20:48,573 - INFO - joeynmt.training - Epoch  97, Step:     9230, Batch Loss:     0.134205, Batch Acc: 0.977167, Tokens per Sec:     3692, Lr: 0.000186\r\n",
      "2024-08-27 15:20:49,158 - INFO - joeynmt.training - Epoch  97, total training loss: 12.84, num. of seqs: 8065, num. of tokens: 80463, 20.9647[sec]\r\n",
      "2024-08-27 15:20:49,158 - INFO - joeynmt.training - EPOCH 98\r\n",
      "2024-08-27 15:20:49,805 - INFO - joeynmt.training - Epoch  98, Step:     9235, Batch Loss:     0.129820, Batch Acc: 0.980414, Tokens per Sec:     4135, Lr: 0.000186\r\n",
      "2024-08-27 15:20:50,854 - INFO - joeynmt.training - Epoch  98, Step:     9240, Batch Loss:     0.119047, Batch Acc: 0.978043, Tokens per Sec:     3909, Lr: 0.000186\r\n",
      "2024-08-27 15:20:51,915 - INFO - joeynmt.training - Epoch  98, Step:     9245, Batch Loss:     0.143395, Batch Acc: 0.981962, Tokens per Sec:     3815, Lr: 0.000186\r\n",
      "2024-08-27 15:20:52,997 - INFO - joeynmt.training - Epoch  98, Step:     9250, Batch Loss:     0.140756, Batch Acc: 0.979717, Tokens per Sec:     4058, Lr: 0.000186\r\n",
      "2024-08-27 15:20:54,081 - INFO - joeynmt.training - Epoch  98, Step:     9255, Batch Loss:     0.117907, Batch Acc: 0.979384, Tokens per Sec:     4166, Lr: 0.000186\r\n",
      "2024-08-27 15:20:55,163 - INFO - joeynmt.training - Epoch  98, Step:     9260, Batch Loss:     0.140093, Batch Acc: 0.979348, Tokens per Sec:     3941, Lr: 0.000186\r\n",
      "2024-08-27 15:20:56,280 - INFO - joeynmt.training - Epoch  98, Step:     9265, Batch Loss:     0.138652, Batch Acc: 0.974904, Tokens per Sec:     3748, Lr: 0.000186\r\n",
      "2024-08-27 15:20:57,414 - INFO - joeynmt.training - Epoch  98, Step:     9270, Batch Loss:     0.137133, Batch Acc: 0.984700, Tokens per Sec:     3690, Lr: 0.000186\r\n",
      "2024-08-27 15:20:58,538 - INFO - joeynmt.training - Epoch  98, Step:     9275, Batch Loss:     0.120282, Batch Acc: 0.982071, Tokens per Sec:     3774, Lr: 0.000186\r\n",
      "2024-08-27 15:20:59,629 - INFO - joeynmt.training - Epoch  98, Step:     9280, Batch Loss:     0.134366, Batch Acc: 0.980098, Tokens per Sec:     4102, Lr: 0.000186\r\n",
      "2024-08-27 15:21:00,699 - INFO - joeynmt.training - Epoch  98, Step:     9285, Batch Loss:     0.162728, Batch Acc: 0.980491, Tokens per Sec:     3884, Lr: 0.000186\r\n",
      "2024-08-27 15:21:01,774 - INFO - joeynmt.training - Epoch  98, Step:     9290, Batch Loss:     0.127186, Batch Acc: 0.982276, Tokens per Sec:     3989, Lr: 0.000186\r\n",
      "2024-08-27 15:21:02,858 - INFO - joeynmt.training - Epoch  98, Step:     9295, Batch Loss:     0.141437, Batch Acc: 0.977267, Tokens per Sec:     3818, Lr: 0.000186\r\n",
      "2024-08-27 15:21:03,945 - INFO - joeynmt.training - Epoch  98, Step:     9300, Batch Loss:     0.126321, Batch Acc: 0.980980, Tokens per Sec:     3775, Lr: 0.000185\r\n",
      "2024-08-27 15:21:05,017 - INFO - joeynmt.training - Epoch  98, Step:     9305, Batch Loss:     0.139394, Batch Acc: 0.978125, Tokens per Sec:     3883, Lr: 0.000185\r\n",
      "2024-08-27 15:21:06,097 - INFO - joeynmt.training - Epoch  98, Step:     9310, Batch Loss:     0.144561, Batch Acc: 0.978739, Tokens per Sec:     3878, Lr: 0.000185\r\n",
      "2024-08-27 15:21:07,197 - INFO - joeynmt.training - Epoch  98, Step:     9315, Batch Loss:     0.137981, Batch Acc: 0.976760, Tokens per Sec:     3993, Lr: 0.000185\r\n",
      "2024-08-27 15:21:08,270 - INFO - joeynmt.training - Epoch  98, Step:     9320, Batch Loss:     0.143217, Batch Acc: 0.978843, Tokens per Sec:     3969, Lr: 0.000185\r\n",
      "2024-08-27 15:21:09,354 - INFO - joeynmt.training - Epoch  98, Step:     9325, Batch Loss:     0.154776, Batch Acc: 0.976830, Tokens per Sec:     3743, Lr: 0.000185\r\n",
      "2024-08-27 15:21:09,832 - INFO - joeynmt.training - Epoch  98, total training loss: 12.63, num. of seqs: 8065, num. of tokens: 80463, 20.6567[sec]\r\n",
      "2024-08-27 15:21:09,832 - INFO - joeynmt.training - EPOCH 99\r\n",
      "2024-08-27 15:21:10,497 - INFO - joeynmt.training - Epoch  99, Step:     9330, Batch Loss:     0.144901, Batch Acc: 0.978437, Tokens per Sec:     3935, Lr: 0.000185\r\n",
      "2024-08-27 15:21:11,585 - INFO - joeynmt.training - Epoch  99, Step:     9335, Batch Loss:     0.129767, Batch Acc: 0.977651, Tokens per Sec:     3868, Lr: 0.000185\r\n",
      "2024-08-27 15:21:12,663 - INFO - joeynmt.training - Epoch  99, Step:     9340, Batch Loss:     0.140318, Batch Acc: 0.974553, Tokens per Sec:     3793, Lr: 0.000185\r\n",
      "2024-08-27 15:21:13,765 - INFO - joeynmt.training - Epoch  99, Step:     9345, Batch Loss:     0.151268, Batch Acc: 0.979430, Tokens per Sec:     3886, Lr: 0.000185\r\n",
      "2024-08-27 15:21:14,852 - INFO - joeynmt.training - Epoch  99, Step:     9350, Batch Loss:     0.126130, Batch Acc: 0.982596, Tokens per Sec:     3806, Lr: 0.000185\r\n",
      "2024-08-27 15:21:15,959 - INFO - joeynmt.training - Epoch  99, Step:     9355, Batch Loss:     0.148846, Batch Acc: 0.980746, Tokens per Sec:     3711, Lr: 0.000185\r\n",
      "2024-08-27 15:21:17,102 - INFO - joeynmt.training - Epoch  99, Step:     9360, Batch Loss:     0.141203, Batch Acc: 0.980913, Tokens per Sec:     3851, Lr: 0.000185\r\n",
      "2024-08-27 15:21:18,197 - INFO - joeynmt.training - Epoch  99, Step:     9365, Batch Loss:     0.125510, Batch Acc: 0.981944, Tokens per Sec:     3949, Lr: 0.000185\r\n",
      "2024-08-27 15:21:19,307 - INFO - joeynmt.training - Epoch  99, Step:     9370, Batch Loss:     0.127436, Batch Acc: 0.980569, Tokens per Sec:     3806, Lr: 0.000185\r\n",
      "2024-08-27 15:21:20,405 - INFO - joeynmt.training - Epoch  99, Step:     9375, Batch Loss:     0.137276, Batch Acc: 0.979853, Tokens per Sec:     3709, Lr: 0.000185\r\n",
      "2024-08-27 15:21:21,500 - INFO - joeynmt.training - Epoch  99, Step:     9380, Batch Loss:     0.127539, Batch Acc: 0.977813, Tokens per Sec:     4033, Lr: 0.000185\r\n",
      "2024-08-27 15:21:22,580 - INFO - joeynmt.training - Epoch  99, Step:     9385, Batch Loss:     0.123595, Batch Acc: 0.979577, Tokens per Sec:     3768, Lr: 0.000185\r\n",
      "2024-08-27 15:21:23,677 - INFO - joeynmt.training - Epoch  99, Step:     9390, Batch Loss:     0.147076, Batch Acc: 0.978167, Tokens per Sec:     4011, Lr: 0.000185\r\n",
      "2024-08-27 15:21:24,750 - INFO - joeynmt.training - Epoch  99, Step:     9395, Batch Loss:     0.134668, Batch Acc: 0.978957, Tokens per Sec:     3988, Lr: 0.000185\r\n",
      "2024-08-27 15:21:25,906 - INFO - joeynmt.training - Epoch  99, Step:     9400, Batch Loss:     0.135120, Batch Acc: 0.977498, Tokens per Sec:     3575, Lr: 0.000185\r\n",
      "2024-08-27 15:21:26,997 - INFO - joeynmt.training - Epoch  99, Step:     9405, Batch Loss:     0.124399, Batch Acc: 0.977540, Tokens per Sec:     3594, Lr: 0.000184\r\n",
      "2024-08-27 15:21:28,061 - INFO - joeynmt.training - Epoch  99, Step:     9410, Batch Loss:     0.126707, Batch Acc: 0.980627, Tokens per Sec:     3934, Lr: 0.000184\r\n",
      "2024-08-27 15:21:29,172 - INFO - joeynmt.training - Epoch  99, Step:     9415, Batch Loss:     0.127500, Batch Acc: 0.980286, Tokens per Sec:     3835, Lr: 0.000184\r\n",
      "2024-08-27 15:21:30,279 - INFO - joeynmt.training - Epoch  99, Step:     9420, Batch Loss:     0.139307, Batch Acc: 0.974674, Tokens per Sec:     3749, Lr: 0.000184\r\n",
      "2024-08-27 15:21:30,874 - INFO - joeynmt.training - Epoch  99, total training loss: 12.60, num. of seqs: 8065, num. of tokens: 80463, 21.0243[sec]\r\n",
      "2024-08-27 15:21:30,874 - INFO - joeynmt.training - EPOCH 100\r\n",
      "2024-08-27 15:21:31,527 - INFO - joeynmt.training - Epoch 100, Step:     9425, Batch Loss:     0.109778, Batch Acc: 0.986249, Tokens per Sec:     4035, Lr: 0.000184\r\n",
      "2024-08-27 15:21:32,615 - INFO - joeynmt.training - Epoch 100, Step:     9430, Batch Loss:     0.134682, Batch Acc: 0.978837, Tokens per Sec:     3954, Lr: 0.000184\r\n",
      "2024-08-27 15:21:33,697 - INFO - joeynmt.training - Epoch 100, Step:     9435, Batch Loss:     0.126299, Batch Acc: 0.978900, Tokens per Sec:     3681, Lr: 0.000184\r\n",
      "2024-08-27 15:21:34,775 - INFO - joeynmt.training - Epoch 100, Step:     9440, Batch Loss:     0.136080, Batch Acc: 0.981060, Tokens per Sec:     3628, Lr: 0.000184\r\n",
      "2024-08-27 15:21:35,863 - INFO - joeynmt.training - Epoch 100, Step:     9445, Batch Loss:     0.116230, Batch Acc: 0.983133, Tokens per Sec:     3816, Lr: 0.000184\r\n",
      "2024-08-27 15:21:36,985 - INFO - joeynmt.training - Epoch 100, Step:     9450, Batch Loss:     0.121193, Batch Acc: 0.979056, Tokens per Sec:     3704, Lr: 0.000184\r\n",
      "2024-08-27 15:21:38,069 - INFO - joeynmt.training - Epoch 100, Step:     9455, Batch Loss:     0.126313, Batch Acc: 0.981949, Tokens per Sec:     3735, Lr: 0.000184\r\n",
      "2024-08-27 15:21:39,172 - INFO - joeynmt.training - Epoch 100, Step:     9460, Batch Loss:     0.125421, Batch Acc: 0.980160, Tokens per Sec:     3975, Lr: 0.000184\r\n",
      "2024-08-27 15:21:40,258 - INFO - joeynmt.training - Epoch 100, Step:     9465, Batch Loss:     0.122088, Batch Acc: 0.979709, Tokens per Sec:     3861, Lr: 0.000184\r\n",
      "2024-08-27 15:21:41,328 - INFO - joeynmt.training - Epoch 100, Step:     9470, Batch Loss:     0.138940, Batch Acc: 0.980501, Tokens per Sec:     3693, Lr: 0.000184\r\n",
      "2024-08-27 15:21:42,431 - INFO - joeynmt.training - Epoch 100, Step:     9475, Batch Loss:     0.136340, Batch Acc: 0.978900, Tokens per Sec:     4041, Lr: 0.000184\r\n",
      "2024-08-27 15:21:43,525 - INFO - joeynmt.training - Epoch 100, Step:     9480, Batch Loss:     0.120191, Batch Acc: 0.982326, Tokens per Sec:     3829, Lr: 0.000184\r\n",
      "2024-08-27 15:21:44,672 - INFO - joeynmt.training - Epoch 100, Step:     9485, Batch Loss:     0.135794, Batch Acc: 0.974782, Tokens per Sec:     3911, Lr: 0.000184\r\n",
      "2024-08-27 15:21:45,753 - INFO - joeynmt.training - Epoch 100, Step:     9490, Batch Loss:     0.127299, Batch Acc: 0.978333, Tokens per Sec:     3631, Lr: 0.000184\r\n",
      "2024-08-27 15:21:46,880 - INFO - joeynmt.training - Epoch 100, Step:     9495, Batch Loss:     0.145224, Batch Acc: 0.972650, Tokens per Sec:     3864, Lr: 0.000184\r\n",
      "2024-08-27 15:21:47,961 - INFO - joeynmt.training - Epoch 100, Step:     9500, Batch Loss:     0.118968, Batch Acc: 0.981706, Tokens per Sec:     3894, Lr: 0.000184\r\n",
      "2024-08-27 15:21:47,962 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=9542\r\n",
      "2024-08-27 15:21:47,962 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:21<00:00, 67.65it/s]\r\n",
      "2024-08-27 15:22:09,546 - INFO - joeynmt.prediction - Generation took 21.5820[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 45379.01 examples/s]\r\n",
      "2024-08-27 15:22:09,922 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:22:09,922 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.15, loss:   6.94, ppl: 1037.70, acc:   0.28, 0.1625[sec]\r\n",
      "2024-08-27 15:22:09,924 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:22:10,070 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:22:10,070 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:22:10,070 - INFO - joeynmt.training - \tHypothesis: comment s’appelle votre sœur\r\n",
      "2024-08-27 15:22:10,395 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:22:10,546 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:22:10,546 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:22:10,547 - INFO - joeynmt.training - \tHypothesis: trois tables\r\n",
      "2024-08-27 15:22:10,846 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:22:10,992 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:22:10,992 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:22:10,992 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 15:22:11,286 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:22:11,431 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:22:11,432 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:22:11,432 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 15:22:12,792 - INFO - joeynmt.training - Epoch 100, Step:     9505, Batch Loss:     0.144843, Batch Acc: 0.979137, Tokens per Sec:     3948, Lr: 0.000183\r\n",
      "2024-08-27 15:22:13,874 - INFO - joeynmt.training - Epoch 100, Step:     9510, Batch Loss:     0.135620, Batch Acc: 0.972985, Tokens per Sec:     4002, Lr: 0.000183\r\n",
      "2024-08-27 15:22:14,963 - INFO - joeynmt.training - Epoch 100, Step:     9515, Batch Loss:     0.145717, Batch Acc: 0.978506, Tokens per Sec:     4234, Lr: 0.000183\r\n",
      "2024-08-27 15:22:15,553 - INFO - joeynmt.training - Epoch 100, total training loss: 12.51, num. of seqs: 8065, num. of tokens: 80463, 20.9005[sec]\r\n",
      "2024-08-27 15:22:15,553 - INFO - joeynmt.training - EPOCH 101\r\n",
      "2024-08-27 15:22:16,218 - INFO - joeynmt.training - Epoch 101, Step:     9520, Batch Loss:     0.118549, Batch Acc: 0.984994, Tokens per Sec:     3932, Lr: 0.000183\r\n",
      "2024-08-27 15:22:17,350 - INFO - joeynmt.training - Epoch 101, Step:     9525, Batch Loss:     0.126803, Batch Acc: 0.984597, Tokens per Sec:     3673, Lr: 0.000183\r\n",
      "2024-08-27 15:22:18,443 - INFO - joeynmt.training - Epoch 101, Step:     9530, Batch Loss:     0.106062, Batch Acc: 0.982631, Tokens per Sec:     3953, Lr: 0.000183\r\n",
      "2024-08-27 15:22:19,543 - INFO - joeynmt.training - Epoch 101, Step:     9535, Batch Loss:     0.143930, Batch Acc: 0.982394, Tokens per Sec:     3822, Lr: 0.000183\r\n",
      "2024-08-27 15:22:20,616 - INFO - joeynmt.training - Epoch 101, Step:     9540, Batch Loss:     0.117858, Batch Acc: 0.984554, Tokens per Sec:     3986, Lr: 0.000183\r\n",
      "2024-08-27 15:22:21,715 - INFO - joeynmt.training - Epoch 101, Step:     9545, Batch Loss:     0.121401, Batch Acc: 0.979620, Tokens per Sec:     3977, Lr: 0.000183\r\n",
      "2024-08-27 15:22:22,815 - INFO - joeynmt.training - Epoch 101, Step:     9550, Batch Loss:     0.131727, Batch Acc: 0.980915, Tokens per Sec:     3955, Lr: 0.000183\r\n",
      "2024-08-27 15:22:23,879 - INFO - joeynmt.training - Epoch 101, Step:     9555, Batch Loss:     0.132174, Batch Acc: 0.980188, Tokens per Sec:     3893, Lr: 0.000183\r\n",
      "2024-08-27 15:22:24,940 - INFO - joeynmt.training - Epoch 101, Step:     9560, Batch Loss:     0.148061, Batch Acc: 0.975321, Tokens per Sec:     3821, Lr: 0.000183\r\n",
      "2024-08-27 15:22:25,983 - INFO - joeynmt.training - Epoch 101, Step:     9565, Batch Loss:     0.121825, Batch Acc: 0.978325, Tokens per Sec:     3896, Lr: 0.000183\r\n",
      "2024-08-27 15:22:27,076 - INFO - joeynmt.training - Epoch 101, Step:     9570, Batch Loss:     0.113064, Batch Acc: 0.983213, Tokens per Sec:     3817, Lr: 0.000183\r\n",
      "2024-08-27 15:22:28,166 - INFO - joeynmt.training - Epoch 101, Step:     9575, Batch Loss:     0.146788, Batch Acc: 0.978338, Tokens per Sec:     3855, Lr: 0.000183\r\n",
      "2024-08-27 15:22:29,233 - INFO - joeynmt.training - Epoch 101, Step:     9580, Batch Loss:     0.124102, Batch Acc: 0.975956, Tokens per Sec:     3900, Lr: 0.000183\r\n",
      "2024-08-27 15:22:30,334 - INFO - joeynmt.training - Epoch 101, Step:     9585, Batch Loss:     0.140855, Batch Acc: 0.977146, Tokens per Sec:     3900, Lr: 0.000183\r\n",
      "2024-08-27 15:22:31,427 - INFO - joeynmt.training - Epoch 101, Step:     9590, Batch Loss:     0.130430, Batch Acc: 0.978438, Tokens per Sec:     3819, Lr: 0.000183\r\n",
      "2024-08-27 15:22:32,520 - INFO - joeynmt.training - Epoch 101, Step:     9595, Batch Loss:     0.137707, Batch Acc: 0.978545, Tokens per Sec:     3930, Lr: 0.000183\r\n",
      "2024-08-27 15:22:33,626 - INFO - joeynmt.training - Epoch 101, Step:     9600, Batch Loss:     0.127665, Batch Acc: 0.981211, Tokens per Sec:     3899, Lr: 0.000183\r\n",
      "2024-08-27 15:22:34,699 - INFO - joeynmt.training - Epoch 101, Step:     9605, Batch Loss:     0.148504, Batch Acc: 0.976805, Tokens per Sec:     3901, Lr: 0.000183\r\n",
      "2024-08-27 15:22:35,763 - INFO - joeynmt.training - Epoch 101, Step:     9610, Batch Loss:     0.154791, Batch Acc: 0.982134, Tokens per Sec:     3948, Lr: 0.000182\r\n",
      "2024-08-27 15:22:36,248 - INFO - joeynmt.training - Epoch 101, total training loss: 12.24, num. of seqs: 8065, num. of tokens: 80463, 20.6777[sec]\r\n",
      "2024-08-27 15:22:36,248 - INFO - joeynmt.training - EPOCH 102\r\n",
      "2024-08-27 15:22:36,924 - INFO - joeynmt.training - Epoch 102, Step:     9615, Batch Loss:     0.136562, Batch Acc: 0.977615, Tokens per Sec:     3661, Lr: 0.000182\r\n",
      "2024-08-27 15:22:38,015 - INFO - joeynmt.training - Epoch 102, Step:     9620, Batch Loss:     0.129426, Batch Acc: 0.981889, Tokens per Sec:     4001, Lr: 0.000182\r\n",
      "2024-08-27 15:22:39,112 - INFO - joeynmt.training - Epoch 102, Step:     9625, Batch Loss:     0.119822, Batch Acc: 0.983983, Tokens per Sec:     3929, Lr: 0.000182\r\n",
      "2024-08-27 15:22:40,193 - INFO - joeynmt.training - Epoch 102, Step:     9630, Batch Loss:     0.133410, Batch Acc: 0.982012, Tokens per Sec:     3913, Lr: 0.000182\r\n",
      "2024-08-27 15:22:41,267 - INFO - joeynmt.training - Epoch 102, Step:     9635, Batch Loss:     0.134916, Batch Acc: 0.982511, Tokens per Sec:     3889, Lr: 0.000182\r\n",
      "2024-08-27 15:22:42,469 - INFO - joeynmt.training - Epoch 102, Step:     9640, Batch Loss:     0.140224, Batch Acc: 0.980792, Tokens per Sec:     3467, Lr: 0.000182\r\n",
      "2024-08-27 15:22:43,553 - INFO - joeynmt.training - Epoch 102, Step:     9645, Batch Loss:     0.125182, Batch Acc: 0.979016, Tokens per Sec:     3959, Lr: 0.000182\r\n",
      "2024-08-27 15:22:44,627 - INFO - joeynmt.training - Epoch 102, Step:     9650, Batch Loss:     0.125417, Batch Acc: 0.981634, Tokens per Sec:     3957, Lr: 0.000182\r\n",
      "2024-08-27 15:22:45,682 - INFO - joeynmt.training - Epoch 102, Step:     9655, Batch Loss:     0.142318, Batch Acc: 0.978004, Tokens per Sec:     4009, Lr: 0.000182\r\n",
      "2024-08-27 15:22:46,812 - INFO - joeynmt.training - Epoch 102, Step:     9660, Batch Loss:     0.127577, Batch Acc: 0.981424, Tokens per Sec:     3719, Lr: 0.000182\r\n",
      "2024-08-27 15:22:47,900 - INFO - joeynmt.training - Epoch 102, Step:     9665, Batch Loss:     0.138899, Batch Acc: 0.981831, Tokens per Sec:     3847, Lr: 0.000182\r\n",
      "2024-08-27 15:22:48,994 - INFO - joeynmt.training - Epoch 102, Step:     9670, Batch Loss:     0.120456, Batch Acc: 0.977185, Tokens per Sec:     4007, Lr: 0.000182\r\n",
      "2024-08-27 15:22:50,080 - INFO - joeynmt.training - Epoch 102, Step:     9675, Batch Loss:     0.113455, Batch Acc: 0.980722, Tokens per Sec:     3775, Lr: 0.000182\r\n",
      "2024-08-27 15:22:51,176 - INFO - joeynmt.training - Epoch 102, Step:     9680, Batch Loss:     0.133881, Batch Acc: 0.973613, Tokens per Sec:     3738, Lr: 0.000182\r\n",
      "2024-08-27 15:22:52,253 - INFO - joeynmt.training - Epoch 102, Step:     9685, Batch Loss:     0.118951, Batch Acc: 0.977085, Tokens per Sec:     3933, Lr: 0.000182\r\n",
      "2024-08-27 15:22:53,385 - INFO - joeynmt.training - Epoch 102, Step:     9690, Batch Loss:     0.144570, Batch Acc: 0.977533, Tokens per Sec:     3855, Lr: 0.000182\r\n",
      "2024-08-27 15:22:54,470 - INFO - joeynmt.training - Epoch 102, Step:     9695, Batch Loss:     0.132160, Batch Acc: 0.976468, Tokens per Sec:     3960, Lr: 0.000182\r\n",
      "2024-08-27 15:22:55,542 - INFO - joeynmt.training - Epoch 102, Step:     9700, Batch Loss:     0.146251, Batch Acc: 0.973797, Tokens per Sec:     3706, Lr: 0.000182\r\n",
      "2024-08-27 15:22:56,639 - INFO - joeynmt.training - Epoch 102, Step:     9705, Batch Loss:     0.127325, Batch Acc: 0.980221, Tokens per Sec:     3965, Lr: 0.000182\r\n",
      "2024-08-27 15:22:57,193 - INFO - joeynmt.training - Epoch 102, total training loss: 12.43, num. of seqs: 8065, num. of tokens: 80463, 20.9279[sec]\r\n",
      "2024-08-27 15:22:57,194 - INFO - joeynmt.training - EPOCH 103\r\n",
      "2024-08-27 15:22:57,842 - INFO - joeynmt.training - Epoch 103, Step:     9710, Batch Loss:     0.114233, Batch Acc: 0.983347, Tokens per Sec:     3820, Lr: 0.000182\r\n",
      "2024-08-27 15:22:58,928 - INFO - joeynmt.training - Epoch 103, Step:     9715, Batch Loss:     0.119160, Batch Acc: 0.983159, Tokens per Sec:     3887, Lr: 0.000181\r\n",
      "2024-08-27 15:23:00,035 - INFO - joeynmt.training - Epoch 103, Step:     9720, Batch Loss:     0.120354, Batch Acc: 0.980941, Tokens per Sec:     3935, Lr: 0.000181\r\n",
      "2024-08-27 15:23:01,124 - INFO - joeynmt.training - Epoch 103, Step:     9725, Batch Loss:     0.133758, Batch Acc: 0.983317, Tokens per Sec:     3802, Lr: 0.000181\r\n",
      "2024-08-27 15:23:02,224 - INFO - joeynmt.training - Epoch 103, Step:     9730, Batch Loss:     0.128056, Batch Acc: 0.977139, Tokens per Sec:     3701, Lr: 0.000181\r\n",
      "2024-08-27 15:23:03,319 - INFO - joeynmt.training - Epoch 103, Step:     9735, Batch Loss:     0.142185, Batch Acc: 0.977600, Tokens per Sec:     3872, Lr: 0.000181\r\n",
      "2024-08-27 15:23:04,407 - INFO - joeynmt.training - Epoch 103, Step:     9740, Batch Loss:     0.144887, Batch Acc: 0.978309, Tokens per Sec:     3777, Lr: 0.000181\r\n",
      "2024-08-27 15:23:05,512 - INFO - joeynmt.training - Epoch 103, Step:     9745, Batch Loss:     0.123512, Batch Acc: 0.982465, Tokens per Sec:     3769, Lr: 0.000181\r\n",
      "2024-08-27 15:23:06,628 - INFO - joeynmt.training - Epoch 103, Step:     9750, Batch Loss:     0.111228, Batch Acc: 0.979864, Tokens per Sec:     3831, Lr: 0.000181\r\n",
      "2024-08-27 15:23:07,770 - INFO - joeynmt.training - Epoch 103, Step:     9755, Batch Loss:     0.128964, Batch Acc: 0.983950, Tokens per Sec:     3927, Lr: 0.000181\r\n",
      "2024-08-27 15:23:08,884 - INFO - joeynmt.training - Epoch 103, Step:     9760, Batch Loss:     0.114173, Batch Acc: 0.982235, Tokens per Sec:     3844, Lr: 0.000181\r\n",
      "2024-08-27 15:23:09,996 - INFO - joeynmt.training - Epoch 103, Step:     9765, Batch Loss:     0.121090, Batch Acc: 0.977314, Tokens per Sec:     3926, Lr: 0.000181\r\n",
      "2024-08-27 15:23:11,091 - INFO - joeynmt.training - Epoch 103, Step:     9770, Batch Loss:     0.125407, Batch Acc: 0.976433, Tokens per Sec:     3764, Lr: 0.000181\r\n",
      "2024-08-27 15:23:12,197 - INFO - joeynmt.training - Epoch 103, Step:     9775, Batch Loss:     0.119494, Batch Acc: 0.983461, Tokens per Sec:     3881, Lr: 0.000181\r\n",
      "2024-08-27 15:23:13,391 - INFO - joeynmt.training - Epoch 103, Step:     9780, Batch Loss:     0.110006, Batch Acc: 0.982930, Tokens per Sec:     3537, Lr: 0.000181\r\n",
      "2024-08-27 15:23:14,499 - INFO - joeynmt.training - Epoch 103, Step:     9785, Batch Loss:     0.112947, Batch Acc: 0.979058, Tokens per Sec:     3795, Lr: 0.000181\r\n",
      "2024-08-27 15:23:15,599 - INFO - joeynmt.training - Epoch 103, Step:     9790, Batch Loss:     0.127851, Batch Acc: 0.981275, Tokens per Sec:     3838, Lr: 0.000181\r\n",
      "2024-08-27 15:23:16,686 - INFO - joeynmt.training - Epoch 103, Step:     9795, Batch Loss:     0.131893, Batch Acc: 0.980097, Tokens per Sec:     3978, Lr: 0.000181\r\n",
      "2024-08-27 15:23:17,785 - INFO - joeynmt.training - Epoch 103, Step:     9800, Batch Loss:     0.117287, Batch Acc: 0.978571, Tokens per Sec:     3696, Lr: 0.000181\r\n",
      "2024-08-27 15:23:18,317 - INFO - joeynmt.training - Epoch 103, total training loss: 12.04, num. of seqs: 8065, num. of tokens: 80463, 21.1066[sec]\r\n",
      "2024-08-27 15:23:18,318 - INFO - joeynmt.training - EPOCH 104\r\n",
      "2024-08-27 15:23:18,968 - INFO - joeynmt.training - Epoch 104, Step:     9805, Batch Loss:     0.108106, Batch Acc: 0.987795, Tokens per Sec:     3809, Lr: 0.000181\r\n",
      "2024-08-27 15:23:20,060 - INFO - joeynmt.training - Epoch 104, Step:     9810, Batch Loss:     0.129234, Batch Acc: 0.983908, Tokens per Sec:     3985, Lr: 0.000181\r\n",
      "2024-08-27 15:23:21,138 - INFO - joeynmt.training - Epoch 104, Step:     9815, Batch Loss:     0.133631, Batch Acc: 0.984039, Tokens per Sec:     3838, Lr: 0.000181\r\n",
      "2024-08-27 15:23:22,241 - INFO - joeynmt.training - Epoch 104, Step:     9820, Batch Loss:     0.117087, Batch Acc: 0.980097, Tokens per Sec:     3736, Lr: 0.000181\r\n",
      "2024-08-27 15:23:23,345 - INFO - joeynmt.training - Epoch 104, Step:     9825, Batch Loss:     0.127013, Batch Acc: 0.982490, Tokens per Sec:     3778, Lr: 0.000180\r\n",
      "2024-08-27 15:23:24,450 - INFO - joeynmt.training - Epoch 104, Step:     9830, Batch Loss:     0.113472, Batch Acc: 0.984016, Tokens per Sec:     3625, Lr: 0.000180\r\n",
      "2024-08-27 15:23:25,551 - INFO - joeynmt.training - Epoch 104, Step:     9835, Batch Loss:     0.133508, Batch Acc: 0.978863, Tokens per Sec:     3870, Lr: 0.000180\r\n",
      "2024-08-27 15:23:26,661 - INFO - joeynmt.training - Epoch 104, Step:     9840, Batch Loss:     0.118520, Batch Acc: 0.982482, Tokens per Sec:     3653, Lr: 0.000180\r\n",
      "2024-08-27 15:23:27,774 - INFO - joeynmt.training - Epoch 104, Step:     9845, Batch Loss:     0.129274, Batch Acc: 0.979478, Tokens per Sec:     3855, Lr: 0.000180\r\n",
      "2024-08-27 15:23:28,867 - INFO - joeynmt.training - Epoch 104, Step:     9850, Batch Loss:     0.131105, Batch Acc: 0.981640, Tokens per Sec:     3742, Lr: 0.000180\r\n",
      "2024-08-27 15:23:29,950 - INFO - joeynmt.training - Epoch 104, Step:     9855, Batch Loss:     0.130447, Batch Acc: 0.983243, Tokens per Sec:     4081, Lr: 0.000180\r\n",
      "2024-08-27 15:23:31,027 - INFO - joeynmt.training - Epoch 104, Step:     9860, Batch Loss:     0.125057, Batch Acc: 0.980797, Tokens per Sec:     3870, Lr: 0.000180\r\n",
      "2024-08-27 15:23:32,111 - INFO - joeynmt.training - Epoch 104, Step:     9865, Batch Loss:     0.130964, Batch Acc: 0.979759, Tokens per Sec:     3832, Lr: 0.000180\r\n",
      "2024-08-27 15:23:33,195 - INFO - joeynmt.training - Epoch 104, Step:     9870, Batch Loss:     0.133866, Batch Acc: 0.983439, Tokens per Sec:     4069, Lr: 0.000180\r\n",
      "2024-08-27 15:23:34,274 - INFO - joeynmt.training - Epoch 104, Step:     9875, Batch Loss:     0.131954, Batch Acc: 0.979597, Tokens per Sec:     3772, Lr: 0.000180\r\n",
      "2024-08-27 15:23:35,361 - INFO - joeynmt.training - Epoch 104, Step:     9880, Batch Loss:     0.134585, Batch Acc: 0.979875, Tokens per Sec:     3979, Lr: 0.000180\r\n",
      "2024-08-27 15:23:36,440 - INFO - joeynmt.training - Epoch 104, Step:     9885, Batch Loss:     0.154344, Batch Acc: 0.977910, Tokens per Sec:     3905, Lr: 0.000180\r\n",
      "2024-08-27 15:23:37,546 - INFO - joeynmt.training - Epoch 104, Step:     9890, Batch Loss:     0.125720, Batch Acc: 0.973941, Tokens per Sec:     3716, Lr: 0.000180\r\n",
      "2024-08-27 15:23:38,636 - INFO - joeynmt.training - Epoch 104, Step:     9895, Batch Loss:     0.132839, Batch Acc: 0.980312, Tokens per Sec:     3823, Lr: 0.000180\r\n",
      "2024-08-27 15:23:39,277 - INFO - joeynmt.training - Epoch 104, total training loss: 12.08, num. of seqs: 8065, num. of tokens: 80463, 20.9421[sec]\r\n",
      "2024-08-27 15:23:39,278 - INFO - joeynmt.training - EPOCH 105\r\n",
      "2024-08-27 15:23:39,718 - INFO - joeynmt.training - Epoch 105, Step:     9900, Batch Loss:     0.118327, Batch Acc: 0.985091, Tokens per Sec:     4146, Lr: 0.000180\r\n",
      "2024-08-27 15:23:40,829 - INFO - joeynmt.training - Epoch 105, Step:     9905, Batch Loss:     0.111035, Batch Acc: 0.980315, Tokens per Sec:     3891, Lr: 0.000180\r\n",
      "2024-08-27 15:23:41,916 - INFO - joeynmt.training - Epoch 105, Step:     9910, Batch Loss:     0.132827, Batch Acc: 0.984269, Tokens per Sec:     3917, Lr: 0.000180\r\n",
      "2024-08-27 15:23:43,004 - INFO - joeynmt.training - Epoch 105, Step:     9915, Batch Loss:     0.105456, Batch Acc: 0.984226, Tokens per Sec:     3849, Lr: 0.000180\r\n",
      "2024-08-27 15:23:44,162 - INFO - joeynmt.training - Epoch 105, Step:     9920, Batch Loss:     0.105944, Batch Acc: 0.986647, Tokens per Sec:     3561, Lr: 0.000180\r\n",
      "2024-08-27 15:23:45,284 - INFO - joeynmt.training - Epoch 105, Step:     9925, Batch Loss:     0.130418, Batch Acc: 0.981052, Tokens per Sec:     3765, Lr: 0.000180\r\n",
      "2024-08-27 15:23:46,375 - INFO - joeynmt.training - Epoch 105, Step:     9930, Batch Loss:     0.108202, Batch Acc: 0.982253, Tokens per Sec:     4031, Lr: 0.000180\r\n",
      "2024-08-27 15:23:47,493 - INFO - joeynmt.training - Epoch 105, Step:     9935, Batch Loss:     0.118966, Batch Acc: 0.982173, Tokens per Sec:     3663, Lr: 0.000179\r\n",
      "2024-08-27 15:23:48,558 - INFO - joeynmt.training - Epoch 105, Step:     9940, Batch Loss:     0.136631, Batch Acc: 0.976535, Tokens per Sec:     3964, Lr: 0.000179\r\n",
      "2024-08-27 15:23:49,670 - INFO - joeynmt.training - Epoch 105, Step:     9945, Batch Loss:     0.114085, Batch Acc: 0.983771, Tokens per Sec:     3768, Lr: 0.000179\r\n",
      "2024-08-27 15:23:50,756 - INFO - joeynmt.training - Epoch 105, Step:     9950, Batch Loss:     0.120513, Batch Acc: 0.983302, Tokens per Sec:     3919, Lr: 0.000179\r\n",
      "2024-08-27 15:23:51,846 - INFO - joeynmt.training - Epoch 105, Step:     9955, Batch Loss:     0.127455, Batch Acc: 0.979372, Tokens per Sec:     3827, Lr: 0.000179\r\n",
      "2024-08-27 15:23:52,954 - INFO - joeynmt.training - Epoch 105, Step:     9960, Batch Loss:     0.106687, Batch Acc: 0.980739, Tokens per Sec:     4033, Lr: 0.000179\r\n",
      "2024-08-27 15:23:54,071 - INFO - joeynmt.training - Epoch 105, Step:     9965, Batch Loss:     0.125718, Batch Acc: 0.981539, Tokens per Sec:     3739, Lr: 0.000179\r\n",
      "2024-08-27 15:23:55,180 - INFO - joeynmt.training - Epoch 105, Step:     9970, Batch Loss:     0.116199, Batch Acc: 0.977060, Tokens per Sec:     3852, Lr: 0.000179\r\n",
      "2024-08-27 15:23:56,362 - INFO - joeynmt.training - Epoch 105, Step:     9975, Batch Loss:     0.128593, Batch Acc: 0.980481, Tokens per Sec:     3731, Lr: 0.000179\r\n",
      "2024-08-27 15:23:57,513 - INFO - joeynmt.training - Epoch 105, Step:     9980, Batch Loss:     0.151635, Batch Acc: 0.981616, Tokens per Sec:     3593, Lr: 0.000179\r\n",
      "2024-08-27 15:23:58,672 - INFO - joeynmt.training - Epoch 105, Step:     9985, Batch Loss:     0.131260, Batch Acc: 0.979833, Tokens per Sec:     3725, Lr: 0.000179\r\n",
      "2024-08-27 15:23:59,769 - INFO - joeynmt.training - Epoch 105, Step:     9990, Batch Loss:     0.121181, Batch Acc: 0.978204, Tokens per Sec:     3516, Lr: 0.000179\r\n",
      "2024-08-27 15:24:00,425 - INFO - joeynmt.training - Epoch 105, total training loss: 11.81, num. of seqs: 8065, num. of tokens: 80463, 21.1306[sec]\r\n",
      "2024-08-27 15:24:00,425 - INFO - joeynmt.training - EPOCH 106\r\n",
      "2024-08-27 15:24:00,864 - INFO - joeynmt.training - Epoch 106, Step:     9995, Batch Loss:     0.123540, Batch Acc: 0.978321, Tokens per Sec:     4130, Lr: 0.000179\r\n",
      "2024-08-27 15:24:01,969 - INFO - joeynmt.training - Epoch 106, Step:    10000, Batch Loss:     0.120677, Batch Acc: 0.983437, Tokens per Sec:     3773, Lr: 0.000179\r\n",
      "2024-08-27 15:24:01,970 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=10042\r\n",
      "2024-08-27 15:24:01,970 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 64.50it/s]\r\n",
      "2024-08-27 15:24:24,610 - INFO - joeynmt.prediction - Generation took 22.6378[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 43013.55 examples/s]\r\n",
      "2024-08-27 15:24:24,989 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:24:24,990 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   4.97, loss:   6.93, ppl: 1020.59, acc:   0.28, 0.1658[sec]\r\n",
      "2024-08-27 15:24:24,991 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:24:25,139 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:24:25,139 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:24:25,139 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 15:24:25,436 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:24:25,581 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:24:25,581 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:24:25,581 - INFO - joeynmt.training - \tHypothesis: trois tables blanches\r\n",
      "2024-08-27 15:24:25,874 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:24:26,019 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:24:26,019 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:24:26,019 - INFO - joeynmt.training - \tHypothesis: cependant l’ancienne lentille et l’autre\r\n",
      "2024-08-27 15:24:26,310 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:24:26,457 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:24:26,458 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:24:26,458 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 15:24:27,902 - INFO - joeynmt.training - Epoch 106, Step:    10005, Batch Loss:     0.119095, Batch Acc: 0.980920, Tokens per Sec:     3627, Lr: 0.000179\r\n",
      "2024-08-27 15:24:29,028 - INFO - joeynmt.training - Epoch 106, Step:    10010, Batch Loss:     0.102699, Batch Acc: 0.983974, Tokens per Sec:     3882, Lr: 0.000179\r\n",
      "2024-08-27 15:24:30,150 - INFO - joeynmt.training - Epoch 106, Step:    10015, Batch Loss:     0.103672, Batch Acc: 0.983800, Tokens per Sec:     3634, Lr: 0.000179\r\n",
      "2024-08-27 15:24:31,250 - INFO - joeynmt.training - Epoch 106, Step:    10020, Batch Loss:     0.130945, Batch Acc: 0.980187, Tokens per Sec:     3996, Lr: 0.000179\r\n",
      "2024-08-27 15:24:32,383 - INFO - joeynmt.training - Epoch 106, Step:    10025, Batch Loss:     0.139974, Batch Acc: 0.982779, Tokens per Sec:     3694, Lr: 0.000179\r\n",
      "2024-08-27 15:24:33,493 - INFO - joeynmt.training - Epoch 106, Step:    10030, Batch Loss:     0.119478, Batch Acc: 0.986814, Tokens per Sec:     3826, Lr: 0.000179\r\n",
      "2024-08-27 15:24:34,613 - INFO - joeynmt.training - Epoch 106, Step:    10035, Batch Loss:     0.110536, Batch Acc: 0.982313, Tokens per Sec:     3840, Lr: 0.000179\r\n",
      "2024-08-27 15:24:35,748 - INFO - joeynmt.training - Epoch 106, Step:    10040, Batch Loss:     0.123552, Batch Acc: 0.985996, Tokens per Sec:     3715, Lr: 0.000179\r\n",
      "2024-08-27 15:24:36,911 - INFO - joeynmt.training - Epoch 106, Step:    10045, Batch Loss:     0.126342, Batch Acc: 0.981677, Tokens per Sec:     3660, Lr: 0.000178\r\n",
      "2024-08-27 15:24:38,019 - INFO - joeynmt.training - Epoch 106, Step:    10050, Batch Loss:     0.121954, Batch Acc: 0.982473, Tokens per Sec:     3661, Lr: 0.000178\r\n",
      "2024-08-27 15:24:39,128 - INFO - joeynmt.training - Epoch 106, Step:    10055, Batch Loss:     0.110631, Batch Acc: 0.983191, Tokens per Sec:     3809, Lr: 0.000178\r\n",
      "2024-08-27 15:24:40,215 - INFO - joeynmt.training - Epoch 106, Step:    10060, Batch Loss:     0.135513, Batch Acc: 0.983588, Tokens per Sec:     3981, Lr: 0.000178\r\n",
      "2024-08-27 15:24:41,302 - INFO - joeynmt.training - Epoch 106, Step:    10065, Batch Loss:     0.123153, Batch Acc: 0.982276, Tokens per Sec:     3946, Lr: 0.000178\r\n",
      "2024-08-27 15:24:42,396 - INFO - joeynmt.training - Epoch 106, Step:    10070, Batch Loss:     0.122667, Batch Acc: 0.983079, Tokens per Sec:     3839, Lr: 0.000178\r\n",
      "2024-08-27 15:24:43,470 - INFO - joeynmt.training - Epoch 106, Step:    10075, Batch Loss:     0.129328, Batch Acc: 0.978885, Tokens per Sec:     3792, Lr: 0.000178\r\n",
      "2024-08-27 15:24:44,545 - INFO - joeynmt.training - Epoch 106, Step:    10080, Batch Loss:     0.118374, Batch Acc: 0.982211, Tokens per Sec:     3924, Lr: 0.000178\r\n",
      "2024-08-27 15:24:45,616 - INFO - joeynmt.training - Epoch 106, Step:    10085, Batch Loss:     0.119556, Batch Acc: 0.978306, Tokens per Sec:     4049, Lr: 0.000178\r\n",
      "2024-08-27 15:24:46,339 - INFO - joeynmt.training - Epoch 106, total training loss: 11.55, num. of seqs: 8065, num. of tokens: 80463, 21.0930[sec]\r\n",
      "2024-08-27 15:24:46,340 - INFO - joeynmt.training - EPOCH 107\r\n",
      "2024-08-27 15:24:46,882 - INFO - joeynmt.training - Epoch 107, Step:    10090, Batch Loss:     0.103495, Batch Acc: 0.988250, Tokens per Sec:     3017, Lr: 0.000178\r\n",
      "2024-08-27 15:24:47,968 - INFO - joeynmt.training - Epoch 107, Step:    10095, Batch Loss:     0.105389, Batch Acc: 0.984784, Tokens per Sec:     4115, Lr: 0.000178\r\n",
      "2024-08-27 15:24:49,047 - INFO - joeynmt.training - Epoch 107, Step:    10100, Batch Loss:     0.132232, Batch Acc: 0.983992, Tokens per Sec:     3710, Lr: 0.000178\r\n",
      "2024-08-27 15:24:50,132 - INFO - joeynmt.training - Epoch 107, Step:    10105, Batch Loss:     0.093155, Batch Acc: 0.983948, Tokens per Sec:     3850, Lr: 0.000178\r\n",
      "2024-08-27 15:24:51,221 - INFO - joeynmt.training - Epoch 107, Step:    10110, Batch Loss:     0.148935, Batch Acc: 0.980415, Tokens per Sec:     3942, Lr: 0.000178\r\n",
      "2024-08-27 15:24:52,340 - INFO - joeynmt.training - Epoch 107, Step:    10115, Batch Loss:     0.099943, Batch Acc: 0.984086, Tokens per Sec:     3818, Lr: 0.000178\r\n",
      "2024-08-27 15:24:53,445 - INFO - joeynmt.training - Epoch 107, Step:    10120, Batch Loss:     0.115761, Batch Acc: 0.980171, Tokens per Sec:     3699, Lr: 0.000178\r\n",
      "2024-08-27 15:24:54,573 - INFO - joeynmt.training - Epoch 107, Step:    10125, Batch Loss:     0.118626, Batch Acc: 0.980650, Tokens per Sec:     3850, Lr: 0.000178\r\n",
      "2024-08-27 15:24:55,671 - INFO - joeynmt.training - Epoch 107, Step:    10130, Batch Loss:     0.126694, Batch Acc: 0.984352, Tokens per Sec:     3789, Lr: 0.000178\r\n",
      "2024-08-27 15:24:56,881 - INFO - joeynmt.training - Epoch 107, Step:    10135, Batch Loss:     0.105640, Batch Acc: 0.983378, Tokens per Sec:     3680, Lr: 0.000178\r\n",
      "2024-08-27 15:24:58,003 - INFO - joeynmt.training - Epoch 107, Step:    10140, Batch Loss:     0.139617, Batch Acc: 0.980312, Tokens per Sec:     3712, Lr: 0.000178\r\n",
      "2024-08-27 15:24:59,132 - INFO - joeynmt.training - Epoch 107, Step:    10145, Batch Loss:     0.139573, Batch Acc: 0.979220, Tokens per Sec:     3798, Lr: 0.000178\r\n",
      "2024-08-27 15:25:00,222 - INFO - joeynmt.training - Epoch 107, Step:    10150, Batch Loss:     0.133490, Batch Acc: 0.977111, Tokens per Sec:     4009, Lr: 0.000178\r\n",
      "2024-08-27 15:25:01,359 - INFO - joeynmt.training - Epoch 107, Step:    10155, Batch Loss:     0.104578, Batch Acc: 0.985423, Tokens per Sec:     3503, Lr: 0.000178\r\n",
      "2024-08-27 15:25:02,460 - INFO - joeynmt.training - Epoch 107, Step:    10160, Batch Loss:     0.130332, Batch Acc: 0.981783, Tokens per Sec:     3792, Lr: 0.000177\r\n",
      "2024-08-27 15:25:03,550 - INFO - joeynmt.training - Epoch 107, Step:    10165, Batch Loss:     0.122982, Batch Acc: 0.980650, Tokens per Sec:     3985, Lr: 0.000177\r\n",
      "2024-08-27 15:25:04,684 - INFO - joeynmt.training - Epoch 107, Step:    10170, Batch Loss:     0.122293, Batch Acc: 0.980641, Tokens per Sec:     3692, Lr: 0.000177\r\n",
      "2024-08-27 15:25:05,764 - INFO - joeynmt.training - Epoch 107, Step:    10175, Batch Loss:     0.133755, Batch Acc: 0.980509, Tokens per Sec:     3897, Lr: 0.000177\r\n",
      "2024-08-27 15:25:06,897 - INFO - joeynmt.training - Epoch 107, Step:    10180, Batch Loss:     0.120107, Batch Acc: 0.981908, Tokens per Sec:     3758, Lr: 0.000177\r\n",
      "2024-08-27 15:25:07,607 - INFO - joeynmt.training - Epoch 107, total training loss: 11.64, num. of seqs: 8065, num. of tokens: 80463, 21.2488[sec]\r\n",
      "2024-08-27 15:25:07,608 - INFO - joeynmt.training - EPOCH 108\r\n",
      "2024-08-27 15:25:08,069 - INFO - joeynmt.training - Epoch 108, Step:    10185, Batch Loss:     0.123037, Batch Acc: 0.978460, Tokens per Sec:     4069, Lr: 0.000177\r\n",
      "2024-08-27 15:25:09,170 - INFO - joeynmt.training - Epoch 108, Step:    10190, Batch Loss:     0.120645, Batch Acc: 0.979617, Tokens per Sec:     3743, Lr: 0.000177\r\n",
      "2024-08-27 15:25:10,260 - INFO - joeynmt.training - Epoch 108, Step:    10195, Batch Loss:     0.101592, Batch Acc: 0.985190, Tokens per Sec:     3904, Lr: 0.000177\r\n",
      "2024-08-27 15:25:11,327 - INFO - joeynmt.training - Epoch 108, Step:    10200, Batch Loss:     0.109215, Batch Acc: 0.981782, Tokens per Sec:     3812, Lr: 0.000177\r\n",
      "2024-08-27 15:25:12,407 - INFO - joeynmt.training - Epoch 108, Step:    10205, Batch Loss:     0.118224, Batch Acc: 0.984179, Tokens per Sec:     3980, Lr: 0.000177\r\n",
      "2024-08-27 15:25:13,483 - INFO - joeynmt.training - Epoch 108, Step:    10210, Batch Loss:     0.136995, Batch Acc: 0.983034, Tokens per Sec:     3839, Lr: 0.000177\r\n",
      "2024-08-27 15:25:14,570 - INFO - joeynmt.training - Epoch 108, Step:    10215, Batch Loss:     0.125940, Batch Acc: 0.981330, Tokens per Sec:     3945, Lr: 0.000177\r\n",
      "2024-08-27 15:25:15,663 - INFO - joeynmt.training - Epoch 108, Step:    10220, Batch Loss:     0.108274, Batch Acc: 0.986723, Tokens per Sec:     3928, Lr: 0.000177\r\n",
      "2024-08-27 15:25:16,784 - INFO - joeynmt.training - Epoch 108, Step:    10225, Batch Loss:     0.124910, Batch Acc: 0.984154, Tokens per Sec:     3604, Lr: 0.000177\r\n",
      "2024-08-27 15:25:17,959 - INFO - joeynmt.training - Epoch 108, Step:    10230, Batch Loss:     0.116064, Batch Acc: 0.983939, Tokens per Sec:     3447, Lr: 0.000177\r\n",
      "2024-08-27 15:25:19,054 - INFO - joeynmt.training - Epoch 108, Step:    10235, Batch Loss:     0.125602, Batch Acc: 0.980055, Tokens per Sec:     3665, Lr: 0.000177\r\n",
      "2024-08-27 15:25:20,146 - INFO - joeynmt.training - Epoch 108, Step:    10240, Batch Loss:     0.105875, Batch Acc: 0.984507, Tokens per Sec:     3903, Lr: 0.000177\r\n",
      "2024-08-27 15:25:21,235 - INFO - joeynmt.training - Epoch 108, Step:    10245, Batch Loss:     0.100304, Batch Acc: 0.983180, Tokens per Sec:     3989, Lr: 0.000177\r\n",
      "2024-08-27 15:25:22,338 - INFO - joeynmt.training - Epoch 108, Step:    10250, Batch Loss:     0.131069, Batch Acc: 0.978582, Tokens per Sec:     3813, Lr: 0.000177\r\n",
      "2024-08-27 15:25:23,431 - INFO - joeynmt.training - Epoch 108, Step:    10255, Batch Loss:     0.111914, Batch Acc: 0.981027, Tokens per Sec:     3956, Lr: 0.000177\r\n",
      "2024-08-27 15:25:24,510 - INFO - joeynmt.training - Epoch 108, Step:    10260, Batch Loss:     0.102322, Batch Acc: 0.981460, Tokens per Sec:     4003, Lr: 0.000177\r\n",
      "2024-08-27 15:25:25,562 - INFO - joeynmt.training - Epoch 108, Step:    10265, Batch Loss:     0.118915, Batch Acc: 0.981172, Tokens per Sec:     3585, Lr: 0.000177\r\n",
      "2024-08-27 15:25:26,651 - INFO - joeynmt.training - Epoch 108, Step:    10270, Batch Loss:     0.126925, Batch Acc: 0.983994, Tokens per Sec:     3964, Lr: 0.000177\r\n",
      "2024-08-27 15:25:27,777 - INFO - joeynmt.training - Epoch 108, Step:    10275, Batch Loss:     0.124773, Batch Acc: 0.984278, Tokens per Sec:     3728, Lr: 0.000176\r\n",
      "2024-08-27 15:25:28,663 - INFO - joeynmt.training - Epoch 108, total training loss: 11.50, num. of seqs: 8065, num. of tokens: 80463, 21.0382[sec]\r\n",
      "2024-08-27 15:25:28,663 - INFO - joeynmt.training - EPOCH 109\r\n",
      "2024-08-27 15:25:28,881 - INFO - joeynmt.training - Epoch 109, Step:    10280, Batch Loss:     0.113457, Batch Acc: 0.985677, Tokens per Sec:     3586, Lr: 0.000176\r\n",
      "2024-08-27 15:25:29,964 - INFO - joeynmt.training - Epoch 109, Step:    10285, Batch Loss:     0.127251, Batch Acc: 0.986145, Tokens per Sec:     4201, Lr: 0.000176\r\n",
      "2024-08-27 15:25:31,045 - INFO - joeynmt.training - Epoch 109, Step:    10290, Batch Loss:     0.101031, Batch Acc: 0.985571, Tokens per Sec:     3977, Lr: 0.000176\r\n",
      "2024-08-27 15:25:32,115 - INFO - joeynmt.training - Epoch 109, Step:    10295, Batch Loss:     0.099011, Batch Acc: 0.986477, Tokens per Sec:     3871, Lr: 0.000176\r\n",
      "2024-08-27 15:25:33,310 - INFO - joeynmt.training - Epoch 109, Step:    10300, Batch Loss:     0.140761, Batch Acc: 0.980475, Tokens per Sec:     3560, Lr: 0.000176\r\n",
      "2024-08-27 15:25:34,393 - INFO - joeynmt.training - Epoch 109, Step:    10305, Batch Loss:     0.117955, Batch Acc: 0.986571, Tokens per Sec:     3853, Lr: 0.000176\r\n",
      "2024-08-27 15:25:35,473 - INFO - joeynmt.training - Epoch 109, Step:    10310, Batch Loss:     0.111246, Batch Acc: 0.986596, Tokens per Sec:     3870, Lr: 0.000176\r\n",
      "2024-08-27 15:25:36,558 - INFO - joeynmt.training - Epoch 109, Step:    10315, Batch Loss:     0.127692, Batch Acc: 0.984008, Tokens per Sec:     3921, Lr: 0.000176\r\n",
      "2024-08-27 15:25:37,668 - INFO - joeynmt.training - Epoch 109, Step:    10320, Batch Loss:     0.125327, Batch Acc: 0.981158, Tokens per Sec:     3878, Lr: 0.000176\r\n",
      "2024-08-27 15:25:38,775 - INFO - joeynmt.training - Epoch 109, Step:    10325, Batch Loss:     0.115376, Batch Acc: 0.985527, Tokens per Sec:     3997, Lr: 0.000176\r\n",
      "2024-08-27 15:25:39,865 - INFO - joeynmt.training - Epoch 109, Step:    10330, Batch Loss:     0.133143, Batch Acc: 0.982477, Tokens per Sec:     3877, Lr: 0.000176\r\n",
      "2024-08-27 15:25:40,943 - INFO - joeynmt.training - Epoch 109, Step:    10335, Batch Loss:     0.132320, Batch Acc: 0.985007, Tokens per Sec:     3900, Lr: 0.000176\r\n",
      "2024-08-27 15:25:42,051 - INFO - joeynmt.training - Epoch 109, Step:    10340, Batch Loss:     0.107328, Batch Acc: 0.983203, Tokens per Sec:     3817, Lr: 0.000176\r\n",
      "2024-08-27 15:25:43,141 - INFO - joeynmt.training - Epoch 109, Step:    10345, Batch Loss:     0.124394, Batch Acc: 0.980568, Tokens per Sec:     3780, Lr: 0.000176\r\n",
      "2024-08-27 15:25:44,211 - INFO - joeynmt.training - Epoch 109, Step:    10350, Batch Loss:     0.116718, Batch Acc: 0.985203, Tokens per Sec:     3918, Lr: 0.000176\r\n",
      "2024-08-27 15:25:45,305 - INFO - joeynmt.training - Epoch 109, Step:    10355, Batch Loss:     0.126260, Batch Acc: 0.980205, Tokens per Sec:     3837, Lr: 0.000176\r\n",
      "2024-08-27 15:25:46,413 - INFO - joeynmt.training - Epoch 109, Step:    10360, Batch Loss:     0.144570, Batch Acc: 0.978955, Tokens per Sec:     3560, Lr: 0.000176\r\n",
      "2024-08-27 15:25:47,532 - INFO - joeynmt.training - Epoch 109, Step:    10365, Batch Loss:     0.102375, Batch Acc: 0.982652, Tokens per Sec:     3764, Lr: 0.000176\r\n",
      "2024-08-27 15:25:48,585 - INFO - joeynmt.training - Epoch 109, Step:    10370, Batch Loss:     0.114838, Batch Acc: 0.978739, Tokens per Sec:     3843, Lr: 0.000176\r\n",
      "2024-08-27 15:25:49,695 - INFO - joeynmt.training - Epoch 109, total training loss: 11.19, num. of seqs: 8065, num. of tokens: 80463, 21.0153[sec]\r\n",
      "2024-08-27 15:25:49,696 - INFO - joeynmt.training - EPOCH 110\r\n",
      "2024-08-27 15:25:49,918 - INFO - joeynmt.training - Epoch 110, Step:    10375, Batch Loss:     0.130348, Batch Acc: 0.968338, Tokens per Sec:     3481, Lr: 0.000176\r\n",
      "2024-08-27 15:25:50,985 - INFO - joeynmt.training - Epoch 110, Step:    10380, Batch Loss:     0.136900, Batch Acc: 0.983269, Tokens per Sec:     4149, Lr: 0.000176\r\n",
      "2024-08-27 15:25:52,074 - INFO - joeynmt.training - Epoch 110, Step:    10385, Batch Loss:     0.100205, Batch Acc: 0.983055, Tokens per Sec:     3740, Lr: 0.000176\r\n",
      "2024-08-27 15:25:53,181 - INFO - joeynmt.training - Epoch 110, Step:    10390, Batch Loss:     0.115906, Batch Acc: 0.981963, Tokens per Sec:     3858, Lr: 0.000175\r\n",
      "2024-08-27 15:25:54,286 - INFO - joeynmt.training - Epoch 110, Step:    10395, Batch Loss:     0.129000, Batch Acc: 0.983121, Tokens per Sec:     3919, Lr: 0.000175\r\n",
      "2024-08-27 15:25:55,371 - INFO - joeynmt.training - Epoch 110, Step:    10400, Batch Loss:     0.108386, Batch Acc: 0.984119, Tokens per Sec:     3772, Lr: 0.000175\r\n",
      "2024-08-27 15:25:56,474 - INFO - joeynmt.training - Epoch 110, Step:    10405, Batch Loss:     0.109629, Batch Acc: 0.982946, Tokens per Sec:     4096, Lr: 0.000175\r\n",
      "2024-08-27 15:25:57,625 - INFO - joeynmt.training - Epoch 110, Step:    10410, Batch Loss:     0.117310, Batch Acc: 0.980970, Tokens per Sec:     3748, Lr: 0.000175\r\n",
      "2024-08-27 15:25:58,719 - INFO - joeynmt.training - Epoch 110, Step:    10415, Batch Loss:     0.113873, Batch Acc: 0.983155, Tokens per Sec:     3852, Lr: 0.000175\r\n",
      "2024-08-27 15:25:59,822 - INFO - joeynmt.training - Epoch 110, Step:    10420, Batch Loss:     0.110924, Batch Acc: 0.982791, Tokens per Sec:     3849, Lr: 0.000175\r\n",
      "2024-08-27 15:26:00,915 - INFO - joeynmt.training - Epoch 110, Step:    10425, Batch Loss:     0.109055, Batch Acc: 0.982352, Tokens per Sec:     3838, Lr: 0.000175\r\n",
      "2024-08-27 15:26:01,994 - INFO - joeynmt.training - Epoch 110, Step:    10430, Batch Loss:     0.123564, Batch Acc: 0.981038, Tokens per Sec:     3911, Lr: 0.000175\r\n",
      "2024-08-27 15:26:03,083 - INFO - joeynmt.training - Epoch 110, Step:    10435, Batch Loss:     0.128076, Batch Acc: 0.979258, Tokens per Sec:     3766, Lr: 0.000175\r\n",
      "2024-08-27 15:26:04,209 - INFO - joeynmt.training - Epoch 110, Step:    10440, Batch Loss:     0.111071, Batch Acc: 0.986375, Tokens per Sec:     3655, Lr: 0.000175\r\n",
      "2024-08-27 15:26:05,309 - INFO - joeynmt.training - Epoch 110, Step:    10445, Batch Loss:     0.123903, Batch Acc: 0.982472, Tokens per Sec:     3995, Lr: 0.000175\r\n",
      "2024-08-27 15:26:06,423 - INFO - joeynmt.training - Epoch 110, Step:    10450, Batch Loss:     0.115455, Batch Acc: 0.982106, Tokens per Sec:     3864, Lr: 0.000175\r\n",
      "2024-08-27 15:26:07,551 - INFO - joeynmt.training - Epoch 110, Step:    10455, Batch Loss:     0.115471, Batch Acc: 0.985229, Tokens per Sec:     3783, Lr: 0.000175\r\n",
      "2024-08-27 15:26:08,652 - INFO - joeynmt.training - Epoch 110, Step:    10460, Batch Loss:     0.128612, Batch Acc: 0.983333, Tokens per Sec:     3707, Lr: 0.000175\r\n",
      "2024-08-27 15:26:09,757 - INFO - joeynmt.training - Epoch 110, Step:    10465, Batch Loss:     0.115497, Batch Acc: 0.984536, Tokens per Sec:     3867, Lr: 0.000175\r\n",
      "2024-08-27 15:26:10,649 - INFO - joeynmt.training - Epoch 110, total training loss: 11.31, num. of seqs: 8065, num. of tokens: 80463, 20.9369[sec]\r\n",
      "2024-08-27 15:26:10,649 - INFO - joeynmt.training - EPOCH 111\r\n",
      "2024-08-27 15:26:10,877 - INFO - joeynmt.training - Epoch 111, Step:    10470, Batch Loss:     0.114181, Batch Acc: 0.986874, Tokens per Sec:     3725, Lr: 0.000175\r\n",
      "2024-08-27 15:26:11,988 - INFO - joeynmt.training - Epoch 111, Step:    10475, Batch Loss:     0.108290, Batch Acc: 0.983603, Tokens per Sec:     3902, Lr: 0.000175\r\n",
      "2024-08-27 15:26:13,079 - INFO - joeynmt.training - Epoch 111, Step:    10480, Batch Loss:     0.116348, Batch Acc: 0.986817, Tokens per Sec:     3825, Lr: 0.000175\r\n",
      "2024-08-27 15:26:14,155 - INFO - joeynmt.training - Epoch 111, Step:    10485, Batch Loss:     0.117116, Batch Acc: 0.983059, Tokens per Sec:     3954, Lr: 0.000175\r\n",
      "2024-08-27 15:26:15,225 - INFO - joeynmt.training - Epoch 111, Step:    10490, Batch Loss:     0.115042, Batch Acc: 0.985407, Tokens per Sec:     3779, Lr: 0.000175\r\n",
      "2024-08-27 15:26:16,317 - INFO - joeynmt.training - Epoch 111, Step:    10495, Batch Loss:     0.116950, Batch Acc: 0.982514, Tokens per Sec:     3881, Lr: 0.000175\r\n",
      "2024-08-27 15:26:17,422 - INFO - joeynmt.training - Epoch 111, Step:    10500, Batch Loss:     0.106608, Batch Acc: 0.982339, Tokens per Sec:     3945, Lr: 0.000175\r\n",
      "2024-08-27 15:26:17,423 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=10542\r\n",
      "2024-08-27 15:26:17,423 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:23<00:00, 63.27it/s]\r\n",
      "2024-08-27 15:26:40,501 - INFO - joeynmt.prediction - Generation took 23.0757[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 43844.98 examples/s]\r\n",
      "2024-08-27 15:26:40,880 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:26:40,880 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   4.84, loss:   6.95, ppl: 1041.11, acc:   0.28, 0.1642[sec]\r\n",
      "2024-08-27 15:26:40,881 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:26:41,329 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:26:41,329 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:26:41,329 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 15:26:41,625 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:26:41,771 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:26:41,771 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:26:41,772 - INFO - joeynmt.training - \tHypothesis: trois maisons\r\n",
      "2024-08-27 15:26:42,064 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:26:42,210 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:26:42,210 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:26:42,210 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 15:26:42,512 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:26:42,678 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:26:42,679 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:26:42,679 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 15:26:44,073 - INFO - joeynmt.training - Epoch 111, Step:    10505, Batch Loss:     0.131644, Batch Acc: 0.983111, Tokens per Sec:     3829, Lr: 0.000175\r\n",
      "2024-08-27 15:26:45,160 - INFO - joeynmt.training - Epoch 111, Step:    10510, Batch Loss:     0.121030, Batch Acc: 0.979967, Tokens per Sec:     3908, Lr: 0.000174\r\n",
      "2024-08-27 15:26:46,252 - INFO - joeynmt.training - Epoch 111, Step:    10515, Batch Loss:     0.135797, Batch Acc: 0.982624, Tokens per Sec:     3745, Lr: 0.000174\r\n",
      "2024-08-27 15:26:47,375 - INFO - joeynmt.training - Epoch 111, Step:    10520, Batch Loss:     0.121509, Batch Acc: 0.981853, Tokens per Sec:     3732, Lr: 0.000174\r\n",
      "2024-08-27 15:26:48,466 - INFO - joeynmt.training - Epoch 111, Step:    10525, Batch Loss:     0.122637, Batch Acc: 0.980722, Tokens per Sec:     3759, Lr: 0.000174\r\n",
      "2024-08-27 15:26:49,575 - INFO - joeynmt.training - Epoch 111, Step:    10530, Batch Loss:     0.110810, Batch Acc: 0.980568, Tokens per Sec:     3715, Lr: 0.000174\r\n",
      "2024-08-27 15:26:50,671 - INFO - joeynmt.training - Epoch 111, Step:    10535, Batch Loss:     0.116176, Batch Acc: 0.981997, Tokens per Sec:     3803, Lr: 0.000174\r\n",
      "2024-08-27 15:26:51,846 - INFO - joeynmt.training - Epoch 111, Step:    10540, Batch Loss:     0.120352, Batch Acc: 0.981632, Tokens per Sec:     3663, Lr: 0.000174\r\n",
      "2024-08-27 15:26:52,958 - INFO - joeynmt.training - Epoch 111, Step:    10545, Batch Loss:     0.117396, Batch Acc: 0.983781, Tokens per Sec:     3883, Lr: 0.000174\r\n",
      "2024-08-27 15:26:54,051 - INFO - joeynmt.training - Epoch 111, Step:    10550, Batch Loss:     0.133508, Batch Acc: 0.982876, Tokens per Sec:     3902, Lr: 0.000174\r\n",
      "2024-08-27 15:26:55,124 - INFO - joeynmt.training - Epoch 111, Step:    10555, Batch Loss:     0.134462, Batch Acc: 0.983322, Tokens per Sec:     4026, Lr: 0.000174\r\n",
      "2024-08-27 15:26:56,197 - INFO - joeynmt.training - Epoch 111, Step:    10560, Batch Loss:     0.115023, Batch Acc: 0.983406, Tokens per Sec:     3653, Lr: 0.000174\r\n",
      "2024-08-27 15:26:57,324 - INFO - joeynmt.training - Epoch 111, Step:    10565, Batch Loss:     0.105283, Batch Acc: 0.980109, Tokens per Sec:     3571, Lr: 0.000174\r\n",
      "2024-08-27 15:26:57,325 - INFO - joeynmt.training - Epoch 111, total training loss: 11.39, num. of seqs: 8065, num. of tokens: 80463, 21.1056[sec]\r\n",
      "2024-08-27 15:26:57,325 - INFO - joeynmt.training - EPOCH 112\r\n",
      "2024-08-27 15:26:58,424 - INFO - joeynmt.training - Epoch 112, Step:    10570, Batch Loss:     0.111255, Batch Acc: 0.983482, Tokens per Sec:     4093, Lr: 0.000174\r\n",
      "2024-08-27 15:26:59,541 - INFO - joeynmt.training - Epoch 112, Step:    10575, Batch Loss:     0.099053, Batch Acc: 0.984579, Tokens per Sec:     3834, Lr: 0.000174\r\n",
      "2024-08-27 15:27:00,698 - INFO - joeynmt.training - Epoch 112, Step:    10580, Batch Loss:     0.116745, Batch Acc: 0.988435, Tokens per Sec:     3515, Lr: 0.000174\r\n",
      "2024-08-27 15:27:01,826 - INFO - joeynmt.training - Epoch 112, Step:    10585, Batch Loss:     0.138324, Batch Acc: 0.983603, Tokens per Sec:     3680, Lr: 0.000174\r\n",
      "2024-08-27 15:27:02,969 - INFO - joeynmt.training - Epoch 112, Step:    10590, Batch Loss:     0.110651, Batch Acc: 0.986888, Tokens per Sec:     3739, Lr: 0.000174\r\n",
      "2024-08-27 15:27:04,090 - INFO - joeynmt.training - Epoch 112, Step:    10595, Batch Loss:     0.128135, Batch Acc: 0.981554, Tokens per Sec:     3870, Lr: 0.000174\r\n",
      "2024-08-27 15:27:05,217 - INFO - joeynmt.training - Epoch 112, Step:    10600, Batch Loss:     0.095875, Batch Acc: 0.987450, Tokens per Sec:     3750, Lr: 0.000174\r\n",
      "2024-08-27 15:27:06,376 - INFO - joeynmt.training - Epoch 112, Step:    10605, Batch Loss:     0.124310, Batch Acc: 0.983295, Tokens per Sec:     3720, Lr: 0.000174\r\n",
      "2024-08-27 15:27:07,541 - INFO - joeynmt.training - Epoch 112, Step:    10610, Batch Loss:     0.143698, Batch Acc: 0.980044, Tokens per Sec:     3529, Lr: 0.000174\r\n",
      "2024-08-27 15:27:08,674 - INFO - joeynmt.training - Epoch 112, Step:    10615, Batch Loss:     0.119211, Batch Acc: 0.985860, Tokens per Sec:     3563, Lr: 0.000174\r\n",
      "2024-08-27 15:27:09,821 - INFO - joeynmt.training - Epoch 112, Step:    10620, Batch Loss:     0.102011, Batch Acc: 0.982096, Tokens per Sec:     3654, Lr: 0.000174\r\n",
      "2024-08-27 15:27:10,962 - INFO - joeynmt.training - Epoch 112, Step:    10625, Batch Loss:     0.156798, Batch Acc: 0.978339, Tokens per Sec:     3925, Lr: 0.000174\r\n",
      "2024-08-27 15:27:12,078 - INFO - joeynmt.training - Epoch 112, Step:    10630, Batch Loss:     0.103012, Batch Acc: 0.985902, Tokens per Sec:     3626, Lr: 0.000174\r\n",
      "2024-08-27 15:27:13,224 - INFO - joeynmt.training - Epoch 112, Step:    10635, Batch Loss:     0.107057, Batch Acc: 0.981641, Tokens per Sec:     3759, Lr: 0.000173\r\n",
      "2024-08-27 15:27:14,364 - INFO - joeynmt.training - Epoch 112, Step:    10640, Batch Loss:     0.124623, Batch Acc: 0.978787, Tokens per Sec:     3807, Lr: 0.000173\r\n",
      "2024-08-27 15:27:15,468 - INFO - joeynmt.training - Epoch 112, Step:    10645, Batch Loss:     0.118293, Batch Acc: 0.985872, Tokens per Sec:     3784, Lr: 0.000173\r\n",
      "2024-08-27 15:27:16,619 - INFO - joeynmt.training - Epoch 112, Step:    10650, Batch Loss:     0.101982, Batch Acc: 0.983213, Tokens per Sec:     3727, Lr: 0.000173\r\n",
      "2024-08-27 15:27:17,829 - INFO - joeynmt.training - Epoch 112, Step:    10655, Batch Loss:     0.110279, Batch Acc: 0.983383, Tokens per Sec:     3585, Lr: 0.000173\r\n",
      "2024-08-27 15:27:18,985 - INFO - joeynmt.training - Epoch 112, Step:    10660, Batch Loss:     0.114684, Batch Acc: 0.982033, Tokens per Sec:     3517, Lr: 0.000173\r\n",
      "2024-08-27 15:27:18,986 - INFO - joeynmt.training - Epoch 112, total training loss: 11.15, num. of seqs: 8065, num. of tokens: 80463, 21.6425[sec]\r\n",
      "2024-08-27 15:27:18,986 - INFO - joeynmt.training - EPOCH 113\r\n",
      "2024-08-27 15:27:20,140 - INFO - joeynmt.training - Epoch 113, Step:    10665, Batch Loss:     0.100957, Batch Acc: 0.984394, Tokens per Sec:     3562, Lr: 0.000173\r\n",
      "2024-08-27 15:27:21,260 - INFO - joeynmt.training - Epoch 113, Step:    10670, Batch Loss:     0.109740, Batch Acc: 0.986059, Tokens per Sec:     3783, Lr: 0.000173\r\n",
      "2024-08-27 15:27:22,402 - INFO - joeynmt.training - Epoch 113, Step:    10675, Batch Loss:     0.123923, Batch Acc: 0.983756, Tokens per Sec:     3560, Lr: 0.000173\r\n",
      "2024-08-27 15:27:23,612 - INFO - joeynmt.training - Epoch 113, Step:    10680, Batch Loss:     0.110404, Batch Acc: 0.982792, Tokens per Sec:     3459, Lr: 0.000173\r\n",
      "2024-08-27 15:27:24,723 - INFO - joeynmt.training - Epoch 113, Step:    10685, Batch Loss:     0.108410, Batch Acc: 0.982240, Tokens per Sec:     3955, Lr: 0.000173\r\n",
      "2024-08-27 15:27:25,788 - INFO - joeynmt.training - Epoch 113, Step:    10690, Batch Loss:     0.139636, Batch Acc: 0.983271, Tokens per Sec:     4046, Lr: 0.000173\r\n",
      "2024-08-27 15:27:26,888 - INFO - joeynmt.training - Epoch 113, Step:    10695, Batch Loss:     0.124802, Batch Acc: 0.983361, Tokens per Sec:     3882, Lr: 0.000173\r\n",
      "2024-08-27 15:27:27,977 - INFO - joeynmt.training - Epoch 113, Step:    10700, Batch Loss:     0.118119, Batch Acc: 0.986959, Tokens per Sec:     3944, Lr: 0.000173\r\n",
      "2024-08-27 15:27:29,057 - INFO - joeynmt.training - Epoch 113, Step:    10705, Batch Loss:     0.098119, Batch Acc: 0.984167, Tokens per Sec:     3689, Lr: 0.000173\r\n",
      "2024-08-27 15:27:30,163 - INFO - joeynmt.training - Epoch 113, Step:    10710, Batch Loss:     0.113219, Batch Acc: 0.982550, Tokens per Sec:     3733, Lr: 0.000173\r\n",
      "2024-08-27 15:27:31,234 - INFO - joeynmt.training - Epoch 113, Step:    10715, Batch Loss:     0.085846, Batch Acc: 0.985187, Tokens per Sec:     3971, Lr: 0.000173\r\n",
      "2024-08-27 15:27:32,301 - INFO - joeynmt.training - Epoch 113, Step:    10720, Batch Loss:     0.125341, Batch Acc: 0.983956, Tokens per Sec:     4093, Lr: 0.000173\r\n",
      "2024-08-27 15:27:33,376 - INFO - joeynmt.training - Epoch 113, Step:    10725, Batch Loss:     0.135758, Batch Acc: 0.983498, Tokens per Sec:     3948, Lr: 0.000173\r\n",
      "2024-08-27 15:27:34,453 - INFO - joeynmt.training - Epoch 113, Step:    10730, Batch Loss:     0.128895, Batch Acc: 0.985078, Tokens per Sec:     3860, Lr: 0.000173\r\n",
      "2024-08-27 15:27:35,531 - INFO - joeynmt.training - Epoch 113, Step:    10735, Batch Loss:     0.133687, Batch Acc: 0.980446, Tokens per Sec:     4038, Lr: 0.000173\r\n",
      "2024-08-27 15:27:36,593 - INFO - joeynmt.training - Epoch 113, Step:    10740, Batch Loss:     0.118823, Batch Acc: 0.985113, Tokens per Sec:     4049, Lr: 0.000173\r\n",
      "2024-08-27 15:27:37,702 - INFO - joeynmt.training - Epoch 113, Step:    10745, Batch Loss:     0.102542, Batch Acc: 0.981673, Tokens per Sec:     3743, Lr: 0.000173\r\n",
      "2024-08-27 15:27:38,778 - INFO - joeynmt.training - Epoch 113, Step:    10750, Batch Loss:     0.132214, Batch Acc: 0.983834, Tokens per Sec:     4083, Lr: 0.000173\r\n",
      "2024-08-27 15:27:39,852 - INFO - joeynmt.training - Epoch 113, Step:    10755, Batch Loss:     0.132646, Batch Acc: 0.981032, Tokens per Sec:     4027, Lr: 0.000172\r\n",
      "2024-08-27 15:27:39,853 - INFO - joeynmt.training - Epoch 113, total training loss: 11.02, num. of seqs: 8065, num. of tokens: 80463, 20.8503[sec]\r\n",
      "2024-08-27 15:27:39,853 - INFO - joeynmt.training - EPOCH 114\r\n",
      "2024-08-27 15:27:40,944 - INFO - joeynmt.training - Epoch 114, Step:    10760, Batch Loss:     0.109127, Batch Acc: 0.986544, Tokens per Sec:     3894, Lr: 0.000172\r\n",
      "2024-08-27 15:27:42,036 - INFO - joeynmt.training - Epoch 114, Step:    10765, Batch Loss:     0.110032, Batch Acc: 0.985400, Tokens per Sec:     3703, Lr: 0.000172\r\n",
      "2024-08-27 15:27:43,130 - INFO - joeynmt.training - Epoch 114, Step:    10770, Batch Loss:     0.120467, Batch Acc: 0.984577, Tokens per Sec:     4032, Lr: 0.000172\r\n",
      "2024-08-27 15:27:44,212 - INFO - joeynmt.training - Epoch 114, Step:    10775, Batch Loss:     0.121235, Batch Acc: 0.985439, Tokens per Sec:     3940, Lr: 0.000172\r\n",
      "2024-08-27 15:27:45,274 - INFO - joeynmt.training - Epoch 114, Step:    10780, Batch Loss:     0.127111, Batch Acc: 0.983429, Tokens per Sec:     4095, Lr: 0.000172\r\n",
      "2024-08-27 15:27:46,340 - INFO - joeynmt.training - Epoch 114, Step:    10785, Batch Loss:     0.106234, Batch Acc: 0.986817, Tokens per Sec:     3916, Lr: 0.000172\r\n",
      "2024-08-27 15:27:47,423 - INFO - joeynmt.training - Epoch 114, Step:    10790, Batch Loss:     0.125091, Batch Acc: 0.985997, Tokens per Sec:     3826, Lr: 0.000172\r\n",
      "2024-08-27 15:27:48,513 - INFO - joeynmt.training - Epoch 114, Step:    10795, Batch Loss:     0.106638, Batch Acc: 0.983834, Tokens per Sec:     3975, Lr: 0.000172\r\n",
      "2024-08-27 15:27:49,583 - INFO - joeynmt.training - Epoch 114, Step:    10800, Batch Loss:     0.130237, Batch Acc: 0.984726, Tokens per Sec:     3921, Lr: 0.000172\r\n",
      "2024-08-27 15:27:50,642 - INFO - joeynmt.training - Epoch 114, Step:    10805, Batch Loss:     0.111327, Batch Acc: 0.984138, Tokens per Sec:     3930, Lr: 0.000172\r\n",
      "2024-08-27 15:27:51,718 - INFO - joeynmt.training - Epoch 114, Step:    10810, Batch Loss:     0.099305, Batch Acc: 0.981840, Tokens per Sec:     3892, Lr: 0.000172\r\n",
      "2024-08-27 15:27:52,786 - INFO - joeynmt.training - Epoch 114, Step:    10815, Batch Loss:     0.123451, Batch Acc: 0.984601, Tokens per Sec:     4016, Lr: 0.000172\r\n",
      "2024-08-27 15:27:53,970 - INFO - joeynmt.training - Epoch 114, Step:    10820, Batch Loss:     0.133447, Batch Acc: 0.984198, Tokens per Sec:     3582, Lr: 0.000172\r\n",
      "2024-08-27 15:27:55,082 - INFO - joeynmt.training - Epoch 114, Step:    10825, Batch Loss:     0.100592, Batch Acc: 0.986366, Tokens per Sec:     3831, Lr: 0.000172\r\n",
      "2024-08-27 15:27:56,187 - INFO - joeynmt.training - Epoch 114, Step:    10830, Batch Loss:     0.112068, Batch Acc: 0.982489, Tokens per Sec:     3878, Lr: 0.000172\r\n",
      "2024-08-27 15:27:57,310 - INFO - joeynmt.training - Epoch 114, Step:    10835, Batch Loss:     0.134555, Batch Acc: 0.978955, Tokens per Sec:     3683, Lr: 0.000172\r\n",
      "2024-08-27 15:27:58,382 - INFO - joeynmt.training - Epoch 114, Step:    10840, Batch Loss:     0.126491, Batch Acc: 0.986639, Tokens per Sec:     3983, Lr: 0.000172\r\n",
      "2024-08-27 15:27:59,465 - INFO - joeynmt.training - Epoch 114, Step:    10845, Batch Loss:     0.107797, Batch Acc: 0.983147, Tokens per Sec:     3890, Lr: 0.000172\r\n",
      "2024-08-27 15:28:00,540 - INFO - joeynmt.training - Epoch 114, Step:    10850, Batch Loss:     0.111761, Batch Acc: 0.978520, Tokens per Sec:     3989, Lr: 0.000172\r\n",
      "2024-08-27 15:28:00,586 - INFO - joeynmt.training - Epoch 114, total training loss: 10.87, num. of seqs: 8065, num. of tokens: 80463, 20.7166[sec]\r\n",
      "2024-08-27 15:28:00,586 - INFO - joeynmt.training - EPOCH 115\r\n",
      "2024-08-27 15:28:01,665 - INFO - joeynmt.training - Epoch 115, Step:    10855, Batch Loss:     0.121481, Batch Acc: 0.986808, Tokens per Sec:     3950, Lr: 0.000172\r\n",
      "2024-08-27 15:28:02,725 - INFO - joeynmt.training - Epoch 115, Step:    10860, Batch Loss:     0.118127, Batch Acc: 0.984691, Tokens per Sec:     3825, Lr: 0.000172\r\n",
      "2024-08-27 15:28:03,782 - INFO - joeynmt.training - Epoch 115, Step:    10865, Batch Loss:     0.114435, Batch Acc: 0.982523, Tokens per Sec:     3735, Lr: 0.000172\r\n",
      "2024-08-27 15:28:04,841 - INFO - joeynmt.training - Epoch 115, Step:    10870, Batch Loss:     0.128243, Batch Acc: 0.984671, Tokens per Sec:     3947, Lr: 0.000172\r\n",
      "2024-08-27 15:28:05,910 - INFO - joeynmt.training - Epoch 115, Step:    10875, Batch Loss:     0.122220, Batch Acc: 0.984316, Tokens per Sec:     3936, Lr: 0.000172\r\n",
      "2024-08-27 15:28:07,031 - INFO - joeynmt.training - Epoch 115, Step:    10880, Batch Loss:     0.111987, Batch Acc: 0.982254, Tokens per Sec:     3725, Lr: 0.000171\r\n",
      "2024-08-27 15:28:08,153 - INFO - joeynmt.training - Epoch 115, Step:    10885, Batch Loss:     0.101680, Batch Acc: 0.984726, Tokens per Sec:     3852, Lr: 0.000171\r\n",
      "2024-08-27 15:28:09,223 - INFO - joeynmt.training - Epoch 115, Step:    10890, Batch Loss:     0.107399, Batch Acc: 0.986420, Tokens per Sec:     3995, Lr: 0.000171\r\n",
      "2024-08-27 15:28:10,305 - INFO - joeynmt.training - Epoch 115, Step:    10895, Batch Loss:     0.100317, Batch Acc: 0.986006, Tokens per Sec:     3897, Lr: 0.000171\r\n",
      "2024-08-27 15:28:11,389 - INFO - joeynmt.training - Epoch 115, Step:    10900, Batch Loss:     0.117246, Batch Acc: 0.981753, Tokens per Sec:     4096, Lr: 0.000171\r\n",
      "2024-08-27 15:28:12,446 - INFO - joeynmt.training - Epoch 115, Step:    10905, Batch Loss:     0.109883, Batch Acc: 0.983959, Tokens per Sec:     3836, Lr: 0.000171\r\n",
      "2024-08-27 15:28:13,517 - INFO - joeynmt.training - Epoch 115, Step:    10910, Batch Loss:     0.103329, Batch Acc: 0.983166, Tokens per Sec:     3996, Lr: 0.000171\r\n",
      "2024-08-27 15:28:14,586 - INFO - joeynmt.training - Epoch 115, Step:    10915, Batch Loss:     0.106343, Batch Acc: 0.982565, Tokens per Sec:     3919, Lr: 0.000171\r\n",
      "2024-08-27 15:28:15,680 - INFO - joeynmt.training - Epoch 115, Step:    10920, Batch Loss:     0.126256, Batch Acc: 0.985594, Tokens per Sec:     3684, Lr: 0.000171\r\n",
      "2024-08-27 15:28:16,801 - INFO - joeynmt.training - Epoch 115, Step:    10925, Batch Loss:     0.113061, Batch Acc: 0.985122, Tokens per Sec:     3900, Lr: 0.000171\r\n",
      "2024-08-27 15:28:17,888 - INFO - joeynmt.training - Epoch 115, Step:    10930, Batch Loss:     0.130360, Batch Acc: 0.984856, Tokens per Sec:     3767, Lr: 0.000171\r\n",
      "2024-08-27 15:28:18,971 - INFO - joeynmt.training - Epoch 115, Step:    10935, Batch Loss:     0.120219, Batch Acc: 0.982299, Tokens per Sec:     3812, Lr: 0.000171\r\n",
      "2024-08-27 15:28:20,052 - INFO - joeynmt.training - Epoch 115, Step:    10940, Batch Loss:     0.106410, Batch Acc: 0.982540, Tokens per Sec:     4080, Lr: 0.000171\r\n",
      "2024-08-27 15:28:21,146 - INFO - joeynmt.training - Epoch 115, Step:    10945, Batch Loss:     0.100952, Batch Acc: 0.979661, Tokens per Sec:     3778, Lr: 0.000171\r\n",
      "2024-08-27 15:28:21,359 - INFO - joeynmt.training - Epoch 115, total training loss: 10.93, num. of seqs: 8065, num. of tokens: 80463, 20.7566[sec]\r\n",
      "2024-08-27 15:28:21,360 - INFO - joeynmt.training - EPOCH 116\r\n",
      "2024-08-27 15:28:22,249 - INFO - joeynmt.training - Epoch 116, Step:    10950, Batch Loss:     0.113705, Batch Acc: 0.983056, Tokens per Sec:     3663, Lr: 0.000171\r\n",
      "2024-08-27 15:28:23,353 - INFO - joeynmt.training - Epoch 116, Step:    10955, Batch Loss:     0.117103, Batch Acc: 0.989764, Tokens per Sec:     3809, Lr: 0.000171\r\n",
      "2024-08-27 15:28:24,456 - INFO - joeynmt.training - Epoch 116, Step:    10960, Batch Loss:     0.110608, Batch Acc: 0.984758, Tokens per Sec:     3930, Lr: 0.000171\r\n",
      "2024-08-27 15:28:25,674 - INFO - joeynmt.training - Epoch 116, Step:    10965, Batch Loss:     0.113372, Batch Acc: 0.983043, Tokens per Sec:     3440, Lr: 0.000171\r\n",
      "2024-08-27 15:28:26,813 - INFO - joeynmt.training - Epoch 116, Step:    10970, Batch Loss:     0.100855, Batch Acc: 0.985241, Tokens per Sec:     3631, Lr: 0.000171\r\n",
      "2024-08-27 15:28:27,899 - INFO - joeynmt.training - Epoch 116, Step:    10975, Batch Loss:     0.107873, Batch Acc: 0.984922, Tokens per Sec:     3971, Lr: 0.000171\r\n",
      "2024-08-27 15:28:28,983 - INFO - joeynmt.training - Epoch 116, Step:    10980, Batch Loss:     0.120005, Batch Acc: 0.982619, Tokens per Sec:     3770, Lr: 0.000171\r\n",
      "2024-08-27 15:28:30,077 - INFO - joeynmt.training - Epoch 116, Step:    10985, Batch Loss:     0.101931, Batch Acc: 0.983879, Tokens per Sec:     3915, Lr: 0.000171\r\n",
      "2024-08-27 15:28:31,145 - INFO - joeynmt.training - Epoch 116, Step:    10990, Batch Loss:     0.109117, Batch Acc: 0.988838, Tokens per Sec:     3691, Lr: 0.000171\r\n",
      "2024-08-27 15:28:32,234 - INFO - joeynmt.training - Epoch 116, Step:    10995, Batch Loss:     0.108472, Batch Acc: 0.983696, Tokens per Sec:     3890, Lr: 0.000171\r\n",
      "2024-08-27 15:28:33,323 - INFO - joeynmt.training - Epoch 116, Step:    11000, Batch Loss:     0.118529, Batch Acc: 0.983706, Tokens per Sec:     3780, Lr: 0.000171\r\n",
      "2024-08-27 15:28:33,324 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=11042\r\n",
      "2024-08-27 15:28:33,324 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 65.89it/s]\r\n",
      "2024-08-27 15:28:55,485 - INFO - joeynmt.prediction - Generation took 22.1591[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44693.63 examples/s]\r\n",
      "2024-08-27 15:28:55,862 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:28:55,862 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   4.99, loss:   6.91, ppl: 1005.49, acc:   0.28, 0.1636[sec]\r\n",
      "2024-08-27 15:28:55,864 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:28:56,033 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:28:56,033 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:28:56,033 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 15:28:56,349 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:28:56,503 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:28:56,503 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:28:56,503 - INFO - joeynmt.training - \tHypothesis: trois maisons en bois\r\n",
      "2024-08-27 15:28:56,836 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:28:56,985 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:28:56,985 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:28:56,985 - INFO - joeynmt.training - \tHypothesis: cependant le vieux était le coup\r\n",
      "2024-08-27 15:28:57,272 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:28:57,416 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:28:57,417 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:28:57,417 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 15:28:58,767 - INFO - joeynmt.training - Epoch 116, Step:    11005, Batch Loss:     0.110840, Batch Acc: 0.984352, Tokens per Sec:     3921, Lr: 0.000171\r\n",
      "2024-08-27 15:28:59,825 - INFO - joeynmt.training - Epoch 116, Step:    11010, Batch Loss:     0.139824, Batch Acc: 0.982956, Tokens per Sec:     4052, Lr: 0.000170\r\n",
      "2024-08-27 15:29:00,894 - INFO - joeynmt.training - Epoch 116, Step:    11015, Batch Loss:     0.134003, Batch Acc: 0.983591, Tokens per Sec:     3994, Lr: 0.000170\r\n",
      "2024-08-27 15:29:01,975 - INFO - joeynmt.training - Epoch 116, Step:    11020, Batch Loss:     0.120863, Batch Acc: 0.985430, Tokens per Sec:     3809, Lr: 0.000170\r\n",
      "2024-08-27 15:29:03,055 - INFO - joeynmt.training - Epoch 116, Step:    11025, Batch Loss:     0.127851, Batch Acc: 0.984084, Tokens per Sec:     3783, Lr: 0.000170\r\n",
      "2024-08-27 15:29:04,169 - INFO - joeynmt.training - Epoch 116, Step:    11030, Batch Loss:     0.127547, Batch Acc: 0.984571, Tokens per Sec:     4076, Lr: 0.000170\r\n",
      "2024-08-27 15:29:05,250 - INFO - joeynmt.training - Epoch 116, Step:    11035, Batch Loss:     0.110213, Batch Acc: 0.983225, Tokens per Sec:     3866, Lr: 0.000170\r\n",
      "2024-08-27 15:29:06,315 - INFO - joeynmt.training - Epoch 116, Step:    11040, Batch Loss:     0.108348, Batch Acc: 0.985763, Tokens per Sec:     3891, Lr: 0.000170\r\n",
      "2024-08-27 15:29:06,795 - INFO - joeynmt.training - Epoch 116, total training loss: 10.73, num. of seqs: 8065, num. of tokens: 80463, 21.0347[sec]\r\n",
      "2024-08-27 15:29:06,795 - INFO - joeynmt.training - EPOCH 117\r\n",
      "2024-08-27 15:29:07,447 - INFO - joeynmt.training - Epoch 117, Step:    11045, Batch Loss:     0.109233, Batch Acc: 0.984646, Tokens per Sec:     3919, Lr: 0.000170\r\n",
      "2024-08-27 15:29:08,541 - INFO - joeynmt.training - Epoch 117, Step:    11050, Batch Loss:     0.094295, Batch Acc: 0.984134, Tokens per Sec:     3978, Lr: 0.000170\r\n",
      "2024-08-27 15:29:09,614 - INFO - joeynmt.training - Epoch 117, Step:    11055, Batch Loss:     0.115278, Batch Acc: 0.984206, Tokens per Sec:     3955, Lr: 0.000170\r\n",
      "2024-08-27 15:29:10,704 - INFO - joeynmt.training - Epoch 117, Step:    11060, Batch Loss:     0.118684, Batch Acc: 0.984138, Tokens per Sec:     3820, Lr: 0.000170\r\n",
      "2024-08-27 15:29:11,793 - INFO - joeynmt.training - Epoch 117, Step:    11065, Batch Loss:     0.115910, Batch Acc: 0.986733, Tokens per Sec:     3881, Lr: 0.000170\r\n",
      "2024-08-27 15:29:12,885 - INFO - joeynmt.training - Epoch 117, Step:    11070, Batch Loss:     0.120145, Batch Acc: 0.984642, Tokens per Sec:     3758, Lr: 0.000170\r\n",
      "2024-08-27 15:29:13,986 - INFO - joeynmt.training - Epoch 117, Step:    11075, Batch Loss:     0.105571, Batch Acc: 0.984034, Tokens per Sec:     3869, Lr: 0.000170\r\n",
      "2024-08-27 15:29:15,071 - INFO - joeynmt.training - Epoch 117, Step:    11080, Batch Loss:     0.120030, Batch Acc: 0.985821, Tokens per Sec:     3839, Lr: 0.000170\r\n",
      "2024-08-27 15:29:16,166 - INFO - joeynmt.training - Epoch 117, Step:    11085, Batch Loss:     0.101768, Batch Acc: 0.981943, Tokens per Sec:     3846, Lr: 0.000170\r\n",
      "2024-08-27 15:29:17,292 - INFO - joeynmt.training - Epoch 117, Step:    11090, Batch Loss:     0.123063, Batch Acc: 0.984474, Tokens per Sec:     3664, Lr: 0.000170\r\n",
      "2024-08-27 15:29:18,376 - INFO - joeynmt.training - Epoch 117, Step:    11095, Batch Loss:     0.124088, Batch Acc: 0.985878, Tokens per Sec:     3855, Lr: 0.000170\r\n",
      "2024-08-27 15:29:19,439 - INFO - joeynmt.training - Epoch 117, Step:    11100, Batch Loss:     0.126833, Batch Acc: 0.982147, Tokens per Sec:     4006, Lr: 0.000170\r\n",
      "2024-08-27 15:29:20,518 - INFO - joeynmt.training - Epoch 117, Step:    11105, Batch Loss:     0.133297, Batch Acc: 0.981588, Tokens per Sec:     4183, Lr: 0.000170\r\n",
      "2024-08-27 15:29:21,570 - INFO - joeynmt.training - Epoch 117, Step:    11110, Batch Loss:     0.123687, Batch Acc: 0.981744, Tokens per Sec:     3960, Lr: 0.000170\r\n",
      "2024-08-27 15:29:22,638 - INFO - joeynmt.training - Epoch 117, Step:    11115, Batch Loss:     0.119001, Batch Acc: 0.984465, Tokens per Sec:     3738, Lr: 0.000170\r\n",
      "2024-08-27 15:29:23,721 - INFO - joeynmt.training - Epoch 117, Step:    11120, Batch Loss:     0.112436, Batch Acc: 0.987694, Tokens per Sec:     3980, Lr: 0.000170\r\n",
      "2024-08-27 15:29:24,809 - INFO - joeynmt.training - Epoch 117, Step:    11125, Batch Loss:     0.132818, Batch Acc: 0.982719, Tokens per Sec:     4047, Lr: 0.000170\r\n",
      "2024-08-27 15:29:25,927 - INFO - joeynmt.training - Epoch 117, Step:    11130, Batch Loss:     0.110135, Batch Acc: 0.979502, Tokens per Sec:     3839, Lr: 0.000170\r\n",
      "2024-08-27 15:29:27,069 - INFO - joeynmt.training - Epoch 117, Step:    11135, Batch Loss:     0.114669, Batch Acc: 0.983910, Tokens per Sec:     3651, Lr: 0.000170\r\n",
      "2024-08-27 15:29:27,623 - INFO - joeynmt.training - Epoch 117, total training loss: 10.83, num. of seqs: 8065, num. of tokens: 80463, 20.8113[sec]\r\n",
      "2024-08-27 15:29:27,623 - INFO - joeynmt.training - EPOCH 118\r\n",
      "2024-08-27 15:29:28,286 - INFO - joeynmt.training - Epoch 118, Step:    11140, Batch Loss:     0.108515, Batch Acc: 0.988760, Tokens per Sec:     3779, Lr: 0.000169\r\n",
      "2024-08-27 15:29:29,367 - INFO - joeynmt.training - Epoch 118, Step:    11145, Batch Loss:     0.113981, Batch Acc: 0.984283, Tokens per Sec:     3947, Lr: 0.000169\r\n",
      "2024-08-27 15:29:30,430 - INFO - joeynmt.training - Epoch 118, Step:    11150, Batch Loss:     0.090945, Batch Acc: 0.990368, Tokens per Sec:     3908, Lr: 0.000169\r\n",
      "2024-08-27 15:29:31,512 - INFO - joeynmt.training - Epoch 118, Step:    11155, Batch Loss:     0.114736, Batch Acc: 0.986848, Tokens per Sec:     3939, Lr: 0.000169\r\n",
      "2024-08-27 15:29:32,594 - INFO - joeynmt.training - Epoch 118, Step:    11160, Batch Loss:     0.093287, Batch Acc: 0.984575, Tokens per Sec:     3837, Lr: 0.000169\r\n",
      "2024-08-27 15:29:33,707 - INFO - joeynmt.training - Epoch 118, Step:    11165, Batch Loss:     0.103526, Batch Acc: 0.986467, Tokens per Sec:     3787, Lr: 0.000169\r\n",
      "2024-08-27 15:29:34,801 - INFO - joeynmt.training - Epoch 118, Step:    11170, Batch Loss:     0.108659, Batch Acc: 0.986520, Tokens per Sec:     3664, Lr: 0.000169\r\n",
      "2024-08-27 15:29:35,903 - INFO - joeynmt.training - Epoch 118, Step:    11175, Batch Loss:     0.119930, Batch Acc: 0.983265, Tokens per Sec:     3958, Lr: 0.000169\r\n",
      "2024-08-27 15:29:37,026 - INFO - joeynmt.training - Epoch 118, Step:    11180, Batch Loss:     0.104226, Batch Acc: 0.984952, Tokens per Sec:     3791, Lr: 0.000169\r\n",
      "2024-08-27 15:29:38,104 - INFO - joeynmt.training - Epoch 118, Step:    11185, Batch Loss:     0.102768, Batch Acc: 0.983998, Tokens per Sec:     3769, Lr: 0.000169\r\n",
      "2024-08-27 15:29:39,156 - INFO - joeynmt.training - Epoch 118, Step:    11190, Batch Loss:     0.106313, Batch Acc: 0.982365, Tokens per Sec:     3668, Lr: 0.000169\r\n",
      "2024-08-27 15:29:40,256 - INFO - joeynmt.training - Epoch 118, Step:    11195, Batch Loss:     0.104098, Batch Acc: 0.984269, Tokens per Sec:     3874, Lr: 0.000169\r\n",
      "2024-08-27 15:29:41,326 - INFO - joeynmt.training - Epoch 118, Step:    11200, Batch Loss:     0.113131, Batch Acc: 0.985555, Tokens per Sec:     4015, Lr: 0.000169\r\n",
      "2024-08-27 15:29:42,426 - INFO - joeynmt.training - Epoch 118, Step:    11205, Batch Loss:     0.106074, Batch Acc: 0.985922, Tokens per Sec:     3875, Lr: 0.000169\r\n",
      "2024-08-27 15:29:43,507 - INFO - joeynmt.training - Epoch 118, Step:    11210, Batch Loss:     0.099798, Batch Acc: 0.985539, Tokens per Sec:     3778, Lr: 0.000169\r\n",
      "2024-08-27 15:29:44,583 - INFO - joeynmt.training - Epoch 118, Step:    11215, Batch Loss:     0.124025, Batch Acc: 0.984415, Tokens per Sec:     3997, Lr: 0.000169\r\n",
      "2024-08-27 15:29:45,648 - INFO - joeynmt.training - Epoch 118, Step:    11220, Batch Loss:     0.119979, Batch Acc: 0.980718, Tokens per Sec:     3897, Lr: 0.000169\r\n",
      "2024-08-27 15:29:46,740 - INFO - joeynmt.training - Epoch 118, Step:    11225, Batch Loss:     0.126170, Batch Acc: 0.982382, Tokens per Sec:     3693, Lr: 0.000169\r\n",
      "2024-08-27 15:29:47,813 - INFO - joeynmt.training - Epoch 118, Step:    11230, Batch Loss:     0.114217, Batch Acc: 0.985721, Tokens per Sec:     4047, Lr: 0.000169\r\n",
      "2024-08-27 15:29:48,511 - INFO - joeynmt.training - Epoch 118, total training loss: 10.63, num. of seqs: 8065, num. of tokens: 80463, 20.8716[sec]\r\n",
      "2024-08-27 15:29:48,511 - INFO - joeynmt.training - EPOCH 119\r\n",
      "2024-08-27 15:29:48,943 - INFO - joeynmt.training - Epoch 119, Step:    11235, Batch Loss:     0.110093, Batch Acc: 0.987403, Tokens per Sec:     3899, Lr: 0.000169\r\n",
      "2024-08-27 15:29:50,012 - INFO - joeynmt.training - Epoch 119, Step:    11240, Batch Loss:     0.115373, Batch Acc: 0.983456, Tokens per Sec:     4075, Lr: 0.000169\r\n",
      "2024-08-27 15:29:51,080 - INFO - joeynmt.training - Epoch 119, Step:    11245, Batch Loss:     0.102409, Batch Acc: 0.985925, Tokens per Sec:     3995, Lr: 0.000169\r\n",
      "2024-08-27 15:29:52,162 - INFO - joeynmt.training - Epoch 119, Step:    11250, Batch Loss:     0.104678, Batch Acc: 0.988819, Tokens per Sec:     3968, Lr: 0.000169\r\n",
      "2024-08-27 15:29:53,257 - INFO - joeynmt.training - Epoch 119, Step:    11255, Batch Loss:     0.101941, Batch Acc: 0.984955, Tokens per Sec:     3890, Lr: 0.000169\r\n",
      "2024-08-27 15:29:54,352 - INFO - joeynmt.training - Epoch 119, Step:    11260, Batch Loss:     0.105340, Batch Acc: 0.988769, Tokens per Sec:     3823, Lr: 0.000169\r\n",
      "2024-08-27 15:29:55,456 - INFO - joeynmt.training - Epoch 119, Step:    11265, Batch Loss:     0.102852, Batch Acc: 0.984770, Tokens per Sec:     3688, Lr: 0.000169\r\n",
      "2024-08-27 15:29:56,613 - INFO - joeynmt.training - Epoch 119, Step:    11270, Batch Loss:     0.099084, Batch Acc: 0.987955, Tokens per Sec:     3736, Lr: 0.000169\r\n",
      "2024-08-27 15:29:57,728 - INFO - joeynmt.training - Epoch 119, Step:    11275, Batch Loss:     0.105243, Batch Acc: 0.987171, Tokens per Sec:     3846, Lr: 0.000168\r\n",
      "2024-08-27 15:29:58,896 - INFO - joeynmt.training - Epoch 119, Step:    11280, Batch Loss:     0.098003, Batch Acc: 0.986095, Tokens per Sec:     3696, Lr: 0.000168\r\n",
      "2024-08-27 15:29:59,988 - INFO - joeynmt.training - Epoch 119, Step:    11285, Batch Loss:     0.104896, Batch Acc: 0.988366, Tokens per Sec:     3781, Lr: 0.000168\r\n",
      "2024-08-27 15:30:01,074 - INFO - joeynmt.training - Epoch 119, Step:    11290, Batch Loss:     0.122428, Batch Acc: 0.983443, Tokens per Sec:     3786, Lr: 0.000168\r\n",
      "2024-08-27 15:30:02,144 - INFO - joeynmt.training - Epoch 119, Step:    11295, Batch Loss:     0.104225, Batch Acc: 0.986393, Tokens per Sec:     3780, Lr: 0.000168\r\n",
      "2024-08-27 15:30:03,226 - INFO - joeynmt.training - Epoch 119, Step:    11300, Batch Loss:     0.112269, Batch Acc: 0.985999, Tokens per Sec:     3763, Lr: 0.000168\r\n",
      "2024-08-27 15:30:04,317 - INFO - joeynmt.training - Epoch 119, Step:    11305, Batch Loss:     0.125050, Batch Acc: 0.986004, Tokens per Sec:     3800, Lr: 0.000168\r\n",
      "2024-08-27 15:30:05,408 - INFO - joeynmt.training - Epoch 119, Step:    11310, Batch Loss:     0.107287, Batch Acc: 0.985156, Tokens per Sec:     4017, Lr: 0.000168\r\n",
      "2024-08-27 15:30:06,518 - INFO - joeynmt.training - Epoch 119, Step:    11315, Batch Loss:     0.120372, Batch Acc: 0.983295, Tokens per Sec:     3940, Lr: 0.000168\r\n",
      "2024-08-27 15:30:07,619 - INFO - joeynmt.training - Epoch 119, Step:    11320, Batch Loss:     0.122802, Batch Acc: 0.983035, Tokens per Sec:     3804, Lr: 0.000168\r\n",
      "2024-08-27 15:30:08,705 - INFO - joeynmt.training - Epoch 119, Step:    11325, Batch Loss:     0.091056, Batch Acc: 0.983536, Tokens per Sec:     4084, Lr: 0.000168\r\n",
      "2024-08-27 15:30:09,403 - INFO - joeynmt.training - Epoch 119, total training loss: 10.27, num. of seqs: 8065, num. of tokens: 80463, 20.8749[sec]\r\n",
      "2024-08-27 15:30:09,403 - INFO - joeynmt.training - EPOCH 120\r\n",
      "2024-08-27 15:30:09,846 - INFO - joeynmt.training - Epoch 120, Step:    11330, Batch Loss:     0.104446, Batch Acc: 0.990981, Tokens per Sec:     4046, Lr: 0.000168\r\n",
      "2024-08-27 15:30:10,938 - INFO - joeynmt.training - Epoch 120, Step:    11335, Batch Loss:     0.100222, Batch Acc: 0.986829, Tokens per Sec:     3757, Lr: 0.000168\r\n",
      "2024-08-27 15:30:12,055 - INFO - joeynmt.training - Epoch 120, Step:    11340, Batch Loss:     0.106113, Batch Acc: 0.985646, Tokens per Sec:     3744, Lr: 0.000168\r\n",
      "2024-08-27 15:30:13,126 - INFO - joeynmt.training - Epoch 120, Step:    11345, Batch Loss:     0.112430, Batch Acc: 0.985734, Tokens per Sec:     3994, Lr: 0.000168\r\n",
      "2024-08-27 15:30:14,185 - INFO - joeynmt.training - Epoch 120, Step:    11350, Batch Loss:     0.100447, Batch Acc: 0.987726, Tokens per Sec:     3925, Lr: 0.000168\r\n",
      "2024-08-27 15:30:15,255 - INFO - joeynmt.training - Epoch 120, Step:    11355, Batch Loss:     0.104720, Batch Acc: 0.987446, Tokens per Sec:     3873, Lr: 0.000168\r\n",
      "2024-08-27 15:30:16,323 - INFO - joeynmt.training - Epoch 120, Step:    11360, Batch Loss:     0.111336, Batch Acc: 0.985688, Tokens per Sec:     4061, Lr: 0.000168\r\n",
      "2024-08-27 15:30:17,415 - INFO - joeynmt.training - Epoch 120, Step:    11365, Batch Loss:     0.111270, Batch Acc: 0.986227, Tokens per Sec:     3726, Lr: 0.000168\r\n",
      "2024-08-27 15:30:18,469 - INFO - joeynmt.training - Epoch 120, Step:    11370, Batch Loss:     0.116137, Batch Acc: 0.984994, Tokens per Sec:     3858, Lr: 0.000168\r\n",
      "2024-08-27 15:30:19,554 - INFO - joeynmt.training - Epoch 120, Step:    11375, Batch Loss:     0.111147, Batch Acc: 0.988823, Tokens per Sec:     4044, Lr: 0.000168\r\n",
      "2024-08-27 15:30:20,608 - INFO - joeynmt.training - Epoch 120, Step:    11380, Batch Loss:     0.105244, Batch Acc: 0.986097, Tokens per Sec:     3823, Lr: 0.000168\r\n",
      "2024-08-27 15:30:21,676 - INFO - joeynmt.training - Epoch 120, Step:    11385, Batch Loss:     0.119773, Batch Acc: 0.986480, Tokens per Sec:     3674, Lr: 0.000168\r\n",
      "2024-08-27 15:30:22,781 - INFO - joeynmt.training - Epoch 120, Step:    11390, Batch Loss:     0.112719, Batch Acc: 0.986525, Tokens per Sec:     3831, Lr: 0.000168\r\n",
      "2024-08-27 15:30:23,873 - INFO - joeynmt.training - Epoch 120, Step:    11395, Batch Loss:     0.121572, Batch Acc: 0.987694, Tokens per Sec:     3947, Lr: 0.000168\r\n",
      "2024-08-27 15:30:24,973 - INFO - joeynmt.training - Epoch 120, Step:    11400, Batch Loss:     0.113986, Batch Acc: 0.987435, Tokens per Sec:     3837, Lr: 0.000168\r\n",
      "2024-08-27 15:30:26,060 - INFO - joeynmt.training - Epoch 120, Step:    11405, Batch Loss:     0.098904, Batch Acc: 0.982392, Tokens per Sec:     3764, Lr: 0.000168\r\n",
      "2024-08-27 15:30:27,187 - INFO - joeynmt.training - Epoch 120, Step:    11410, Batch Loss:     0.101544, Batch Acc: 0.986971, Tokens per Sec:     3883, Lr: 0.000167\r\n",
      "2024-08-27 15:30:28,280 - INFO - joeynmt.training - Epoch 120, Step:    11415, Batch Loss:     0.119750, Batch Acc: 0.984300, Tokens per Sec:     3791, Lr: 0.000167\r\n",
      "2024-08-27 15:30:29,372 - INFO - joeynmt.training - Epoch 120, Step:    11420, Batch Loss:     0.103383, Batch Acc: 0.986621, Tokens per Sec:     3973, Lr: 0.000167\r\n",
      "2024-08-27 15:30:30,370 - INFO - joeynmt.training - Epoch 120, total training loss: 10.22, num. of seqs: 8065, num. of tokens: 80463, 20.9499[sec]\r\n",
      "2024-08-27 15:30:30,370 - INFO - joeynmt.training - EPOCH 121\r\n",
      "2024-08-27 15:30:30,598 - INFO - joeynmt.training - Epoch 121, Step:    11425, Batch Loss:     0.102423, Batch Acc: 0.990066, Tokens per Sec:     4034, Lr: 0.000167\r\n",
      "2024-08-27 15:30:31,736 - INFO - joeynmt.training - Epoch 121, Step:    11430, Batch Loss:     0.113188, Batch Acc: 0.984357, Tokens per Sec:     3880, Lr: 0.000167\r\n",
      "2024-08-27 15:30:32,828 - INFO - joeynmt.training - Epoch 121, Step:    11435, Batch Loss:     0.105387, Batch Acc: 0.988848, Tokens per Sec:     3696, Lr: 0.000167\r\n",
      "2024-08-27 15:30:33,904 - INFO - joeynmt.training - Epoch 121, Step:    11440, Batch Loss:     0.090897, Batch Acc: 0.985612, Tokens per Sec:     3879, Lr: 0.000167\r\n",
      "2024-08-27 15:30:34,970 - INFO - joeynmt.training - Epoch 121, Step:    11445, Batch Loss:     0.095043, Batch Acc: 0.988819, Tokens per Sec:     3863, Lr: 0.000167\r\n",
      "2024-08-27 15:30:36,067 - INFO - joeynmt.training - Epoch 121, Step:    11450, Batch Loss:     0.104385, Batch Acc: 0.986652, Tokens per Sec:     4031, Lr: 0.000167\r\n",
      "2024-08-27 15:30:37,168 - INFO - joeynmt.training - Epoch 121, Step:    11455, Batch Loss:     0.101226, Batch Acc: 0.987750, Tokens per Sec:     3857, Lr: 0.000167\r\n",
      "2024-08-27 15:30:38,248 - INFO - joeynmt.training - Epoch 121, Step:    11460, Batch Loss:     0.090903, Batch Acc: 0.987592, Tokens per Sec:     3883, Lr: 0.000167\r\n",
      "2024-08-27 15:30:39,310 - INFO - joeynmt.training - Epoch 121, Step:    11465, Batch Loss:     0.112755, Batch Acc: 0.986477, Tokens per Sec:     3832, Lr: 0.000167\r\n",
      "2024-08-27 15:30:40,380 - INFO - joeynmt.training - Epoch 121, Step:    11470, Batch Loss:     0.113032, Batch Acc: 0.984712, Tokens per Sec:     3852, Lr: 0.000167\r\n",
      "2024-08-27 15:30:41,476 - INFO - joeynmt.training - Epoch 121, Step:    11475, Batch Loss:     0.131147, Batch Acc: 0.984797, Tokens per Sec:     3783, Lr: 0.000167\r\n",
      "2024-08-27 15:30:42,556 - INFO - joeynmt.training - Epoch 121, Step:    11480, Batch Loss:     0.090575, Batch Acc: 0.986318, Tokens per Sec:     3795, Lr: 0.000167\r\n",
      "2024-08-27 15:30:43,671 - INFO - joeynmt.training - Epoch 121, Step:    11485, Batch Loss:     0.093204, Batch Acc: 0.985649, Tokens per Sec:     3937, Lr: 0.000167\r\n",
      "2024-08-27 15:30:44,768 - INFO - joeynmt.training - Epoch 121, Step:    11490, Batch Loss:     0.116010, Batch Acc: 0.982920, Tokens per Sec:     4007, Lr: 0.000167\r\n",
      "2024-08-27 15:30:45,831 - INFO - joeynmt.training - Epoch 121, Step:    11495, Batch Loss:     0.113880, Batch Acc: 0.984258, Tokens per Sec:     4003, Lr: 0.000167\r\n",
      "2024-08-27 15:30:46,946 - INFO - joeynmt.training - Epoch 121, Step:    11500, Batch Loss:     0.112508, Batch Acc: 0.983758, Tokens per Sec:     3591, Lr: 0.000167\r\n",
      "2024-08-27 15:30:46,947 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=11542\r\n",
      "2024-08-27 15:30:46,948 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:23<00:00, 63.08it/s]\r\n",
      "2024-08-27 15:31:10,094 - INFO - joeynmt.prediction - Generation took 23.1445[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44809.51 examples/s]\r\n",
      "2024-08-27 15:31:10,472 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:31:10,472 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.13, loss:   6.98, ppl: 1072.36, acc:   0.28, 0.1640[sec]\r\n",
      "2024-08-27 15:31:10,474 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:31:10,620 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:31:10,620 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:31:10,620 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 15:31:10,911 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:31:11,059 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:31:11,060 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:31:11,060 - INFO - joeynmt.training - \tHypothesis: trois tables blanches\r\n",
      "2024-08-27 15:31:11,355 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:31:11,502 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:31:11,502 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:31:11,502 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 15:31:11,791 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:31:11,936 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:31:11,937 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:31:11,937 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 15:31:13,302 - INFO - joeynmt.training - Epoch 121, Step:    11505, Batch Loss:     0.105706, Batch Acc: 0.985560, Tokens per Sec:     3808, Lr: 0.000167\r\n",
      "2024-08-27 15:31:14,378 - INFO - joeynmt.training - Epoch 121, Step:    11510, Batch Loss:     0.106719, Batch Acc: 0.983904, Tokens per Sec:     4044, Lr: 0.000167\r\n",
      "2024-08-27 15:31:15,451 - INFO - joeynmt.training - Epoch 121, Step:    11515, Batch Loss:     0.101019, Batch Acc: 0.983023, Tokens per Sec:     4011, Lr: 0.000167\r\n",
      "2024-08-27 15:31:16,430 - INFO - joeynmt.training - Epoch 121, total training loss: 10.19, num. of seqs: 8065, num. of tokens: 80463, 20.7622[sec]\r\n",
      "2024-08-27 15:31:16,430 - INFO - joeynmt.training - EPOCH 122\r\n",
      "2024-08-27 15:31:16,651 - INFO - joeynmt.training - Epoch 122, Step:    11520, Batch Loss:     0.094236, Batch Acc: 0.988764, Tokens per Sec:     3280, Lr: 0.000167\r\n",
      "2024-08-27 15:31:17,754 - INFO - joeynmt.training - Epoch 122, Step:    11525, Batch Loss:     0.117473, Batch Acc: 0.986137, Tokens per Sec:     3863, Lr: 0.000167\r\n",
      "2024-08-27 15:31:18,822 - INFO - joeynmt.training - Epoch 122, Step:    11530, Batch Loss:     0.095330, Batch Acc: 0.986973, Tokens per Sec:     3955, Lr: 0.000167\r\n",
      "2024-08-27 15:31:19,896 - INFO - joeynmt.training - Epoch 122, Step:    11535, Batch Loss:     0.115688, Batch Acc: 0.984561, Tokens per Sec:     3923, Lr: 0.000167\r\n",
      "2024-08-27 15:31:20,969 - INFO - joeynmt.training - Epoch 122, Step:    11540, Batch Loss:     0.117407, Batch Acc: 0.986031, Tokens per Sec:     3871, Lr: 0.000167\r\n",
      "2024-08-27 15:31:22,063 - INFO - joeynmt.training - Epoch 122, Step:    11545, Batch Loss:     0.110977, Batch Acc: 0.985209, Tokens per Sec:     3958, Lr: 0.000166\r\n",
      "2024-08-27 15:31:23,137 - INFO - joeynmt.training - Epoch 122, Step:    11550, Batch Loss:     0.106874, Batch Acc: 0.983825, Tokens per Sec:     3916, Lr: 0.000166\r\n",
      "2024-08-27 15:31:24,227 - INFO - joeynmt.training - Epoch 122, Step:    11555, Batch Loss:     0.123351, Batch Acc: 0.983227, Tokens per Sec:     3888, Lr: 0.000166\r\n",
      "2024-08-27 15:31:25,288 - INFO - joeynmt.training - Epoch 122, Step:    11560, Batch Loss:     0.119555, Batch Acc: 0.987321, Tokens per Sec:     4017, Lr: 0.000166\r\n",
      "2024-08-27 15:31:26,367 - INFO - joeynmt.training - Epoch 122, Step:    11565, Batch Loss:     0.112973, Batch Acc: 0.986208, Tokens per Sec:     4100, Lr: 0.000166\r\n",
      "2024-08-27 15:31:27,488 - INFO - joeynmt.training - Epoch 122, Step:    11570, Batch Loss:     0.113518, Batch Acc: 0.984267, Tokens per Sec:     3858, Lr: 0.000166\r\n",
      "2024-08-27 15:31:28,697 - INFO - joeynmt.training - Epoch 122, Step:    11575, Batch Loss:     0.104503, Batch Acc: 0.986648, Tokens per Sec:     3532, Lr: 0.000166\r\n",
      "2024-08-27 15:31:29,797 - INFO - joeynmt.training - Epoch 122, Step:    11580, Batch Loss:     0.104947, Batch Acc: 0.984802, Tokens per Sec:     3891, Lr: 0.000166\r\n",
      "2024-08-27 15:31:30,889 - INFO - joeynmt.training - Epoch 122, Step:    11585, Batch Loss:     0.118278, Batch Acc: 0.984410, Tokens per Sec:     4055, Lr: 0.000166\r\n",
      "2024-08-27 15:31:32,035 - INFO - joeynmt.training - Epoch 122, Step:    11590, Batch Loss:     0.109081, Batch Acc: 0.985754, Tokens per Sec:     3495, Lr: 0.000166\r\n",
      "2024-08-27 15:31:33,304 - INFO - joeynmt.training - Epoch 122, Step:    11595, Batch Loss:     0.109211, Batch Acc: 0.987010, Tokens per Sec:     3400, Lr: 0.000166\r\n",
      "2024-08-27 15:31:34,406 - INFO - joeynmt.training - Epoch 122, Step:    11600, Batch Loss:     0.108812, Batch Acc: 0.984478, Tokens per Sec:     3859, Lr: 0.000166\r\n",
      "2024-08-27 15:31:35,501 - INFO - joeynmt.training - Epoch 122, Step:    11605, Batch Loss:     0.114039, Batch Acc: 0.984138, Tokens per Sec:     3921, Lr: 0.000166\r\n",
      "2024-08-27 15:31:36,584 - INFO - joeynmt.training - Epoch 122, Step:    11610, Batch Loss:     0.116063, Batch Acc: 0.985922, Tokens per Sec:     3872, Lr: 0.000166\r\n",
      "2024-08-27 15:31:37,381 - INFO - joeynmt.training - Epoch 122, total training loss: 10.14, num. of seqs: 8065, num. of tokens: 80463, 20.9343[sec]\r\n",
      "2024-08-27 15:31:37,382 - INFO - joeynmt.training - EPOCH 123\r\n",
      "2024-08-27 15:31:37,829 - INFO - joeynmt.training - Epoch 123, Step:    11615, Batch Loss:     0.106740, Batch Acc: 0.985006, Tokens per Sec:     3913, Lr: 0.000166\r\n",
      "2024-08-27 15:31:38,935 - INFO - joeynmt.training - Epoch 123, Step:    11620, Batch Loss:     0.107670, Batch Acc: 0.987118, Tokens per Sec:     3934, Lr: 0.000166\r\n",
      "2024-08-27 15:31:40,007 - INFO - joeynmt.training - Epoch 123, Step:    11625, Batch Loss:     0.112068, Batch Acc: 0.983766, Tokens per Sec:     3736, Lr: 0.000166\r\n",
      "2024-08-27 15:31:41,084 - INFO - joeynmt.training - Epoch 123, Step:    11630, Batch Loss:     0.110067, Batch Acc: 0.986493, Tokens per Sec:     3649, Lr: 0.000166\r\n",
      "2024-08-27 15:31:42,225 - INFO - joeynmt.training - Epoch 123, Step:    11635, Batch Loss:     0.090635, Batch Acc: 0.988686, Tokens per Sec:     3641, Lr: 0.000166\r\n",
      "2024-08-27 15:31:43,539 - INFO - joeynmt.training - Epoch 123, Step:    11640, Batch Loss:     0.093793, Batch Acc: 0.989225, Tokens per Sec:     3322, Lr: 0.000166\r\n",
      "2024-08-27 15:31:44,627 - INFO - joeynmt.training - Epoch 123, Step:    11645, Batch Loss:     0.096405, Batch Acc: 0.986833, Tokens per Sec:     3843, Lr: 0.000166\r\n",
      "2024-08-27 15:31:45,699 - INFO - joeynmt.training - Epoch 123, Step:    11650, Batch Loss:     0.099245, Batch Acc: 0.986221, Tokens per Sec:     3996, Lr: 0.000166\r\n",
      "2024-08-27 15:31:46,806 - INFO - joeynmt.training - Epoch 123, Step:    11655, Batch Loss:     0.129259, Batch Acc: 0.986190, Tokens per Sec:     3665, Lr: 0.000166\r\n",
      "2024-08-27 15:31:47,896 - INFO - joeynmt.training - Epoch 123, Step:    11660, Batch Loss:     0.089044, Batch Acc: 0.989227, Tokens per Sec:     3920, Lr: 0.000166\r\n",
      "2024-08-27 15:31:48,980 - INFO - joeynmt.training - Epoch 123, Step:    11665, Batch Loss:     0.112305, Batch Acc: 0.986127, Tokens per Sec:     3993, Lr: 0.000166\r\n",
      "2024-08-27 15:31:50,070 - INFO - joeynmt.training - Epoch 123, Step:    11670, Batch Loss:     0.121441, Batch Acc: 0.988087, Tokens per Sec:     3776, Lr: 0.000166\r\n",
      "2024-08-27 15:31:51,180 - INFO - joeynmt.training - Epoch 123, Step:    11675, Batch Loss:     0.102078, Batch Acc: 0.985996, Tokens per Sec:     3798, Lr: 0.000166\r\n",
      "2024-08-27 15:31:52,276 - INFO - joeynmt.training - Epoch 123, Step:    11680, Batch Loss:     0.095921, Batch Acc: 0.989872, Tokens per Sec:     3695, Lr: 0.000166\r\n",
      "2024-08-27 15:31:53,387 - INFO - joeynmt.training - Epoch 123, Step:    11685, Batch Loss:     0.104027, Batch Acc: 0.985085, Tokens per Sec:     3745, Lr: 0.000165\r\n",
      "2024-08-27 15:31:54,509 - INFO - joeynmt.training - Epoch 123, Step:    11690, Batch Loss:     0.100703, Batch Acc: 0.987152, Tokens per Sec:     3679, Lr: 0.000165\r\n",
      "2024-08-27 15:31:55,626 - INFO - joeynmt.training - Epoch 123, Step:    11695, Batch Loss:     0.090849, Batch Acc: 0.986685, Tokens per Sec:     3899, Lr: 0.000165\r\n",
      "2024-08-27 15:31:56,780 - INFO - joeynmt.training - Epoch 123, Step:    11700, Batch Loss:     0.134241, Batch Acc: 0.983614, Tokens per Sec:     3652, Lr: 0.000165\r\n",
      "2024-08-27 15:31:57,878 - INFO - joeynmt.training - Epoch 123, Step:    11705, Batch Loss:     0.113758, Batch Acc: 0.984633, Tokens per Sec:     4034, Lr: 0.000165\r\n",
      "2024-08-27 15:31:58,747 - INFO - joeynmt.training - Epoch 123, total training loss: 10.16, num. of seqs: 8065, num. of tokens: 80463, 21.3471[sec]\r\n",
      "2024-08-27 15:31:58,747 - INFO - joeynmt.training - EPOCH 124\r\n",
      "2024-08-27 15:31:58,973 - INFO - joeynmt.training - Epoch 124, Step:    11710, Batch Loss:     0.092319, Batch Acc: 0.987327, Tokens per Sec:     3893, Lr: 0.000165\r\n",
      "2024-08-27 15:32:00,078 - INFO - joeynmt.training - Epoch 124, Step:    11715, Batch Loss:     0.107218, Batch Acc: 0.986414, Tokens per Sec:     3869, Lr: 0.000165\r\n",
      "2024-08-27 15:32:01,159 - INFO - joeynmt.training - Epoch 124, Step:    11720, Batch Loss:     0.095373, Batch Acc: 0.984375, Tokens per Sec:     3909, Lr: 0.000165\r\n",
      "2024-08-27 15:32:02,236 - INFO - joeynmt.training - Epoch 124, Step:    11725, Batch Loss:     0.103626, Batch Acc: 0.985469, Tokens per Sec:     3838, Lr: 0.000165\r\n",
      "2024-08-27 15:32:03,413 - INFO - joeynmt.training - Epoch 124, Step:    11730, Batch Loss:     0.110180, Batch Acc: 0.983359, Tokens per Sec:     3831, Lr: 0.000165\r\n",
      "2024-08-27 15:32:04,567 - INFO - joeynmt.training - Epoch 124, Step:    11735, Batch Loss:     0.136845, Batch Acc: 0.984742, Tokens per Sec:     3468, Lr: 0.000165\r\n",
      "2024-08-27 15:32:05,652 - INFO - joeynmt.training - Epoch 124, Step:    11740, Batch Loss:     0.100594, Batch Acc: 0.987897, Tokens per Sec:     3885, Lr: 0.000165\r\n",
      "2024-08-27 15:32:06,764 - INFO - joeynmt.training - Epoch 124, Step:    11745, Batch Loss:     0.117266, Batch Acc: 0.985464, Tokens per Sec:     3590, Lr: 0.000165\r\n",
      "2024-08-27 15:32:07,822 - INFO - joeynmt.training - Epoch 124, Step:    11750, Batch Loss:     0.108035, Batch Acc: 0.990028, Tokens per Sec:     3699, Lr: 0.000165\r\n",
      "2024-08-27 15:32:08,907 - INFO - joeynmt.training - Epoch 124, Step:    11755, Batch Loss:     0.105372, Batch Acc: 0.987654, Tokens per Sec:     3885, Lr: 0.000165\r\n",
      "2024-08-27 15:32:09,988 - INFO - joeynmt.training - Epoch 124, Step:    11760, Batch Loss:     0.103430, Batch Acc: 0.985314, Tokens per Sec:     4037, Lr: 0.000165\r\n",
      "2024-08-27 15:32:11,059 - INFO - joeynmt.training - Epoch 124, Step:    11765, Batch Loss:     0.103218, Batch Acc: 0.987873, Tokens per Sec:     4158, Lr: 0.000165\r\n",
      "2024-08-27 15:32:12,136 - INFO - joeynmt.training - Epoch 124, Step:    11770, Batch Loss:     0.116640, Batch Acc: 0.986114, Tokens per Sec:     4014, Lr: 0.000165\r\n",
      "2024-08-27 15:32:13,203 - INFO - joeynmt.training - Epoch 124, Step:    11775, Batch Loss:     0.099842, Batch Acc: 0.986809, Tokens per Sec:     3770, Lr: 0.000165\r\n",
      "2024-08-27 15:32:14,273 - INFO - joeynmt.training - Epoch 124, Step:    11780, Batch Loss:     0.112337, Batch Acc: 0.987447, Tokens per Sec:     3949, Lr: 0.000165\r\n",
      "2024-08-27 15:32:15,337 - INFO - joeynmt.training - Epoch 124, Step:    11785, Batch Loss:     0.113840, Batch Acc: 0.986217, Tokens per Sec:     3956, Lr: 0.000165\r\n",
      "2024-08-27 15:32:16,411 - INFO - joeynmt.training - Epoch 124, Step:    11790, Batch Loss:     0.102510, Batch Acc: 0.986111, Tokens per Sec:     4094, Lr: 0.000165\r\n",
      "2024-08-27 15:32:17,502 - INFO - joeynmt.training - Epoch 124, Step:    11795, Batch Loss:     0.097956, Batch Acc: 0.986654, Tokens per Sec:     3847, Lr: 0.000165\r\n",
      "2024-08-27 15:32:18,589 - INFO - joeynmt.training - Epoch 124, Step:    11800, Batch Loss:     0.098568, Batch Acc: 0.986273, Tokens per Sec:     3958, Lr: 0.000165\r\n",
      "2024-08-27 15:32:19,554 - INFO - joeynmt.training - Epoch 124, total training loss: 10.16, num. of seqs: 8065, num. of tokens: 80463, 20.7899[sec]\r\n",
      "2024-08-27 15:32:19,555 - INFO - joeynmt.training - EPOCH 125\r\n",
      "2024-08-27 15:32:19,778 - INFO - joeynmt.training - Epoch 125, Step:    11805, Batch Loss:     0.121403, Batch Acc: 0.981971, Tokens per Sec:     3791, Lr: 0.000165\r\n",
      "2024-08-27 15:32:20,857 - INFO - joeynmt.training - Epoch 125, Step:    11810, Batch Loss:     0.083278, Batch Acc: 0.989857, Tokens per Sec:     3748, Lr: 0.000165\r\n",
      "2024-08-27 15:32:21,945 - INFO - joeynmt.training - Epoch 125, Step:    11815, Batch Loss:     0.110502, Batch Acc: 0.986891, Tokens per Sec:     3930, Lr: 0.000165\r\n",
      "2024-08-27 15:32:23,027 - INFO - joeynmt.training - Epoch 125, Step:    11820, Batch Loss:     0.100345, Batch Acc: 0.989065, Tokens per Sec:     3976, Lr: 0.000165\r\n",
      "2024-08-27 15:32:24,127 - INFO - joeynmt.training - Epoch 125, Step:    11825, Batch Loss:     0.116315, Batch Acc: 0.990452, Tokens per Sec:     3907, Lr: 0.000165\r\n",
      "2024-08-27 15:32:25,208 - INFO - joeynmt.training - Epoch 125, Step:    11830, Batch Loss:     0.104042, Batch Acc: 0.986412, Tokens per Sec:     4016, Lr: 0.000164\r\n",
      "2024-08-27 15:32:26,274 - INFO - joeynmt.training - Epoch 125, Step:    11835, Batch Loss:     0.104641, Batch Acc: 0.990153, Tokens per Sec:     3814, Lr: 0.000164\r\n",
      "2024-08-27 15:32:27,393 - INFO - joeynmt.training - Epoch 125, Step:    11840, Batch Loss:     0.103073, Batch Acc: 0.989464, Tokens per Sec:     3818, Lr: 0.000164\r\n",
      "2024-08-27 15:32:28,495 - INFO - joeynmt.training - Epoch 125, Step:    11845, Batch Loss:     0.134362, Batch Acc: 0.984447, Tokens per Sec:     3739, Lr: 0.000164\r\n",
      "2024-08-27 15:32:29,601 - INFO - joeynmt.training - Epoch 125, Step:    11850, Batch Loss:     0.111470, Batch Acc: 0.986682, Tokens per Sec:     3938, Lr: 0.000164\r\n",
      "2024-08-27 15:32:30,708 - INFO - joeynmt.training - Epoch 125, Step:    11855, Batch Loss:     0.108933, Batch Acc: 0.988060, Tokens per Sec:     4012, Lr: 0.000164\r\n",
      "2024-08-27 15:32:31,807 - INFO - joeynmt.training - Epoch 125, Step:    11860, Batch Loss:     0.097587, Batch Acc: 0.985127, Tokens per Sec:     3917, Lr: 0.000164\r\n",
      "2024-08-27 15:32:32,919 - INFO - joeynmt.training - Epoch 125, Step:    11865, Batch Loss:     0.103468, Batch Acc: 0.987430, Tokens per Sec:     3866, Lr: 0.000164\r\n",
      "2024-08-27 15:32:34,025 - INFO - joeynmt.training - Epoch 125, Step:    11870, Batch Loss:     0.104363, Batch Acc: 0.985175, Tokens per Sec:     3785, Lr: 0.000164\r\n",
      "2024-08-27 15:32:35,202 - INFO - joeynmt.training - Epoch 125, Step:    11875, Batch Loss:     0.109384, Batch Acc: 0.985842, Tokens per Sec:     3603, Lr: 0.000164\r\n",
      "2024-08-27 15:32:36,308 - INFO - joeynmt.training - Epoch 125, Step:    11880, Batch Loss:     0.113872, Batch Acc: 0.985259, Tokens per Sec:     3804, Lr: 0.000164\r\n",
      "2024-08-27 15:32:37,435 - INFO - joeynmt.training - Epoch 125, Step:    11885, Batch Loss:     0.121728, Batch Acc: 0.982786, Tokens per Sec:     3921, Lr: 0.000164\r\n",
      "2024-08-27 15:32:38,515 - INFO - joeynmt.training - Epoch 125, Step:    11890, Batch Loss:     0.110152, Batch Acc: 0.988459, Tokens per Sec:     3854, Lr: 0.000164\r\n",
      "2024-08-27 15:32:39,595 - INFO - joeynmt.training - Epoch 125, Step:    11895, Batch Loss:     0.111354, Batch Acc: 0.985905, Tokens per Sec:     3813, Lr: 0.000164\r\n",
      "2024-08-27 15:32:40,463 - INFO - joeynmt.training - Epoch 125, total training loss: 9.93, num. of seqs: 8065, num. of tokens: 80463, 20.8912[sec]\r\n",
      "2024-08-27 15:32:40,463 - INFO - joeynmt.training - EPOCH 126\r\n",
      "2024-08-27 15:32:40,687 - INFO - joeynmt.training - Epoch 126, Step:    11900, Batch Loss:     0.093006, Batch Acc: 0.990981, Tokens per Sec:     4031, Lr: 0.000164\r\n",
      "2024-08-27 15:32:41,774 - INFO - joeynmt.training - Epoch 126, Step:    11905, Batch Loss:     0.089350, Batch Acc: 0.987278, Tokens per Sec:     3833, Lr: 0.000164\r\n",
      "2024-08-27 15:32:42,868 - INFO - joeynmt.training - Epoch 126, Step:    11910, Batch Loss:     0.096024, Batch Acc: 0.985711, Tokens per Sec:     3841, Lr: 0.000164\r\n",
      "2024-08-27 15:32:43,987 - INFO - joeynmt.training - Epoch 126, Step:    11915, Batch Loss:     0.101918, Batch Acc: 0.985704, Tokens per Sec:     3880, Lr: 0.000164\r\n",
      "2024-08-27 15:32:45,073 - INFO - joeynmt.training - Epoch 126, Step:    11920, Batch Loss:     0.092089, Batch Acc: 0.989339, Tokens per Sec:     3889, Lr: 0.000164\r\n",
      "2024-08-27 15:32:46,159 - INFO - joeynmt.training - Epoch 126, Step:    11925, Batch Loss:     0.095584, Batch Acc: 0.986700, Tokens per Sec:     3740, Lr: 0.000164\r\n",
      "2024-08-27 15:32:47,309 - INFO - joeynmt.training - Epoch 126, Step:    11930, Batch Loss:     0.095240, Batch Acc: 0.991276, Tokens per Sec:     3692, Lr: 0.000164\r\n",
      "2024-08-27 15:32:48,388 - INFO - joeynmt.training - Epoch 126, Step:    11935, Batch Loss:     0.110486, Batch Acc: 0.984641, Tokens per Sec:     3862, Lr: 0.000164\r\n",
      "2024-08-27 15:32:49,477 - INFO - joeynmt.training - Epoch 126, Step:    11940, Batch Loss:     0.097454, Batch Acc: 0.986761, Tokens per Sec:     3889, Lr: 0.000164\r\n",
      "2024-08-27 15:32:50,554 - INFO - joeynmt.training - Epoch 126, Step:    11945, Batch Loss:     0.094290, Batch Acc: 0.990129, Tokens per Sec:     4048, Lr: 0.000164\r\n",
      "2024-08-27 15:32:51,634 - INFO - joeynmt.training - Epoch 126, Step:    11950, Batch Loss:     0.098732, Batch Acc: 0.986753, Tokens per Sec:     3986, Lr: 0.000164\r\n",
      "2024-08-27 15:32:52,703 - INFO - joeynmt.training - Epoch 126, Step:    11955, Batch Loss:     0.122216, Batch Acc: 0.983170, Tokens per Sec:     3725, Lr: 0.000164\r\n",
      "2024-08-27 15:32:53,783 - INFO - joeynmt.training - Epoch 126, Step:    11960, Batch Loss:     0.101566, Batch Acc: 0.985425, Tokens per Sec:     4069, Lr: 0.000164\r\n",
      "2024-08-27 15:32:54,852 - INFO - joeynmt.training - Epoch 126, Step:    11965, Batch Loss:     0.101245, Batch Acc: 0.986139, Tokens per Sec:     3782, Lr: 0.000164\r\n",
      "2024-08-27 15:32:55,949 - INFO - joeynmt.training - Epoch 126, Step:    11970, Batch Loss:     0.102439, Batch Acc: 0.984345, Tokens per Sec:     3845, Lr: 0.000164\r\n",
      "2024-08-27 15:32:57,078 - INFO - joeynmt.training - Epoch 126, Step:    11975, Batch Loss:     0.112361, Batch Acc: 0.984927, Tokens per Sec:     3763, Lr: 0.000163\r\n",
      "2024-08-27 15:32:58,159 - INFO - joeynmt.training - Epoch 126, Step:    11980, Batch Loss:     0.110530, Batch Acc: 0.989151, Tokens per Sec:     3926, Lr: 0.000163\r\n",
      "2024-08-27 15:32:59,248 - INFO - joeynmt.training - Epoch 126, Step:    11985, Batch Loss:     0.112100, Batch Acc: 0.986404, Tokens per Sec:     3784, Lr: 0.000163\r\n",
      "2024-08-27 15:33:00,343 - INFO - joeynmt.training - Epoch 126, Step:    11990, Batch Loss:     0.097818, Batch Acc: 0.988142, Tokens per Sec:     3931, Lr: 0.000163\r\n",
      "2024-08-27 15:33:01,385 - INFO - joeynmt.training - Epoch 126, total training loss: 9.71, num. of seqs: 8065, num. of tokens: 80463, 20.9052[sec]\r\n",
      "2024-08-27 15:33:01,386 - INFO - joeynmt.training - EPOCH 127\r\n",
      "2024-08-27 15:33:01,623 - INFO - joeynmt.training - Epoch 127, Step:    11995, Batch Loss:     0.097446, Batch Acc: 0.985126, Tokens per Sec:     3758, Lr: 0.000163\r\n",
      "2024-08-27 15:33:02,774 - INFO - joeynmt.training - Epoch 127, Step:    12000, Batch Loss:     0.109602, Batch Acc: 0.986330, Tokens per Sec:     3687, Lr: 0.000163\r\n",
      "2024-08-27 15:33:02,775 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=12042\r\n",
      "2024-08-27 15:33:02,775 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:23<00:00, 61.26it/s]\r\n",
      "2024-08-27 15:33:26,612 - INFO - joeynmt.prediction - Generation took 23.8348[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44060.40 examples/s]\r\n",
      "2024-08-27 15:33:27,008 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:33:27,009 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   4.88, loss:   6.97, ppl: 1064.69, acc:   0.28, 0.1654[sec]\r\n",
      "2024-08-27 15:33:27,010 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:33:27,157 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:33:27,157 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:33:27,157 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 15:33:27,448 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:33:27,594 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:33:27,594 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:33:27,595 - INFO - joeynmt.training - \tHypothesis: trois tables blanches\r\n",
      "2024-08-27 15:33:27,888 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:33:28,032 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:33:28,032 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:33:28,032 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 15:33:28,323 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:33:28,468 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:33:28,468 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:33:28,468 - INFO - joeynmt.training - \tHypothesis: un tour et demi\r\n",
      "2024-08-27 15:33:29,856 - INFO - joeynmt.training - Epoch 127, Step:    12005, Batch Loss:     0.091347, Batch Acc: 0.984970, Tokens per Sec:     3782, Lr: 0.000163\r\n",
      "2024-08-27 15:33:30,960 - INFO - joeynmt.training - Epoch 127, Step:    12010, Batch Loss:     0.088611, Batch Acc: 0.987497, Tokens per Sec:     3843, Lr: 0.000163\r\n",
      "2024-08-27 15:33:32,051 - INFO - joeynmt.training - Epoch 127, Step:    12015, Batch Loss:     0.109877, Batch Acc: 0.985518, Tokens per Sec:     3865, Lr: 0.000163\r\n",
      "2024-08-27 15:33:33,154 - INFO - joeynmt.training - Epoch 127, Step:    12020, Batch Loss:     0.105940, Batch Acc: 0.986571, Tokens per Sec:     3919, Lr: 0.000163\r\n",
      "2024-08-27 15:33:34,232 - INFO - joeynmt.training - Epoch 127, Step:    12025, Batch Loss:     0.098341, Batch Acc: 0.987985, Tokens per Sec:     4016, Lr: 0.000163\r\n",
      "2024-08-27 15:33:35,316 - INFO - joeynmt.training - Epoch 127, Step:    12030, Batch Loss:     0.098022, Batch Acc: 0.987416, Tokens per Sec:     3961, Lr: 0.000163\r\n",
      "2024-08-27 15:33:36,399 - INFO - joeynmt.training - Epoch 127, Step:    12035, Batch Loss:     0.106454, Batch Acc: 0.986787, Tokens per Sec:     3777, Lr: 0.000163\r\n",
      "2024-08-27 15:33:37,566 - INFO - joeynmt.training - Epoch 127, Step:    12040, Batch Loss:     0.106177, Batch Acc: 0.988514, Tokens per Sec:     3433, Lr: 0.000163\r\n",
      "2024-08-27 15:33:38,647 - INFO - joeynmt.training - Epoch 127, Step:    12045, Batch Loss:     0.097615, Batch Acc: 0.988802, Tokens per Sec:     3886, Lr: 0.000163\r\n",
      "2024-08-27 15:33:39,716 - INFO - joeynmt.training - Epoch 127, Step:    12050, Batch Loss:     0.090050, Batch Acc: 0.990066, Tokens per Sec:     3956, Lr: 0.000163\r\n",
      "2024-08-27 15:33:40,813 - INFO - joeynmt.training - Epoch 127, Step:    12055, Batch Loss:     0.107069, Batch Acc: 0.987267, Tokens per Sec:     4013, Lr: 0.000163\r\n",
      "2024-08-27 15:33:41,897 - INFO - joeynmt.training - Epoch 127, Step:    12060, Batch Loss:     0.115304, Batch Acc: 0.984331, Tokens per Sec:     3594, Lr: 0.000163\r\n",
      "2024-08-27 15:33:42,988 - INFO - joeynmt.training - Epoch 127, Step:    12065, Batch Loss:     0.098701, Batch Acc: 0.990057, Tokens per Sec:     3875, Lr: 0.000163\r\n",
      "2024-08-27 15:33:44,105 - INFO - joeynmt.training - Epoch 127, Step:    12070, Batch Loss:     0.109176, Batch Acc: 0.986458, Tokens per Sec:     3768, Lr: 0.000163\r\n",
      "2024-08-27 15:33:45,172 - INFO - joeynmt.training - Epoch 127, Step:    12075, Batch Loss:     0.098565, Batch Acc: 0.987509, Tokens per Sec:     3905, Lr: 0.000163\r\n",
      "2024-08-27 15:33:46,242 - INFO - joeynmt.training - Epoch 127, Step:    12080, Batch Loss:     0.114110, Batch Acc: 0.984896, Tokens per Sec:     3838, Lr: 0.000163\r\n",
      "2024-08-27 15:33:47,339 - INFO - joeynmt.training - Epoch 127, Step:    12085, Batch Loss:     0.101995, Batch Acc: 0.984078, Tokens per Sec:     3839, Lr: 0.000163\r\n",
      "2024-08-27 15:33:48,391 - INFO - joeynmt.training - Epoch 127, total training loss: 9.72, num. of seqs: 8065, num. of tokens: 80463, 20.9967[sec]\r\n",
      "2024-08-27 15:33:48,391 - INFO - joeynmt.training - EPOCH 128\r\n",
      "2024-08-27 15:33:48,614 - INFO - joeynmt.training - Epoch 128, Step:    12090, Batch Loss:     0.093280, Batch Acc: 0.992457, Tokens per Sec:     4236, Lr: 0.000163\r\n",
      "2024-08-27 15:33:49,682 - INFO - joeynmt.training - Epoch 128, Step:    12095, Batch Loss:     0.121490, Batch Acc: 0.987363, Tokens per Sec:     3932, Lr: 0.000163\r\n",
      "2024-08-27 15:33:50,756 - INFO - joeynmt.training - Epoch 128, Step:    12100, Batch Loss:     0.102179, Batch Acc: 0.987860, Tokens per Sec:     3911, Lr: 0.000163\r\n",
      "2024-08-27 15:33:51,839 - INFO - joeynmt.training - Epoch 128, Step:    12105, Batch Loss:     0.106266, Batch Acc: 0.988804, Tokens per Sec:     4127, Lr: 0.000163\r\n",
      "2024-08-27 15:33:52,890 - INFO - joeynmt.training - Epoch 128, Step:    12110, Batch Loss:     0.094136, Batch Acc: 0.987708, Tokens per Sec:     3717, Lr: 0.000163\r\n",
      "2024-08-27 15:33:53,980 - INFO - joeynmt.training - Epoch 128, Step:    12115, Batch Loss:     0.087511, Batch Acc: 0.989759, Tokens per Sec:     3855, Lr: 0.000163\r\n",
      "2024-08-27 15:33:55,068 - INFO - joeynmt.training - Epoch 128, Step:    12120, Batch Loss:     0.084641, Batch Acc: 0.987013, Tokens per Sec:     3824, Lr: 0.000162\r\n",
      "2024-08-27 15:33:56,156 - INFO - joeynmt.training - Epoch 128, Step:    12125, Batch Loss:     0.098654, Batch Acc: 0.984998, Tokens per Sec:     3923, Lr: 0.000162\r\n",
      "2024-08-27 15:33:57,285 - INFO - joeynmt.training - Epoch 128, Step:    12130, Batch Loss:     0.100970, Batch Acc: 0.988549, Tokens per Sec:     3793, Lr: 0.000162\r\n",
      "2024-08-27 15:33:58,356 - INFO - joeynmt.training - Epoch 128, Step:    12135, Batch Loss:     0.110974, Batch Acc: 0.988010, Tokens per Sec:     3896, Lr: 0.000162\r\n",
      "2024-08-27 15:33:59,475 - INFO - joeynmt.training - Epoch 128, Step:    12140, Batch Loss:     0.103839, Batch Acc: 0.986615, Tokens per Sec:     3944, Lr: 0.000162\r\n",
      "2024-08-27 15:34:00,571 - INFO - joeynmt.training - Epoch 128, Step:    12145, Batch Loss:     0.097158, Batch Acc: 0.987324, Tokens per Sec:     3888, Lr: 0.000162\r\n",
      "2024-08-27 15:34:01,652 - INFO - joeynmt.training - Epoch 128, Step:    12150, Batch Loss:     0.090915, Batch Acc: 0.989504, Tokens per Sec:     3881, Lr: 0.000162\r\n",
      "2024-08-27 15:34:02,734 - INFO - joeynmt.training - Epoch 128, Step:    12155, Batch Loss:     0.104456, Batch Acc: 0.986827, Tokens per Sec:     4000, Lr: 0.000162\r\n",
      "2024-08-27 15:34:03,837 - INFO - joeynmt.training - Epoch 128, Step:    12160, Batch Loss:     0.109969, Batch Acc: 0.988803, Tokens per Sec:     3891, Lr: 0.000162\r\n",
      "2024-08-27 15:34:04,976 - INFO - joeynmt.training - Epoch 128, Step:    12165, Batch Loss:     0.084130, Batch Acc: 0.988253, Tokens per Sec:     3590, Lr: 0.000162\r\n",
      "2024-08-27 15:34:06,078 - INFO - joeynmt.training - Epoch 128, Step:    12170, Batch Loss:     0.096930, Batch Acc: 0.988273, Tokens per Sec:     3947, Lr: 0.000162\r\n",
      "2024-08-27 15:34:07,188 - INFO - joeynmt.training - Epoch 128, Step:    12175, Batch Loss:     0.115273, Batch Acc: 0.984952, Tokens per Sec:     3834, Lr: 0.000162\r\n",
      "2024-08-27 15:34:08,320 - INFO - joeynmt.training - Epoch 128, Step:    12180, Batch Loss:     0.104215, Batch Acc: 0.989008, Tokens per Sec:     3781, Lr: 0.000162\r\n",
      "2024-08-27 15:34:09,235 - INFO - joeynmt.training - Epoch 128, total training loss: 9.49, num. of seqs: 8065, num. of tokens: 80463, 20.8283[sec]\r\n",
      "2024-08-27 15:34:09,236 - INFO - joeynmt.training - EPOCH 129\r\n",
      "2024-08-27 15:34:09,461 - INFO - joeynmt.training - Epoch 129, Step:    12185, Batch Loss:     0.084275, Batch Acc: 0.991463, Tokens per Sec:     3699, Lr: 0.000162\r\n",
      "2024-08-27 15:34:10,553 - INFO - joeynmt.training - Epoch 129, Step:    12190, Batch Loss:     0.083145, Batch Acc: 0.987082, Tokens per Sec:     3616, Lr: 0.000162\r\n",
      "2024-08-27 15:34:11,686 - INFO - joeynmt.training - Epoch 129, Step:    12195, Batch Loss:     0.109475, Batch Acc: 0.988243, Tokens per Sec:     3981, Lr: 0.000162\r\n",
      "2024-08-27 15:34:12,801 - INFO - joeynmt.training - Epoch 129, Step:    12200, Batch Loss:     0.093295, Batch Acc: 0.989095, Tokens per Sec:     3868, Lr: 0.000162\r\n",
      "2024-08-27 15:34:13,930 - INFO - joeynmt.training - Epoch 129, Step:    12205, Batch Loss:     0.098919, Batch Acc: 0.988699, Tokens per Sec:     3844, Lr: 0.000162\r\n",
      "2024-08-27 15:34:15,028 - INFO - joeynmt.training - Epoch 129, Step:    12210, Batch Loss:     0.097253, Batch Acc: 0.991701, Tokens per Sec:     3511, Lr: 0.000162\r\n",
      "2024-08-27 15:34:16,134 - INFO - joeynmt.training - Epoch 129, Step:    12215, Batch Loss:     0.088006, Batch Acc: 0.988514, Tokens per Sec:     3783, Lr: 0.000162\r\n",
      "2024-08-27 15:34:17,368 - INFO - joeynmt.training - Epoch 129, Step:    12220, Batch Loss:     0.089033, Batch Acc: 0.990178, Tokens per Sec:     3549, Lr: 0.000162\r\n",
      "2024-08-27 15:34:18,473 - INFO - joeynmt.training - Epoch 129, Step:    12225, Batch Loss:     0.090753, Batch Acc: 0.986351, Tokens per Sec:     3714, Lr: 0.000162\r\n",
      "2024-08-27 15:34:19,574 - INFO - joeynmt.training - Epoch 129, Step:    12230, Batch Loss:     0.095097, Batch Acc: 0.986515, Tokens per Sec:     3910, Lr: 0.000162\r\n",
      "2024-08-27 15:34:20,690 - INFO - joeynmt.training - Epoch 129, Step:    12235, Batch Loss:     0.095230, Batch Acc: 0.987897, Tokens per Sec:     3777, Lr: 0.000162\r\n",
      "2024-08-27 15:34:21,827 - INFO - joeynmt.training - Epoch 129, Step:    12240, Batch Loss:     0.103162, Batch Acc: 0.986295, Tokens per Sec:     3724, Lr: 0.000162\r\n",
      "2024-08-27 15:34:22,963 - INFO - joeynmt.training - Epoch 129, Step:    12245, Batch Loss:     0.090786, Batch Acc: 0.988307, Tokens per Sec:     3919, Lr: 0.000162\r\n",
      "2024-08-27 15:34:24,079 - INFO - joeynmt.training - Epoch 129, Step:    12250, Batch Loss:     0.112883, Batch Acc: 0.986031, Tokens per Sec:     3724, Lr: 0.000162\r\n",
      "2024-08-27 15:34:25,182 - INFO - joeynmt.training - Epoch 129, Step:    12255, Batch Loss:     0.101444, Batch Acc: 0.987730, Tokens per Sec:     3696, Lr: 0.000162\r\n",
      "2024-08-27 15:34:26,296 - INFO - joeynmt.training - Epoch 129, Step:    12260, Batch Loss:     0.117036, Batch Acc: 0.985191, Tokens per Sec:     3701, Lr: 0.000162\r\n",
      "2024-08-27 15:34:27,448 - INFO - joeynmt.training - Epoch 129, Step:    12265, Batch Loss:     0.106678, Batch Acc: 0.983534, Tokens per Sec:     3904, Lr: 0.000162\r\n",
      "2024-08-27 15:34:28,587 - INFO - joeynmt.training - Epoch 129, Step:    12270, Batch Loss:     0.108919, Batch Acc: 0.987144, Tokens per Sec:     3755, Lr: 0.000161\r\n",
      "2024-08-27 15:34:29,700 - INFO - joeynmt.training - Epoch 129, Step:    12275, Batch Loss:     0.112735, Batch Acc: 0.984930, Tokens per Sec:     3699, Lr: 0.000161\r\n",
      "2024-08-27 15:34:30,644 - INFO - joeynmt.training - Epoch 129, total training loss: 9.36, num. of seqs: 8065, num. of tokens: 80463, 21.3920[sec]\r\n",
      "2024-08-27 15:34:30,645 - INFO - joeynmt.training - EPOCH 130\r\n",
      "2024-08-27 15:34:30,875 - INFO - joeynmt.training - Epoch 130, Step:    12280, Batch Loss:     0.098243, Batch Acc: 0.988877, Tokens per Sec:     3975, Lr: 0.000161\r\n",
      "2024-08-27 15:34:31,981 - INFO - joeynmt.training - Epoch 130, Step:    12285, Batch Loss:     0.095031, Batch Acc: 0.984966, Tokens per Sec:     3974, Lr: 0.000161\r\n",
      "2024-08-27 15:34:33,082 - INFO - joeynmt.training - Epoch 130, Step:    12290, Batch Loss:     0.121353, Batch Acc: 0.987685, Tokens per Sec:     3615, Lr: 0.000161\r\n",
      "2024-08-27 15:34:34,173 - INFO - joeynmt.training - Epoch 130, Step:    12295, Batch Loss:     0.094896, Batch Acc: 0.988561, Tokens per Sec:     3849, Lr: 0.000161\r\n",
      "2024-08-27 15:34:35,304 - INFO - joeynmt.training - Epoch 130, Step:    12300, Batch Loss:     0.092390, Batch Acc: 0.990760, Tokens per Sec:     3828, Lr: 0.000161\r\n",
      "2024-08-27 15:34:36,423 - INFO - joeynmt.training - Epoch 130, Step:    12305, Batch Loss:     0.099091, Batch Acc: 0.989180, Tokens per Sec:     3719, Lr: 0.000161\r\n",
      "2024-08-27 15:34:37,581 - INFO - joeynmt.training - Epoch 130, Step:    12310, Batch Loss:     0.098174, Batch Acc: 0.990248, Tokens per Sec:     3722, Lr: 0.000161\r\n",
      "2024-08-27 15:34:38,678 - INFO - joeynmt.training - Epoch 130, Step:    12315, Batch Loss:     0.084282, Batch Acc: 0.989262, Tokens per Sec:     3909, Lr: 0.000161\r\n",
      "2024-08-27 15:34:39,924 - INFO - joeynmt.training - Epoch 130, Step:    12320, Batch Loss:     0.094339, Batch Acc: 0.989798, Tokens per Sec:     3463, Lr: 0.000161\r\n",
      "2024-08-27 15:34:41,075 - INFO - joeynmt.training - Epoch 130, Step:    12325, Batch Loss:     0.092739, Batch Acc: 0.989109, Tokens per Sec:     3591, Lr: 0.000161\r\n",
      "2024-08-27 15:34:42,174 - INFO - joeynmt.training - Epoch 130, Step:    12330, Batch Loss:     0.096340, Batch Acc: 0.988135, Tokens per Sec:     3836, Lr: 0.000161\r\n",
      "2024-08-27 15:34:43,264 - INFO - joeynmt.training - Epoch 130, Step:    12335, Batch Loss:     0.114640, Batch Acc: 0.985152, Tokens per Sec:     3896, Lr: 0.000161\r\n",
      "2024-08-27 15:34:44,397 - INFO - joeynmt.training - Epoch 130, Step:    12340, Batch Loss:     0.110199, Batch Acc: 0.984684, Tokens per Sec:     3749, Lr: 0.000161\r\n",
      "2024-08-27 15:34:45,505 - INFO - joeynmt.training - Epoch 130, Step:    12345, Batch Loss:     0.099430, Batch Acc: 0.988573, Tokens per Sec:     3872, Lr: 0.000161\r\n",
      "2024-08-27 15:34:46,618 - INFO - joeynmt.training - Epoch 130, Step:    12350, Batch Loss:     0.104563, Batch Acc: 0.988276, Tokens per Sec:     3680, Lr: 0.000161\r\n",
      "2024-08-27 15:34:47,760 - INFO - joeynmt.training - Epoch 130, Step:    12355, Batch Loss:     0.108667, Batch Acc: 0.986142, Tokens per Sec:     3605, Lr: 0.000161\r\n",
      "2024-08-27 15:34:48,852 - INFO - joeynmt.training - Epoch 130, Step:    12360, Batch Loss:     0.095724, Batch Acc: 0.985698, Tokens per Sec:     4034, Lr: 0.000161\r\n",
      "2024-08-27 15:34:49,958 - INFO - joeynmt.training - Epoch 130, Step:    12365, Batch Loss:     0.123299, Batch Acc: 0.985071, Tokens per Sec:     3878, Lr: 0.000161\r\n",
      "2024-08-27 15:34:51,075 - INFO - joeynmt.training - Epoch 130, Step:    12370, Batch Loss:     0.086864, Batch Acc: 0.988101, Tokens per Sec:     3914, Lr: 0.000161\r\n",
      "2024-08-27 15:34:51,896 - INFO - joeynmt.training - Epoch 130, total training loss: 9.32, num. of seqs: 8065, num. of tokens: 80463, 21.2337[sec]\r\n",
      "2024-08-27 15:34:51,896 - INFO - joeynmt.training - EPOCH 131\r\n",
      "2024-08-27 15:34:52,371 - INFO - joeynmt.training - Epoch 131, Step:    12375, Batch Loss:     0.116393, Batch Acc: 0.982530, Tokens per Sec:     3523, Lr: 0.000161\r\n",
      "2024-08-27 15:34:53,490 - INFO - joeynmt.training - Epoch 131, Step:    12380, Batch Loss:     0.088904, Batch Acc: 0.989677, Tokens per Sec:     3987, Lr: 0.000161\r\n",
      "2024-08-27 15:34:54,600 - INFO - joeynmt.training - Epoch 131, Step:    12385, Batch Loss:     0.099487, Batch Acc: 0.988092, Tokens per Sec:     3785, Lr: 0.000161\r\n",
      "2024-08-27 15:34:55,693 - INFO - joeynmt.training - Epoch 131, Step:    12390, Batch Loss:     0.104443, Batch Acc: 0.987342, Tokens per Sec:     3975, Lr: 0.000161\r\n",
      "2024-08-27 15:34:56,860 - INFO - joeynmt.training - Epoch 131, Step:    12395, Batch Loss:     0.106794, Batch Acc: 0.987203, Tokens per Sec:     3685, Lr: 0.000161\r\n",
      "2024-08-27 15:34:57,983 - INFO - joeynmt.training - Epoch 131, Step:    12400, Batch Loss:     0.101898, Batch Acc: 0.990693, Tokens per Sec:     3829, Lr: 0.000161\r\n",
      "2024-08-27 15:34:59,111 - INFO - joeynmt.training - Epoch 131, Step:    12405, Batch Loss:     0.113702, Batch Acc: 0.986845, Tokens per Sec:     3777, Lr: 0.000161\r\n",
      "2024-08-27 15:35:00,233 - INFO - joeynmt.training - Epoch 131, Step:    12410, Batch Loss:     0.097137, Batch Acc: 0.987578, Tokens per Sec:     3735, Lr: 0.000161\r\n",
      "2024-08-27 15:35:01,342 - INFO - joeynmt.training - Epoch 131, Step:    12415, Batch Loss:     0.093028, Batch Acc: 0.989591, Tokens per Sec:     3727, Lr: 0.000161\r\n",
      "2024-08-27 15:35:02,452 - INFO - joeynmt.training - Epoch 131, Step:    12420, Batch Loss:     0.096192, Batch Acc: 0.988350, Tokens per Sec:     3871, Lr: 0.000161\r\n",
      "2024-08-27 15:35:03,547 - INFO - joeynmt.training - Epoch 131, Step:    12425, Batch Loss:     0.081625, Batch Acc: 0.988663, Tokens per Sec:     3868, Lr: 0.000160\r\n",
      "2024-08-27 15:35:04,666 - INFO - joeynmt.training - Epoch 131, Step:    12430, Batch Loss:     0.103054, Batch Acc: 0.985972, Tokens per Sec:     3761, Lr: 0.000160\r\n",
      "2024-08-27 15:35:05,780 - INFO - joeynmt.training - Epoch 131, Step:    12435, Batch Loss:     0.103078, Batch Acc: 0.988284, Tokens per Sec:     3909, Lr: 0.000160\r\n",
      "2024-08-27 15:35:06,925 - INFO - joeynmt.training - Epoch 131, Step:    12440, Batch Loss:     0.096033, Batch Acc: 0.987415, Tokens per Sec:     3612, Lr: 0.000160\r\n",
      "2024-08-27 15:35:08,050 - INFO - joeynmt.training - Epoch 131, Step:    12445, Batch Loss:     0.096596, Batch Acc: 0.985578, Tokens per Sec:     3638, Lr: 0.000160\r\n",
      "2024-08-27 15:35:09,158 - INFO - joeynmt.training - Epoch 131, Step:    12450, Batch Loss:     0.100386, Batch Acc: 0.985517, Tokens per Sec:     3929, Lr: 0.000160\r\n",
      "2024-08-27 15:35:10,247 - INFO - joeynmt.training - Epoch 131, Step:    12455, Batch Loss:     0.091926, Batch Acc: 0.989340, Tokens per Sec:     3620, Lr: 0.000160\r\n",
      "2024-08-27 15:35:11,447 - INFO - joeynmt.training - Epoch 131, Step:    12460, Batch Loss:     0.091295, Batch Acc: 0.988541, Tokens per Sec:     3491, Lr: 0.000160\r\n",
      "2024-08-27 15:35:12,569 - INFO - joeynmt.training - Epoch 131, Step:    12465, Batch Loss:     0.095927, Batch Acc: 0.988687, Tokens per Sec:     3787, Lr: 0.000160\r\n",
      "2024-08-27 15:35:13,297 - INFO - joeynmt.training - Epoch 131, total training loss: 9.43, num. of seqs: 8065, num. of tokens: 80463, 21.3840[sec]\r\n",
      "2024-08-27 15:35:13,298 - INFO - joeynmt.training - EPOCH 132\r\n",
      "2024-08-27 15:35:13,747 - INFO - joeynmt.training - Epoch 132, Step:    12470, Batch Loss:     0.098863, Batch Acc: 0.984967, Tokens per Sec:     3739, Lr: 0.000160\r\n",
      "2024-08-27 15:35:14,857 - INFO - joeynmt.training - Epoch 132, Step:    12475, Batch Loss:     0.070818, Batch Acc: 0.988413, Tokens per Sec:     3888, Lr: 0.000160\r\n",
      "2024-08-27 15:35:15,971 - INFO - joeynmt.training - Epoch 132, Step:    12480, Batch Loss:     0.098125, Batch Acc: 0.986500, Tokens per Sec:     3728, Lr: 0.000160\r\n",
      "2024-08-27 15:35:17,175 - INFO - joeynmt.training - Epoch 132, Step:    12485, Batch Loss:     0.092097, Batch Acc: 0.988253, Tokens per Sec:     3394, Lr: 0.000160\r\n",
      "2024-08-27 15:35:18,494 - INFO - joeynmt.training - Epoch 132, Step:    12490, Batch Loss:     0.093291, Batch Acc: 0.990394, Tokens per Sec:     3239, Lr: 0.000160\r\n",
      "2024-08-27 15:35:19,598 - INFO - joeynmt.training - Epoch 132, Step:    12495, Batch Loss:     0.089785, Batch Acc: 0.991201, Tokens per Sec:     3810, Lr: 0.000160\r\n",
      "2024-08-27 15:35:20,744 - INFO - joeynmt.training - Epoch 132, Step:    12500, Batch Loss:     0.108532, Batch Acc: 0.985756, Tokens per Sec:     3864, Lr: 0.000160\r\n",
      "2024-08-27 15:35:20,745 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=12542\r\n",
      "2024-08-27 15:35:20,745 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 63.97it/s]\r\n",
      "2024-08-27 15:35:43,570 - INFO - joeynmt.prediction - Generation took 22.8233[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44465.90 examples/s]\r\n",
      "2024-08-27 15:35:43,971 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:35:43,971 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.04, loss:   6.96, ppl: 1053.99, acc:   0.28, 0.1669[sec]\r\n",
      "2024-08-27 15:35:43,973 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:35:44,119 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:35:44,119 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:35:44,119 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 15:35:44,414 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:35:44,561 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:35:44,561 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:35:44,561 - INFO - joeynmt.training - \tHypothesis: trois tables blanches\r\n",
      "2024-08-27 15:35:44,854 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:35:45,002 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:35:45,002 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:35:45,002 - INFO - joeynmt.training - \tHypothesis: le groupe disparaît avec lui\r\n",
      "2024-08-27 15:35:45,294 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:35:45,442 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:35:45,442 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:35:45,442 - INFO - joeynmt.training - \tHypothesis: une trentaine de personnes\r\n",
      "2024-08-27 15:35:46,855 - INFO - joeynmt.training - Epoch 132, Step:    12505, Batch Loss:     0.100509, Batch Acc: 0.989484, Tokens per Sec:     3656, Lr: 0.000160\r\n",
      "2024-08-27 15:35:47,945 - INFO - joeynmt.training - Epoch 132, Step:    12510, Batch Loss:     0.090264, Batch Acc: 0.989255, Tokens per Sec:     3675, Lr: 0.000160\r\n",
      "2024-08-27 15:35:49,100 - INFO - joeynmt.training - Epoch 132, Step:    12515, Batch Loss:     0.095587, Batch Acc: 0.987560, Tokens per Sec:     3619, Lr: 0.000160\r\n",
      "2024-08-27 15:35:50,209 - INFO - joeynmt.training - Epoch 132, Step:    12520, Batch Loss:     0.100236, Batch Acc: 0.986755, Tokens per Sec:     3681, Lr: 0.000160\r\n",
      "2024-08-27 15:35:51,295 - INFO - joeynmt.training - Epoch 132, Step:    12525, Batch Loss:     0.086196, Batch Acc: 0.989111, Tokens per Sec:     3639, Lr: 0.000160\r\n",
      "2024-08-27 15:35:52,379 - INFO - joeynmt.training - Epoch 132, Step:    12530, Batch Loss:     0.109204, Batch Acc: 0.988037, Tokens per Sec:     3935, Lr: 0.000160\r\n",
      "2024-08-27 15:35:53,478 - INFO - joeynmt.training - Epoch 132, Step:    12535, Batch Loss:     0.102268, Batch Acc: 0.987677, Tokens per Sec:     3918, Lr: 0.000160\r\n",
      "2024-08-27 15:35:54,596 - INFO - joeynmt.training - Epoch 132, Step:    12540, Batch Loss:     0.110099, Batch Acc: 0.988652, Tokens per Sec:     4021, Lr: 0.000160\r\n",
      "2024-08-27 15:35:55,709 - INFO - joeynmt.training - Epoch 132, Step:    12545, Batch Loss:     0.098241, Batch Acc: 0.984991, Tokens per Sec:     3832, Lr: 0.000160\r\n",
      "2024-08-27 15:35:56,891 - INFO - joeynmt.training - Epoch 132, Step:    12550, Batch Loss:     0.110749, Batch Acc: 0.985819, Tokens per Sec:     3465, Lr: 0.000160\r\n",
      "2024-08-27 15:35:58,009 - INFO - joeynmt.training - Epoch 132, Step:    12555, Batch Loss:     0.089835, Batch Acc: 0.987222, Tokens per Sec:     3782, Lr: 0.000160\r\n",
      "2024-08-27 15:35:59,141 - INFO - joeynmt.training - Epoch 132, Step:    12560, Batch Loss:     0.086302, Batch Acc: 0.989661, Tokens per Sec:     3676, Lr: 0.000160\r\n",
      "2024-08-27 15:36:00,049 - INFO - joeynmt.training - Epoch 132, total training loss: 9.41, num. of seqs: 8065, num. of tokens: 80463, 21.7411[sec]\r\n",
      "2024-08-27 15:36:00,049 - INFO - joeynmt.training - EPOCH 133\r\n",
      "2024-08-27 15:36:00,279 - INFO - joeynmt.training - Epoch 133, Step:    12565, Batch Loss:     0.084883, Batch Acc: 0.989691, Tokens per Sec:     3440, Lr: 0.000160\r\n",
      "2024-08-27 15:36:01,399 - INFO - joeynmt.training - Epoch 133, Step:    12570, Batch Loss:     0.083280, Batch Acc: 0.987534, Tokens per Sec:     3657, Lr: 0.000160\r\n",
      "2024-08-27 15:36:02,530 - INFO - joeynmt.training - Epoch 133, Step:    12575, Batch Loss:     0.100676, Batch Acc: 0.988294, Tokens per Sec:     3928, Lr: 0.000160\r\n",
      "2024-08-27 15:36:03,646 - INFO - joeynmt.training - Epoch 133, Step:    12580, Batch Loss:     0.102358, Batch Acc: 0.988146, Tokens per Sec:     4009, Lr: 0.000159\r\n",
      "2024-08-27 15:36:04,736 - INFO - joeynmt.training - Epoch 133, Step:    12585, Batch Loss:     0.091562, Batch Acc: 0.990345, Tokens per Sec:     3801, Lr: 0.000159\r\n",
      "2024-08-27 15:36:05,821 - INFO - joeynmt.training - Epoch 133, Step:    12590, Batch Loss:     0.085915, Batch Acc: 0.989313, Tokens per Sec:     4055, Lr: 0.000159\r\n",
      "2024-08-27 15:36:06,947 - INFO - joeynmt.training - Epoch 133, Step:    12595, Batch Loss:     0.100474, Batch Acc: 0.987454, Tokens per Sec:     3828, Lr: 0.000159\r\n",
      "2024-08-27 15:36:08,030 - INFO - joeynmt.training - Epoch 133, Step:    12600, Batch Loss:     0.096087, Batch Acc: 0.986733, Tokens per Sec:     3690, Lr: 0.000159\r\n",
      "2024-08-27 15:36:09,125 - INFO - joeynmt.training - Epoch 133, Step:    12605, Batch Loss:     0.095763, Batch Acc: 0.988213, Tokens per Sec:     3798, Lr: 0.000159\r\n",
      "2024-08-27 15:36:10,238 - INFO - joeynmt.training - Epoch 133, Step:    12610, Batch Loss:     0.108779, Batch Acc: 0.985809, Tokens per Sec:     4056, Lr: 0.000159\r\n",
      "2024-08-27 15:36:11,357 - INFO - joeynmt.training - Epoch 133, Step:    12615, Batch Loss:     0.095204, Batch Acc: 0.989838, Tokens per Sec:     3694, Lr: 0.000159\r\n",
      "2024-08-27 15:36:12,451 - INFO - joeynmt.training - Epoch 133, Step:    12620, Batch Loss:     0.092283, Batch Acc: 0.988227, Tokens per Sec:     3887, Lr: 0.000159\r\n",
      "2024-08-27 15:36:13,631 - INFO - joeynmt.training - Epoch 133, Step:    12625, Batch Loss:     0.099489, Batch Acc: 0.987509, Tokens per Sec:     3528, Lr: 0.000159\r\n",
      "2024-08-27 15:36:14,750 - INFO - joeynmt.training - Epoch 133, Step:    12630, Batch Loss:     0.099665, Batch Acc: 0.988756, Tokens per Sec:     3740, Lr: 0.000159\r\n",
      "2024-08-27 15:36:15,862 - INFO - joeynmt.training - Epoch 133, Step:    12635, Batch Loss:     0.087923, Batch Acc: 0.988410, Tokens per Sec:     3882, Lr: 0.000159\r\n",
      "2024-08-27 15:36:16,996 - INFO - joeynmt.training - Epoch 133, Step:    12640, Batch Loss:     0.080014, Batch Acc: 0.989776, Tokens per Sec:     3624, Lr: 0.000159\r\n",
      "2024-08-27 15:36:18,096 - INFO - joeynmt.training - Epoch 133, Step:    12645, Batch Loss:     0.085372, Batch Acc: 0.986915, Tokens per Sec:     3754, Lr: 0.000159\r\n",
      "2024-08-27 15:36:19,206 - INFO - joeynmt.training - Epoch 133, Step:    12650, Batch Loss:     0.100744, Batch Acc: 0.986190, Tokens per Sec:     3655, Lr: 0.000159\r\n",
      "2024-08-27 15:36:20,332 - INFO - joeynmt.training - Epoch 133, Step:    12655, Batch Loss:     0.092956, Batch Acc: 0.984931, Tokens per Sec:     3775, Lr: 0.000159\r\n",
      "2024-08-27 15:36:21,300 - INFO - joeynmt.training - Epoch 133, total training loss: 9.20, num. of seqs: 8065, num. of tokens: 80463, 21.2336[sec]\r\n",
      "2024-08-27 15:36:21,300 - INFO - joeynmt.training - EPOCH 134\r\n",
      "2024-08-27 15:36:21,533 - INFO - joeynmt.training - Epoch 134, Step:    12660, Batch Loss:     0.100454, Batch Acc: 0.987119, Tokens per Sec:     3735, Lr: 0.000159\r\n",
      "2024-08-27 15:36:22,645 - INFO - joeynmt.training - Epoch 134, Step:    12665, Batch Loss:     0.105212, Batch Acc: 0.991172, Tokens per Sec:     3669, Lr: 0.000159\r\n",
      "2024-08-27 15:36:23,758 - INFO - joeynmt.training - Epoch 134, Step:    12670, Batch Loss:     0.085058, Batch Acc: 0.989636, Tokens per Sec:     3556, Lr: 0.000159\r\n",
      "2024-08-27 15:36:24,877 - INFO - joeynmt.training - Epoch 134, Step:    12675, Batch Loss:     0.086458, Batch Acc: 0.988791, Tokens per Sec:     3752, Lr: 0.000159\r\n",
      "2024-08-27 15:36:26,008 - INFO - joeynmt.training - Epoch 134, Step:    12680, Batch Loss:     0.083564, Batch Acc: 0.987830, Tokens per Sec:     3852, Lr: 0.000159\r\n",
      "2024-08-27 15:36:27,151 - INFO - joeynmt.training - Epoch 134, Step:    12685, Batch Loss:     0.107118, Batch Acc: 0.989161, Tokens per Sec:     3718, Lr: 0.000159\r\n",
      "2024-08-27 15:36:28,244 - INFO - joeynmt.training - Epoch 134, Step:    12690, Batch Loss:     0.092627, Batch Acc: 0.990469, Tokens per Sec:     3648, Lr: 0.000159\r\n",
      "2024-08-27 15:36:29,351 - INFO - joeynmt.training - Epoch 134, Step:    12695, Batch Loss:     0.085583, Batch Acc: 0.990041, Tokens per Sec:     3722, Lr: 0.000159\r\n",
      "2024-08-27 15:36:30,457 - INFO - joeynmt.training - Epoch 134, Step:    12700, Batch Loss:     0.103746, Batch Acc: 0.989930, Tokens per Sec:     3863, Lr: 0.000159\r\n",
      "2024-08-27 15:36:31,573 - INFO - joeynmt.training - Epoch 134, Step:    12705, Batch Loss:     0.097842, Batch Acc: 0.986604, Tokens per Sec:     3815, Lr: 0.000159\r\n",
      "2024-08-27 15:36:32,691 - INFO - joeynmt.training - Epoch 134, Step:    12710, Batch Loss:     0.088794, Batch Acc: 0.988962, Tokens per Sec:     3811, Lr: 0.000159\r\n",
      "2024-08-27 15:36:33,809 - INFO - joeynmt.training - Epoch 134, Step:    12715, Batch Loss:     0.081453, Batch Acc: 0.989505, Tokens per Sec:     3667, Lr: 0.000159\r\n",
      "2024-08-27 15:36:34,926 - INFO - joeynmt.training - Epoch 134, Step:    12720, Batch Loss:     0.102094, Batch Acc: 0.987957, Tokens per Sec:     3794, Lr: 0.000159\r\n",
      "2024-08-27 15:36:36,028 - INFO - joeynmt.training - Epoch 134, Step:    12725, Batch Loss:     0.082947, Batch Acc: 0.988347, Tokens per Sec:     3738, Lr: 0.000159\r\n",
      "2024-08-27 15:36:37,155 - INFO - joeynmt.training - Epoch 134, Step:    12730, Batch Loss:     0.091201, Batch Acc: 0.989164, Tokens per Sec:     3771, Lr: 0.000159\r\n",
      "2024-08-27 15:36:38,264 - INFO - joeynmt.training - Epoch 134, Step:    12735, Batch Loss:     0.100207, Batch Acc: 0.987777, Tokens per Sec:     3911, Lr: 0.000159\r\n",
      "2024-08-27 15:36:39,357 - INFO - joeynmt.training - Epoch 134, Step:    12740, Batch Loss:     0.082511, Batch Acc: 0.989688, Tokens per Sec:     3909, Lr: 0.000158\r\n",
      "2024-08-27 15:36:40,449 - INFO - joeynmt.training - Epoch 134, Step:    12745, Batch Loss:     0.095787, Batch Acc: 0.989848, Tokens per Sec:     3972, Lr: 0.000158\r\n",
      "2024-08-27 15:36:41,536 - INFO - joeynmt.training - Epoch 134, Step:    12750, Batch Loss:     0.105861, Batch Acc: 0.986914, Tokens per Sec:     3869, Lr: 0.000158\r\n",
      "2024-08-27 15:36:42,625 - INFO - joeynmt.training - Epoch 134, Step:    12755, Batch Loss:     0.108362, Batch Acc: 0.989163, Tokens per Sec:     3729, Lr: 0.000158\r\n",
      "2024-08-27 15:36:42,626 - INFO - joeynmt.training - Epoch 134, total training loss: 9.10, num. of seqs: 8065, num. of tokens: 80463, 21.3078[sec]\r\n",
      "2024-08-27 15:36:42,626 - INFO - joeynmt.training - EPOCH 135\r\n",
      "2024-08-27 15:36:43,729 - INFO - joeynmt.training - Epoch 135, Step:    12760, Batch Loss:     0.105886, Batch Acc: 0.990630, Tokens per Sec:     3786, Lr: 0.000158\r\n",
      "2024-08-27 15:36:44,893 - INFO - joeynmt.training - Epoch 135, Step:    12765, Batch Loss:     0.100470, Batch Acc: 0.989753, Tokens per Sec:     3689, Lr: 0.000158\r\n",
      "2024-08-27 15:36:45,990 - INFO - joeynmt.training - Epoch 135, Step:    12770, Batch Loss:     0.109773, Batch Acc: 0.987317, Tokens per Sec:     3740, Lr: 0.000158\r\n",
      "2024-08-27 15:36:47,117 - INFO - joeynmt.training - Epoch 135, Step:    12775, Batch Loss:     0.096515, Batch Acc: 0.990371, Tokens per Sec:     3688, Lr: 0.000158\r\n",
      "2024-08-27 15:36:48,214 - INFO - joeynmt.training - Epoch 135, Step:    12780, Batch Loss:     0.089990, Batch Acc: 0.987595, Tokens per Sec:     3971, Lr: 0.000158\r\n",
      "2024-08-27 15:36:49,303 - INFO - joeynmt.training - Epoch 135, Step:    12785, Batch Loss:     0.093678, Batch Acc: 0.990368, Tokens per Sec:     3818, Lr: 0.000158\r\n",
      "2024-08-27 15:36:50,401 - INFO - joeynmt.training - Epoch 135, Step:    12790, Batch Loss:     0.090636, Batch Acc: 0.988805, Tokens per Sec:     3987, Lr: 0.000158\r\n",
      "2024-08-27 15:36:51,510 - INFO - joeynmt.training - Epoch 135, Step:    12795, Batch Loss:     0.096995, Batch Acc: 0.988988, Tokens per Sec:     3853, Lr: 0.000158\r\n",
      "2024-08-27 15:36:52,581 - INFO - joeynmt.training - Epoch 135, Step:    12800, Batch Loss:     0.100779, Batch Acc: 0.990359, Tokens per Sec:     3877, Lr: 0.000158\r\n",
      "2024-08-27 15:36:53,668 - INFO - joeynmt.training - Epoch 135, Step:    12805, Batch Loss:     0.095390, Batch Acc: 0.986502, Tokens per Sec:     3956, Lr: 0.000158\r\n",
      "2024-08-27 15:36:54,752 - INFO - joeynmt.training - Epoch 135, Step:    12810, Batch Loss:     0.107473, Batch Acc: 0.987292, Tokens per Sec:     3995, Lr: 0.000158\r\n",
      "2024-08-27 15:36:55,829 - INFO - joeynmt.training - Epoch 135, Step:    12815, Batch Loss:     0.088125, Batch Acc: 0.989706, Tokens per Sec:     3789, Lr: 0.000158\r\n",
      "2024-08-27 15:36:57,005 - INFO - joeynmt.training - Epoch 135, Step:    12820, Batch Loss:     0.093987, Batch Acc: 0.989417, Tokens per Sec:     3618, Lr: 0.000158\r\n",
      "2024-08-27 15:36:58,090 - INFO - joeynmt.training - Epoch 135, Step:    12825, Batch Loss:     0.105597, Batch Acc: 0.987805, Tokens per Sec:     3933, Lr: 0.000158\r\n",
      "2024-08-27 15:36:59,187 - INFO - joeynmt.training - Epoch 135, Step:    12830, Batch Loss:     0.092186, Batch Acc: 0.988945, Tokens per Sec:     3961, Lr: 0.000158\r\n",
      "2024-08-27 15:37:00,288 - INFO - joeynmt.training - Epoch 135, Step:    12835, Batch Loss:     0.102135, Batch Acc: 0.985202, Tokens per Sec:     3928, Lr: 0.000158\r\n",
      "2024-08-27 15:37:01,437 - INFO - joeynmt.training - Epoch 135, Step:    12840, Batch Loss:     0.085785, Batch Acc: 0.991642, Tokens per Sec:     3753, Lr: 0.000158\r\n",
      "2024-08-27 15:37:02,537 - INFO - joeynmt.training - Epoch 135, Step:    12845, Batch Loss:     0.099536, Batch Acc: 0.987669, Tokens per Sec:     3760, Lr: 0.000158\r\n",
      "2024-08-27 15:37:03,641 - INFO - joeynmt.training - Epoch 135, Step:    12850, Batch Loss:     0.108019, Batch Acc: 0.987841, Tokens per Sec:     3652, Lr: 0.000158\r\n",
      "2024-08-27 15:37:03,688 - INFO - joeynmt.training - Epoch 135, total training loss: 9.09, num. of seqs: 8065, num. of tokens: 80463, 21.0462[sec]\r\n",
      "2024-08-27 15:37:03,689 - INFO - joeynmt.training - EPOCH 136\r\n",
      "2024-08-27 15:37:04,828 - INFO - joeynmt.training - Epoch 136, Step:    12855, Batch Loss:     0.088833, Batch Acc: 0.988381, Tokens per Sec:     3640, Lr: 0.000158\r\n",
      "2024-08-27 15:37:05,932 - INFO - joeynmt.training - Epoch 136, Step:    12860, Batch Loss:     0.087231, Batch Acc: 0.987773, Tokens per Sec:     3779, Lr: 0.000158\r\n",
      "2024-08-27 15:37:07,063 - INFO - joeynmt.training - Epoch 136, Step:    12865, Batch Loss:     0.093227, Batch Acc: 0.984266, Tokens per Sec:     3541, Lr: 0.000158\r\n",
      "2024-08-27 15:37:08,171 - INFO - joeynmt.training - Epoch 136, Step:    12870, Batch Loss:     0.098501, Batch Acc: 0.990024, Tokens per Sec:     3714, Lr: 0.000158\r\n",
      "2024-08-27 15:37:09,248 - INFO - joeynmt.training - Epoch 136, Step:    12875, Batch Loss:     0.096648, Batch Acc: 0.990579, Tokens per Sec:     3943, Lr: 0.000158\r\n",
      "2024-08-27 15:37:10,353 - INFO - joeynmt.training - Epoch 136, Step:    12880, Batch Loss:     0.085175, Batch Acc: 0.990126, Tokens per Sec:     3943, Lr: 0.000158\r\n",
      "2024-08-27 15:37:11,482 - INFO - joeynmt.training - Epoch 136, Step:    12885, Batch Loss:     0.086595, Batch Acc: 0.990723, Tokens per Sec:     3631, Lr: 0.000158\r\n",
      "2024-08-27 15:37:12,595 - INFO - joeynmt.training - Epoch 136, Step:    12890, Batch Loss:     0.098660, Batch Acc: 0.989374, Tokens per Sec:     3977, Lr: 0.000158\r\n",
      "2024-08-27 15:37:13,684 - INFO - joeynmt.training - Epoch 136, Step:    12895, Batch Loss:     0.104833, Batch Acc: 0.990385, Tokens per Sec:     3821, Lr: 0.000158\r\n",
      "2024-08-27 15:37:14,778 - INFO - joeynmt.training - Epoch 136, Step:    12900, Batch Loss:     0.097368, Batch Acc: 0.987799, Tokens per Sec:     3748, Lr: 0.000157\r\n",
      "2024-08-27 15:37:15,979 - INFO - joeynmt.training - Epoch 136, Step:    12905, Batch Loss:     0.102519, Batch Acc: 0.990124, Tokens per Sec:     3626, Lr: 0.000157\r\n",
      "2024-08-27 15:37:17,116 - INFO - joeynmt.training - Epoch 136, Step:    12910, Batch Loss:     0.106362, Batch Acc: 0.988276, Tokens per Sec:     3828, Lr: 0.000157\r\n",
      "2024-08-27 15:37:18,220 - INFO - joeynmt.training - Epoch 136, Step:    12915, Batch Loss:     0.103492, Batch Acc: 0.986593, Tokens per Sec:     3923, Lr: 0.000157\r\n",
      "2024-08-27 15:37:19,297 - INFO - joeynmt.training - Epoch 136, Step:    12920, Batch Loss:     0.081414, Batch Acc: 0.988987, Tokens per Sec:     3796, Lr: 0.000157\r\n",
      "2024-08-27 15:37:20,382 - INFO - joeynmt.training - Epoch 136, Step:    12925, Batch Loss:     0.097857, Batch Acc: 0.989825, Tokens per Sec:     3896, Lr: 0.000157\r\n",
      "2024-08-27 15:37:21,466 - INFO - joeynmt.training - Epoch 136, Step:    12930, Batch Loss:     0.093551, Batch Acc: 0.991357, Tokens per Sec:     3847, Lr: 0.000157\r\n",
      "2024-08-27 15:37:22,673 - INFO - joeynmt.training - Epoch 136, Step:    12935, Batch Loss:     0.083313, Batch Acc: 0.986714, Tokens per Sec:     3493, Lr: 0.000157\r\n",
      "2024-08-27 15:37:23,763 - INFO - joeynmt.training - Epoch 136, Step:    12940, Batch Loss:     0.091635, Batch Acc: 0.990087, Tokens per Sec:     3707, Lr: 0.000157\r\n",
      "2024-08-27 15:37:24,858 - INFO - joeynmt.training - Epoch 136, Step:    12945, Batch Loss:     0.103681, Batch Acc: 0.987630, Tokens per Sec:     3765, Lr: 0.000157\r\n",
      "2024-08-27 15:37:25,076 - INFO - joeynmt.training - Epoch 136, total training loss: 9.08, num. of seqs: 8065, num. of tokens: 80463, 21.3694[sec]\r\n",
      "2024-08-27 15:37:25,076 - INFO - joeynmt.training - EPOCH 137\r\n",
      "2024-08-27 15:37:25,975 - INFO - joeynmt.training - Epoch 137, Step:    12950, Batch Loss:     0.089865, Batch Acc: 0.990577, Tokens per Sec:     3907, Lr: 0.000157\r\n",
      "2024-08-27 15:37:27,101 - INFO - joeynmt.training - Epoch 137, Step:    12955, Batch Loss:     0.086053, Batch Acc: 0.990223, Tokens per Sec:     3547, Lr: 0.000157\r\n",
      "2024-08-27 15:37:28,201 - INFO - joeynmt.training - Epoch 137, Step:    12960, Batch Loss:     0.094162, Batch Acc: 0.990985, Tokens per Sec:     3936, Lr: 0.000157\r\n",
      "2024-08-27 15:37:29,294 - INFO - joeynmt.training - Epoch 137, Step:    12965, Batch Loss:     0.100031, Batch Acc: 0.988846, Tokens per Sec:     3530, Lr: 0.000157\r\n",
      "2024-08-27 15:37:30,390 - INFO - joeynmt.training - Epoch 137, Step:    12970, Batch Loss:     0.095803, Batch Acc: 0.990177, Tokens per Sec:     3810, Lr: 0.000157\r\n",
      "2024-08-27 15:37:31,506 - INFO - joeynmt.training - Epoch 137, Step:    12975, Batch Loss:     0.096579, Batch Acc: 0.990444, Tokens per Sec:     3941, Lr: 0.000157\r\n",
      "2024-08-27 15:37:32,608 - INFO - joeynmt.training - Epoch 137, Step:    12980, Batch Loss:     0.095678, Batch Acc: 0.991198, Tokens per Sec:     4023, Lr: 0.000157\r\n",
      "2024-08-27 15:37:33,693 - INFO - joeynmt.training - Epoch 137, Step:    12985, Batch Loss:     0.090240, Batch Acc: 0.988954, Tokens per Sec:     3925, Lr: 0.000157\r\n",
      "2024-08-27 15:37:34,776 - INFO - joeynmt.training - Epoch 137, Step:    12990, Batch Loss:     0.094660, Batch Acc: 0.987971, Tokens per Sec:     3993, Lr: 0.000157\r\n",
      "2024-08-27 15:37:35,892 - INFO - joeynmt.training - Epoch 137, Step:    12995, Batch Loss:     0.107007, Batch Acc: 0.987082, Tokens per Sec:     3886, Lr: 0.000157\r\n",
      "2024-08-27 15:37:37,021 - INFO - joeynmt.training - Epoch 137, Step:    13000, Batch Loss:     0.106108, Batch Acc: 0.987043, Tokens per Sec:     3489, Lr: 0.000157\r\n",
      "2024-08-27 15:37:37,022 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=13042\r\n",
      "2024-08-27 15:37:37,023 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 64.59it/s]\r\n",
      "2024-08-27 15:37:59,630 - INFO - joeynmt.prediction - Generation took 22.6056[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 43401.46 examples/s]\r\n",
      "2024-08-27 15:38:00,010 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:38:00,010 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   4.90, loss:   6.94, ppl: 1035.91, acc:   0.28, 0.1638[sec]\r\n",
      "2024-08-27 15:38:00,012 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:38:00,159 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:38:00,160 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:38:00,160 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 15:38:00,453 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:38:00,599 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:38:00,600 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:38:00,600 - INFO - joeynmt.training - \tHypothesis: trois tables blanches\r\n",
      "2024-08-27 15:38:00,892 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:38:01,037 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:38:01,037 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:38:01,037 - INFO - joeynmt.training - \tHypothesis: le service passagers n'est pas en activité actuellement\r\n",
      "2024-08-27 15:38:01,329 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:38:01,475 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:38:01,476 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:38:01,476 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 15:38:02,882 - INFO - joeynmt.training - Epoch 137, Step:    13005, Batch Loss:     0.110859, Batch Acc: 0.988128, Tokens per Sec:     3934, Lr: 0.000157\r\n",
      "2024-08-27 15:38:03,975 - INFO - joeynmt.training - Epoch 137, Step:    13010, Batch Loss:     0.100579, Batch Acc: 0.987721, Tokens per Sec:     3727, Lr: 0.000157\r\n",
      "2024-08-27 15:38:05,069 - INFO - joeynmt.training - Epoch 137, Step:    13015, Batch Loss:     0.118650, Batch Acc: 0.985236, Tokens per Sec:     3719, Lr: 0.000157\r\n",
      "2024-08-27 15:38:06,179 - INFO - joeynmt.training - Epoch 137, Step:    13020, Batch Loss:     0.126898, Batch Acc: 0.985977, Tokens per Sec:     3920, Lr: 0.000157\r\n",
      "2024-08-27 15:38:07,314 - INFO - joeynmt.training - Epoch 137, Step:    13025, Batch Loss:     0.085966, Batch Acc: 0.988829, Tokens per Sec:     3790, Lr: 0.000157\r\n",
      "2024-08-27 15:38:08,439 - INFO - joeynmt.training - Epoch 137, Step:    13030, Batch Loss:     0.094513, Batch Acc: 0.987709, Tokens per Sec:     3618, Lr: 0.000157\r\n",
      "2024-08-27 15:38:09,595 - INFO - joeynmt.training - Epoch 137, Step:    13035, Batch Loss:     0.089899, Batch Acc: 0.986330, Tokens per Sec:     3733, Lr: 0.000157\r\n",
      "2024-08-27 15:38:10,730 - INFO - joeynmt.training - Epoch 137, Step:    13040, Batch Loss:     0.092335, Batch Acc: 0.991870, Tokens per Sec:     3687, Lr: 0.000157\r\n",
      "2024-08-27 15:38:11,107 - INFO - joeynmt.training - Epoch 137, total training loss: 9.06, num. of seqs: 8065, num. of tokens: 80463, 21.2681[sec]\r\n",
      "2024-08-27 15:38:11,108 - INFO - joeynmt.training - EPOCH 138\r\n",
      "2024-08-27 15:38:12,009 - INFO - joeynmt.training - Epoch 138, Step:    13045, Batch Loss:     0.090767, Batch Acc: 0.991589, Tokens per Sec:     3709, Lr: 0.000157\r\n",
      "2024-08-27 15:38:13,114 - INFO - joeynmt.training - Epoch 138, Step:    13050, Batch Loss:     0.090010, Batch Acc: 0.988891, Tokens per Sec:     3995, Lr: 0.000157\r\n",
      "2024-08-27 15:38:14,213 - INFO - joeynmt.training - Epoch 138, Step:    13055, Batch Loss:     0.091627, Batch Acc: 0.991101, Tokens per Sec:     3891, Lr: 0.000157\r\n",
      "2024-08-27 15:38:15,315 - INFO - joeynmt.training - Epoch 138, Step:    13060, Batch Loss:     0.102004, Batch Acc: 0.989179, Tokens per Sec:     3858, Lr: 0.000157\r\n",
      "2024-08-27 15:38:16,416 - INFO - joeynmt.training - Epoch 138, Step:    13065, Batch Loss:     0.097924, Batch Acc: 0.989146, Tokens per Sec:     3768, Lr: 0.000157\r\n",
      "2024-08-27 15:38:17,555 - INFO - joeynmt.training - Epoch 138, Step:    13070, Batch Loss:     0.096822, Batch Acc: 0.988059, Tokens per Sec:     3751, Lr: 0.000156\r\n",
      "2024-08-27 15:38:18,768 - INFO - joeynmt.training - Epoch 138, Step:    13075, Batch Loss:     0.115870, Batch Acc: 0.985892, Tokens per Sec:     3450, Lr: 0.000156\r\n",
      "2024-08-27 15:38:19,865 - INFO - joeynmt.training - Epoch 138, Step:    13080, Batch Loss:     0.088836, Batch Acc: 0.990661, Tokens per Sec:     3908, Lr: 0.000156\r\n",
      "2024-08-27 15:38:20,966 - INFO - joeynmt.training - Epoch 138, Step:    13085, Batch Loss:     0.089748, Batch Acc: 0.989759, Tokens per Sec:     3815, Lr: 0.000156\r\n",
      "2024-08-27 15:38:22,066 - INFO - joeynmt.training - Epoch 138, Step:    13090, Batch Loss:     0.098577, Batch Acc: 0.987972, Tokens per Sec:     3858, Lr: 0.000156\r\n",
      "2024-08-27 15:38:23,172 - INFO - joeynmt.training - Epoch 138, Step:    13095, Batch Loss:     0.097012, Batch Acc: 0.988244, Tokens per Sec:     3846, Lr: 0.000156\r\n",
      "2024-08-27 15:38:24,390 - INFO - joeynmt.training - Epoch 138, Step:    13100, Batch Loss:     0.089752, Batch Acc: 0.983875, Tokens per Sec:     3415, Lr: 0.000156\r\n",
      "2024-08-27 15:38:25,493 - INFO - joeynmt.training - Epoch 138, Step:    13105, Batch Loss:     0.094718, Batch Acc: 0.989404, Tokens per Sec:     3679, Lr: 0.000156\r\n",
      "2024-08-27 15:38:26,591 - INFO - joeynmt.training - Epoch 138, Step:    13110, Batch Loss:     0.104672, Batch Acc: 0.986997, Tokens per Sec:     3785, Lr: 0.000156\r\n",
      "2024-08-27 15:38:27,712 - INFO - joeynmt.training - Epoch 138, Step:    13115, Batch Loss:     0.101771, Batch Acc: 0.985758, Tokens per Sec:     3762, Lr: 0.000156\r\n",
      "2024-08-27 15:38:28,803 - INFO - joeynmt.training - Epoch 138, Step:    13120, Batch Loss:     0.085381, Batch Acc: 0.988230, Tokens per Sec:     3896, Lr: 0.000156\r\n",
      "2024-08-27 15:38:29,916 - INFO - joeynmt.training - Epoch 138, Step:    13125, Batch Loss:     0.090408, Batch Acc: 0.988120, Tokens per Sec:     3859, Lr: 0.000156\r\n",
      "2024-08-27 15:38:31,020 - INFO - joeynmt.training - Epoch 138, Step:    13130, Batch Loss:     0.115687, Batch Acc: 0.989755, Tokens per Sec:     3804, Lr: 0.000156\r\n",
      "2024-08-27 15:38:32,125 - INFO - joeynmt.training - Epoch 138, Step:    13135, Batch Loss:     0.110633, Batch Acc: 0.987740, Tokens per Sec:     3914, Lr: 0.000156\r\n",
      "2024-08-27 15:38:32,396 - INFO - joeynmt.training - Epoch 138, total training loss: 9.05, num. of seqs: 8065, num. of tokens: 80463, 21.2715[sec]\r\n",
      "2024-08-27 15:38:32,397 - INFO - joeynmt.training - EPOCH 139\r\n",
      "2024-08-27 15:38:33,298 - INFO - joeynmt.training - Epoch 139, Step:    13140, Batch Loss:     0.098278, Batch Acc: 0.988415, Tokens per Sec:     3943, Lr: 0.000156\r\n",
      "2024-08-27 15:38:34,404 - INFO - joeynmt.training - Epoch 139, Step:    13145, Batch Loss:     0.080193, Batch Acc: 0.989162, Tokens per Sec:     3756, Lr: 0.000156\r\n",
      "2024-08-27 15:38:35,508 - INFO - joeynmt.training - Epoch 139, Step:    13150, Batch Loss:     0.093333, Batch Acc: 0.989712, Tokens per Sec:     3964, Lr: 0.000156\r\n",
      "2024-08-27 15:38:36,616 - INFO - joeynmt.training - Epoch 139, Step:    13155, Batch Loss:     0.088236, Batch Acc: 0.990640, Tokens per Sec:     3668, Lr: 0.000156\r\n",
      "2024-08-27 15:38:37,767 - INFO - joeynmt.training - Epoch 139, Step:    13160, Batch Loss:     0.090592, Batch Acc: 0.988927, Tokens per Sec:     3769, Lr: 0.000156\r\n",
      "2024-08-27 15:38:38,886 - INFO - joeynmt.training - Epoch 139, Step:    13165, Batch Loss:     0.102091, Batch Acc: 0.990113, Tokens per Sec:     3888, Lr: 0.000156\r\n",
      "2024-08-27 15:38:39,984 - INFO - joeynmt.training - Epoch 139, Step:    13170, Batch Loss:     0.101863, Batch Acc: 0.988569, Tokens per Sec:     3825, Lr: 0.000156\r\n",
      "2024-08-27 15:38:41,068 - INFO - joeynmt.training - Epoch 139, Step:    13175, Batch Loss:     0.098664, Batch Acc: 0.987790, Tokens per Sec:     3857, Lr: 0.000156\r\n",
      "2024-08-27 15:38:42,165 - INFO - joeynmt.training - Epoch 139, Step:    13180, Batch Loss:     0.106280, Batch Acc: 0.986324, Tokens per Sec:     3868, Lr: 0.000156\r\n",
      "2024-08-27 15:38:43,253 - INFO - joeynmt.training - Epoch 139, Step:    13185, Batch Loss:     0.089552, Batch Acc: 0.989756, Tokens per Sec:     3950, Lr: 0.000156\r\n",
      "2024-08-27 15:38:44,367 - INFO - joeynmt.training - Epoch 139, Step:    13190, Batch Loss:     0.094345, Batch Acc: 0.986689, Tokens per Sec:     3778, Lr: 0.000156\r\n",
      "2024-08-27 15:38:45,480 - INFO - joeynmt.training - Epoch 139, Step:    13195, Batch Loss:     0.098382, Batch Acc: 0.988375, Tokens per Sec:     3946, Lr: 0.000156\r\n",
      "2024-08-27 15:38:46,592 - INFO - joeynmt.training - Epoch 139, Step:    13200, Batch Loss:     0.093188, Batch Acc: 0.990287, Tokens per Sec:     3796, Lr: 0.000156\r\n",
      "2024-08-27 15:38:47,696 - INFO - joeynmt.training - Epoch 139, Step:    13205, Batch Loss:     0.080156, Batch Acc: 0.988511, Tokens per Sec:     3709, Lr: 0.000156\r\n",
      "2024-08-27 15:38:48,803 - INFO - joeynmt.training - Epoch 139, Step:    13210, Batch Loss:     0.087999, Batch Acc: 0.988625, Tokens per Sec:     3654, Lr: 0.000156\r\n",
      "2024-08-27 15:38:50,024 - INFO - joeynmt.training - Epoch 139, Step:    13215, Batch Loss:     0.093005, Batch Acc: 0.987660, Tokens per Sec:     3587, Lr: 0.000156\r\n",
      "2024-08-27 15:38:51,125 - INFO - joeynmt.training - Epoch 139, Step:    13220, Batch Loss:     0.083269, Batch Acc: 0.989261, Tokens per Sec:     3640, Lr: 0.000156\r\n",
      "2024-08-27 15:38:52,259 - INFO - joeynmt.training - Epoch 139, Step:    13225, Batch Loss:     0.087552, Batch Acc: 0.990787, Tokens per Sec:     3926, Lr: 0.000156\r\n",
      "2024-08-27 15:38:53,371 - INFO - joeynmt.training - Epoch 139, Step:    13230, Batch Loss:     0.103012, Batch Acc: 0.984036, Tokens per Sec:     3607, Lr: 0.000156\r\n",
      "2024-08-27 15:38:53,643 - INFO - joeynmt.training - Epoch 139, total training loss: 8.97, num. of seqs: 8065, num. of tokens: 80463, 21.2293[sec]\r\n",
      "2024-08-27 15:38:53,643 - INFO - joeynmt.training - EPOCH 140\r\n",
      "2024-08-27 15:38:54,523 - INFO - joeynmt.training - Epoch 140, Step:    13235, Batch Loss:     0.088902, Batch Acc: 0.989165, Tokens per Sec:     3899, Lr: 0.000155\r\n",
      "2024-08-27 15:38:55,618 - INFO - joeynmt.training - Epoch 140, Step:    13240, Batch Loss:     0.092045, Batch Acc: 0.987885, Tokens per Sec:     3770, Lr: 0.000155\r\n",
      "2024-08-27 15:38:56,787 - INFO - joeynmt.training - Epoch 140, Step:    13245, Batch Loss:     0.106573, Batch Acc: 0.987603, Tokens per Sec:     3728, Lr: 0.000155\r\n",
      "2024-08-27 15:38:57,908 - INFO - joeynmt.training - Epoch 140, Step:    13250, Batch Loss:     0.106032, Batch Acc: 0.988744, Tokens per Sec:     4045, Lr: 0.000155\r\n",
      "2024-08-27 15:38:59,020 - INFO - joeynmt.training - Epoch 140, Step:    13255, Batch Loss:     0.092437, Batch Acc: 0.990786, Tokens per Sec:     3906, Lr: 0.000155\r\n",
      "2024-08-27 15:39:00,112 - INFO - joeynmt.training - Epoch 140, Step:    13260, Batch Loss:     0.101454, Batch Acc: 0.990062, Tokens per Sec:     3875, Lr: 0.000155\r\n",
      "2024-08-27 15:39:01,209 - INFO - joeynmt.training - Epoch 140, Step:    13265, Batch Loss:     0.089764, Batch Acc: 0.989882, Tokens per Sec:     3697, Lr: 0.000155\r\n",
      "2024-08-27 15:39:02,278 - INFO - joeynmt.training - Epoch 140, Step:    13270, Batch Loss:     0.095052, Batch Acc: 0.990689, Tokens per Sec:     3818, Lr: 0.000155\r\n",
      "2024-08-27 15:39:03,378 - INFO - joeynmt.training - Epoch 140, Step:    13275, Batch Loss:     0.085731, Batch Acc: 0.989927, Tokens per Sec:     3883, Lr: 0.000155\r\n",
      "2024-08-27 15:39:04,479 - INFO - joeynmt.training - Epoch 140, Step:    13280, Batch Loss:     0.092716, Batch Acc: 0.989432, Tokens per Sec:     3869, Lr: 0.000155\r\n",
      "2024-08-27 15:39:05,591 - INFO - joeynmt.training - Epoch 140, Step:    13285, Batch Loss:     0.094728, Batch Acc: 0.987270, Tokens per Sec:     3819, Lr: 0.000155\r\n",
      "2024-08-27 15:39:06,727 - INFO - joeynmt.training - Epoch 140, Step:    13290, Batch Loss:     0.089811, Batch Acc: 0.988741, Tokens per Sec:     3832, Lr: 0.000155\r\n",
      "2024-08-27 15:39:07,844 - INFO - joeynmt.training - Epoch 140, Step:    13295, Batch Loss:     0.090807, Batch Acc: 0.988855, Tokens per Sec:     3778, Lr: 0.000155\r\n",
      "2024-08-27 15:39:08,956 - INFO - joeynmt.training - Epoch 140, Step:    13300, Batch Loss:     0.098515, Batch Acc: 0.989741, Tokens per Sec:     3684, Lr: 0.000155\r\n",
      "2024-08-27 15:39:10,054 - INFO - joeynmt.training - Epoch 140, Step:    13305, Batch Loss:     0.102545, Batch Acc: 0.988990, Tokens per Sec:     3808, Lr: 0.000155\r\n",
      "2024-08-27 15:39:11,159 - INFO - joeynmt.training - Epoch 140, Step:    13310, Batch Loss:     0.092935, Batch Acc: 0.989232, Tokens per Sec:     3699, Lr: 0.000155\r\n",
      "2024-08-27 15:39:12,272 - INFO - joeynmt.training - Epoch 140, Step:    13315, Batch Loss:     0.086497, Batch Acc: 0.988131, Tokens per Sec:     3866, Lr: 0.000155\r\n",
      "2024-08-27 15:39:13,374 - INFO - joeynmt.training - Epoch 140, Step:    13320, Batch Loss:     0.092211, Batch Acc: 0.992126, Tokens per Sec:     3806, Lr: 0.000155\r\n",
      "2024-08-27 15:39:14,488 - INFO - joeynmt.training - Epoch 140, Step:    13325, Batch Loss:     0.093225, Batch Acc: 0.990947, Tokens per Sec:     3670, Lr: 0.000155\r\n",
      "2024-08-27 15:39:14,826 - INFO - joeynmt.training - Epoch 140, total training loss: 8.81, num. of seqs: 8065, num. of tokens: 80463, 21.1657[sec]\r\n",
      "2024-08-27 15:39:14,826 - INFO - joeynmt.training - EPOCH 141\r\n",
      "2024-08-27 15:39:15,705 - INFO - joeynmt.training - Epoch 141, Step:    13330, Batch Loss:     0.096673, Batch Acc: 0.989286, Tokens per Sec:     3839, Lr: 0.000155\r\n",
      "2024-08-27 15:39:16,832 - INFO - joeynmt.training - Epoch 141, Step:    13335, Batch Loss:     0.122973, Batch Acc: 0.987899, Tokens per Sec:     3817, Lr: 0.000155\r\n",
      "2024-08-27 15:39:17,920 - INFO - joeynmt.training - Epoch 141, Step:    13340, Batch Loss:     0.084881, Batch Acc: 0.991507, Tokens per Sec:     3897, Lr: 0.000155\r\n",
      "2024-08-27 15:39:19,004 - INFO - joeynmt.training - Epoch 141, Step:    13345, Batch Loss:     0.090194, Batch Acc: 0.991332, Tokens per Sec:     3727, Lr: 0.000155\r\n",
      "2024-08-27 15:39:20,097 - INFO - joeynmt.training - Epoch 141, Step:    13350, Batch Loss:     0.094094, Batch Acc: 0.990506, Tokens per Sec:     3857, Lr: 0.000155\r\n",
      "2024-08-27 15:39:21,449 - INFO - joeynmt.training - Epoch 141, Step:    13355, Batch Loss:     0.089880, Batch Acc: 0.991523, Tokens per Sec:     3230, Lr: 0.000155\r\n",
      "2024-08-27 15:39:22,571 - INFO - joeynmt.training - Epoch 141, Step:    13360, Batch Loss:     0.079524, Batch Acc: 0.988012, Tokens per Sec:     3721, Lr: 0.000155\r\n",
      "2024-08-27 15:39:23,670 - INFO - joeynmt.training - Epoch 141, Step:    13365, Batch Loss:     0.103406, Batch Acc: 0.987757, Tokens per Sec:     3719, Lr: 0.000155\r\n",
      "2024-08-27 15:39:24,786 - INFO - joeynmt.training - Epoch 141, Step:    13370, Batch Loss:     0.096173, Batch Acc: 0.988650, Tokens per Sec:     3792, Lr: 0.000155\r\n",
      "2024-08-27 15:39:25,880 - INFO - joeynmt.training - Epoch 141, Step:    13375, Batch Loss:     0.086672, Batch Acc: 0.990872, Tokens per Sec:     3807, Lr: 0.000155\r\n",
      "2024-08-27 15:39:27,129 - INFO - joeynmt.training - Epoch 141, Step:    13380, Batch Loss:     0.104419, Batch Acc: 0.991465, Tokens per Sec:     3286, Lr: 0.000155\r\n",
      "2024-08-27 15:39:28,224 - INFO - joeynmt.training - Epoch 141, Step:    13385, Batch Loss:     0.097305, Batch Acc: 0.989724, Tokens per Sec:     3736, Lr: 0.000155\r\n",
      "2024-08-27 15:39:29,309 - INFO - joeynmt.training - Epoch 141, Step:    13390, Batch Loss:     0.083756, Batch Acc: 0.991034, Tokens per Sec:     3701, Lr: 0.000155\r\n",
      "2024-08-27 15:39:30,415 - INFO - joeynmt.training - Epoch 141, Step:    13395, Batch Loss:     0.096020, Batch Acc: 0.986012, Tokens per Sec:     3817, Lr: 0.000155\r\n",
      "2024-08-27 15:39:31,545 - INFO - joeynmt.training - Epoch 141, Step:    13400, Batch Loss:     0.105414, Batch Acc: 0.988496, Tokens per Sec:     4003, Lr: 0.000155\r\n",
      "2024-08-27 15:39:32,636 - INFO - joeynmt.training - Epoch 141, Step:    13405, Batch Loss:     0.089911, Batch Acc: 0.990349, Tokens per Sec:     3707, Lr: 0.000155\r\n",
      "2024-08-27 15:39:33,769 - INFO - joeynmt.training - Epoch 141, Step:    13410, Batch Loss:     0.090300, Batch Acc: 0.989009, Tokens per Sec:     4099, Lr: 0.000154\r\n",
      "2024-08-27 15:39:34,873 - INFO - joeynmt.training - Epoch 141, Step:    13415, Batch Loss:     0.094528, Batch Acc: 0.987760, Tokens per Sec:     3924, Lr: 0.000154\r\n",
      "2024-08-27 15:39:35,973 - INFO - joeynmt.training - Epoch 141, Step:    13420, Batch Loss:     0.081667, Batch Acc: 0.986985, Tokens per Sec:     3773, Lr: 0.000154\r\n",
      "2024-08-27 15:39:36,296 - INFO - joeynmt.training - Epoch 141, total training loss: 8.76, num. of seqs: 8065, num. of tokens: 80463, 21.4529[sec]\r\n",
      "2024-08-27 15:39:36,296 - INFO - joeynmt.training - EPOCH 142\r\n",
      "2024-08-27 15:39:37,223 - INFO - joeynmt.training - Epoch 142, Step:    13425, Batch Loss:     0.094284, Batch Acc: 0.989628, Tokens per Sec:     3760, Lr: 0.000154\r\n",
      "2024-08-27 15:39:38,315 - INFO - joeynmt.training - Epoch 142, Step:    13430, Batch Loss:     0.087263, Batch Acc: 0.992036, Tokens per Sec:     3914, Lr: 0.000154\r\n",
      "2024-08-27 15:39:39,438 - INFO - joeynmt.training - Epoch 142, Step:    13435, Batch Loss:     0.085523, Batch Acc: 0.990938, Tokens per Sec:     3934, Lr: 0.000154\r\n",
      "2024-08-27 15:39:40,537 - INFO - joeynmt.training - Epoch 142, Step:    13440, Batch Loss:     0.101598, Batch Acc: 0.990705, Tokens per Sec:     3820, Lr: 0.000154\r\n",
      "2024-08-27 15:39:41,612 - INFO - joeynmt.training - Epoch 142, Step:    13445, Batch Loss:     0.086063, Batch Acc: 0.991626, Tokens per Sec:     3669, Lr: 0.000154\r\n",
      "2024-08-27 15:39:42,709 - INFO - joeynmt.training - Epoch 142, Step:    13450, Batch Loss:     0.084477, Batch Acc: 0.989983, Tokens per Sec:     3735, Lr: 0.000154\r\n",
      "2024-08-27 15:39:43,813 - INFO - joeynmt.training - Epoch 142, Step:    13455, Batch Loss:     0.093714, Batch Acc: 0.988448, Tokens per Sec:     3763, Lr: 0.000154\r\n",
      "2024-08-27 15:39:44,919 - INFO - joeynmt.training - Epoch 142, Step:    13460, Batch Loss:     0.089572, Batch Acc: 0.986864, Tokens per Sec:     3859, Lr: 0.000154\r\n",
      "2024-08-27 15:39:45,987 - INFO - joeynmt.training - Epoch 142, Step:    13465, Batch Loss:     0.094785, Batch Acc: 0.987116, Tokens per Sec:     3781, Lr: 0.000154\r\n",
      "2024-08-27 15:39:47,101 - INFO - joeynmt.training - Epoch 142, Step:    13470, Batch Loss:     0.103099, Batch Acc: 0.986909, Tokens per Sec:     3707, Lr: 0.000154\r\n",
      "2024-08-27 15:39:48,213 - INFO - joeynmt.training - Epoch 142, Step:    13475, Batch Loss:     0.114251, Batch Acc: 0.985752, Tokens per Sec:     3727, Lr: 0.000154\r\n",
      "2024-08-27 15:39:49,315 - INFO - joeynmt.training - Epoch 142, Step:    13480, Batch Loss:     0.088068, Batch Acc: 0.986944, Tokens per Sec:     3753, Lr: 0.000154\r\n",
      "2024-08-27 15:39:50,422 - INFO - joeynmt.training - Epoch 142, Step:    13485, Batch Loss:     0.086814, Batch Acc: 0.990570, Tokens per Sec:     3836, Lr: 0.000154\r\n",
      "2024-08-27 15:39:51,584 - INFO - joeynmt.training - Epoch 142, Step:    13490, Batch Loss:     0.100024, Batch Acc: 0.987570, Tokens per Sec:     3672, Lr: 0.000154\r\n",
      "2024-08-27 15:39:52,737 - INFO - joeynmt.training - Epoch 142, Step:    13495, Batch Loss:     0.112168, Batch Acc: 0.989118, Tokens per Sec:     3666, Lr: 0.000154\r\n",
      "2024-08-27 15:39:53,851 - INFO - joeynmt.training - Epoch 142, Step:    13500, Batch Loss:     0.091108, Batch Acc: 0.991296, Tokens per Sec:     3615, Lr: 0.000154\r\n",
      "2024-08-27 15:39:53,852 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=13542\r\n",
      "2024-08-27 15:39:53,852 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 66.03it/s]\r\n",
      "2024-08-27 15:40:15,966 - INFO - joeynmt.prediction - Generation took 22.1122[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 45378.68 examples/s]\r\n",
      "2024-08-27 15:40:16,350 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:40:16,350 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.66, loss:   6.91, ppl: 1004.65, acc:   0.28, 0.1702[sec]\r\n",
      "2024-08-27 15:40:16,351 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\r\n",
      "2024-08-27 15:40:16,664 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/13500.ckpt.\r\n",
      "2024-08-27 15:40:16,665 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/7500.ckpt\r\n",
      "2024-08-27 15:40:16,694 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:40:16,859 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:40:16,860 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:40:16,860 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 15:40:17,154 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:40:17,301 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:40:17,301 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:40:17,301 - INFO - joeynmt.training - \tHypothesis: trois montée de l'enclos\r\n",
      "2024-08-27 15:40:17,598 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:40:17,745 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:40:17,746 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:40:17,746 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 15:40:18,042 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:40:18,189 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:40:18,189 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:40:18,189 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 15:40:19,579 - INFO - joeynmt.training - Epoch 142, Step:    13505, Batch Loss:     0.077947, Batch Acc: 0.987025, Tokens per Sec:     3883, Lr: 0.000154\r\n",
      "2024-08-27 15:40:20,700 - INFO - joeynmt.training - Epoch 142, Step:    13510, Batch Loss:     0.086237, Batch Acc: 0.989688, Tokens per Sec:     3898, Lr: 0.000154\r\n",
      "2024-08-27 15:40:21,835 - INFO - joeynmt.training - Epoch 142, Step:    13515, Batch Loss:     0.097556, Batch Acc: 0.986275, Tokens per Sec:     3659, Lr: 0.000154\r\n",
      "2024-08-27 15:40:22,332 - INFO - joeynmt.training - Epoch 142, total training loss: 8.96, num. of seqs: 8065, num. of tokens: 80463, 21.3821[sec]\r\n",
      "2024-08-27 15:40:22,333 - INFO - joeynmt.training - EPOCH 143\r\n",
      "2024-08-27 15:40:23,076 - INFO - joeynmt.training - Epoch 143, Step:    13520, Batch Loss:     0.078365, Batch Acc: 0.991542, Tokens per Sec:     3521, Lr: 0.000154\r\n",
      "2024-08-27 15:40:24,326 - INFO - joeynmt.training - Epoch 143, Step:    13525, Batch Loss:     0.091048, Batch Acc: 0.990454, Tokens per Sec:     3438, Lr: 0.000154\r\n",
      "2024-08-27 15:40:25,418 - INFO - joeynmt.training - Epoch 143, Step:    13530, Batch Loss:     0.091309, Batch Acc: 0.990235, Tokens per Sec:     3941, Lr: 0.000154\r\n",
      "2024-08-27 15:40:26,515 - INFO - joeynmt.training - Epoch 143, Step:    13535, Batch Loss:     0.083359, Batch Acc: 0.992959, Tokens per Sec:     3888, Lr: 0.000154\r\n",
      "2024-08-27 15:40:27,692 - INFO - joeynmt.training - Epoch 143, Step:    13540, Batch Loss:     0.080113, Batch Acc: 0.991005, Tokens per Sec:     3778, Lr: 0.000154\r\n",
      "2024-08-27 15:40:28,894 - INFO - joeynmt.training - Epoch 143, Step:    13545, Batch Loss:     0.075934, Batch Acc: 0.991328, Tokens per Sec:     3361, Lr: 0.000154\r\n",
      "2024-08-27 15:40:30,006 - INFO - joeynmt.training - Epoch 143, Step:    13550, Batch Loss:     0.090061, Batch Acc: 0.990830, Tokens per Sec:     3828, Lr: 0.000154\r\n",
      "2024-08-27 15:40:31,105 - INFO - joeynmt.training - Epoch 143, Step:    13555, Batch Loss:     0.096135, Batch Acc: 0.990324, Tokens per Sec:     4045, Lr: 0.000154\r\n",
      "2024-08-27 15:40:32,224 - INFO - joeynmt.training - Epoch 143, Step:    13560, Batch Loss:     0.083855, Batch Acc: 0.990076, Tokens per Sec:     3786, Lr: 0.000154\r\n",
      "2024-08-27 15:40:33,324 - INFO - joeynmt.training - Epoch 143, Step:    13565, Batch Loss:     0.086599, Batch Acc: 0.988064, Tokens per Sec:     3811, Lr: 0.000154\r\n",
      "2024-08-27 15:40:34,423 - INFO - joeynmt.training - Epoch 143, Step:    13570, Batch Loss:     0.093803, Batch Acc: 0.987042, Tokens per Sec:     3653, Lr: 0.000154\r\n",
      "2024-08-27 15:40:35,541 - INFO - joeynmt.training - Epoch 143, Step:    13575, Batch Loss:     0.088353, Batch Acc: 0.988912, Tokens per Sec:     3955, Lr: 0.000154\r\n",
      "2024-08-27 15:40:36,672 - INFO - joeynmt.training - Epoch 143, Step:    13580, Batch Loss:     0.083733, Batch Acc: 0.989067, Tokens per Sec:     3641, Lr: 0.000154\r\n",
      "2024-08-27 15:40:37,809 - INFO - joeynmt.training - Epoch 143, Step:    13585, Batch Loss:     0.105038, Batch Acc: 0.987838, Tokens per Sec:     3618, Lr: 0.000153\r\n",
      "2024-08-27 15:40:38,917 - INFO - joeynmt.training - Epoch 143, Step:    13590, Batch Loss:     0.075811, Batch Acc: 0.988976, Tokens per Sec:     3688, Lr: 0.000153\r\n",
      "2024-08-27 15:40:40,041 - INFO - joeynmt.training - Epoch 143, Step:    13595, Batch Loss:     0.093001, Batch Acc: 0.987862, Tokens per Sec:     3812, Lr: 0.000153\r\n",
      "2024-08-27 15:40:41,141 - INFO - joeynmt.training - Epoch 143, Step:    13600, Batch Loss:     0.094533, Batch Acc: 0.990723, Tokens per Sec:     3826, Lr: 0.000153\r\n",
      "2024-08-27 15:40:42,246 - INFO - joeynmt.training - Epoch 143, Step:    13605, Batch Loss:     0.082401, Batch Acc: 0.987839, Tokens per Sec:     3575, Lr: 0.000153\r\n",
      "2024-08-27 15:40:43,324 - INFO - joeynmt.training - Epoch 143, Step:    13610, Batch Loss:     0.093038, Batch Acc: 0.991731, Tokens per Sec:     3706, Lr: 0.000153\r\n",
      "2024-08-27 15:40:43,925 - INFO - joeynmt.training - Epoch 143, total training loss: 8.55, num. of seqs: 8065, num. of tokens: 80463, 21.5742[sec]\r\n",
      "2024-08-27 15:40:43,926 - INFO - joeynmt.training - EPOCH 144\r\n",
      "2024-08-27 15:40:44,602 - INFO - joeynmt.training - Epoch 144, Step:    13615, Batch Loss:     0.095866, Batch Acc: 0.990287, Tokens per Sec:     3677, Lr: 0.000153\r\n",
      "2024-08-27 15:40:45,698 - INFO - joeynmt.training - Epoch 144, Step:    13620, Batch Loss:     0.099250, Batch Acc: 0.989215, Tokens per Sec:     3893, Lr: 0.000153\r\n",
      "2024-08-27 15:40:46,844 - INFO - joeynmt.training - Epoch 144, Step:    13625, Batch Loss:     0.095321, Batch Acc: 0.990650, Tokens per Sec:     3735, Lr: 0.000153\r\n",
      "2024-08-27 15:40:47,946 - INFO - joeynmt.training - Epoch 144, Step:    13630, Batch Loss:     0.102037, Batch Acc: 0.989494, Tokens per Sec:     3719, Lr: 0.000153\r\n",
      "2024-08-27 15:40:49,072 - INFO - joeynmt.training - Epoch 144, Step:    13635, Batch Loss:     0.105779, Batch Acc: 0.990736, Tokens per Sec:     3836, Lr: 0.000153\r\n",
      "2024-08-27 15:40:50,157 - INFO - joeynmt.training - Epoch 144, Step:    13640, Batch Loss:     0.091033, Batch Acc: 0.985987, Tokens per Sec:     4013, Lr: 0.000153\r\n",
      "2024-08-27 15:40:51,271 - INFO - joeynmt.training - Epoch 144, Step:    13645, Batch Loss:     0.084293, Batch Acc: 0.989796, Tokens per Sec:     3963, Lr: 0.000153\r\n",
      "2024-08-27 15:40:52,374 - INFO - joeynmt.training - Epoch 144, Step:    13650, Batch Loss:     0.088389, Batch Acc: 0.991823, Tokens per Sec:     3772, Lr: 0.000153\r\n",
      "2024-08-27 15:40:53,479 - INFO - joeynmt.training - Epoch 144, Step:    13655, Batch Loss:     0.083597, Batch Acc: 0.990229, Tokens per Sec:     3800, Lr: 0.000153\r\n",
      "2024-08-27 15:40:54,675 - INFO - joeynmt.training - Epoch 144, Step:    13660, Batch Loss:     0.080273, Batch Acc: 0.988915, Tokens per Sec:     3621, Lr: 0.000153\r\n",
      "2024-08-27 15:40:55,827 - INFO - joeynmt.training - Epoch 144, Step:    13665, Batch Loss:     0.081185, Batch Acc: 0.990343, Tokens per Sec:     3781, Lr: 0.000153\r\n",
      "2024-08-27 15:40:56,995 - INFO - joeynmt.training - Epoch 144, Step:    13670, Batch Loss:     0.100240, Batch Acc: 0.988194, Tokens per Sec:     3627, Lr: 0.000153\r\n",
      "2024-08-27 15:40:58,107 - INFO - joeynmt.training - Epoch 144, Step:    13675, Batch Loss:     0.095383, Batch Acc: 0.989924, Tokens per Sec:     3661, Lr: 0.000153\r\n",
      "2024-08-27 15:40:59,205 - INFO - joeynmt.training - Epoch 144, Step:    13680, Batch Loss:     0.099522, Batch Acc: 0.989051, Tokens per Sec:     3748, Lr: 0.000153\r\n",
      "2024-08-27 15:41:00,297 - INFO - joeynmt.training - Epoch 144, Step:    13685, Batch Loss:     0.082900, Batch Acc: 0.991304, Tokens per Sec:     3581, Lr: 0.000153\r\n",
      "2024-08-27 15:41:01,402 - INFO - joeynmt.training - Epoch 144, Step:    13690, Batch Loss:     0.093514, Batch Acc: 0.987323, Tokens per Sec:     3714, Lr: 0.000153\r\n",
      "2024-08-27 15:41:02,488 - INFO - joeynmt.training - Epoch 144, Step:    13695, Batch Loss:     0.084502, Batch Acc: 0.990869, Tokens per Sec:     3736, Lr: 0.000153\r\n",
      "2024-08-27 15:41:03,593 - INFO - joeynmt.training - Epoch 144, Step:    13700, Batch Loss:     0.090721, Batch Acc: 0.988595, Tokens per Sec:     3967, Lr: 0.000153\r\n",
      "2024-08-27 15:41:04,690 - INFO - joeynmt.training - Epoch 144, Step:    13705, Batch Loss:     0.097696, Batch Acc: 0.990716, Tokens per Sec:     3834, Lr: 0.000153\r\n",
      "2024-08-27 15:41:05,286 - INFO - joeynmt.training - Epoch 144, total training loss: 8.59, num. of seqs: 8065, num. of tokens: 80463, 21.3420[sec]\r\n",
      "2024-08-27 15:41:05,287 - INFO - joeynmt.training - EPOCH 145\r\n",
      "2024-08-27 15:41:05,953 - INFO - joeynmt.training - Epoch 145, Step:    13710, Batch Loss:     0.084826, Batch Acc: 0.990977, Tokens per Sec:     3850, Lr: 0.000153\r\n",
      "2024-08-27 15:41:07,069 - INFO - joeynmt.training - Epoch 145, Step:    13715, Batch Loss:     0.082263, Batch Acc: 0.990835, Tokens per Sec:     3620, Lr: 0.000153\r\n",
      "2024-08-27 15:41:08,160 - INFO - joeynmt.training - Epoch 145, Step:    13720, Batch Loss:     0.084446, Batch Acc: 0.989080, Tokens per Sec:     3780, Lr: 0.000153\r\n",
      "2024-08-27 15:41:09,262 - INFO - joeynmt.training - Epoch 145, Step:    13725, Batch Loss:     0.082329, Batch Acc: 0.986866, Tokens per Sec:     3939, Lr: 0.000153\r\n",
      "2024-08-27 15:41:10,366 - INFO - joeynmt.training - Epoch 145, Step:    13730, Batch Loss:     0.085947, Batch Acc: 0.991698, Tokens per Sec:     3821, Lr: 0.000153\r\n",
      "2024-08-27 15:41:11,490 - INFO - joeynmt.training - Epoch 145, Step:    13735, Batch Loss:     0.094797, Batch Acc: 0.991907, Tokens per Sec:     3739, Lr: 0.000153\r\n",
      "2024-08-27 15:41:12,618 - INFO - joeynmt.training - Epoch 145, Step:    13740, Batch Loss:     0.086455, Batch Acc: 0.992015, Tokens per Sec:     3666, Lr: 0.000153\r\n",
      "2024-08-27 15:41:13,764 - INFO - joeynmt.training - Epoch 145, Step:    13745, Batch Loss:     0.102446, Batch Acc: 0.987400, Tokens per Sec:     3815, Lr: 0.000153\r\n",
      "2024-08-27 15:41:14,890 - INFO - joeynmt.training - Epoch 145, Step:    13750, Batch Loss:     0.096902, Batch Acc: 0.990092, Tokens per Sec:     3766, Lr: 0.000153\r\n",
      "2024-08-27 15:41:16,017 - INFO - joeynmt.training - Epoch 145, Step:    13755, Batch Loss:     0.093224, Batch Acc: 0.989488, Tokens per Sec:     3886, Lr: 0.000153\r\n",
      "2024-08-27 15:41:17,175 - INFO - joeynmt.training - Epoch 145, Step:    13760, Batch Loss:     0.089489, Batch Acc: 0.989242, Tokens per Sec:     3694, Lr: 0.000152\r\n",
      "2024-08-27 15:41:18,285 - INFO - joeynmt.training - Epoch 145, Step:    13765, Batch Loss:     0.084533, Batch Acc: 0.988296, Tokens per Sec:     3851, Lr: 0.000152\r\n",
      "2024-08-27 15:41:19,377 - INFO - joeynmt.training - Epoch 145, Step:    13770, Batch Loss:     0.088047, Batch Acc: 0.990139, Tokens per Sec:     3904, Lr: 0.000152\r\n",
      "2024-08-27 15:41:20,473 - INFO - joeynmt.training - Epoch 145, Step:    13775, Batch Loss:     0.089453, Batch Acc: 0.989022, Tokens per Sec:     3740, Lr: 0.000152\r\n",
      "2024-08-27 15:41:21,580 - INFO - joeynmt.training - Epoch 145, Step:    13780, Batch Loss:     0.097672, Batch Acc: 0.990072, Tokens per Sec:     4007, Lr: 0.000152\r\n",
      "2024-08-27 15:41:22,677 - INFO - joeynmt.training - Epoch 145, Step:    13785, Batch Loss:     0.093172, Batch Acc: 0.985524, Tokens per Sec:     3843, Lr: 0.000152\r\n",
      "2024-08-27 15:41:23,764 - INFO - joeynmt.training - Epoch 145, Step:    13790, Batch Loss:     0.091506, Batch Acc: 0.991384, Tokens per Sec:     3527, Lr: 0.000152\r\n",
      "2024-08-27 15:41:24,858 - INFO - joeynmt.training - Epoch 145, Step:    13795, Batch Loss:     0.097706, Batch Acc: 0.988841, Tokens per Sec:     4016, Lr: 0.000152\r\n",
      "2024-08-27 15:41:26,085 - INFO - joeynmt.training - Epoch 145, Step:    13800, Batch Loss:     0.085042, Batch Acc: 0.991238, Tokens per Sec:     3445, Lr: 0.000152\r\n",
      "2024-08-27 15:41:26,593 - INFO - joeynmt.training - Epoch 145, total training loss: 8.63, num. of seqs: 8065, num. of tokens: 80463, 21.2885[sec]\r\n",
      "2024-08-27 15:41:26,593 - INFO - joeynmt.training - EPOCH 146\r\n",
      "2024-08-27 15:41:27,281 - INFO - joeynmt.training - Epoch 146, Step:    13805, Batch Loss:     0.100408, Batch Acc: 0.990340, Tokens per Sec:     3785, Lr: 0.000152\r\n",
      "2024-08-27 15:41:28,378 - INFO - joeynmt.training - Epoch 146, Step:    13810, Batch Loss:     0.074077, Batch Acc: 0.992970, Tokens per Sec:     3762, Lr: 0.000152\r\n",
      "2024-08-27 15:41:29,481 - INFO - joeynmt.training - Epoch 146, Step:    13815, Batch Loss:     0.087094, Batch Acc: 0.989247, Tokens per Sec:     3882, Lr: 0.000152\r\n",
      "2024-08-27 15:41:30,564 - INFO - joeynmt.training - Epoch 146, Step:    13820, Batch Loss:     0.091679, Batch Acc: 0.991502, Tokens per Sec:     4020, Lr: 0.000152\r\n",
      "2024-08-27 15:41:31,667 - INFO - joeynmt.training - Epoch 146, Step:    13825, Batch Loss:     0.106621, Batch Acc: 0.989970, Tokens per Sec:     3891, Lr: 0.000152\r\n",
      "2024-08-27 15:41:32,797 - INFO - joeynmt.training - Epoch 146, Step:    13830, Batch Loss:     0.096701, Batch Acc: 0.988259, Tokens per Sec:     3920, Lr: 0.000152\r\n",
      "2024-08-27 15:41:33,889 - INFO - joeynmt.training - Epoch 146, Step:    13835, Batch Loss:     0.069445, Batch Acc: 0.991876, Tokens per Sec:     3611, Lr: 0.000152\r\n",
      "2024-08-27 15:41:34,996 - INFO - joeynmt.training - Epoch 146, Step:    13840, Batch Loss:     0.089158, Batch Acc: 0.992509, Tokens per Sec:     3863, Lr: 0.000152\r\n",
      "2024-08-27 15:41:36,105 - INFO - joeynmt.training - Epoch 146, Step:    13845, Batch Loss:     0.091425, Batch Acc: 0.990711, Tokens per Sec:     3882, Lr: 0.000152\r\n",
      "2024-08-27 15:41:37,252 - INFO - joeynmt.training - Epoch 146, Step:    13850, Batch Loss:     0.106140, Batch Acc: 0.990203, Tokens per Sec:     3831, Lr: 0.000152\r\n",
      "2024-08-27 15:41:38,362 - INFO - joeynmt.training - Epoch 146, Step:    13855, Batch Loss:     0.099866, Batch Acc: 0.988450, Tokens per Sec:     3902, Lr: 0.000152\r\n",
      "2024-08-27 15:41:39,459 - INFO - joeynmt.training - Epoch 146, Step:    13860, Batch Loss:     0.073799, Batch Acc: 0.989558, Tokens per Sec:     3755, Lr: 0.000152\r\n",
      "2024-08-27 15:41:40,558 - INFO - joeynmt.training - Epoch 146, Step:    13865, Batch Loss:     0.075522, Batch Acc: 0.988372, Tokens per Sec:     3604, Lr: 0.000152\r\n",
      "2024-08-27 15:41:41,668 - INFO - joeynmt.training - Epoch 146, Step:    13870, Batch Loss:     0.087544, Batch Acc: 0.990767, Tokens per Sec:     3806, Lr: 0.000152\r\n",
      "2024-08-27 15:41:42,778 - INFO - joeynmt.training - Epoch 146, Step:    13875, Batch Loss:     0.091876, Batch Acc: 0.987873, Tokens per Sec:     3567, Lr: 0.000152\r\n",
      "2024-08-27 15:41:43,917 - INFO - joeynmt.training - Epoch 146, Step:    13880, Batch Loss:     0.090307, Batch Acc: 0.991497, Tokens per Sec:     3718, Lr: 0.000152\r\n",
      "2024-08-27 15:41:45,044 - INFO - joeynmt.training - Epoch 146, Step:    13885, Batch Loss:     0.100419, Batch Acc: 0.988274, Tokens per Sec:     3788, Lr: 0.000152\r\n",
      "2024-08-27 15:41:46,136 - INFO - joeynmt.training - Epoch 146, Step:    13890, Batch Loss:     0.079670, Batch Acc: 0.991576, Tokens per Sec:     3807, Lr: 0.000152\r\n",
      "2024-08-27 15:41:47,264 - INFO - joeynmt.training - Epoch 146, Step:    13895, Batch Loss:     0.091910, Batch Acc: 0.988849, Tokens per Sec:     3738, Lr: 0.000152\r\n",
      "2024-08-27 15:41:47,808 - INFO - joeynmt.training - Epoch 146, total training loss: 8.40, num. of seqs: 8065, num. of tokens: 80463, 21.1986[sec]\r\n",
      "2024-08-27 15:41:47,808 - INFO - joeynmt.training - EPOCH 147\r\n",
      "2024-08-27 15:41:48,475 - INFO - joeynmt.training - Epoch 147, Step:    13900, Batch Loss:     0.086746, Batch Acc: 0.992509, Tokens per Sec:     3631, Lr: 0.000152\r\n",
      "2024-08-27 15:41:49,563 - INFO - joeynmt.training - Epoch 147, Step:    13905, Batch Loss:     0.086908, Batch Acc: 0.993924, Tokens per Sec:     3936, Lr: 0.000152\r\n",
      "2024-08-27 15:41:50,634 - INFO - joeynmt.training - Epoch 147, Step:    13910, Batch Loss:     0.089420, Batch Acc: 0.991313, Tokens per Sec:     3657, Lr: 0.000152\r\n",
      "2024-08-27 15:41:51,710 - INFO - joeynmt.training - Epoch 147, Step:    13915, Batch Loss:     0.082981, Batch Acc: 0.991803, Tokens per Sec:     3856, Lr: 0.000152\r\n",
      "2024-08-27 15:41:52,808 - INFO - joeynmt.training - Epoch 147, Step:    13920, Batch Loss:     0.079709, Batch Acc: 0.990051, Tokens per Sec:     3941, Lr: 0.000152\r\n",
      "2024-08-27 15:41:53,905 - INFO - joeynmt.training - Epoch 147, Step:    13925, Batch Loss:     0.097568, Batch Acc: 0.989864, Tokens per Sec:     3958, Lr: 0.000152\r\n",
      "2024-08-27 15:41:54,980 - INFO - joeynmt.training - Epoch 147, Step:    13930, Batch Loss:     0.095434, Batch Acc: 0.990649, Tokens per Sec:     3682, Lr: 0.000152\r\n",
      "2024-08-27 15:41:56,087 - INFO - joeynmt.training - Epoch 147, Step:    13935, Batch Loss:     0.092524, Batch Acc: 0.989558, Tokens per Sec:     3724, Lr: 0.000152\r\n",
      "2024-08-27 15:41:57,321 - INFO - joeynmt.training - Epoch 147, Step:    13940, Batch Loss:     0.076278, Batch Acc: 0.989952, Tokens per Sec:     3551, Lr: 0.000152\r\n",
      "2024-08-27 15:41:58,433 - INFO - joeynmt.training - Epoch 147, Step:    13945, Batch Loss:     0.095884, Batch Acc: 0.989630, Tokens per Sec:     3643, Lr: 0.000151\r\n",
      "2024-08-27 15:41:59,535 - INFO - joeynmt.training - Epoch 147, Step:    13950, Batch Loss:     0.085397, Batch Acc: 0.992330, Tokens per Sec:     3791, Lr: 0.000151\r\n",
      "2024-08-27 15:42:00,664 - INFO - joeynmt.training - Epoch 147, Step:    13955, Batch Loss:     0.078992, Batch Acc: 0.989433, Tokens per Sec:     3940, Lr: 0.000151\r\n",
      "2024-08-27 15:42:01,824 - INFO - joeynmt.training - Epoch 147, Step:    13960, Batch Loss:     0.089614, Batch Acc: 0.991420, Tokens per Sec:     3621, Lr: 0.000151\r\n",
      "2024-08-27 15:42:03,017 - INFO - joeynmt.training - Epoch 147, Step:    13965, Batch Loss:     0.085288, Batch Acc: 0.988750, Tokens per Sec:     3281, Lr: 0.000151\r\n",
      "2024-08-27 15:42:04,129 - INFO - joeynmt.training - Epoch 147, Step:    13970, Batch Loss:     0.091469, Batch Acc: 0.990575, Tokens per Sec:     4009, Lr: 0.000151\r\n",
      "2024-08-27 15:42:05,243 - INFO - joeynmt.training - Epoch 147, Step:    13975, Batch Loss:     0.091558, Batch Acc: 0.989732, Tokens per Sec:     4026, Lr: 0.000151\r\n",
      "2024-08-27 15:42:06,355 - INFO - joeynmt.training - Epoch 147, Step:    13980, Batch Loss:     0.086014, Batch Acc: 0.988307, Tokens per Sec:     4001, Lr: 0.000151\r\n",
      "2024-08-27 15:42:07,484 - INFO - joeynmt.training - Epoch 147, Step:    13985, Batch Loss:     0.090064, Batch Acc: 0.987983, Tokens per Sec:     3762, Lr: 0.000151\r\n",
      "2024-08-27 15:42:08,572 - INFO - joeynmt.training - Epoch 147, Step:    13990, Batch Loss:     0.094604, Batch Acc: 0.988809, Tokens per Sec:     3943, Lr: 0.000151\r\n",
      "2024-08-27 15:42:09,070 - INFO - joeynmt.training - Epoch 147, total training loss: 8.33, num. of seqs: 8065, num. of tokens: 80463, 21.2429[sec]\r\n",
      "2024-08-27 15:42:09,070 - INFO - joeynmt.training - EPOCH 148\r\n",
      "2024-08-27 15:42:09,730 - INFO - joeynmt.training - Epoch 148, Step:    13995, Batch Loss:     0.083404, Batch Acc: 0.994827, Tokens per Sec:     3831, Lr: 0.000151\r\n",
      "2024-08-27 15:42:10,818 - INFO - joeynmt.training - Epoch 148, Step:    14000, Batch Loss:     0.086034, Batch Acc: 0.990458, Tokens per Sec:     3855, Lr: 0.000151\r\n",
      "2024-08-27 15:42:10,819 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=14042\r\n",
      "2024-08-27 15:42:10,820 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:21<00:00, 66.65it/s]\r\n",
      "2024-08-27 15:42:32,726 - INFO - joeynmt.prediction - Generation took 21.9048[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 43924.89 examples/s]\r\n",
      "2024-08-27 15:42:33,111 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:42:33,111 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.29, loss:   6.92, ppl: 1015.52, acc:   0.29, 0.1691[sec]\r\n",
      "2024-08-27 15:42:33,423 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/14000.ckpt.\r\n",
      "2024-08-27 15:42:33,424 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/8000.ckpt\r\n",
      "2024-08-27 15:42:33,449 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:42:33,598 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:42:33,598 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:42:33,598 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 15:42:33,896 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:42:34,041 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:42:34,041 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:42:34,042 - INFO - joeynmt.training - \tHypothesis: trois éléphants\r\n",
      "2024-08-27 15:42:34,333 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:42:34,481 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:42:34,481 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:42:34,481 - INFO - joeynmt.training - \tHypothesis: le groupe disparaît avec lui\r\n",
      "2024-08-27 15:42:34,777 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:42:34,924 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:42:34,924 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:42:34,924 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 15:42:36,343 - INFO - joeynmt.training - Epoch 148, Step:    14005, Batch Loss:     0.081567, Batch Acc: 0.991972, Tokens per Sec:     3768, Lr: 0.000151\r\n",
      "2024-08-27 15:42:37,471 - INFO - joeynmt.training - Epoch 148, Step:    14010, Batch Loss:     0.074685, Batch Acc: 0.993755, Tokens per Sec:     3552, Lr: 0.000151\r\n",
      "2024-08-27 15:42:38,570 - INFO - joeynmt.training - Epoch 148, Step:    14015, Batch Loss:     0.083803, Batch Acc: 0.992421, Tokens per Sec:     3722, Lr: 0.000151\r\n",
      "2024-08-27 15:42:39,687 - INFO - joeynmt.training - Epoch 148, Step:    14020, Batch Loss:     0.080290, Batch Acc: 0.991828, Tokens per Sec:     3838, Lr: 0.000151\r\n",
      "2024-08-27 15:42:40,791 - INFO - joeynmt.training - Epoch 148, Step:    14025, Batch Loss:     0.070259, Batch Acc: 0.990792, Tokens per Sec:     3937, Lr: 0.000151\r\n",
      "2024-08-27 15:42:41,903 - INFO - joeynmt.training - Epoch 148, Step:    14030, Batch Loss:     0.087435, Batch Acc: 0.990277, Tokens per Sec:     3793, Lr: 0.000151\r\n",
      "2024-08-27 15:42:43,019 - INFO - joeynmt.training - Epoch 148, Step:    14035, Batch Loss:     0.083353, Batch Acc: 0.989112, Tokens per Sec:     3623, Lr: 0.000151\r\n",
      "2024-08-27 15:42:44,139 - INFO - joeynmt.training - Epoch 148, Step:    14040, Batch Loss:     0.094672, Batch Acc: 0.990000, Tokens per Sec:     4023, Lr: 0.000151\r\n",
      "2024-08-27 15:42:45,268 - INFO - joeynmt.training - Epoch 148, Step:    14045, Batch Loss:     0.078422, Batch Acc: 0.989315, Tokens per Sec:     3814, Lr: 0.000151\r\n",
      "2024-08-27 15:42:46,373 - INFO - joeynmt.training - Epoch 148, Step:    14050, Batch Loss:     0.075839, Batch Acc: 0.992561, Tokens per Sec:     3653, Lr: 0.000151\r\n",
      "2024-08-27 15:42:47,506 - INFO - joeynmt.training - Epoch 148, Step:    14055, Batch Loss:     0.091753, Batch Acc: 0.989593, Tokens per Sec:     3818, Lr: 0.000151\r\n",
      "2024-08-27 15:42:48,600 - INFO - joeynmt.training - Epoch 148, Step:    14060, Batch Loss:     0.081319, Batch Acc: 0.990034, Tokens per Sec:     4036, Lr: 0.000151\r\n",
      "2024-08-27 15:42:49,692 - INFO - joeynmt.training - Epoch 148, Step:    14065, Batch Loss:     0.087119, Batch Acc: 0.987203, Tokens per Sec:     3940, Lr: 0.000151\r\n",
      "2024-08-27 15:42:50,775 - INFO - joeynmt.training - Epoch 148, Step:    14070, Batch Loss:     0.088730, Batch Acc: 0.990677, Tokens per Sec:     3863, Lr: 0.000151\r\n",
      "2024-08-27 15:42:51,856 - INFO - joeynmt.training - Epoch 148, Step:    14075, Batch Loss:     0.079703, Batch Acc: 0.989417, Tokens per Sec:     3764, Lr: 0.000151\r\n",
      "2024-08-27 15:42:52,957 - INFO - joeynmt.training - Epoch 148, Step:    14080, Batch Loss:     0.088114, Batch Acc: 0.992476, Tokens per Sec:     3864, Lr: 0.000151\r\n",
      "2024-08-27 15:42:54,065 - INFO - joeynmt.training - Epoch 148, Step:    14085, Batch Loss:     0.088789, Batch Acc: 0.988983, Tokens per Sec:     3850, Lr: 0.000151\r\n",
      "2024-08-27 15:42:54,644 - INFO - joeynmt.training - Epoch 148, total training loss: 8.25, num. of seqs: 8065, num. of tokens: 80463, 21.1574[sec]\r\n",
      "2024-08-27 15:42:54,644 - INFO - joeynmt.training - EPOCH 149\r\n",
      "2024-08-27 15:42:55,308 - INFO - joeynmt.training - Epoch 149, Step:    14090, Batch Loss:     0.085028, Batch Acc: 0.990789, Tokens per Sec:     3782, Lr: 0.000151\r\n",
      "2024-08-27 15:42:56,422 - INFO - joeynmt.training - Epoch 149, Step:    14095, Batch Loss:     0.080338, Batch Acc: 0.989157, Tokens per Sec:     3728, Lr: 0.000151\r\n",
      "2024-08-27 15:42:57,560 - INFO - joeynmt.training - Epoch 149, Step:    14100, Batch Loss:     0.095785, Batch Acc: 0.989379, Tokens per Sec:     3726, Lr: 0.000151\r\n",
      "2024-08-27 15:42:58,656 - INFO - joeynmt.training - Epoch 149, Step:    14105, Batch Loss:     0.084529, Batch Acc: 0.988470, Tokens per Sec:     3801, Lr: 0.000151\r\n",
      "2024-08-27 15:42:59,963 - INFO - joeynmt.training - Epoch 149, Step:    14110, Batch Loss:     0.076002, Batch Acc: 0.993522, Tokens per Sec:     3310, Lr: 0.000151\r\n",
      "2024-08-27 15:43:01,082 - INFO - joeynmt.training - Epoch 149, Step:    14115, Batch Loss:     0.072174, Batch Acc: 0.992243, Tokens per Sec:     3803, Lr: 0.000151\r\n",
      "2024-08-27 15:43:02,193 - INFO - joeynmt.training - Epoch 149, Step:    14120, Batch Loss:     0.100863, Batch Acc: 0.989777, Tokens per Sec:     3789, Lr: 0.000151\r\n",
      "2024-08-27 15:43:03,311 - INFO - joeynmt.training - Epoch 149, Step:    14125, Batch Loss:     0.109117, Batch Acc: 0.989342, Tokens per Sec:     3860, Lr: 0.000151\r\n",
      "2024-08-27 15:43:04,453 - INFO - joeynmt.training - Epoch 149, Step:    14130, Batch Loss:     0.075596, Batch Acc: 0.991262, Tokens per Sec:     3813, Lr: 0.000150\r\n",
      "2024-08-27 15:43:05,538 - INFO - joeynmt.training - Epoch 149, Step:    14135, Batch Loss:     0.080956, Batch Acc: 0.990151, Tokens per Sec:     3837, Lr: 0.000150\r\n",
      "2024-08-27 15:43:06,642 - INFO - joeynmt.training - Epoch 149, Step:    14140, Batch Loss:     0.087801, Batch Acc: 0.991683, Tokens per Sec:     3814, Lr: 0.000150\r\n",
      "2024-08-27 15:43:07,761 - INFO - joeynmt.training - Epoch 149, Step:    14145, Batch Loss:     0.093364, Batch Acc: 0.990619, Tokens per Sec:     3815, Lr: 0.000150\r\n",
      "2024-08-27 15:43:08,869 - INFO - joeynmt.training - Epoch 149, Step:    14150, Batch Loss:     0.091419, Batch Acc: 0.990788, Tokens per Sec:     3725, Lr: 0.000150\r\n",
      "2024-08-27 15:43:09,961 - INFO - joeynmt.training - Epoch 149, Step:    14155, Batch Loss:     0.072091, Batch Acc: 0.990993, Tokens per Sec:     3765, Lr: 0.000150\r\n",
      "2024-08-27 15:43:11,041 - INFO - joeynmt.training - Epoch 149, Step:    14160, Batch Loss:     0.093804, Batch Acc: 0.990550, Tokens per Sec:     3923, Lr: 0.000150\r\n",
      "2024-08-27 15:43:12,120 - INFO - joeynmt.training - Epoch 149, Step:    14165, Batch Loss:     0.074725, Batch Acc: 0.993162, Tokens per Sec:     4068, Lr: 0.000150\r\n",
      "2024-08-27 15:43:13,194 - INFO - joeynmt.training - Epoch 149, Step:    14170, Batch Loss:     0.101857, Batch Acc: 0.988118, Tokens per Sec:     3839, Lr: 0.000150\r\n",
      "2024-08-27 15:43:14,280 - INFO - joeynmt.training - Epoch 149, Step:    14175, Batch Loss:     0.088305, Batch Acc: 0.987267, Tokens per Sec:     3765, Lr: 0.000150\r\n",
      "2024-08-27 15:43:15,361 - INFO - joeynmt.training - Epoch 149, Step:    14180, Batch Loss:     0.069213, Batch Acc: 0.992160, Tokens per Sec:     3895, Lr: 0.000150\r\n",
      "2024-08-27 15:43:15,898 - INFO - joeynmt.training - Epoch 149, total training loss: 8.21, num. of seqs: 8065, num. of tokens: 80463, 21.2366[sec]\r\n",
      "2024-08-27 15:43:15,898 - INFO - joeynmt.training - EPOCH 150\r\n",
      "2024-08-27 15:43:16,569 - INFO - joeynmt.training - Epoch 150, Step:    14185, Batch Loss:     0.076824, Batch Acc: 0.990808, Tokens per Sec:     3915, Lr: 0.000150\r\n",
      "2024-08-27 15:43:17,689 - INFO - joeynmt.training - Epoch 150, Step:    14190, Batch Loss:     0.071941, Batch Acc: 0.993000, Tokens per Sec:     3703, Lr: 0.000150\r\n",
      "2024-08-27 15:43:18,776 - INFO - joeynmt.training - Epoch 150, Step:    14195, Batch Loss:     0.086344, Batch Acc: 0.990976, Tokens per Sec:     3976, Lr: 0.000150\r\n",
      "2024-08-27 15:43:19,875 - INFO - joeynmt.training - Epoch 150, Step:    14200, Batch Loss:     0.104226, Batch Acc: 0.989857, Tokens per Sec:     3953, Lr: 0.000150\r\n",
      "2024-08-27 15:43:20,976 - INFO - joeynmt.training - Epoch 150, Step:    14205, Batch Loss:     0.092952, Batch Acc: 0.990868, Tokens per Sec:     3979, Lr: 0.000150\r\n",
      "2024-08-27 15:43:22,077 - INFO - joeynmt.training - Epoch 150, Step:    14210, Batch Loss:     0.089568, Batch Acc: 0.991878, Tokens per Sec:     3804, Lr: 0.000150\r\n",
      "2024-08-27 15:43:23,165 - INFO - joeynmt.training - Epoch 150, Step:    14215, Batch Loss:     0.089596, Batch Acc: 0.991352, Tokens per Sec:     3721, Lr: 0.000150\r\n",
      "2024-08-27 15:43:24,255 - INFO - joeynmt.training - Epoch 150, Step:    14220, Batch Loss:     0.096817, Batch Acc: 0.991238, Tokens per Sec:     3982, Lr: 0.000150\r\n",
      "2024-08-27 15:43:25,372 - INFO - joeynmt.training - Epoch 150, Step:    14225, Batch Loss:     0.075642, Batch Acc: 0.991230, Tokens per Sec:     3983, Lr: 0.000150\r\n",
      "2024-08-27 15:43:26,487 - INFO - joeynmt.training - Epoch 150, Step:    14230, Batch Loss:     0.076397, Batch Acc: 0.993289, Tokens per Sec:     3878, Lr: 0.000150\r\n",
      "2024-08-27 15:43:27,627 - INFO - joeynmt.training - Epoch 150, Step:    14235, Batch Loss:     0.098271, Batch Acc: 0.989026, Tokens per Sec:     3840, Lr: 0.000150\r\n",
      "2024-08-27 15:43:28,738 - INFO - joeynmt.training - Epoch 150, Step:    14240, Batch Loss:     0.086571, Batch Acc: 0.992058, Tokens per Sec:     3968, Lr: 0.000150\r\n",
      "2024-08-27 15:43:29,835 - INFO - joeynmt.training - Epoch 150, Step:    14245, Batch Loss:     0.085547, Batch Acc: 0.988539, Tokens per Sec:     3740, Lr: 0.000150\r\n",
      "2024-08-27 15:43:31,028 - INFO - joeynmt.training - Epoch 150, Step:    14250, Batch Loss:     0.083918, Batch Acc: 0.991332, Tokens per Sec:     3386, Lr: 0.000150\r\n",
      "2024-08-27 15:43:32,117 - INFO - joeynmt.training - Epoch 150, Step:    14255, Batch Loss:     0.090049, Batch Acc: 0.990647, Tokens per Sec:     3539, Lr: 0.000150\r\n",
      "2024-08-27 15:43:33,214 - INFO - joeynmt.training - Epoch 150, Step:    14260, Batch Loss:     0.091949, Batch Acc: 0.990238, Tokens per Sec:     3645, Lr: 0.000150\r\n",
      "2024-08-27 15:43:34,304 - INFO - joeynmt.training - Epoch 150, Step:    14265, Batch Loss:     0.093831, Batch Acc: 0.989718, Tokens per Sec:     3748, Lr: 0.000150\r\n",
      "2024-08-27 15:43:35,413 - INFO - joeynmt.training - Epoch 150, Step:    14270, Batch Loss:     0.075176, Batch Acc: 0.993137, Tokens per Sec:     3683, Lr: 0.000150\r\n",
      "2024-08-27 15:43:36,502 - INFO - joeynmt.training - Epoch 150, Step:    14275, Batch Loss:     0.098770, Batch Acc: 0.991167, Tokens per Sec:     3954, Lr: 0.000150\r\n",
      "2024-08-27 15:43:37,135 - INFO - joeynmt.training - Epoch 150, total training loss: 8.02, num. of seqs: 8065, num. of tokens: 80463, 21.2192[sec]\r\n",
      "2024-08-27 15:43:37,135 - INFO - joeynmt.training - EPOCH 151\r\n",
      "2024-08-27 15:43:37,820 - INFO - joeynmt.training - Epoch 151, Step:    14280, Batch Loss:     0.080276, Batch Acc: 0.991788, Tokens per Sec:     3935, Lr: 0.000150\r\n",
      "2024-08-27 15:43:38,925 - INFO - joeynmt.training - Epoch 151, Step:    14285, Batch Loss:     0.083620, Batch Acc: 0.990691, Tokens per Sec:     3695, Lr: 0.000150\r\n",
      "2024-08-27 15:43:40,018 - INFO - joeynmt.training - Epoch 151, Step:    14290, Batch Loss:     0.084506, Batch Acc: 0.990257, Tokens per Sec:     3854, Lr: 0.000150\r\n",
      "2024-08-27 15:43:41,119 - INFO - joeynmt.training - Epoch 151, Step:    14295, Batch Loss:     0.081929, Batch Acc: 0.992460, Tokens per Sec:     3858, Lr: 0.000150\r\n",
      "2024-08-27 15:43:42,256 - INFO - joeynmt.training - Epoch 151, Step:    14300, Batch Loss:     0.084455, Batch Acc: 0.990966, Tokens per Sec:     3799, Lr: 0.000150\r\n",
      "2024-08-27 15:43:43,373 - INFO - joeynmt.training - Epoch 151, Step:    14305, Batch Loss:     0.089083, Batch Acc: 0.992113, Tokens per Sec:     3749, Lr: 0.000150\r\n",
      "2024-08-27 15:43:44,477 - INFO - joeynmt.training - Epoch 151, Step:    14310, Batch Loss:     0.085133, Batch Acc: 0.989822, Tokens per Sec:     3829, Lr: 0.000150\r\n",
      "2024-08-27 15:43:45,601 - INFO - joeynmt.training - Epoch 151, Step:    14315, Batch Loss:     0.090146, Batch Acc: 0.987819, Tokens per Sec:     3800, Lr: 0.000150\r\n",
      "2024-08-27 15:43:46,722 - INFO - joeynmt.training - Epoch 151, Step:    14320, Batch Loss:     0.077654, Batch Acc: 0.991850, Tokens per Sec:     3725, Lr: 0.000149\r\n",
      "2024-08-27 15:43:47,826 - INFO - joeynmt.training - Epoch 151, Step:    14325, Batch Loss:     0.092040, Batch Acc: 0.990526, Tokens per Sec:     3824, Lr: 0.000149\r\n",
      "2024-08-27 15:43:48,929 - INFO - joeynmt.training - Epoch 151, Step:    14330, Batch Loss:     0.086940, Batch Acc: 0.991537, Tokens per Sec:     3858, Lr: 0.000149\r\n",
      "2024-08-27 15:43:50,023 - INFO - joeynmt.training - Epoch 151, Step:    14335, Batch Loss:     0.084297, Batch Acc: 0.988730, Tokens per Sec:     3897, Lr: 0.000149\r\n",
      "2024-08-27 15:43:51,114 - INFO - joeynmt.training - Epoch 151, Step:    14340, Batch Loss:     0.099285, Batch Acc: 0.987851, Tokens per Sec:     3850, Lr: 0.000149\r\n",
      "2024-08-27 15:43:52,202 - INFO - joeynmt.training - Epoch 151, Step:    14345, Batch Loss:     0.084276, Batch Acc: 0.989454, Tokens per Sec:     3926, Lr: 0.000149\r\n",
      "2024-08-27 15:43:53,299 - INFO - joeynmt.training - Epoch 151, Step:    14350, Batch Loss:     0.094767, Batch Acc: 0.990211, Tokens per Sec:     4098, Lr: 0.000149\r\n",
      "2024-08-27 15:43:54,388 - INFO - joeynmt.training - Epoch 151, Step:    14355, Batch Loss:     0.071809, Batch Acc: 0.992667, Tokens per Sec:     4011, Lr: 0.000149\r\n",
      "2024-08-27 15:43:55,481 - INFO - joeynmt.training - Epoch 151, Step:    14360, Batch Loss:     0.074127, Batch Acc: 0.988703, Tokens per Sec:     3729, Lr: 0.000149\r\n",
      "2024-08-27 15:43:56,610 - INFO - joeynmt.training - Epoch 151, Step:    14365, Batch Loss:     0.089063, Batch Acc: 0.991388, Tokens per Sec:     3705, Lr: 0.000149\r\n",
      "2024-08-27 15:43:57,737 - INFO - joeynmt.training - Epoch 151, Step:    14370, Batch Loss:     0.077792, Batch Acc: 0.990277, Tokens per Sec:     3744, Lr: 0.000149\r\n",
      "2024-08-27 15:43:58,169 - INFO - joeynmt.training - Epoch 151, total training loss: 8.10, num. of seqs: 8065, num. of tokens: 80463, 21.0164[sec]\r\n",
      "2024-08-27 15:43:58,169 - INFO - joeynmt.training - EPOCH 152\r\n",
      "2024-08-27 15:43:58,812 - INFO - joeynmt.training - Epoch 152, Step:    14375, Batch Loss:     0.085204, Batch Acc: 0.996199, Tokens per Sec:     3704, Lr: 0.000149\r\n",
      "2024-08-27 15:43:59,891 - INFO - joeynmt.training - Epoch 152, Step:    14380, Batch Loss:     0.080545, Batch Acc: 0.992197, Tokens per Sec:     3922, Lr: 0.000149\r\n",
      "2024-08-27 15:44:00,986 - INFO - joeynmt.training - Epoch 152, Step:    14385, Batch Loss:     0.092063, Batch Acc: 0.989161, Tokens per Sec:     3879, Lr: 0.000149\r\n",
      "2024-08-27 15:44:02,144 - INFO - joeynmt.training - Epoch 152, Step:    14390, Batch Loss:     0.085896, Batch Acc: 0.989688, Tokens per Sec:     3437, Lr: 0.000149\r\n",
      "2024-08-27 15:44:03,260 - INFO - joeynmt.training - Epoch 152, Step:    14395, Batch Loss:     0.094215, Batch Acc: 0.989290, Tokens per Sec:     3851, Lr: 0.000149\r\n",
      "2024-08-27 15:44:04,382 - INFO - joeynmt.training - Epoch 152, Step:    14400, Batch Loss:     0.083948, Batch Acc: 0.992994, Tokens per Sec:     3818, Lr: 0.000149\r\n",
      "2024-08-27 15:44:05,484 - INFO - joeynmt.training - Epoch 152, Step:    14405, Batch Loss:     0.084271, Batch Acc: 0.990426, Tokens per Sec:     3795, Lr: 0.000149\r\n",
      "2024-08-27 15:44:06,609 - INFO - joeynmt.training - Epoch 152, Step:    14410, Batch Loss:     0.091371, Batch Acc: 0.989993, Tokens per Sec:     3641, Lr: 0.000149\r\n",
      "2024-08-27 15:44:07,728 - INFO - joeynmt.training - Epoch 152, Step:    14415, Batch Loss:     0.082973, Batch Acc: 0.992145, Tokens per Sec:     3757, Lr: 0.000149\r\n",
      "2024-08-27 15:44:08,846 - INFO - joeynmt.training - Epoch 152, Step:    14420, Batch Loss:     0.084838, Batch Acc: 0.993351, Tokens per Sec:     3636, Lr: 0.000149\r\n",
      "2024-08-27 15:44:09,948 - INFO - joeynmt.training - Epoch 152, Step:    14425, Batch Loss:     0.096070, Batch Acc: 0.989215, Tokens per Sec:     3872, Lr: 0.000149\r\n",
      "2024-08-27 15:44:11,047 - INFO - joeynmt.training - Epoch 152, Step:    14430, Batch Loss:     0.077234, Batch Acc: 0.991959, Tokens per Sec:     3737, Lr: 0.000149\r\n",
      "2024-08-27 15:44:12,148 - INFO - joeynmt.training - Epoch 152, Step:    14435, Batch Loss:     0.092888, Batch Acc: 0.990606, Tokens per Sec:     3678, Lr: 0.000149\r\n",
      "2024-08-27 15:44:13,263 - INFO - joeynmt.training - Epoch 152, Step:    14440, Batch Loss:     0.098814, Batch Acc: 0.990058, Tokens per Sec:     3700, Lr: 0.000149\r\n",
      "2024-08-27 15:44:14,364 - INFO - joeynmt.training - Epoch 152, Step:    14445, Batch Loss:     0.097185, Batch Acc: 0.990620, Tokens per Sec:     3682, Lr: 0.000149\r\n",
      "2024-08-27 15:44:15,461 - INFO - joeynmt.training - Epoch 152, Step:    14450, Batch Loss:     0.094727, Batch Acc: 0.989441, Tokens per Sec:     3801, Lr: 0.000149\r\n",
      "2024-08-27 15:44:16,570 - INFO - joeynmt.training - Epoch 152, Step:    14455, Batch Loss:     0.089379, Batch Acc: 0.988790, Tokens per Sec:     3862, Lr: 0.000149\r\n",
      "2024-08-27 15:44:17,695 - INFO - joeynmt.training - Epoch 152, Step:    14460, Batch Loss:     0.093399, Batch Acc: 0.990205, Tokens per Sec:     3908, Lr: 0.000149\r\n",
      "2024-08-27 15:44:18,786 - INFO - joeynmt.training - Epoch 152, Step:    14465, Batch Loss:     0.091546, Batch Acc: 0.990644, Tokens per Sec:     4016, Lr: 0.000149\r\n",
      "2024-08-27 15:44:19,498 - INFO - joeynmt.training - Epoch 152, total training loss: 8.26, num. of seqs: 8065, num. of tokens: 80463, 21.3125[sec]\r\n",
      "2024-08-27 15:44:19,499 - INFO - joeynmt.training - EPOCH 153\r\n",
      "2024-08-27 15:44:19,951 - INFO - joeynmt.training - Epoch 153, Step:    14470, Batch Loss:     0.085868, Batch Acc: 0.994135, Tokens per Sec:     3809, Lr: 0.000149\r\n",
      "2024-08-27 15:44:21,040 - INFO - joeynmt.training - Epoch 153, Step:    14475, Batch Loss:     0.085049, Batch Acc: 0.988926, Tokens per Sec:     3816, Lr: 0.000149\r\n",
      "2024-08-27 15:44:22,151 - INFO - joeynmt.training - Epoch 153, Step:    14480, Batch Loss:     0.088907, Batch Acc: 0.991698, Tokens per Sec:     3796, Lr: 0.000149\r\n",
      "2024-08-27 15:44:23,275 - INFO - joeynmt.training - Epoch 153, Step:    14485, Batch Loss:     0.078644, Batch Acc: 0.991914, Tokens per Sec:     3743, Lr: 0.000149\r\n",
      "2024-08-27 15:44:24,404 - INFO - joeynmt.training - Epoch 153, Step:    14490, Batch Loss:     0.092514, Batch Acc: 0.990501, Tokens per Sec:     4014, Lr: 0.000149\r\n",
      "2024-08-27 15:44:25,519 - INFO - joeynmt.training - Epoch 153, Step:    14495, Batch Loss:     0.074829, Batch Acc: 0.991868, Tokens per Sec:     3862, Lr: 0.000149\r\n",
      "2024-08-27 15:44:26,622 - INFO - joeynmt.training - Epoch 153, Step:    14500, Batch Loss:     0.078447, Batch Acc: 0.993556, Tokens per Sec:     3943, Lr: 0.000149\r\n",
      "2024-08-27 15:44:26,623 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=14542\r\n",
      "2024-08-27 15:44:26,623 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 64.54it/s]\r\n",
      "2024-08-27 15:44:49,246 - INFO - joeynmt.prediction - Generation took 22.6212[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44514.34 examples/s]\r\n",
      "2024-08-27 15:44:49,629 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:44:49,629 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.35, loss:   6.91, ppl: 1004.50, acc:   0.28, 0.1670[sec]\r\n",
      "2024-08-27 15:44:49,938 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/14500.ckpt.\r\n",
      "2024-08-27 15:44:49,939 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/9000.ckpt\r\n",
      "2024-08-27 15:44:49,963 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:44:50,110 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:44:50,110 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:44:50,111 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 15:44:50,405 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:44:50,552 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:44:50,552 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:44:50,552 - INFO - joeynmt.training - \tHypothesis: trois montée de l'enclos\r\n",
      "2024-08-27 15:44:50,848 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:44:50,994 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:44:50,994 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:44:50,995 - INFO - joeynmt.training - \tHypothesis: le service passagers n'est pas en activité actuellement\r\n",
      "2024-08-27 15:44:51,289 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:44:51,436 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:44:51,436 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:44:51,436 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 15:44:52,839 - INFO - joeynmt.training - Epoch 153, Step:    14505, Batch Loss:     0.078214, Batch Acc: 0.992327, Tokens per Sec:     3997, Lr: 0.000149\r\n",
      "2024-08-27 15:44:53,939 - INFO - joeynmt.training - Epoch 153, Step:    14510, Batch Loss:     0.084383, Batch Acc: 0.987725, Tokens per Sec:     4150, Lr: 0.000149\r\n",
      "2024-08-27 15:44:55,016 - INFO - joeynmt.training - Epoch 153, Step:    14515, Batch Loss:     0.086573, Batch Acc: 0.990690, Tokens per Sec:     3891, Lr: 0.000148\r\n",
      "2024-08-27 15:44:56,114 - INFO - joeynmt.training - Epoch 153, Step:    14520, Batch Loss:     0.088712, Batch Acc: 0.992228, Tokens per Sec:     3869, Lr: 0.000148\r\n",
      "2024-08-27 15:44:57,243 - INFO - joeynmt.training - Epoch 153, Step:    14525, Batch Loss:     0.070978, Batch Acc: 0.993156, Tokens per Sec:     3495, Lr: 0.000148\r\n",
      "2024-08-27 15:44:58,329 - INFO - joeynmt.training - Epoch 153, Step:    14530, Batch Loss:     0.078149, Batch Acc: 0.986776, Tokens per Sec:     3834, Lr: 0.000148\r\n",
      "2024-08-27 15:44:59,413 - INFO - joeynmt.training - Epoch 153, Step:    14535, Batch Loss:     0.073508, Batch Acc: 0.988688, Tokens per Sec:     4080, Lr: 0.000148\r\n",
      "2024-08-27 15:45:00,497 - INFO - joeynmt.training - Epoch 153, Step:    14540, Batch Loss:     0.085444, Batch Acc: 0.992295, Tokens per Sec:     3952, Lr: 0.000148\r\n",
      "2024-08-27 15:45:01,578 - INFO - joeynmt.training - Epoch 153, Step:    14545, Batch Loss:     0.089272, Batch Acc: 0.991189, Tokens per Sec:     3784, Lr: 0.000148\r\n",
      "2024-08-27 15:45:02,711 - INFO - joeynmt.training - Epoch 153, Step:    14550, Batch Loss:     0.088664, Batch Acc: 0.986706, Tokens per Sec:     3921, Lr: 0.000148\r\n",
      "2024-08-27 15:45:03,829 - INFO - joeynmt.training - Epoch 153, Step:    14555, Batch Loss:     0.083167, Batch Acc: 0.990435, Tokens per Sec:     3742, Lr: 0.000148\r\n",
      "2024-08-27 15:45:04,980 - INFO - joeynmt.training - Epoch 153, Step:    14560, Batch Loss:     0.083210, Batch Acc: 0.989352, Tokens per Sec:     3754, Lr: 0.000148\r\n",
      "2024-08-27 15:45:05,422 - INFO - joeynmt.training - Epoch 153, total training loss: 8.08, num. of seqs: 8065, num. of tokens: 80463, 20.7987[sec]\r\n",
      "2024-08-27 15:45:05,422 - INFO - joeynmt.training - EPOCH 154\r\n",
      "2024-08-27 15:45:06,089 - INFO - joeynmt.training - Epoch 154, Step:    14565, Batch Loss:     0.083174, Batch Acc: 0.993696, Tokens per Sec:     3826, Lr: 0.000148\r\n",
      "2024-08-27 15:45:07,220 - INFO - joeynmt.training - Epoch 154, Step:    14570, Batch Loss:     0.080332, Batch Acc: 0.994222, Tokens per Sec:     3675, Lr: 0.000148\r\n",
      "2024-08-27 15:45:08,342 - INFO - joeynmt.training - Epoch 154, Step:    14575, Batch Loss:     0.073036, Batch Acc: 0.991558, Tokens per Sec:     3697, Lr: 0.000148\r\n",
      "2024-08-27 15:45:09,441 - INFO - joeynmt.training - Epoch 154, Step:    14580, Batch Loss:     0.083260, Batch Acc: 0.993926, Tokens per Sec:     3747, Lr: 0.000148\r\n",
      "2024-08-27 15:45:10,549 - INFO - joeynmt.training - Epoch 154, Step:    14585, Batch Loss:     0.084547, Batch Acc: 0.990541, Tokens per Sec:     4013, Lr: 0.000148\r\n",
      "2024-08-27 15:45:11,663 - INFO - joeynmt.training - Epoch 154, Step:    14590, Batch Loss:     0.086948, Batch Acc: 0.988180, Tokens per Sec:     3798, Lr: 0.000148\r\n",
      "2024-08-27 15:45:12,770 - INFO - joeynmt.training - Epoch 154, Step:    14595, Batch Loss:     0.084803, Batch Acc: 0.992398, Tokens per Sec:     3924, Lr: 0.000148\r\n",
      "2024-08-27 15:45:13,870 - INFO - joeynmt.training - Epoch 154, Step:    14600, Batch Loss:     0.080123, Batch Acc: 0.992013, Tokens per Sec:     3871, Lr: 0.000148\r\n",
      "2024-08-27 15:45:14,959 - INFO - joeynmt.training - Epoch 154, Step:    14605, Batch Loss:     0.084676, Batch Acc: 0.993271, Tokens per Sec:     3963, Lr: 0.000148\r\n",
      "2024-08-27 15:45:16,047 - INFO - joeynmt.training - Epoch 154, Step:    14610, Batch Loss:     0.094734, Batch Acc: 0.989969, Tokens per Sec:     3848, Lr: 0.000148\r\n",
      "2024-08-27 15:45:17,158 - INFO - joeynmt.training - Epoch 154, Step:    14615, Batch Loss:     0.085991, Batch Acc: 0.991870, Tokens per Sec:     3768, Lr: 0.000148\r\n",
      "2024-08-27 15:45:18,243 - INFO - joeynmt.training - Epoch 154, Step:    14620, Batch Loss:     0.083182, Batch Acc: 0.989813, Tokens per Sec:     3892, Lr: 0.000148\r\n",
      "2024-08-27 15:45:19,360 - INFO - joeynmt.training - Epoch 154, Step:    14625, Batch Loss:     0.085975, Batch Acc: 0.992281, Tokens per Sec:     3830, Lr: 0.000148\r\n",
      "2024-08-27 15:45:20,457 - INFO - joeynmt.training - Epoch 154, Step:    14630, Batch Loss:     0.078031, Batch Acc: 0.991646, Tokens per Sec:     3713, Lr: 0.000148\r\n",
      "2024-08-27 15:45:21,572 - INFO - joeynmt.training - Epoch 154, Step:    14635, Batch Loss:     0.079615, Batch Acc: 0.992165, Tokens per Sec:     3780, Lr: 0.000148\r\n",
      "2024-08-27 15:45:22,689 - INFO - joeynmt.training - Epoch 154, Step:    14640, Batch Loss:     0.081019, Batch Acc: 0.991251, Tokens per Sec:     3787, Lr: 0.000148\r\n",
      "2024-08-27 15:45:23,789 - INFO - joeynmt.training - Epoch 154, Step:    14645, Batch Loss:     0.077766, Batch Acc: 0.990412, Tokens per Sec:     3795, Lr: 0.000148\r\n",
      "2024-08-27 15:45:24,892 - INFO - joeynmt.training - Epoch 154, Step:    14650, Batch Loss:     0.083199, Batch Acc: 0.989727, Tokens per Sec:     3886, Lr: 0.000148\r\n",
      "2024-08-27 15:45:25,985 - INFO - joeynmt.training - Epoch 154, Step:    14655, Batch Loss:     0.084958, Batch Acc: 0.990633, Tokens per Sec:     3617, Lr: 0.000148\r\n",
      "2024-08-27 15:45:26,587 - INFO - joeynmt.training - Epoch 154, total training loss: 7.88, num. of seqs: 8065, num. of tokens: 80463, 21.1488[sec]\r\n",
      "2024-08-27 15:45:26,587 - INFO - joeynmt.training - EPOCH 155\r\n",
      "2024-08-27 15:45:27,290 - INFO - joeynmt.training - Epoch 155, Step:    14660, Batch Loss:     0.086867, Batch Acc: 0.993625, Tokens per Sec:     3369, Lr: 0.000148\r\n",
      "2024-08-27 15:45:28,377 - INFO - joeynmt.training - Epoch 155, Step:    14665, Batch Loss:     0.077892, Batch Acc: 0.991369, Tokens per Sec:     3841, Lr: 0.000148\r\n",
      "2024-08-27 15:45:29,502 - INFO - joeynmt.training - Epoch 155, Step:    14670, Batch Loss:     0.077451, Batch Acc: 0.991799, Tokens per Sec:     3795, Lr: 0.000148\r\n",
      "2024-08-27 15:45:30,600 - INFO - joeynmt.training - Epoch 155, Step:    14675, Batch Loss:     0.070968, Batch Acc: 0.993745, Tokens per Sec:     3790, Lr: 0.000148\r\n",
      "2024-08-27 15:45:31,699 - INFO - joeynmt.training - Epoch 155, Step:    14680, Batch Loss:     0.083528, Batch Acc: 0.989401, Tokens per Sec:     3694, Lr: 0.000148\r\n",
      "2024-08-27 15:45:32,806 - INFO - joeynmt.training - Epoch 155, Step:    14685, Batch Loss:     0.077970, Batch Acc: 0.993967, Tokens per Sec:     3745, Lr: 0.000148\r\n",
      "2024-08-27 15:45:33,914 - INFO - joeynmt.training - Epoch 155, Step:    14690, Batch Loss:     0.080386, Batch Acc: 0.993804, Tokens per Sec:     3936, Lr: 0.000148\r\n",
      "2024-08-27 15:45:35,056 - INFO - joeynmt.training - Epoch 155, Step:    14695, Batch Loss:     0.077949, Batch Acc: 0.990656, Tokens per Sec:     3845, Lr: 0.000148\r\n",
      "2024-08-27 15:45:36,235 - INFO - joeynmt.training - Epoch 155, Step:    14700, Batch Loss:     0.080632, Batch Acc: 0.992115, Tokens per Sec:     3659, Lr: 0.000148\r\n",
      "2024-08-27 15:45:37,374 - INFO - joeynmt.training - Epoch 155, Step:    14705, Batch Loss:     0.074919, Batch Acc: 0.991770, Tokens per Sec:     3629, Lr: 0.000148\r\n",
      "2024-08-27 15:45:38,473 - INFO - joeynmt.training - Epoch 155, Step:    14710, Batch Loss:     0.077806, Batch Acc: 0.991417, Tokens per Sec:     3712, Lr: 0.000147\r\n",
      "2024-08-27 15:45:39,581 - INFO - joeynmt.training - Epoch 155, Step:    14715, Batch Loss:     0.077987, Batch Acc: 0.992512, Tokens per Sec:     3740, Lr: 0.000147\r\n",
      "2024-08-27 15:45:40,683 - INFO - joeynmt.training - Epoch 155, Step:    14720, Batch Loss:     0.083823, Batch Acc: 0.991274, Tokens per Sec:     3955, Lr: 0.000147\r\n",
      "2024-08-27 15:45:41,761 - INFO - joeynmt.training - Epoch 155, Step:    14725, Batch Loss:     0.070590, Batch Acc: 0.992757, Tokens per Sec:     3845, Lr: 0.000147\r\n",
      "2024-08-27 15:45:42,892 - INFO - joeynmt.training - Epoch 155, Step:    14730, Batch Loss:     0.081582, Batch Acc: 0.991934, Tokens per Sec:     3837, Lr: 0.000147\r\n",
      "2024-08-27 15:45:44,002 - INFO - joeynmt.training - Epoch 155, Step:    14735, Batch Loss:     0.091509, Batch Acc: 0.992726, Tokens per Sec:     3718, Lr: 0.000147\r\n",
      "2024-08-27 15:45:45,109 - INFO - joeynmt.training - Epoch 155, Step:    14740, Batch Loss:     0.083311, Batch Acc: 0.992842, Tokens per Sec:     3916, Lr: 0.000147\r\n",
      "2024-08-27 15:45:46,200 - INFO - joeynmt.training - Epoch 155, Step:    14745, Batch Loss:     0.078648, Batch Acc: 0.990191, Tokens per Sec:     3833, Lr: 0.000147\r\n",
      "2024-08-27 15:45:47,336 - INFO - joeynmt.training - Epoch 155, Step:    14750, Batch Loss:     0.082237, Batch Acc: 0.990618, Tokens per Sec:     3661, Lr: 0.000147\r\n",
      "2024-08-27 15:45:47,956 - INFO - joeynmt.training - Epoch 155, total training loss: 7.80, num. of seqs: 8065, num. of tokens: 80463, 21.3514[sec]\r\n",
      "2024-08-27 15:45:47,956 - INFO - joeynmt.training - EPOCH 156\r\n",
      "2024-08-27 15:45:48,636 - INFO - joeynmt.training - Epoch 156, Step:    14755, Batch Loss:     0.085033, Batch Acc: 0.988998, Tokens per Sec:     3634, Lr: 0.000147\r\n",
      "2024-08-27 15:45:49,735 - INFO - joeynmt.training - Epoch 156, Step:    14760, Batch Loss:     0.069836, Batch Acc: 0.992806, Tokens per Sec:     3794, Lr: 0.000147\r\n",
      "2024-08-27 15:45:50,835 - INFO - joeynmt.training - Epoch 156, Step:    14765, Batch Loss:     0.087273, Batch Acc: 0.991969, Tokens per Sec:     3740, Lr: 0.000147\r\n",
      "2024-08-27 15:45:51,928 - INFO - joeynmt.training - Epoch 156, Step:    14770, Batch Loss:     0.071511, Batch Acc: 0.992696, Tokens per Sec:     3886, Lr: 0.000147\r\n",
      "2024-08-27 15:45:53,019 - INFO - joeynmt.training - Epoch 156, Step:    14775, Batch Loss:     0.079367, Batch Acc: 0.990431, Tokens per Sec:     3834, Lr: 0.000147\r\n",
      "2024-08-27 15:45:54,089 - INFO - joeynmt.training - Epoch 156, Step:    14780, Batch Loss:     0.073768, Batch Acc: 0.993459, Tokens per Sec:     3715, Lr: 0.000147\r\n",
      "2024-08-27 15:45:55,169 - INFO - joeynmt.training - Epoch 156, Step:    14785, Batch Loss:     0.091910, Batch Acc: 0.987560, Tokens per Sec:     3876, Lr: 0.000147\r\n",
      "2024-08-27 15:45:56,261 - INFO - joeynmt.training - Epoch 156, Step:    14790, Batch Loss:     0.088461, Batch Acc: 0.989319, Tokens per Sec:     3689, Lr: 0.000147\r\n",
      "2024-08-27 15:45:57,386 - INFO - joeynmt.training - Epoch 156, Step:    14795, Batch Loss:     0.080350, Batch Acc: 0.992530, Tokens per Sec:     3690, Lr: 0.000147\r\n",
      "2024-08-27 15:45:58,461 - INFO - joeynmt.training - Epoch 156, Step:    14800, Batch Loss:     0.093135, Batch Acc: 0.991835, Tokens per Sec:     3875, Lr: 0.000147\r\n",
      "2024-08-27 15:45:59,554 - INFO - joeynmt.training - Epoch 156, Step:    14805, Batch Loss:     0.080470, Batch Acc: 0.990833, Tokens per Sec:     3595, Lr: 0.000147\r\n",
      "2024-08-27 15:46:00,645 - INFO - joeynmt.training - Epoch 156, Step:    14810, Batch Loss:     0.082597, Batch Acc: 0.989375, Tokens per Sec:     3797, Lr: 0.000147\r\n",
      "2024-08-27 15:46:01,728 - INFO - joeynmt.training - Epoch 156, Step:    14815, Batch Loss:     0.085773, Batch Acc: 0.991132, Tokens per Sec:     3962, Lr: 0.000147\r\n",
      "2024-08-27 15:46:02,827 - INFO - joeynmt.training - Epoch 156, Step:    14820, Batch Loss:     0.079493, Batch Acc: 0.991393, Tokens per Sec:     4122, Lr: 0.000147\r\n",
      "2024-08-27 15:46:03,938 - INFO - joeynmt.training - Epoch 156, Step:    14825, Batch Loss:     0.071834, Batch Acc: 0.992375, Tokens per Sec:     4015, Lr: 0.000147\r\n",
      "2024-08-27 15:46:05,033 - INFO - joeynmt.training - Epoch 156, Step:    14830, Batch Loss:     0.086245, Batch Acc: 0.991436, Tokens per Sec:     3736, Lr: 0.000147\r\n",
      "2024-08-27 15:46:06,175 - INFO - joeynmt.training - Epoch 156, Step:    14835, Batch Loss:     0.086235, Batch Acc: 0.991578, Tokens per Sec:     3644, Lr: 0.000147\r\n",
      "2024-08-27 15:46:07,390 - INFO - joeynmt.training - Epoch 156, Step:    14840, Batch Loss:     0.093009, Batch Acc: 0.991310, Tokens per Sec:     3600, Lr: 0.000147\r\n",
      "2024-08-27 15:46:08,486 - INFO - joeynmt.training - Epoch 156, Step:    14845, Batch Loss:     0.095211, Batch Acc: 0.989977, Tokens per Sec:     3916, Lr: 0.000147\r\n",
      "2024-08-27 15:46:09,188 - INFO - joeynmt.training - Epoch 156, total training loss: 8.00, num. of seqs: 8065, num. of tokens: 80463, 21.2147[sec]\r\n",
      "2024-08-27 15:46:09,188 - INFO - joeynmt.training - EPOCH 157\r\n",
      "2024-08-27 15:46:09,639 - INFO - joeynmt.training - Epoch 157, Step:    14850, Batch Loss:     0.087092, Batch Acc: 0.992420, Tokens per Sec:     3839, Lr: 0.000147\r\n",
      "2024-08-27 15:46:10,731 - INFO - joeynmt.training - Epoch 157, Step:    14855, Batch Loss:     0.078719, Batch Acc: 0.991254, Tokens per Sec:     3667, Lr: 0.000147\r\n",
      "2024-08-27 15:46:11,826 - INFO - joeynmt.training - Epoch 157, Step:    14860, Batch Loss:     0.090564, Batch Acc: 0.992843, Tokens per Sec:     3702, Lr: 0.000147\r\n",
      "2024-08-27 15:46:12,914 - INFO - joeynmt.training - Epoch 157, Step:    14865, Batch Loss:     0.081365, Batch Acc: 0.991642, Tokens per Sec:     3743, Lr: 0.000147\r\n",
      "2024-08-27 15:46:14,013 - INFO - joeynmt.training - Epoch 157, Step:    14870, Batch Loss:     0.072852, Batch Acc: 0.995048, Tokens per Sec:     4045, Lr: 0.000147\r\n",
      "2024-08-27 15:46:15,099 - INFO - joeynmt.training - Epoch 157, Step:    14875, Batch Loss:     0.080901, Batch Acc: 0.991288, Tokens per Sec:     3912, Lr: 0.000147\r\n",
      "2024-08-27 15:46:16,192 - INFO - joeynmt.training - Epoch 157, Step:    14880, Batch Loss:     0.079600, Batch Acc: 0.988541, Tokens per Sec:     3914, Lr: 0.000147\r\n",
      "2024-08-27 15:46:17,309 - INFO - joeynmt.training - Epoch 157, Step:    14885, Batch Loss:     0.078246, Batch Acc: 0.990961, Tokens per Sec:     3769, Lr: 0.000147\r\n",
      "2024-08-27 15:46:18,404 - INFO - joeynmt.training - Epoch 157, Step:    14890, Batch Loss:     0.068198, Batch Acc: 0.994058, Tokens per Sec:     3843, Lr: 0.000147\r\n",
      "2024-08-27 15:46:19,512 - INFO - joeynmt.training - Epoch 157, Step:    14895, Batch Loss:     0.071769, Batch Acc: 0.992056, Tokens per Sec:     3864, Lr: 0.000147\r\n",
      "2024-08-27 15:46:20,627 - INFO - joeynmt.training - Epoch 157, Step:    14900, Batch Loss:     0.076214, Batch Acc: 0.990907, Tokens per Sec:     3848, Lr: 0.000147\r\n",
      "2024-08-27 15:46:21,755 - INFO - joeynmt.training - Epoch 157, Step:    14905, Batch Loss:     0.077035, Batch Acc: 0.992699, Tokens per Sec:     3889, Lr: 0.000147\r\n",
      "2024-08-27 15:46:22,880 - INFO - joeynmt.training - Epoch 157, Step:    14910, Batch Loss:     0.095528, Batch Acc: 0.989615, Tokens per Sec:     3769, Lr: 0.000146\r\n",
      "2024-08-27 15:46:23,985 - INFO - joeynmt.training - Epoch 157, Step:    14915, Batch Loss:     0.082243, Batch Acc: 0.989845, Tokens per Sec:     3925, Lr: 0.000146\r\n",
      "2024-08-27 15:46:25,056 - INFO - joeynmt.training - Epoch 157, Step:    14920, Batch Loss:     0.072471, Batch Acc: 0.991985, Tokens per Sec:     3962, Lr: 0.000146\r\n",
      "2024-08-27 15:46:26,145 - INFO - joeynmt.training - Epoch 157, Step:    14925, Batch Loss:     0.081777, Batch Acc: 0.990235, Tokens per Sec:     3954, Lr: 0.000146\r\n",
      "2024-08-27 15:46:27,270 - INFO - joeynmt.training - Epoch 157, Step:    14930, Batch Loss:     0.105787, Batch Acc: 0.989357, Tokens per Sec:     3760, Lr: 0.000146\r\n",
      "2024-08-27 15:46:28,343 - INFO - joeynmt.training - Epoch 157, Step:    14935, Batch Loss:     0.082951, Batch Acc: 0.993408, Tokens per Sec:     3679, Lr: 0.000146\r\n",
      "2024-08-27 15:46:29,435 - INFO - joeynmt.training - Epoch 157, Step:    14940, Batch Loss:     0.088602, Batch Acc: 0.987694, Tokens per Sec:     3945, Lr: 0.000146\r\n",
      "2024-08-27 15:46:30,109 - INFO - joeynmt.training - Epoch 157, total training loss: 7.92, num. of seqs: 8065, num. of tokens: 80463, 20.9039[sec]\r\n",
      "2024-08-27 15:46:30,109 - INFO - joeynmt.training - EPOCH 158\r\n",
      "2024-08-27 15:46:30,552 - INFO - joeynmt.training - Epoch 158, Step:    14945, Batch Loss:     0.092262, Batch Acc: 0.990313, Tokens per Sec:     3993, Lr: 0.000146\r\n",
      "2024-08-27 15:46:31,649 - INFO - joeynmt.training - Epoch 158, Step:    14950, Batch Loss:     0.095330, Batch Acc: 0.991546, Tokens per Sec:     3670, Lr: 0.000146\r\n",
      "2024-08-27 15:46:32,759 - INFO - joeynmt.training - Epoch 158, Step:    14955, Batch Loss:     0.083283, Batch Acc: 0.992117, Tokens per Sec:     3771, Lr: 0.000146\r\n",
      "2024-08-27 15:46:33,857 - INFO - joeynmt.training - Epoch 158, Step:    14960, Batch Loss:     0.078877, Batch Acc: 0.991234, Tokens per Sec:     3953, Lr: 0.000146\r\n",
      "2024-08-27 15:46:34,954 - INFO - joeynmt.training - Epoch 158, Step:    14965, Batch Loss:     0.079764, Batch Acc: 0.990652, Tokens per Sec:     4000, Lr: 0.000146\r\n",
      "2024-08-27 15:46:36,053 - INFO - joeynmt.training - Epoch 158, Step:    14970, Batch Loss:     0.085705, Batch Acc: 0.992804, Tokens per Sec:     3669, Lr: 0.000146\r\n",
      "2024-08-27 15:46:37,190 - INFO - joeynmt.training - Epoch 158, Step:    14975, Batch Loss:     0.080346, Batch Acc: 0.993884, Tokens per Sec:     3743, Lr: 0.000146\r\n",
      "2024-08-27 15:46:38,460 - INFO - joeynmt.training - Epoch 158, Step:    14980, Batch Loss:     0.084297, Batch Acc: 0.992195, Tokens per Sec:     3230, Lr: 0.000146\r\n",
      "2024-08-27 15:46:39,547 - INFO - joeynmt.training - Epoch 158, Step:    14985, Batch Loss:     0.084767, Batch Acc: 0.992231, Tokens per Sec:     3674, Lr: 0.000146\r\n",
      "2024-08-27 15:46:40,653 - INFO - joeynmt.training - Epoch 158, Step:    14990, Batch Loss:     0.082142, Batch Acc: 0.989287, Tokens per Sec:     3713, Lr: 0.000146\r\n",
      "2024-08-27 15:46:41,756 - INFO - joeynmt.training - Epoch 158, Step:    14995, Batch Loss:     0.084558, Batch Acc: 0.990155, Tokens per Sec:     3688, Lr: 0.000146\r\n",
      "2024-08-27 15:46:42,864 - INFO - joeynmt.training - Epoch 158, Step:    15000, Batch Loss:     0.079957, Batch Acc: 0.988949, Tokens per Sec:     3841, Lr: 0.000146\r\n",
      "2024-08-27 15:46:42,864 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=15042\r\n",
      "2024-08-27 15:46:42,865 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 64.69it/s]\r\n",
      "2024-08-27 15:47:05,436 - INFO - joeynmt.prediction - Generation took 22.5690[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44341.27 examples/s]\r\n",
      "2024-08-27 15:47:05,820 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:47:05,820 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.24, loss:   6.92, ppl: 1010.09, acc:   0.28, 0.1687[sec]\r\n",
      "2024-08-27 15:47:05,821 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:47:05,968 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:47:05,968 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:47:05,968 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 15:47:06,260 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:47:06,407 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:47:06,407 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:47:06,407 - INFO - joeynmt.training - \tHypothesis: trois tables blanches\r\n",
      "2024-08-27 15:47:06,709 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:47:06,864 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:47:06,865 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:47:06,865 - INFO - joeynmt.training - \tHypothesis: le service passagers n'est pas en activité actuellement\r\n",
      "2024-08-27 15:47:07,157 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:47:07,302 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:47:07,303 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:47:07,303 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 15:47:08,739 - INFO - joeynmt.training - Epoch 158, Step:    15005, Batch Loss:     0.089664, Batch Acc: 0.990531, Tokens per Sec:     3512, Lr: 0.000146\r\n",
      "2024-08-27 15:47:09,890 - INFO - joeynmt.training - Epoch 158, Step:    15010, Batch Loss:     0.072401, Batch Acc: 0.992966, Tokens per Sec:     3829, Lr: 0.000146\r\n",
      "2024-08-27 15:47:10,978 - INFO - joeynmt.training - Epoch 158, Step:    15015, Batch Loss:     0.081332, Batch Acc: 0.989998, Tokens per Sec:     3771, Lr: 0.000146\r\n",
      "2024-08-27 15:47:12,077 - INFO - joeynmt.training - Epoch 158, Step:    15020, Batch Loss:     0.082355, Batch Acc: 0.990682, Tokens per Sec:     4006, Lr: 0.000146\r\n",
      "2024-08-27 15:47:13,167 - INFO - joeynmt.training - Epoch 158, Step:    15025, Batch Loss:     0.073615, Batch Acc: 0.992160, Tokens per Sec:     3980, Lr: 0.000146\r\n",
      "2024-08-27 15:47:14,241 - INFO - joeynmt.training - Epoch 158, Step:    15030, Batch Loss:     0.083243, Batch Acc: 0.991126, Tokens per Sec:     4097, Lr: 0.000146\r\n",
      "2024-08-27 15:47:15,337 - INFO - joeynmt.training - Epoch 158, Step:    15035, Batch Loss:     0.092570, Batch Acc: 0.989645, Tokens per Sec:     3879, Lr: 0.000146\r\n",
      "2024-08-27 15:47:16,153 - INFO - joeynmt.training - Epoch 158, total training loss: 7.89, num. of seqs: 8065, num. of tokens: 80463, 21.2955[sec]\r\n",
      "2024-08-27 15:47:16,153 - INFO - joeynmt.training - EPOCH 159\r\n",
      "2024-08-27 15:47:16,593 - INFO - joeynmt.training - Epoch 159, Step:    15040, Batch Loss:     0.074145, Batch Acc: 0.993602, Tokens per Sec:     3595, Lr: 0.000146\r\n",
      "2024-08-27 15:47:17,695 - INFO - joeynmt.training - Epoch 159, Step:    15045, Batch Loss:     0.076418, Batch Acc: 0.990518, Tokens per Sec:     3733, Lr: 0.000146\r\n",
      "2024-08-27 15:47:18,780 - INFO - joeynmt.training - Epoch 159, Step:    15050, Batch Loss:     0.095994, Batch Acc: 0.990833, Tokens per Sec:     3722, Lr: 0.000146\r\n",
      "2024-08-27 15:47:19,877 - INFO - joeynmt.training - Epoch 159, Step:    15055, Batch Loss:     0.081239, Batch Acc: 0.992318, Tokens per Sec:     3920, Lr: 0.000146\r\n",
      "2024-08-27 15:47:20,966 - INFO - joeynmt.training - Epoch 159, Step:    15060, Batch Loss:     0.082522, Batch Acc: 0.995017, Tokens per Sec:     3870, Lr: 0.000146\r\n",
      "2024-08-27 15:47:22,065 - INFO - joeynmt.training - Epoch 159, Step:    15065, Batch Loss:     0.084564, Batch Acc: 0.993481, Tokens per Sec:     3913, Lr: 0.000146\r\n",
      "2024-08-27 15:47:23,161 - INFO - joeynmt.training - Epoch 159, Step:    15070, Batch Loss:     0.078532, Batch Acc: 0.992717, Tokens per Sec:     4009, Lr: 0.000146\r\n",
      "2024-08-27 15:47:24,250 - INFO - joeynmt.training - Epoch 159, Step:    15075, Batch Loss:     0.080669, Batch Acc: 0.994147, Tokens per Sec:     3927, Lr: 0.000146\r\n",
      "2024-08-27 15:47:25,343 - INFO - joeynmt.training - Epoch 159, Step:    15080, Batch Loss:     0.079383, Batch Acc: 0.990637, Tokens per Sec:     4008, Lr: 0.000146\r\n",
      "2024-08-27 15:47:26,452 - INFO - joeynmt.training - Epoch 159, Step:    15085, Batch Loss:     0.082209, Batch Acc: 0.993472, Tokens per Sec:     3732, Lr: 0.000146\r\n",
      "2024-08-27 15:47:27,582 - INFO - joeynmt.training - Epoch 159, Step:    15090, Batch Loss:     0.086214, Batch Acc: 0.992483, Tokens per Sec:     3533, Lr: 0.000146\r\n",
      "2024-08-27 15:47:28,695 - INFO - joeynmt.training - Epoch 159, Step:    15095, Batch Loss:     0.074716, Batch Acc: 0.993382, Tokens per Sec:     3803, Lr: 0.000146\r\n",
      "2024-08-27 15:47:29,828 - INFO - joeynmt.training - Epoch 159, Step:    15100, Batch Loss:     0.089720, Batch Acc: 0.990192, Tokens per Sec:     4143, Lr: 0.000146\r\n",
      "2024-08-27 15:47:30,927 - INFO - joeynmt.training - Epoch 159, Step:    15105, Batch Loss:     0.080528, Batch Acc: 0.990424, Tokens per Sec:     3803, Lr: 0.000146\r\n",
      "2024-08-27 15:47:32,027 - INFO - joeynmt.training - Epoch 159, Step:    15110, Batch Loss:     0.084533, Batch Acc: 0.990459, Tokens per Sec:     3626, Lr: 0.000146\r\n",
      "2024-08-27 15:47:33,134 - INFO - joeynmt.training - Epoch 159, Step:    15115, Batch Loss:     0.100981, Batch Acc: 0.992108, Tokens per Sec:     3893, Lr: 0.000146\r\n",
      "2024-08-27 15:47:34,231 - INFO - joeynmt.training - Epoch 159, Step:    15120, Batch Loss:     0.080754, Batch Acc: 0.992455, Tokens per Sec:     3988, Lr: 0.000145\r\n",
      "2024-08-27 15:47:35,308 - INFO - joeynmt.training - Epoch 159, Step:    15125, Batch Loss:     0.074070, Batch Acc: 0.993612, Tokens per Sec:     3928, Lr: 0.000145\r\n",
      "2024-08-27 15:47:36,402 - INFO - joeynmt.training - Epoch 159, Step:    15130, Batch Loss:     0.097224, Batch Acc: 0.990989, Tokens per Sec:     3960, Lr: 0.000145\r\n",
      "2024-08-27 15:47:37,079 - INFO - joeynmt.training - Epoch 159, total training loss: 7.63, num. of seqs: 8065, num. of tokens: 80463, 20.9083[sec]\r\n",
      "2024-08-27 15:47:37,079 - INFO - joeynmt.training - EPOCH 160\r\n",
      "2024-08-27 15:47:37,514 - INFO - joeynmt.training - Epoch 160, Step:    15135, Batch Loss:     0.078274, Batch Acc: 0.993080, Tokens per Sec:     4025, Lr: 0.000145\r\n",
      "2024-08-27 15:47:38,598 - INFO - joeynmt.training - Epoch 160, Step:    15140, Batch Loss:     0.067863, Batch Acc: 0.995206, Tokens per Sec:     3851, Lr: 0.000145\r\n",
      "2024-08-27 15:47:39,707 - INFO - joeynmt.training - Epoch 160, Step:    15145, Batch Loss:     0.072072, Batch Acc: 0.992481, Tokens per Sec:     3841, Lr: 0.000145\r\n",
      "2024-08-27 15:47:40,858 - INFO - joeynmt.training - Epoch 160, Step:    15150, Batch Loss:     0.077000, Batch Acc: 0.994183, Tokens per Sec:     3736, Lr: 0.000145\r\n",
      "2024-08-27 15:47:41,945 - INFO - joeynmt.training - Epoch 160, Step:    15155, Batch Loss:     0.087005, Batch Acc: 0.991445, Tokens per Sec:     3873, Lr: 0.000145\r\n",
      "2024-08-27 15:47:43,022 - INFO - joeynmt.training - Epoch 160, Step:    15160, Batch Loss:     0.071313, Batch Acc: 0.994882, Tokens per Sec:     3812, Lr: 0.000145\r\n",
      "2024-08-27 15:47:44,107 - INFO - joeynmt.training - Epoch 160, Step:    15165, Batch Loss:     0.078700, Batch Acc: 0.990787, Tokens per Sec:     3704, Lr: 0.000145\r\n",
      "2024-08-27 15:47:45,205 - INFO - joeynmt.training - Epoch 160, Step:    15170, Batch Loss:     0.067911, Batch Acc: 0.991969, Tokens per Sec:     3745, Lr: 0.000145\r\n",
      "2024-08-27 15:47:46,296 - INFO - joeynmt.training - Epoch 160, Step:    15175, Batch Loss:     0.092074, Batch Acc: 0.991663, Tokens per Sec:     3960, Lr: 0.000145\r\n",
      "2024-08-27 15:47:47,441 - INFO - joeynmt.training - Epoch 160, Step:    15180, Batch Loss:     0.097639, Batch Acc: 0.990867, Tokens per Sec:     3732, Lr: 0.000145\r\n",
      "2024-08-27 15:47:48,539 - INFO - joeynmt.training - Epoch 160, Step:    15185, Batch Loss:     0.077879, Batch Acc: 0.992616, Tokens per Sec:     3827, Lr: 0.000145\r\n",
      "2024-08-27 15:47:49,632 - INFO - joeynmt.training - Epoch 160, Step:    15190, Batch Loss:     0.082268, Batch Acc: 0.991521, Tokens per Sec:     3887, Lr: 0.000145\r\n",
      "2024-08-27 15:47:50,743 - INFO - joeynmt.training - Epoch 160, Step:    15195, Batch Loss:     0.067605, Batch Acc: 0.993932, Tokens per Sec:     3860, Lr: 0.000145\r\n",
      "2024-08-27 15:47:51,831 - INFO - joeynmt.training - Epoch 160, Step:    15200, Batch Loss:     0.093798, Batch Acc: 0.991501, Tokens per Sec:     3896, Lr: 0.000145\r\n",
      "2024-08-27 15:47:52,930 - INFO - joeynmt.training - Epoch 160, Step:    15205, Batch Loss:     0.079015, Batch Acc: 0.992147, Tokens per Sec:     4057, Lr: 0.000145\r\n",
      "2024-08-27 15:47:54,017 - INFO - joeynmt.training - Epoch 160, Step:    15210, Batch Loss:     0.079338, Batch Acc: 0.990946, Tokens per Sec:     3863, Lr: 0.000145\r\n",
      "2024-08-27 15:47:55,092 - INFO - joeynmt.training - Epoch 160, Step:    15215, Batch Loss:     0.085286, Batch Acc: 0.989258, Tokens per Sec:     3726, Lr: 0.000145\r\n",
      "2024-08-27 15:47:56,198 - INFO - joeynmt.training - Epoch 160, Step:    15220, Batch Loss:     0.084804, Batch Acc: 0.992539, Tokens per Sec:     3881, Lr: 0.000145\r\n",
      "2024-08-27 15:47:57,329 - INFO - joeynmt.training - Epoch 160, Step:    15225, Batch Loss:     0.081273, Batch Acc: 0.990987, Tokens per Sec:     3829, Lr: 0.000145\r\n",
      "2024-08-27 15:47:58,079 - INFO - joeynmt.training - Epoch 160, total training loss: 7.71, num. of seqs: 8065, num. of tokens: 80463, 20.9821[sec]\r\n",
      "2024-08-27 15:47:58,079 - INFO - joeynmt.training - EPOCH 161\r\n",
      "2024-08-27 15:47:58,536 - INFO - joeynmt.training - Epoch 161, Step:    15230, Batch Loss:     0.064109, Batch Acc: 0.994616, Tokens per Sec:     3284, Lr: 0.000145\r\n",
      "2024-08-27 15:47:59,629 - INFO - joeynmt.training - Epoch 161, Step:    15235, Batch Loss:     0.068568, Batch Acc: 0.994200, Tokens per Sec:     4103, Lr: 0.000145\r\n",
      "2024-08-27 15:48:00,723 - INFO - joeynmt.training - Epoch 161, Step:    15240, Batch Loss:     0.089843, Batch Acc: 0.990695, Tokens per Sec:     4032, Lr: 0.000145\r\n",
      "2024-08-27 15:48:01,839 - INFO - joeynmt.training - Epoch 161, Step:    15245, Batch Loss:     0.074381, Batch Acc: 0.990997, Tokens per Sec:     3882, Lr: 0.000145\r\n",
      "2024-08-27 15:48:02,936 - INFO - joeynmt.training - Epoch 161, Step:    15250, Batch Loss:     0.081521, Batch Acc: 0.993364, Tokens per Sec:     3986, Lr: 0.000145\r\n",
      "2024-08-27 15:48:04,030 - INFO - joeynmt.training - Epoch 161, Step:    15255, Batch Loss:     0.068016, Batch Acc: 0.993986, Tokens per Sec:     3801, Lr: 0.000145\r\n",
      "2024-08-27 15:48:05,092 - INFO - joeynmt.training - Epoch 161, Step:    15260, Batch Loss:     0.074055, Batch Acc: 0.993156, Tokens per Sec:     3856, Lr: 0.000145\r\n",
      "2024-08-27 15:48:06,186 - INFO - joeynmt.training - Epoch 161, Step:    15265, Batch Loss:     0.081515, Batch Acc: 0.990194, Tokens per Sec:     3918, Lr: 0.000145\r\n",
      "2024-08-27 15:48:07,303 - INFO - joeynmt.training - Epoch 161, Step:    15270, Batch Loss:     0.061219, Batch Acc: 0.993224, Tokens per Sec:     3835, Lr: 0.000145\r\n",
      "2024-08-27 15:48:08,390 - INFO - joeynmt.training - Epoch 161, Step:    15275, Batch Loss:     0.082854, Batch Acc: 0.991550, Tokens per Sec:     3813, Lr: 0.000145\r\n",
      "2024-08-27 15:48:09,473 - INFO - joeynmt.training - Epoch 161, Step:    15280, Batch Loss:     0.092777, Batch Acc: 0.991323, Tokens per Sec:     3835, Lr: 0.000145\r\n",
      "2024-08-27 15:48:10,576 - INFO - joeynmt.training - Epoch 161, Step:    15285, Batch Loss:     0.069335, Batch Acc: 0.992530, Tokens per Sec:     3641, Lr: 0.000145\r\n",
      "2024-08-27 15:48:11,815 - INFO - joeynmt.training - Epoch 161, Step:    15290, Batch Loss:     0.077695, Batch Acc: 0.992004, Tokens per Sec:     3534, Lr: 0.000145\r\n",
      "2024-08-27 15:48:12,921 - INFO - joeynmt.training - Epoch 161, Step:    15295, Batch Loss:     0.080670, Batch Acc: 0.992199, Tokens per Sec:     3829, Lr: 0.000145\r\n",
      "2024-08-27 15:48:14,029 - INFO - joeynmt.training - Epoch 161, Step:    15300, Batch Loss:     0.074932, Batch Acc: 0.989230, Tokens per Sec:     3939, Lr: 0.000145\r\n",
      "2024-08-27 15:48:15,131 - INFO - joeynmt.training - Epoch 161, Step:    15305, Batch Loss:     0.084125, Batch Acc: 0.992406, Tokens per Sec:     3829, Lr: 0.000145\r\n",
      "2024-08-27 15:48:16,222 - INFO - joeynmt.training - Epoch 161, Step:    15310, Batch Loss:     0.078508, Batch Acc: 0.990982, Tokens per Sec:     3864, Lr: 0.000145\r\n",
      "2024-08-27 15:48:17,354 - INFO - joeynmt.training - Epoch 161, Step:    15315, Batch Loss:     0.074442, Batch Acc: 0.991641, Tokens per Sec:     4018, Lr: 0.000145\r\n",
      "2024-08-27 15:48:18,458 - INFO - joeynmt.training - Epoch 161, Step:    15320, Batch Loss:     0.077682, Batch Acc: 0.992816, Tokens per Sec:     3787, Lr: 0.000145\r\n",
      "2024-08-27 15:48:19,050 - INFO - joeynmt.training - Epoch 161, total training loss: 7.52, num. of seqs: 8065, num. of tokens: 80463, 20.9534[sec]\r\n",
      "2024-08-27 15:48:19,050 - INFO - joeynmt.training - EPOCH 162\r\n",
      "2024-08-27 15:48:19,712 - INFO - joeynmt.training - Epoch 162, Step:    15325, Batch Loss:     0.088540, Batch Acc: 0.992357, Tokens per Sec:     3779, Lr: 0.000145\r\n",
      "2024-08-27 15:48:20,802 - INFO - joeynmt.training - Epoch 162, Step:    15330, Batch Loss:     0.080750, Batch Acc: 0.992893, Tokens per Sec:     3876, Lr: 0.000144\r\n",
      "2024-08-27 15:48:21,899 - INFO - joeynmt.training - Epoch 162, Step:    15335, Batch Loss:     0.079936, Batch Acc: 0.993578, Tokens per Sec:     3974, Lr: 0.000144\r\n",
      "2024-08-27 15:48:23,002 - INFO - joeynmt.training - Epoch 162, Step:    15340, Batch Loss:     0.080018, Batch Acc: 0.992457, Tokens per Sec:     3970, Lr: 0.000144\r\n",
      "2024-08-27 15:48:24,083 - INFO - joeynmt.training - Epoch 162, Step:    15345, Batch Loss:     0.075578, Batch Acc: 0.991665, Tokens per Sec:     3777, Lr: 0.000144\r\n",
      "2024-08-27 15:48:25,183 - INFO - joeynmt.training - Epoch 162, Step:    15350, Batch Loss:     0.094766, Batch Acc: 0.989867, Tokens per Sec:     3769, Lr: 0.000144\r\n",
      "2024-08-27 15:48:26,261 - INFO - joeynmt.training - Epoch 162, Step:    15355, Batch Loss:     0.083348, Batch Acc: 0.991975, Tokens per Sec:     3934, Lr: 0.000144\r\n",
      "2024-08-27 15:48:27,373 - INFO - joeynmt.training - Epoch 162, Step:    15360, Batch Loss:     0.074589, Batch Acc: 0.992105, Tokens per Sec:     3647, Lr: 0.000144\r\n",
      "2024-08-27 15:48:28,480 - INFO - joeynmt.training - Epoch 162, Step:    15365, Batch Loss:     0.071701, Batch Acc: 0.990568, Tokens per Sec:     3834, Lr: 0.000144\r\n",
      "2024-08-27 15:48:29,580 - INFO - joeynmt.training - Epoch 162, Step:    15370, Batch Loss:     0.071543, Batch Acc: 0.995106, Tokens per Sec:     3716, Lr: 0.000144\r\n",
      "2024-08-27 15:48:30,678 - INFO - joeynmt.training - Epoch 162, Step:    15375, Batch Loss:     0.076945, Batch Acc: 0.992244, Tokens per Sec:     3760, Lr: 0.000144\r\n",
      "2024-08-27 15:48:31,780 - INFO - joeynmt.training - Epoch 162, Step:    15380, Batch Loss:     0.085476, Batch Acc: 0.991860, Tokens per Sec:     3907, Lr: 0.000144\r\n",
      "2024-08-27 15:48:32,882 - INFO - joeynmt.training - Epoch 162, Step:    15385, Batch Loss:     0.088278, Batch Acc: 0.990903, Tokens per Sec:     3889, Lr: 0.000144\r\n",
      "2024-08-27 15:48:33,981 - INFO - joeynmt.training - Epoch 162, Step:    15390, Batch Loss:     0.094080, Batch Acc: 0.989001, Tokens per Sec:     3891, Lr: 0.000144\r\n",
      "2024-08-27 15:48:35,070 - INFO - joeynmt.training - Epoch 162, Step:    15395, Batch Loss:     0.072034, Batch Acc: 0.989624, Tokens per Sec:     3808, Lr: 0.000144\r\n",
      "2024-08-27 15:48:36,136 - INFO - joeynmt.training - Epoch 162, Step:    15400, Batch Loss:     0.082995, Batch Acc: 0.991046, Tokens per Sec:     3670, Lr: 0.000144\r\n",
      "2024-08-27 15:48:37,263 - INFO - joeynmt.training - Epoch 162, Step:    15405, Batch Loss:     0.096240, Batch Acc: 0.990657, Tokens per Sec:     3613, Lr: 0.000144\r\n",
      "2024-08-27 15:48:38,343 - INFO - joeynmt.training - Epoch 162, Step:    15410, Batch Loss:     0.090887, Batch Acc: 0.992679, Tokens per Sec:     4047, Lr: 0.000144\r\n",
      "2024-08-27 15:48:39,427 - INFO - joeynmt.training - Epoch 162, Step:    15415, Batch Loss:     0.076779, Batch Acc: 0.993194, Tokens per Sec:     4072, Lr: 0.000144\r\n",
      "2024-08-27 15:48:40,013 - INFO - joeynmt.training - Epoch 162, total training loss: 7.64, num. of seqs: 8065, num. of tokens: 80463, 20.9458[sec]\r\n",
      "2024-08-27 15:48:40,013 - INFO - joeynmt.training - EPOCH 163\r\n",
      "2024-08-27 15:48:40,664 - INFO - joeynmt.training - Epoch 163, Step:    15420, Batch Loss:     0.079899, Batch Acc: 0.993156, Tokens per Sec:     4065, Lr: 0.000144\r\n",
      "2024-08-27 15:48:41,756 - INFO - joeynmt.training - Epoch 163, Step:    15425, Batch Loss:     0.074675, Batch Acc: 0.993282, Tokens per Sec:     3821, Lr: 0.000144\r\n",
      "2024-08-27 15:48:43,044 - INFO - joeynmt.training - Epoch 163, Step:    15430, Batch Loss:     0.083313, Batch Acc: 0.988878, Tokens per Sec:     3071, Lr: 0.000144\r\n",
      "2024-08-27 15:48:44,183 - INFO - joeynmt.training - Epoch 163, Step:    15435, Batch Loss:     0.068585, Batch Acc: 0.992796, Tokens per Sec:     3783, Lr: 0.000144\r\n",
      "2024-08-27 15:48:45,300 - INFO - joeynmt.training - Epoch 163, Step:    15440, Batch Loss:     0.082735, Batch Acc: 0.991653, Tokens per Sec:     3861, Lr: 0.000144\r\n",
      "2024-08-27 15:48:46,487 - INFO - joeynmt.training - Epoch 163, Step:    15445, Batch Loss:     0.083780, Batch Acc: 0.990159, Tokens per Sec:     3597, Lr: 0.000144\r\n",
      "2024-08-27 15:48:47,613 - INFO - joeynmt.training - Epoch 163, Step:    15450, Batch Loss:     0.091757, Batch Acc: 0.991284, Tokens per Sec:     3875, Lr: 0.000144\r\n",
      "2024-08-27 15:48:48,709 - INFO - joeynmt.training - Epoch 163, Step:    15455, Batch Loss:     0.078163, Batch Acc: 0.994161, Tokens per Sec:     3752, Lr: 0.000144\r\n",
      "2024-08-27 15:48:49,813 - INFO - joeynmt.training - Epoch 163, Step:    15460, Batch Loss:     0.076074, Batch Acc: 0.990626, Tokens per Sec:     3868, Lr: 0.000144\r\n",
      "2024-08-27 15:48:50,911 - INFO - joeynmt.training - Epoch 163, Step:    15465, Batch Loss:     0.095289, Batch Acc: 0.993516, Tokens per Sec:     3935, Lr: 0.000144\r\n",
      "2024-08-27 15:48:52,010 - INFO - joeynmt.training - Epoch 163, Step:    15470, Batch Loss:     0.081670, Batch Acc: 0.992685, Tokens per Sec:     3857, Lr: 0.000144\r\n",
      "2024-08-27 15:48:53,102 - INFO - joeynmt.training - Epoch 163, Step:    15475, Batch Loss:     0.086680, Batch Acc: 0.991517, Tokens per Sec:     3781, Lr: 0.000144\r\n",
      "2024-08-27 15:48:54,213 - INFO - joeynmt.training - Epoch 163, Step:    15480, Batch Loss:     0.083171, Batch Acc: 0.990538, Tokens per Sec:     3903, Lr: 0.000144\r\n",
      "2024-08-27 15:48:55,308 - INFO - joeynmt.training - Epoch 163, Step:    15485, Batch Loss:     0.093813, Batch Acc: 0.989867, Tokens per Sec:     3787, Lr: 0.000144\r\n",
      "2024-08-27 15:48:56,466 - INFO - joeynmt.training - Epoch 163, Step:    15490, Batch Loss:     0.073216, Batch Acc: 0.991006, Tokens per Sec:     3557, Lr: 0.000144\r\n",
      "2024-08-27 15:48:57,615 - INFO - joeynmt.training - Epoch 163, Step:    15495, Batch Loss:     0.080412, Batch Acc: 0.992678, Tokens per Sec:     3569, Lr: 0.000144\r\n",
      "2024-08-27 15:48:58,716 - INFO - joeynmt.training - Epoch 163, Step:    15500, Batch Loss:     0.083538, Batch Acc: 0.990817, Tokens per Sec:     3957, Lr: 0.000144\r\n",
      "2024-08-27 15:48:58,717 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=15542\r\n",
      "2024-08-27 15:48:58,717 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 64.86it/s]\r\n",
      "2024-08-27 15:49:21,231 - INFO - joeynmt.prediction - Generation took 22.5111[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 43445.47 examples/s]\r\n",
      "2024-08-27 15:49:21,617 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:49:21,617 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.22, loss:   6.88, ppl: 973.32, acc:   0.28, 0.1716[sec]\r\n",
      "2024-08-27 15:49:21,619 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:49:21,766 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:49:21,766 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:49:21,766 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 15:49:22,059 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:49:22,206 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:49:22,206 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:49:22,206 - INFO - joeynmt.training - \tHypothesis: trois tables blanches\r\n",
      "2024-08-27 15:49:22,523 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:49:22,669 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:49:22,669 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:49:22,669 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 15:49:22,962 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:49:23,108 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:49:23,108 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:49:23,108 - INFO - joeynmt.training - \tHypothesis: une et unième\r\n",
      "2024-08-27 15:49:24,500 - INFO - joeynmt.training - Epoch 163, Step:    15505, Batch Loss:     0.082115, Batch Acc: 0.991755, Tokens per Sec:     3876, Lr: 0.000144\r\n",
      "2024-08-27 15:49:25,593 - INFO - joeynmt.training - Epoch 163, Step:    15510, Batch Loss:     0.089920, Batch Acc: 0.990436, Tokens per Sec:     3924, Lr: 0.000144\r\n",
      "2024-08-27 15:49:26,126 - INFO - joeynmt.training - Epoch 163, total training loss: 7.71, num. of seqs: 8065, num. of tokens: 80463, 21.4089[sec]\r\n",
      "2024-08-27 15:49:26,127 - INFO - joeynmt.training - EPOCH 164\r\n",
      "2024-08-27 15:49:26,816 - INFO - joeynmt.training - Epoch 164, Step:    15515, Batch Loss:     0.073762, Batch Acc: 0.992357, Tokens per Sec:     3629, Lr: 0.000144\r\n",
      "2024-08-27 15:49:27,881 - INFO - joeynmt.training - Epoch 164, Step:    15520, Batch Loss:     0.075839, Batch Acc: 0.994135, Tokens per Sec:     3845, Lr: 0.000144\r\n",
      "2024-08-27 15:49:28,973 - INFO - joeynmt.training - Epoch 164, Step:    15525, Batch Loss:     0.073890, Batch Acc: 0.993207, Tokens per Sec:     3911, Lr: 0.000144\r\n",
      "2024-08-27 15:49:30,060 - INFO - joeynmt.training - Epoch 164, Step:    15530, Batch Loss:     0.081161, Batch Acc: 0.995200, Tokens per Sec:     4029, Lr: 0.000144\r\n",
      "2024-08-27 15:49:31,150 - INFO - joeynmt.training - Epoch 164, Step:    15535, Batch Loss:     0.082576, Batch Acc: 0.990806, Tokens per Sec:     3894, Lr: 0.000144\r\n",
      "2024-08-27 15:49:32,241 - INFO - joeynmt.training - Epoch 164, Step:    15540, Batch Loss:     0.076595, Batch Acc: 0.989065, Tokens per Sec:     3942, Lr: 0.000143\r\n",
      "2024-08-27 15:49:33,340 - INFO - joeynmt.training - Epoch 164, Step:    15545, Batch Loss:     0.077731, Batch Acc: 0.992548, Tokens per Sec:     3908, Lr: 0.000143\r\n",
      "2024-08-27 15:49:34,450 - INFO - joeynmt.training - Epoch 164, Step:    15550, Batch Loss:     0.077159, Batch Acc: 0.992692, Tokens per Sec:     3701, Lr: 0.000143\r\n",
      "2024-08-27 15:49:35,584 - INFO - joeynmt.training - Epoch 164, Step:    15555, Batch Loss:     0.083843, Batch Acc: 0.993227, Tokens per Sec:     3781, Lr: 0.000143\r\n",
      "2024-08-27 15:49:36,702 - INFO - joeynmt.training - Epoch 164, Step:    15560, Batch Loss:     0.076552, Batch Acc: 0.992765, Tokens per Sec:     3835, Lr: 0.000143\r\n",
      "2024-08-27 15:49:37,818 - INFO - joeynmt.training - Epoch 164, Step:    15565, Batch Loss:     0.077258, Batch Acc: 0.991963, Tokens per Sec:     3683, Lr: 0.000143\r\n",
      "2024-08-27 15:49:38,900 - INFO - joeynmt.training - Epoch 164, Step:    15570, Batch Loss:     0.077047, Batch Acc: 0.990678, Tokens per Sec:     3965, Lr: 0.000143\r\n",
      "2024-08-27 15:49:40,003 - INFO - joeynmt.training - Epoch 164, Step:    15575, Batch Loss:     0.078830, Batch Acc: 0.991620, Tokens per Sec:     3900, Lr: 0.000143\r\n",
      "2024-08-27 15:49:41,087 - INFO - joeynmt.training - Epoch 164, Step:    15580, Batch Loss:     0.076134, Batch Acc: 0.992635, Tokens per Sec:     3885, Lr: 0.000143\r\n",
      "2024-08-27 15:49:42,187 - INFO - joeynmt.training - Epoch 164, Step:    15585, Batch Loss:     0.083645, Batch Acc: 0.992532, Tokens per Sec:     4017, Lr: 0.000143\r\n",
      "2024-08-27 15:49:43,265 - INFO - joeynmt.training - Epoch 164, Step:    15590, Batch Loss:     0.103408, Batch Acc: 0.988319, Tokens per Sec:     3894, Lr: 0.000143\r\n",
      "2024-08-27 15:49:44,372 - INFO - joeynmt.training - Epoch 164, Step:    15595, Batch Loss:     0.079045, Batch Acc: 0.993327, Tokens per Sec:     3792, Lr: 0.000143\r\n",
      "2024-08-27 15:49:45,563 - INFO - joeynmt.training - Epoch 164, Step:    15600, Batch Loss:     0.090393, Batch Acc: 0.991811, Tokens per Sec:     3488, Lr: 0.000143\r\n",
      "2024-08-27 15:49:46,661 - INFO - joeynmt.training - Epoch 164, Step:    15605, Batch Loss:     0.078001, Batch Acc: 0.991421, Tokens per Sec:     3614, Lr: 0.000143\r\n",
      "2024-08-27 15:49:47,170 - INFO - joeynmt.training - Epoch 164, total training loss: 7.53, num. of seqs: 8065, num. of tokens: 80463, 21.0257[sec]\r\n",
      "2024-08-27 15:49:47,170 - INFO - joeynmt.training - EPOCH 165\r\n",
      "2024-08-27 15:49:47,851 - INFO - joeynmt.training - Epoch 165, Step:    15610, Batch Loss:     0.069226, Batch Acc: 0.995104, Tokens per Sec:     3621, Lr: 0.000143\r\n",
      "2024-08-27 15:49:48,947 - INFO - joeynmt.training - Epoch 165, Step:    15615, Batch Loss:     0.084145, Batch Acc: 0.993517, Tokens per Sec:     3943, Lr: 0.000143\r\n",
      "2024-08-27 15:49:50,030 - INFO - joeynmt.training - Epoch 165, Step:    15620, Batch Loss:     0.088884, Batch Acc: 0.992233, Tokens per Sec:     3925, Lr: 0.000143\r\n",
      "2024-08-27 15:49:51,108 - INFO - joeynmt.training - Epoch 165, Step:    15625, Batch Loss:     0.076249, Batch Acc: 0.991939, Tokens per Sec:     4031, Lr: 0.000143\r\n",
      "2024-08-27 15:49:52,189 - INFO - joeynmt.training - Epoch 165, Step:    15630, Batch Loss:     0.068892, Batch Acc: 0.993885, Tokens per Sec:     3787, Lr: 0.000143\r\n",
      "2024-08-27 15:49:53,279 - INFO - joeynmt.training - Epoch 165, Step:    15635, Batch Loss:     0.076020, Batch Acc: 0.992521, Tokens per Sec:     3805, Lr: 0.000143\r\n",
      "2024-08-27 15:49:54,411 - INFO - joeynmt.training - Epoch 165, Step:    15640, Batch Loss:     0.071162, Batch Acc: 0.992402, Tokens per Sec:     3840, Lr: 0.000143\r\n",
      "2024-08-27 15:49:55,487 - INFO - joeynmt.training - Epoch 165, Step:    15645, Batch Loss:     0.076453, Batch Acc: 0.991168, Tokens per Sec:     3788, Lr: 0.000143\r\n",
      "2024-08-27 15:49:56,570 - INFO - joeynmt.training - Epoch 165, Step:    15650, Batch Loss:     0.068879, Batch Acc: 0.991263, Tokens per Sec:     3703, Lr: 0.000143\r\n",
      "2024-08-27 15:49:57,704 - INFO - joeynmt.training - Epoch 165, Step:    15655, Batch Loss:     0.093635, Batch Acc: 0.992814, Tokens per Sec:     3685, Lr: 0.000143\r\n",
      "2024-08-27 15:49:58,815 - INFO - joeynmt.training - Epoch 165, Step:    15660, Batch Loss:     0.082609, Batch Acc: 0.991545, Tokens per Sec:     3939, Lr: 0.000143\r\n",
      "2024-08-27 15:49:59,899 - INFO - joeynmt.training - Epoch 165, Step:    15665, Batch Loss:     0.075327, Batch Acc: 0.992508, Tokens per Sec:     3822, Lr: 0.000143\r\n",
      "2024-08-27 15:50:00,997 - INFO - joeynmt.training - Epoch 165, Step:    15670, Batch Loss:     0.081245, Batch Acc: 0.992931, Tokens per Sec:     3866, Lr: 0.000143\r\n",
      "2024-08-27 15:50:02,084 - INFO - joeynmt.training - Epoch 165, Step:    15675, Batch Loss:     0.079873, Batch Acc: 0.990440, Tokens per Sec:     3851, Lr: 0.000143\r\n",
      "2024-08-27 15:50:03,169 - INFO - joeynmt.training - Epoch 165, Step:    15680, Batch Loss:     0.075095, Batch Acc: 0.992117, Tokens per Sec:     3860, Lr: 0.000143\r\n",
      "2024-08-27 15:50:04,263 - INFO - joeynmt.training - Epoch 165, Step:    15685, Batch Loss:     0.069168, Batch Acc: 0.990113, Tokens per Sec:     3888, Lr: 0.000143\r\n",
      "2024-08-27 15:50:05,375 - INFO - joeynmt.training - Epoch 165, Step:    15690, Batch Loss:     0.073240, Batch Acc: 0.993128, Tokens per Sec:     4056, Lr: 0.000143\r\n",
      "2024-08-27 15:50:06,487 - INFO - joeynmt.training - Epoch 165, Step:    15695, Batch Loss:     0.081043, Batch Acc: 0.989617, Tokens per Sec:     3902, Lr: 0.000143\r\n",
      "2024-08-27 15:50:07,611 - INFO - joeynmt.training - Epoch 165, Step:    15700, Batch Loss:     0.079070, Batch Acc: 0.992844, Tokens per Sec:     3731, Lr: 0.000143\r\n",
      "2024-08-27 15:50:08,099 - INFO - joeynmt.training - Epoch 165, total training loss: 7.43, num. of seqs: 8065, num. of tokens: 80463, 20.9113[sec]\r\n",
      "2024-08-27 15:50:08,099 - INFO - joeynmt.training - EPOCH 166\r\n",
      "2024-08-27 15:50:08,755 - INFO - joeynmt.training - Epoch 166, Step:    15705, Batch Loss:     0.074115, Batch Acc: 0.990826, Tokens per Sec:     3682, Lr: 0.000143\r\n",
      "2024-08-27 15:50:09,882 - INFO - joeynmt.training - Epoch 166, Step:    15710, Batch Loss:     0.084507, Batch Acc: 0.995123, Tokens per Sec:     3820, Lr: 0.000143\r\n",
      "2024-08-27 15:50:10,984 - INFO - joeynmt.training - Epoch 166, Step:    15715, Batch Loss:     0.085833, Batch Acc: 0.990926, Tokens per Sec:     4005, Lr: 0.000143\r\n",
      "2024-08-27 15:50:12,088 - INFO - joeynmt.training - Epoch 166, Step:    15720, Batch Loss:     0.078254, Batch Acc: 0.991553, Tokens per Sec:     3863, Lr: 0.000143\r\n",
      "2024-08-27 15:50:13,192 - INFO - joeynmt.training - Epoch 166, Step:    15725, Batch Loss:     0.085896, Batch Acc: 0.991504, Tokens per Sec:     3947, Lr: 0.000143\r\n",
      "2024-08-27 15:50:14,282 - INFO - joeynmt.training - Epoch 166, Step:    15730, Batch Loss:     0.078130, Batch Acc: 0.992272, Tokens per Sec:     3801, Lr: 0.000143\r\n",
      "2024-08-27 15:50:15,387 - INFO - joeynmt.training - Epoch 166, Step:    15735, Batch Loss:     0.072842, Batch Acc: 0.994660, Tokens per Sec:     3730, Lr: 0.000143\r\n",
      "2024-08-27 15:50:16,593 - INFO - joeynmt.training - Epoch 166, Step:    15740, Batch Loss:     0.074682, Batch Acc: 0.991140, Tokens per Sec:     3467, Lr: 0.000143\r\n",
      "2024-08-27 15:50:17,733 - INFO - joeynmt.training - Epoch 166, Step:    15745, Batch Loss:     0.092196, Batch Acc: 0.991159, Tokens per Sec:     3770, Lr: 0.000143\r\n",
      "2024-08-27 15:50:18,833 - INFO - joeynmt.training - Epoch 166, Step:    15750, Batch Loss:     0.087047, Batch Acc: 0.994871, Tokens per Sec:     3727, Lr: 0.000143\r\n",
      "2024-08-27 15:50:19,933 - INFO - joeynmt.training - Epoch 166, Step:    15755, Batch Loss:     0.095282, Batch Acc: 0.991361, Tokens per Sec:     3894, Lr: 0.000143\r\n",
      "2024-08-27 15:50:21,029 - INFO - joeynmt.training - Epoch 166, Step:    15760, Batch Loss:     0.082076, Batch Acc: 0.990871, Tokens per Sec:     3899, Lr: 0.000142\r\n",
      "2024-08-27 15:50:22,128 - INFO - joeynmt.training - Epoch 166, Step:    15765, Batch Loss:     0.075790, Batch Acc: 0.992344, Tokens per Sec:     3809, Lr: 0.000142\r\n",
      "2024-08-27 15:50:23,234 - INFO - joeynmt.training - Epoch 166, Step:    15770, Batch Loss:     0.090479, Batch Acc: 0.990892, Tokens per Sec:     3872, Lr: 0.000142\r\n",
      "2024-08-27 15:50:24,338 - INFO - joeynmt.training - Epoch 166, Step:    15775, Batch Loss:     0.073583, Batch Acc: 0.993945, Tokens per Sec:     3743, Lr: 0.000142\r\n",
      "2024-08-27 15:50:25,437 - INFO - joeynmt.training - Epoch 166, Step:    15780, Batch Loss:     0.072181, Batch Acc: 0.992465, Tokens per Sec:     3744, Lr: 0.000142\r\n",
      "2024-08-27 15:50:26,550 - INFO - joeynmt.training - Epoch 166, Step:    15785, Batch Loss:     0.076022, Batch Acc: 0.992256, Tokens per Sec:     3715, Lr: 0.000142\r\n",
      "2024-08-27 15:50:27,683 - INFO - joeynmt.training - Epoch 166, Step:    15790, Batch Loss:     0.075362, Batch Acc: 0.991483, Tokens per Sec:     3734, Lr: 0.000142\r\n",
      "2024-08-27 15:50:28,779 - INFO - joeynmt.training - Epoch 166, Step:    15795, Batch Loss:     0.071599, Batch Acc: 0.991473, Tokens per Sec:     3853, Lr: 0.000142\r\n",
      "2024-08-27 15:50:29,360 - INFO - joeynmt.training - Epoch 166, total training loss: 7.52, num. of seqs: 8065, num. of tokens: 80463, 21.2435[sec]\r\n",
      "2024-08-27 15:50:29,360 - INFO - joeynmt.training - EPOCH 167\r\n",
      "2024-08-27 15:50:30,020 - INFO - joeynmt.training - Epoch 167, Step:    15800, Batch Loss:     0.078761, Batch Acc: 0.992949, Tokens per Sec:     3896, Lr: 0.000142\r\n",
      "2024-08-27 15:50:31,116 - INFO - joeynmt.training - Epoch 167, Step:    15805, Batch Loss:     0.077451, Batch Acc: 0.992600, Tokens per Sec:     3823, Lr: 0.000142\r\n",
      "2024-08-27 15:50:32,225 - INFO - joeynmt.training - Epoch 167, Step:    15810, Batch Loss:     0.068398, Batch Acc: 0.993376, Tokens per Sec:     3950, Lr: 0.000142\r\n",
      "2024-08-27 15:50:33,318 - INFO - joeynmt.training - Epoch 167, Step:    15815, Batch Loss:     0.085295, Batch Acc: 0.992178, Tokens per Sec:     3864, Lr: 0.000142\r\n",
      "2024-08-27 15:50:34,391 - INFO - joeynmt.training - Epoch 167, Step:    15820, Batch Loss:     0.080623, Batch Acc: 0.991925, Tokens per Sec:     3695, Lr: 0.000142\r\n",
      "2024-08-27 15:50:35,475 - INFO - joeynmt.training - Epoch 167, Step:    15825, Batch Loss:     0.078829, Batch Acc: 0.992861, Tokens per Sec:     3880, Lr: 0.000142\r\n",
      "2024-08-27 15:50:36,581 - INFO - joeynmt.training - Epoch 167, Step:    15830, Batch Loss:     0.081954, Batch Acc: 0.989227, Tokens per Sec:     3863, Lr: 0.000142\r\n",
      "2024-08-27 15:50:37,701 - INFO - joeynmt.training - Epoch 167, Step:    15835, Batch Loss:     0.073978, Batch Acc: 0.994553, Tokens per Sec:     3607, Lr: 0.000142\r\n",
      "2024-08-27 15:50:38,819 - INFO - joeynmt.training - Epoch 167, Step:    15840, Batch Loss:     0.075936, Batch Acc: 0.992865, Tokens per Sec:     3890, Lr: 0.000142\r\n",
      "2024-08-27 15:50:39,919 - INFO - joeynmt.training - Epoch 167, Step:    15845, Batch Loss:     0.071992, Batch Acc: 0.993097, Tokens per Sec:     3820, Lr: 0.000142\r\n",
      "2024-08-27 15:50:41,014 - INFO - joeynmt.training - Epoch 167, Step:    15850, Batch Loss:     0.077139, Batch Acc: 0.992969, Tokens per Sec:     4032, Lr: 0.000142\r\n",
      "2024-08-27 15:50:42,120 - INFO - joeynmt.training - Epoch 167, Step:    15855, Batch Loss:     0.093170, Batch Acc: 0.988257, Tokens per Sec:     3850, Lr: 0.000142\r\n",
      "2024-08-27 15:50:43,230 - INFO - joeynmt.training - Epoch 167, Step:    15860, Batch Loss:     0.072493, Batch Acc: 0.992506, Tokens per Sec:     3849, Lr: 0.000142\r\n",
      "2024-08-27 15:50:44,330 - INFO - joeynmt.training - Epoch 167, Step:    15865, Batch Loss:     0.090951, Batch Acc: 0.991373, Tokens per Sec:     3692, Lr: 0.000142\r\n",
      "2024-08-27 15:50:45,444 - INFO - joeynmt.training - Epoch 167, Step:    15870, Batch Loss:     0.080672, Batch Acc: 0.991205, Tokens per Sec:     3780, Lr: 0.000142\r\n",
      "2024-08-27 15:50:46,565 - INFO - joeynmt.training - Epoch 167, Step:    15875, Batch Loss:     0.081441, Batch Acc: 0.991687, Tokens per Sec:     3649, Lr: 0.000142\r\n",
      "2024-08-27 15:50:47,843 - INFO - joeynmt.training - Epoch 167, Step:    15880, Batch Loss:     0.076197, Batch Acc: 0.991457, Tokens per Sec:     3207, Lr: 0.000142\r\n",
      "2024-08-27 15:50:48,944 - INFO - joeynmt.training - Epoch 167, Step:    15885, Batch Loss:     0.078741, Batch Acc: 0.992663, Tokens per Sec:     3843, Lr: 0.000142\r\n",
      "2024-08-27 15:50:50,034 - INFO - joeynmt.training - Epoch 167, Step:    15890, Batch Loss:     0.088904, Batch Acc: 0.990615, Tokens per Sec:     3911, Lr: 0.000142\r\n",
      "2024-08-27 15:50:50,624 - INFO - joeynmt.training - Epoch 167, total training loss: 7.49, num. of seqs: 8065, num. of tokens: 80463, 21.2453[sec]\r\n",
      "2024-08-27 15:50:50,624 - INFO - joeynmt.training - EPOCH 168\r\n",
      "2024-08-27 15:50:51,286 - INFO - joeynmt.training - Epoch 168, Step:    15895, Batch Loss:     0.073791, Batch Acc: 0.994715, Tokens per Sec:     3742, Lr: 0.000142\r\n",
      "2024-08-27 15:50:52,381 - INFO - joeynmt.training - Epoch 168, Step:    15900, Batch Loss:     0.068912, Batch Acc: 0.992295, Tokens per Sec:     3911, Lr: 0.000142\r\n",
      "2024-08-27 15:50:53,476 - INFO - joeynmt.training - Epoch 168, Step:    15905, Batch Loss:     0.077889, Batch Acc: 0.992450, Tokens per Sec:     3754, Lr: 0.000142\r\n",
      "2024-08-27 15:50:54,589 - INFO - joeynmt.training - Epoch 168, Step:    15910, Batch Loss:     0.078628, Batch Acc: 0.993682, Tokens per Sec:     3984, Lr: 0.000142\r\n",
      "2024-08-27 15:50:55,681 - INFO - joeynmt.training - Epoch 168, Step:    15915, Batch Loss:     0.077529, Batch Acc: 0.993550, Tokens per Sec:     3834, Lr: 0.000142\r\n",
      "2024-08-27 15:50:56,846 - INFO - joeynmt.training - Epoch 168, Step:    15920, Batch Loss:     0.080045, Batch Acc: 0.991661, Tokens per Sec:     3606, Lr: 0.000142\r\n",
      "2024-08-27 15:50:57,940 - INFO - joeynmt.training - Epoch 168, Step:    15925, Batch Loss:     0.077725, Batch Acc: 0.992218, Tokens per Sec:     3760, Lr: 0.000142\r\n",
      "2024-08-27 15:50:59,047 - INFO - joeynmt.training - Epoch 168, Step:    15930, Batch Loss:     0.070588, Batch Acc: 0.993070, Tokens per Sec:     3784, Lr: 0.000142\r\n",
      "2024-08-27 15:51:00,127 - INFO - joeynmt.training - Epoch 168, Step:    15935, Batch Loss:     0.081528, Batch Acc: 0.991715, Tokens per Sec:     3690, Lr: 0.000142\r\n",
      "2024-08-27 15:51:01,220 - INFO - joeynmt.training - Epoch 168, Step:    15940, Batch Loss:     0.083444, Batch Acc: 0.992781, Tokens per Sec:     3932, Lr: 0.000142\r\n",
      "2024-08-27 15:51:02,312 - INFO - joeynmt.training - Epoch 168, Step:    15945, Batch Loss:     0.072930, Batch Acc: 0.995090, Tokens per Sec:     3731, Lr: 0.000142\r\n",
      "2024-08-27 15:51:03,395 - INFO - joeynmt.training - Epoch 168, Step:    15950, Batch Loss:     0.094158, Batch Acc: 0.990888, Tokens per Sec:     3957, Lr: 0.000142\r\n",
      "2024-08-27 15:51:04,483 - INFO - joeynmt.training - Epoch 168, Step:    15955, Batch Loss:     0.086643, Batch Acc: 0.992385, Tokens per Sec:     3864, Lr: 0.000142\r\n",
      "2024-08-27 15:51:05,573 - INFO - joeynmt.training - Epoch 168, Step:    15960, Batch Loss:     0.075782, Batch Acc: 0.991059, Tokens per Sec:     3903, Lr: 0.000142\r\n",
      "2024-08-27 15:51:06,672 - INFO - joeynmt.training - Epoch 168, Step:    15965, Batch Loss:     0.079835, Batch Acc: 0.991373, Tokens per Sec:     4009, Lr: 0.000142\r\n",
      "2024-08-27 15:51:07,785 - INFO - joeynmt.training - Epoch 168, Step:    15970, Batch Loss:     0.076015, Batch Acc: 0.990711, Tokens per Sec:     3874, Lr: 0.000142\r\n",
      "2024-08-27 15:51:08,867 - INFO - joeynmt.training - Epoch 168, Step:    15975, Batch Loss:     0.087954, Batch Acc: 0.992925, Tokens per Sec:     3790, Lr: 0.000142\r\n",
      "2024-08-27 15:51:09,971 - INFO - joeynmt.training - Epoch 168, Step:    15980, Batch Loss:     0.078075, Batch Acc: 0.990036, Tokens per Sec:     3820, Lr: 0.000142\r\n",
      "2024-08-27 15:51:11,068 - INFO - joeynmt.training - Epoch 168, Step:    15985, Batch Loss:     0.070923, Batch Acc: 0.991108, Tokens per Sec:     3795, Lr: 0.000141\r\n",
      "2024-08-27 15:51:11,666 - INFO - joeynmt.training - Epoch 168, total training loss: 7.31, num. of seqs: 8065, num. of tokens: 80463, 21.0246[sec]\r\n",
      "2024-08-27 15:51:11,667 - INFO - joeynmt.training - EPOCH 169\r\n",
      "2024-08-27 15:51:12,329 - INFO - joeynmt.training - Epoch 169, Step:    15990, Batch Loss:     0.087516, Batch Acc: 0.990543, Tokens per Sec:     3694, Lr: 0.000141\r\n",
      "2024-08-27 15:51:13,432 - INFO - joeynmt.training - Epoch 169, Step:    15995, Batch Loss:     0.078913, Batch Acc: 0.992244, Tokens per Sec:     3744, Lr: 0.000141\r\n",
      "2024-08-27 15:51:14,532 - INFO - joeynmt.training - Epoch 169, Step:    16000, Batch Loss:     0.070933, Batch Acc: 0.992039, Tokens per Sec:     3884, Lr: 0.000141\r\n",
      "2024-08-27 15:51:14,533 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=16042\r\n",
      "2024-08-27 15:51:14,533 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 65.43it/s]\r\n",
      "2024-08-27 15:51:36,851 - INFO - joeynmt.prediction - Generation took 22.3159[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44812.11 examples/s]\r\n",
      "2024-08-27 15:51:37,232 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:51:37,232 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.26, loss:   6.88, ppl: 974.75, acc:   0.29, 0.1667[sec]\r\n",
      "2024-08-27 15:51:37,234 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:51:37,381 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:51:37,381 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:51:37,382 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 15:51:37,673 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:51:37,820 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:51:37,820 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:51:37,820 - INFO - joeynmt.training - \tHypothesis: trois montée de l'enclos\r\n",
      "2024-08-27 15:51:38,114 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:51:38,260 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:51:38,260 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:51:38,260 - INFO - joeynmt.training - \tHypothesis: le service passagers n'est pas en activité actuellement\r\n",
      "2024-08-27 15:51:38,553 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:51:38,699 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:51:38,700 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:51:38,700 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 15:51:40,111 - INFO - joeynmt.training - Epoch 169, Step:    16005, Batch Loss:     0.075473, Batch Acc: 0.993374, Tokens per Sec:     3918, Lr: 0.000141\r\n",
      "2024-08-27 15:51:41,227 - INFO - joeynmt.training - Epoch 169, Step:    16010, Batch Loss:     0.081720, Batch Acc: 0.992208, Tokens per Sec:     3795, Lr: 0.000141\r\n",
      "2024-08-27 15:51:42,322 - INFO - joeynmt.training - Epoch 169, Step:    16015, Batch Loss:     0.069088, Batch Acc: 0.990889, Tokens per Sec:     3712, Lr: 0.000141\r\n",
      "2024-08-27 15:51:43,424 - INFO - joeynmt.training - Epoch 169, Step:    16020, Batch Loss:     0.086406, Batch Acc: 0.992281, Tokens per Sec:     3884, Lr: 0.000141\r\n",
      "2024-08-27 15:51:44,543 - INFO - joeynmt.training - Epoch 169, Step:    16025, Batch Loss:     0.074028, Batch Acc: 0.992631, Tokens per Sec:     3762, Lr: 0.000141\r\n",
      "2024-08-27 15:51:45,647 - INFO - joeynmt.training - Epoch 169, Step:    16030, Batch Loss:     0.077192, Batch Acc: 0.992662, Tokens per Sec:     3953, Lr: 0.000141\r\n",
      "2024-08-27 15:51:46,772 - INFO - joeynmt.training - Epoch 169, Step:    16035, Batch Loss:     0.065860, Batch Acc: 0.992821, Tokens per Sec:     3717, Lr: 0.000141\r\n",
      "2024-08-27 15:51:47,863 - INFO - joeynmt.training - Epoch 169, Step:    16040, Batch Loss:     0.075775, Batch Acc: 0.991395, Tokens per Sec:     3942, Lr: 0.000141\r\n",
      "2024-08-27 15:51:48,968 - INFO - joeynmt.training - Epoch 169, Step:    16045, Batch Loss:     0.070889, Batch Acc: 0.991811, Tokens per Sec:     3874, Lr: 0.000141\r\n",
      "2024-08-27 15:51:50,148 - INFO - joeynmt.training - Epoch 169, Step:    16050, Batch Loss:     0.094092, Batch Acc: 0.990633, Tokens per Sec:     3439, Lr: 0.000141\r\n",
      "2024-08-27 15:51:51,259 - INFO - joeynmt.training - Epoch 169, Step:    16055, Batch Loss:     0.076296, Batch Acc: 0.993351, Tokens per Sec:     4063, Lr: 0.000141\r\n",
      "2024-08-27 15:51:52,341 - INFO - joeynmt.training - Epoch 169, Step:    16060, Batch Loss:     0.083928, Batch Acc: 0.992120, Tokens per Sec:     3874, Lr: 0.000141\r\n",
      "2024-08-27 15:51:53,427 - INFO - joeynmt.training - Epoch 169, Step:    16065, Batch Loss:     0.086795, Batch Acc: 0.991916, Tokens per Sec:     3876, Lr: 0.000141\r\n",
      "2024-08-27 15:51:54,524 - INFO - joeynmt.training - Epoch 169, Step:    16070, Batch Loss:     0.072996, Batch Acc: 0.993677, Tokens per Sec:     4039, Lr: 0.000141\r\n",
      "2024-08-27 15:51:55,619 - INFO - joeynmt.training - Epoch 169, Step:    16075, Batch Loss:     0.100724, Batch Acc: 0.990663, Tokens per Sec:     4013, Lr: 0.000141\r\n",
      "2024-08-27 15:51:56,796 - INFO - joeynmt.training - Epoch 169, Step:    16080, Batch Loss:     0.083959, Batch Acc: 0.991203, Tokens per Sec:     3574, Lr: 0.000141\r\n",
      "2024-08-27 15:51:57,176 - INFO - joeynmt.training - Epoch 169, total training loss: 7.36, num. of seqs: 8065, num. of tokens: 80463, 21.0304[sec]\r\n",
      "2024-08-27 15:51:57,176 - INFO - joeynmt.training - EPOCH 170\r\n",
      "2024-08-27 15:51:58,044 - INFO - joeynmt.training - Epoch 170, Step:    16085, Batch Loss:     0.073698, Batch Acc: 0.994207, Tokens per Sec:     3797, Lr: 0.000141\r\n",
      "2024-08-27 15:51:59,132 - INFO - joeynmt.training - Epoch 170, Step:    16090, Batch Loss:     0.078988, Batch Acc: 0.992730, Tokens per Sec:     3924, Lr: 0.000141\r\n",
      "2024-08-27 15:52:00,223 - INFO - joeynmt.training - Epoch 170, Step:    16095, Batch Loss:     0.074274, Batch Acc: 0.991449, Tokens per Sec:     3754, Lr: 0.000141\r\n",
      "2024-08-27 15:52:01,361 - INFO - joeynmt.training - Epoch 170, Step:    16100, Batch Loss:     0.067592, Batch Acc: 0.992627, Tokens per Sec:     3814, Lr: 0.000141\r\n",
      "2024-08-27 15:52:02,471 - INFO - joeynmt.training - Epoch 170, Step:    16105, Batch Loss:     0.079180, Batch Acc: 0.992164, Tokens per Sec:     3911, Lr: 0.000141\r\n",
      "2024-08-27 15:52:03,567 - INFO - joeynmt.training - Epoch 170, Step:    16110, Batch Loss:     0.077279, Batch Acc: 0.994639, Tokens per Sec:     3920, Lr: 0.000141\r\n",
      "2024-08-27 15:52:04,662 - INFO - joeynmt.training - Epoch 170, Step:    16115, Batch Loss:     0.075540, Batch Acc: 0.993337, Tokens per Sec:     3563, Lr: 0.000141\r\n",
      "2024-08-27 15:52:05,761 - INFO - joeynmt.training - Epoch 170, Step:    16120, Batch Loss:     0.084334, Batch Acc: 0.993087, Tokens per Sec:     3818, Lr: 0.000141\r\n",
      "2024-08-27 15:52:06,905 - INFO - joeynmt.training - Epoch 170, Step:    16125, Batch Loss:     0.085798, Batch Acc: 0.992145, Tokens per Sec:     3677, Lr: 0.000141\r\n",
      "2024-08-27 15:52:08,001 - INFO - joeynmt.training - Epoch 170, Step:    16130, Batch Loss:     0.079747, Batch Acc: 0.991501, Tokens per Sec:     3866, Lr: 0.000141\r\n",
      "2024-08-27 15:52:09,087 - INFO - joeynmt.training - Epoch 170, Step:    16135, Batch Loss:     0.078680, Batch Acc: 0.992130, Tokens per Sec:     3979, Lr: 0.000141\r\n",
      "2024-08-27 15:52:10,176 - INFO - joeynmt.training - Epoch 170, Step:    16140, Batch Loss:     0.077604, Batch Acc: 0.992674, Tokens per Sec:     3764, Lr: 0.000141\r\n",
      "2024-08-27 15:52:11,267 - INFO - joeynmt.training - Epoch 170, Step:    16145, Batch Loss:     0.081646, Batch Acc: 0.992610, Tokens per Sec:     3970, Lr: 0.000141\r\n",
      "2024-08-27 15:52:12,382 - INFO - joeynmt.training - Epoch 170, Step:    16150, Batch Loss:     0.074369, Batch Acc: 0.993461, Tokens per Sec:     3845, Lr: 0.000141\r\n",
      "2024-08-27 15:52:13,466 - INFO - joeynmt.training - Epoch 170, Step:    16155, Batch Loss:     0.065962, Batch Acc: 0.991274, Tokens per Sec:     3913, Lr: 0.000141\r\n",
      "2024-08-27 15:52:14,556 - INFO - joeynmt.training - Epoch 170, Step:    16160, Batch Loss:     0.082627, Batch Acc: 0.990873, Tokens per Sec:     3922, Lr: 0.000141\r\n",
      "2024-08-27 15:52:15,639 - INFO - joeynmt.training - Epoch 170, Step:    16165, Batch Loss:     0.077184, Batch Acc: 0.994105, Tokens per Sec:     3917, Lr: 0.000141\r\n",
      "2024-08-27 15:52:16,766 - INFO - joeynmt.training - Epoch 170, Step:    16170, Batch Loss:     0.080097, Batch Acc: 0.991919, Tokens per Sec:     3844, Lr: 0.000141\r\n",
      "2024-08-27 15:52:17,845 - INFO - joeynmt.training - Epoch 170, Step:    16175, Batch Loss:     0.072667, Batch Acc: 0.989853, Tokens per Sec:     3842, Lr: 0.000141\r\n",
      "2024-08-27 15:52:18,112 - INFO - joeynmt.training - Epoch 170, total training loss: 7.25, num. of seqs: 8065, num. of tokens: 80463, 20.9182[sec]\r\n",
      "2024-08-27 15:52:18,112 - INFO - joeynmt.training - EPOCH 171\r\n",
      "2024-08-27 15:52:18,973 - INFO - joeynmt.training - Epoch 171, Step:    16180, Batch Loss:     0.065917, Batch Acc: 0.993530, Tokens per Sec:     3789, Lr: 0.000141\r\n",
      "2024-08-27 15:52:20,089 - INFO - joeynmt.training - Epoch 171, Step:    16185, Batch Loss:     0.102253, Batch Acc: 0.989576, Tokens per Sec:     3700, Lr: 0.000141\r\n",
      "2024-08-27 15:52:21,299 - INFO - joeynmt.training - Epoch 171, Step:    16190, Batch Loss:     0.083846, Batch Acc: 0.991377, Tokens per Sec:     3453, Lr: 0.000141\r\n",
      "2024-08-27 15:52:22,457 - INFO - joeynmt.training - Epoch 171, Step:    16195, Batch Loss:     0.071184, Batch Acc: 0.992560, Tokens per Sec:     3714, Lr: 0.000141\r\n",
      "2024-08-27 15:52:23,576 - INFO - joeynmt.training - Epoch 171, Step:    16200, Batch Loss:     0.073271, Batch Acc: 0.994123, Tokens per Sec:     3803, Lr: 0.000141\r\n",
      "2024-08-27 15:52:24,666 - INFO - joeynmt.training - Epoch 171, Step:    16205, Batch Loss:     0.071589, Batch Acc: 0.993374, Tokens per Sec:     3741, Lr: 0.000141\r\n",
      "2024-08-27 15:52:25,762 - INFO - joeynmt.training - Epoch 171, Step:    16210, Batch Loss:     0.091236, Batch Acc: 0.992191, Tokens per Sec:     3860, Lr: 0.000141\r\n",
      "2024-08-27 15:52:26,895 - INFO - joeynmt.training - Epoch 171, Step:    16215, Batch Loss:     0.101466, Batch Acc: 0.991394, Tokens per Sec:     3694, Lr: 0.000140\r\n",
      "2024-08-27 15:52:28,019 - INFO - joeynmt.training - Epoch 171, Step:    16220, Batch Loss:     0.074246, Batch Acc: 0.991028, Tokens per Sec:     3869, Lr: 0.000140\r\n",
      "2024-08-27 15:52:29,141 - INFO - joeynmt.training - Epoch 171, Step:    16225, Batch Loss:     0.092572, Batch Acc: 0.989954, Tokens per Sec:     4086, Lr: 0.000140\r\n",
      "2024-08-27 15:52:30,228 - INFO - joeynmt.training - Epoch 171, Step:    16230, Batch Loss:     0.064966, Batch Acc: 0.991860, Tokens per Sec:     3843, Lr: 0.000140\r\n",
      "2024-08-27 15:52:31,335 - INFO - joeynmt.training - Epoch 171, Step:    16235, Batch Loss:     0.070492, Batch Acc: 0.994220, Tokens per Sec:     3910, Lr: 0.000140\r\n",
      "2024-08-27 15:52:32,422 - INFO - joeynmt.training - Epoch 171, Step:    16240, Batch Loss:     0.085909, Batch Acc: 0.989801, Tokens per Sec:     3700, Lr: 0.000140\r\n",
      "2024-08-27 15:52:33,514 - INFO - joeynmt.training - Epoch 171, Step:    16245, Batch Loss:     0.078911, Batch Acc: 0.994592, Tokens per Sec:     4065, Lr: 0.000140\r\n",
      "2024-08-27 15:52:34,608 - INFO - joeynmt.training - Epoch 171, Step:    16250, Batch Loss:     0.067058, Batch Acc: 0.992302, Tokens per Sec:     3923, Lr: 0.000140\r\n",
      "2024-08-27 15:52:35,705 - INFO - joeynmt.training - Epoch 171, Step:    16255, Batch Loss:     0.085437, Batch Acc: 0.992229, Tokens per Sec:     3640, Lr: 0.000140\r\n",
      "2024-08-27 15:52:36,852 - INFO - joeynmt.training - Epoch 171, Step:    16260, Batch Loss:     0.075506, Batch Acc: 0.993787, Tokens per Sec:     3648, Lr: 0.000140\r\n",
      "2024-08-27 15:52:37,978 - INFO - joeynmt.training - Epoch 171, Step:    16265, Batch Loss:     0.071509, Batch Acc: 0.993469, Tokens per Sec:     3674, Lr: 0.000140\r\n",
      "2024-08-27 15:52:39,102 - INFO - joeynmt.training - Epoch 171, Step:    16270, Batch Loss:     0.076647, Batch Acc: 0.993777, Tokens per Sec:     3863, Lr: 0.000140\r\n",
      "2024-08-27 15:52:39,422 - INFO - joeynmt.training - Epoch 171, total training loss: 7.38, num. of seqs: 8065, num. of tokens: 80463, 21.2928[sec]\r\n",
      "2024-08-27 15:52:39,423 - INFO - joeynmt.training - EPOCH 172\r\n",
      "2024-08-27 15:52:40,326 - INFO - joeynmt.training - Epoch 172, Step:    16275, Batch Loss:     0.075290, Batch Acc: 0.992898, Tokens per Sec:     3916, Lr: 0.000140\r\n",
      "2024-08-27 15:52:41,441 - INFO - joeynmt.training - Epoch 172, Step:    16280, Batch Loss:     0.074626, Batch Acc: 0.992770, Tokens per Sec:     3475, Lr: 0.000140\r\n",
      "2024-08-27 15:52:42,528 - INFO - joeynmt.training - Epoch 172, Step:    16285, Batch Loss:     0.063879, Batch Acc: 0.993926, Tokens per Sec:     3788, Lr: 0.000140\r\n",
      "2024-08-27 15:52:43,627 - INFO - joeynmt.training - Epoch 172, Step:    16290, Batch Loss:     0.065418, Batch Acc: 0.995228, Tokens per Sec:     3816, Lr: 0.000140\r\n",
      "2024-08-27 15:52:44,719 - INFO - joeynmt.training - Epoch 172, Step:    16295, Batch Loss:     0.081393, Batch Acc: 0.991082, Tokens per Sec:     3699, Lr: 0.000140\r\n",
      "2024-08-27 15:52:45,831 - INFO - joeynmt.training - Epoch 172, Step:    16300, Batch Loss:     0.099756, Batch Acc: 0.992497, Tokens per Sec:     3839, Lr: 0.000140\r\n",
      "2024-08-27 15:52:46,982 - INFO - joeynmt.training - Epoch 172, Step:    16305, Batch Loss:     0.072759, Batch Acc: 0.992106, Tokens per Sec:     3744, Lr: 0.000140\r\n",
      "2024-08-27 15:52:48,063 - INFO - joeynmt.training - Epoch 172, Step:    16310, Batch Loss:     0.077473, Batch Acc: 0.991699, Tokens per Sec:     4014, Lr: 0.000140\r\n",
      "2024-08-27 15:52:49,158 - INFO - joeynmt.training - Epoch 172, Step:    16315, Batch Loss:     0.080561, Batch Acc: 0.992671, Tokens per Sec:     3867, Lr: 0.000140\r\n",
      "2024-08-27 15:52:50,250 - INFO - joeynmt.training - Epoch 172, Step:    16320, Batch Loss:     0.071341, Batch Acc: 0.991874, Tokens per Sec:     3721, Lr: 0.000140\r\n",
      "2024-08-27 15:52:51,385 - INFO - joeynmt.training - Epoch 172, Step:    16325, Batch Loss:     0.083536, Batch Acc: 0.991206, Tokens per Sec:     3810, Lr: 0.000140\r\n",
      "2024-08-27 15:52:52,582 - INFO - joeynmt.training - Epoch 172, Step:    16330, Batch Loss:     0.073060, Batch Acc: 0.993879, Tokens per Sec:     3551, Lr: 0.000140\r\n",
      "2024-08-27 15:52:53,710 - INFO - joeynmt.training - Epoch 172, Step:    16335, Batch Loss:     0.074813, Batch Acc: 0.991841, Tokens per Sec:     3696, Lr: 0.000140\r\n",
      "2024-08-27 15:52:54,838 - INFO - joeynmt.training - Epoch 172, Step:    16340, Batch Loss:     0.074204, Batch Acc: 0.991986, Tokens per Sec:     3654, Lr: 0.000140\r\n",
      "2024-08-27 15:52:55,974 - INFO - joeynmt.training - Epoch 172, Step:    16345, Batch Loss:     0.072364, Batch Acc: 0.993946, Tokens per Sec:     3784, Lr: 0.000140\r\n",
      "2024-08-27 15:52:57,119 - INFO - joeynmt.training - Epoch 172, Step:    16350, Batch Loss:     0.071728, Batch Acc: 0.992060, Tokens per Sec:     3739, Lr: 0.000140\r\n",
      "2024-08-27 15:52:58,219 - INFO - joeynmt.training - Epoch 172, Step:    16355, Batch Loss:     0.080909, Batch Acc: 0.991730, Tokens per Sec:     3740, Lr: 0.000140\r\n",
      "2024-08-27 15:52:59,313 - INFO - joeynmt.training - Epoch 172, Step:    16360, Batch Loss:     0.073266, Batch Acc: 0.993204, Tokens per Sec:     3902, Lr: 0.000140\r\n",
      "2024-08-27 15:53:00,405 - INFO - joeynmt.training - Epoch 172, Step:    16365, Batch Loss:     0.070854, Batch Acc: 0.992226, Tokens per Sec:     3891, Lr: 0.000140\r\n",
      "2024-08-27 15:53:00,778 - INFO - joeynmt.training - Epoch 172, total training loss: 7.38, num. of seqs: 8065, num. of tokens: 80463, 21.3378[sec]\r\n",
      "2024-08-27 15:53:00,778 - INFO - joeynmt.training - EPOCH 173\r\n",
      "2024-08-27 15:53:01,661 - INFO - joeynmt.training - Epoch 173, Step:    16370, Batch Loss:     0.080390, Batch Acc: 0.993395, Tokens per Sec:     3792, Lr: 0.000140\r\n",
      "2024-08-27 15:53:02,771 - INFO - joeynmt.training - Epoch 173, Step:    16375, Batch Loss:     0.076595, Batch Acc: 0.993750, Tokens per Sec:     3892, Lr: 0.000140\r\n",
      "2024-08-27 15:53:03,871 - INFO - joeynmt.training - Epoch 173, Step:    16380, Batch Loss:     0.070927, Batch Acc: 0.990905, Tokens per Sec:     4002, Lr: 0.000140\r\n",
      "2024-08-27 15:53:04,969 - INFO - joeynmt.training - Epoch 173, Step:    16385, Batch Loss:     0.074655, Batch Acc: 0.992456, Tokens per Sec:     3744, Lr: 0.000140\r\n",
      "2024-08-27 15:53:06,062 - INFO - joeynmt.training - Epoch 173, Step:    16390, Batch Loss:     0.085291, Batch Acc: 0.993057, Tokens per Sec:     3826, Lr: 0.000140\r\n",
      "2024-08-27 15:53:07,192 - INFO - joeynmt.training - Epoch 173, Step:    16395, Batch Loss:     0.084072, Batch Acc: 0.991878, Tokens per Sec:     3597, Lr: 0.000140\r\n",
      "2024-08-27 15:53:08,298 - INFO - joeynmt.training - Epoch 173, Step:    16400, Batch Loss:     0.072557, Batch Acc: 0.994603, Tokens per Sec:     3857, Lr: 0.000140\r\n",
      "2024-08-27 15:53:09,404 - INFO - joeynmt.training - Epoch 173, Step:    16405, Batch Loss:     0.083638, Batch Acc: 0.990639, Tokens per Sec:     3867, Lr: 0.000140\r\n",
      "2024-08-27 15:53:10,509 - INFO - joeynmt.training - Epoch 173, Step:    16410, Batch Loss:     0.067537, Batch Acc: 0.993944, Tokens per Sec:     3739, Lr: 0.000140\r\n",
      "2024-08-27 15:53:11,594 - INFO - joeynmt.training - Epoch 173, Step:    16415, Batch Loss:     0.078250, Batch Acc: 0.993271, Tokens per Sec:     3836, Lr: 0.000140\r\n",
      "2024-08-27 15:53:12,687 - INFO - joeynmt.training - Epoch 173, Step:    16420, Batch Loss:     0.091799, Batch Acc: 0.991742, Tokens per Sec:     3771, Lr: 0.000140\r\n",
      "2024-08-27 15:53:13,792 - INFO - joeynmt.training - Epoch 173, Step:    16425, Batch Loss:     0.077165, Batch Acc: 0.992916, Tokens per Sec:     3833, Lr: 0.000140\r\n",
      "2024-08-27 15:53:14,890 - INFO - joeynmt.training - Epoch 173, Step:    16430, Batch Loss:     0.086326, Batch Acc: 0.992675, Tokens per Sec:     3857, Lr: 0.000140\r\n",
      "2024-08-27 15:53:15,998 - INFO - joeynmt.training - Epoch 173, Step:    16435, Batch Loss:     0.070884, Batch Acc: 0.992559, Tokens per Sec:     3762, Lr: 0.000140\r\n",
      "2024-08-27 15:53:17,121 - INFO - joeynmt.training - Epoch 173, Step:    16440, Batch Loss:     0.081850, Batch Acc: 0.990260, Tokens per Sec:     3569, Lr: 0.000140\r\n",
      "2024-08-27 15:53:18,235 - INFO - joeynmt.training - Epoch 173, Step:    16445, Batch Loss:     0.084109, Batch Acc: 0.993665, Tokens per Sec:     3828, Lr: 0.000139\r\n",
      "2024-08-27 15:53:19,328 - INFO - joeynmt.training - Epoch 173, Step:    16450, Batch Loss:     0.078303, Batch Acc: 0.992584, Tokens per Sec:     3949, Lr: 0.000139\r\n",
      "2024-08-27 15:53:20,431 - INFO - joeynmt.training - Epoch 173, Step:    16455, Batch Loss:     0.069405, Batch Acc: 0.992430, Tokens per Sec:     3835, Lr: 0.000139\r\n",
      "2024-08-27 15:53:21,553 - INFO - joeynmt.training - Epoch 173, Step:    16460, Batch Loss:     0.073286, Batch Acc: 0.994135, Tokens per Sec:     3955, Lr: 0.000139\r\n",
      "2024-08-27 15:53:21,879 - INFO - joeynmt.training - Epoch 173, total training loss: 7.26, num. of seqs: 8065, num. of tokens: 80463, 21.0827[sec]\r\n",
      "2024-08-27 15:53:21,879 - INFO - joeynmt.training - EPOCH 174\r\n",
      "2024-08-27 15:53:22,779 - INFO - joeynmt.training - Epoch 174, Step:    16465, Batch Loss:     0.083607, Batch Acc: 0.993833, Tokens per Sec:     3799, Lr: 0.000139\r\n",
      "2024-08-27 15:53:23,989 - INFO - joeynmt.training - Epoch 174, Step:    16470, Batch Loss:     0.064454, Batch Acc: 0.993984, Tokens per Sec:     3574, Lr: 0.000139\r\n",
      "2024-08-27 15:53:25,095 - INFO - joeynmt.training - Epoch 174, Step:    16475, Batch Loss:     0.083610, Batch Acc: 0.992539, Tokens per Sec:     3881, Lr: 0.000139\r\n",
      "2024-08-27 15:53:26,219 - INFO - joeynmt.training - Epoch 174, Step:    16480, Batch Loss:     0.067143, Batch Acc: 0.995589, Tokens per Sec:     3632, Lr: 0.000139\r\n",
      "2024-08-27 15:53:27,441 - INFO - joeynmt.training - Epoch 174, Step:    16485, Batch Loss:     0.073611, Batch Acc: 0.992640, Tokens per Sec:     3562, Lr: 0.000139\r\n",
      "2024-08-27 15:53:28,544 - INFO - joeynmt.training - Epoch 174, Step:    16490, Batch Loss:     0.081365, Batch Acc: 0.993370, Tokens per Sec:     3830, Lr: 0.000139\r\n",
      "2024-08-27 15:53:29,629 - INFO - joeynmt.training - Epoch 174, Step:    16495, Batch Loss:     0.068120, Batch Acc: 0.993415, Tokens per Sec:     3783, Lr: 0.000139\r\n",
      "2024-08-27 15:53:30,747 - INFO - joeynmt.training - Epoch 174, Step:    16500, Batch Loss:     0.068856, Batch Acc: 0.992695, Tokens per Sec:     3551, Lr: 0.000139\r\n",
      "2024-08-27 15:53:30,748 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=16542\r\n",
      "2024-08-27 15:53:30,748 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:21<00:00, 66.42it/s]\r\n",
      "2024-08-27 15:53:52,732 - INFO - joeynmt.prediction - Generation took 21.9821[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 43908.94 examples/s]\r\n",
      "2024-08-27 15:53:53,119 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:53:53,119 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.14, loss:   6.89, ppl: 979.29, acc:   0.29, 0.1712[sec]\r\n",
      "2024-08-27 15:53:53,121 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:53:53,575 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:53:53,575 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:53:53,575 - INFO - joeynmt.training - \tHypothesis: comment vas tu\r\n",
      "2024-08-27 15:53:53,866 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:53:54,013 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:53:54,013 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:53:54,013 - INFO - joeynmt.training - \tHypothesis: trois tables blanches\r\n",
      "2024-08-27 15:53:54,306 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:53:54,469 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:53:54,469 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:53:54,469 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 15:53:54,778 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:53:54,932 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:53:54,932 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:53:54,932 - INFO - joeynmt.training - \tHypothesis: une trentaine de personnes\r\n",
      "2024-08-27 15:53:56,342 - INFO - joeynmt.training - Epoch 174, Step:    16505, Batch Loss:     0.071563, Batch Acc: 0.993160, Tokens per Sec:     3944, Lr: 0.000139\r\n",
      "2024-08-27 15:53:57,465 - INFO - joeynmt.training - Epoch 174, Step:    16510, Batch Loss:     0.068250, Batch Acc: 0.994458, Tokens per Sec:     3698, Lr: 0.000139\r\n",
      "2024-08-27 15:53:58,579 - INFO - joeynmt.training - Epoch 174, Step:    16515, Batch Loss:     0.092881, Batch Acc: 0.990119, Tokens per Sec:     3910, Lr: 0.000139\r\n",
      "2024-08-27 15:53:59,690 - INFO - joeynmt.training - Epoch 174, Step:    16520, Batch Loss:     0.085153, Batch Acc: 0.992440, Tokens per Sec:     3813, Lr: 0.000139\r\n",
      "2024-08-27 15:54:00,796 - INFO - joeynmt.training - Epoch 174, Step:    16525, Batch Loss:     0.080180, Batch Acc: 0.993215, Tokens per Sec:     3735, Lr: 0.000139\r\n",
      "2024-08-27 15:54:01,893 - INFO - joeynmt.training - Epoch 174, Step:    16530, Batch Loss:     0.079084, Batch Acc: 0.991457, Tokens per Sec:     3738, Lr: 0.000139\r\n",
      "2024-08-27 15:54:03,002 - INFO - joeynmt.training - Epoch 174, Step:    16535, Batch Loss:     0.070230, Batch Acc: 0.993661, Tokens per Sec:     3984, Lr: 0.000139\r\n",
      "2024-08-27 15:54:04,117 - INFO - joeynmt.training - Epoch 174, Step:    16540, Batch Loss:     0.076652, Batch Acc: 0.993337, Tokens per Sec:     3771, Lr: 0.000139\r\n",
      "2024-08-27 15:54:05,250 - INFO - joeynmt.training - Epoch 174, Step:    16545, Batch Loss:     0.088437, Batch Acc: 0.990223, Tokens per Sec:     3884, Lr: 0.000139\r\n",
      "2024-08-27 15:54:06,375 - INFO - joeynmt.training - Epoch 174, Step:    16550, Batch Loss:     0.081851, Batch Acc: 0.991479, Tokens per Sec:     3760, Lr: 0.000139\r\n",
      "2024-08-27 15:54:07,516 - INFO - joeynmt.training - Epoch 174, Step:    16555, Batch Loss:     0.073259, Batch Acc: 0.993689, Tokens per Sec:     3752, Lr: 0.000139\r\n",
      "2024-08-27 15:54:07,779 - INFO - joeynmt.training - Epoch 174, total training loss: 7.18, num. of seqs: 8065, num. of tokens: 80463, 21.4007[sec]\r\n",
      "2024-08-27 15:54:07,780 - INFO - joeynmt.training - EPOCH 175\r\n",
      "2024-08-27 15:54:08,675 - INFO - joeynmt.training - Epoch 175, Step:    16560, Batch Loss:     0.074299, Batch Acc: 0.994870, Tokens per Sec:     3936, Lr: 0.000139\r\n",
      "2024-08-27 15:54:09,753 - INFO - joeynmt.training - Epoch 175, Step:    16565, Batch Loss:     0.070651, Batch Acc: 0.993525, Tokens per Sec:     3873, Lr: 0.000139\r\n",
      "2024-08-27 15:54:10,837 - INFO - joeynmt.training - Epoch 175, Step:    16570, Batch Loss:     0.081912, Batch Acc: 0.991039, Tokens per Sec:     3809, Lr: 0.000139\r\n",
      "2024-08-27 15:54:11,941 - INFO - joeynmt.training - Epoch 175, Step:    16575, Batch Loss:     0.078842, Batch Acc: 0.991980, Tokens per Sec:     3955, Lr: 0.000139\r\n",
      "2024-08-27 15:54:13,024 - INFO - joeynmt.training - Epoch 175, Step:    16580, Batch Loss:     0.068070, Batch Acc: 0.992326, Tokens per Sec:     3853, Lr: 0.000139\r\n",
      "2024-08-27 15:54:14,100 - INFO - joeynmt.training - Epoch 175, Step:    16585, Batch Loss:     0.069030, Batch Acc: 0.994963, Tokens per Sec:     3876, Lr: 0.000139\r\n",
      "2024-08-27 15:54:15,180 - INFO - joeynmt.training - Epoch 175, Step:    16590, Batch Loss:     0.086482, Batch Acc: 0.992745, Tokens per Sec:     3959, Lr: 0.000139\r\n",
      "2024-08-27 15:54:16,273 - INFO - joeynmt.training - Epoch 175, Step:    16595, Batch Loss:     0.074926, Batch Acc: 0.993015, Tokens per Sec:     3933, Lr: 0.000139\r\n",
      "2024-08-27 15:54:17,392 - INFO - joeynmt.training - Epoch 175, Step:    16600, Batch Loss:     0.071350, Batch Acc: 0.993692, Tokens per Sec:     3826, Lr: 0.000139\r\n",
      "2024-08-27 15:54:18,485 - INFO - joeynmt.training - Epoch 175, Step:    16605, Batch Loss:     0.072415, Batch Acc: 0.995279, Tokens per Sec:     3877, Lr: 0.000139\r\n",
      "2024-08-27 15:54:19,573 - INFO - joeynmt.training - Epoch 175, Step:    16610, Batch Loss:     0.069426, Batch Acc: 0.994606, Tokens per Sec:     3921, Lr: 0.000139\r\n",
      "2024-08-27 15:54:20,651 - INFO - joeynmt.training - Epoch 175, Step:    16615, Batch Loss:     0.070543, Batch Acc: 0.992848, Tokens per Sec:     3766, Lr: 0.000139\r\n",
      "2024-08-27 15:54:21,731 - INFO - joeynmt.training - Epoch 175, Step:    16620, Batch Loss:     0.079465, Batch Acc: 0.994760, Tokens per Sec:     3712, Lr: 0.000139\r\n",
      "2024-08-27 15:54:22,828 - INFO - joeynmt.training - Epoch 175, Step:    16625, Batch Loss:     0.071146, Batch Acc: 0.992058, Tokens per Sec:     3908, Lr: 0.000139\r\n",
      "2024-08-27 15:54:23,926 - INFO - joeynmt.training - Epoch 175, Step:    16630, Batch Loss:     0.065912, Batch Acc: 0.992234, Tokens per Sec:     4105, Lr: 0.000139\r\n",
      "2024-08-27 15:54:25,011 - INFO - joeynmt.training - Epoch 175, Step:    16635, Batch Loss:     0.076662, Batch Acc: 0.993527, Tokens per Sec:     3846, Lr: 0.000139\r\n",
      "2024-08-27 15:54:26,202 - INFO - joeynmt.training - Epoch 175, Step:    16640, Batch Loss:     0.092639, Batch Acc: 0.994318, Tokens per Sec:     3550, Lr: 0.000139\r\n",
      "2024-08-27 15:54:27,354 - INFO - joeynmt.training - Epoch 175, Step:    16645, Batch Loss:     0.076860, Batch Acc: 0.995495, Tokens per Sec:     3472, Lr: 0.000139\r\n",
      "2024-08-27 15:54:28,465 - INFO - joeynmt.training - Epoch 175, Step:    16650, Batch Loss:     0.082257, Batch Acc: 0.991882, Tokens per Sec:     3771, Lr: 0.000139\r\n",
      "2024-08-27 15:54:28,860 - INFO - joeynmt.training - Epoch 175, total training loss: 7.06, num. of seqs: 8065, num. of tokens: 80463, 21.0631[sec]\r\n",
      "2024-08-27 15:54:28,860 - INFO - joeynmt.training - EPOCH 176\r\n",
      "2024-08-27 15:54:29,808 - INFO - joeynmt.training - Epoch 176, Step:    16655, Batch Loss:     0.072219, Batch Acc: 0.994205, Tokens per Sec:     3845, Lr: 0.000139\r\n",
      "2024-08-27 15:54:30,913 - INFO - joeynmt.training - Epoch 176, Step:    16660, Batch Loss:     0.066465, Batch Acc: 0.993207, Tokens per Sec:     3734, Lr: 0.000139\r\n",
      "2024-08-27 15:54:32,010 - INFO - joeynmt.training - Epoch 176, Step:    16665, Batch Loss:     0.079323, Batch Acc: 0.993240, Tokens per Sec:     3778, Lr: 0.000139\r\n",
      "2024-08-27 15:54:33,138 - INFO - joeynmt.training - Epoch 176, Step:    16670, Batch Loss:     0.074289, Batch Acc: 0.993539, Tokens per Sec:     3846, Lr: 0.000139\r\n",
      "2024-08-27 15:54:34,239 - INFO - joeynmt.training - Epoch 176, Step:    16675, Batch Loss:     0.068910, Batch Acc: 0.995965, Tokens per Sec:     3826, Lr: 0.000139\r\n",
      "2024-08-27 15:54:35,349 - INFO - joeynmt.training - Epoch 176, Step:    16680, Batch Loss:     0.072575, Batch Acc: 0.993730, Tokens per Sec:     3740, Lr: 0.000139\r\n",
      "2024-08-27 15:54:36,464 - INFO - joeynmt.training - Epoch 176, Step:    16685, Batch Loss:     0.081706, Batch Acc: 0.990734, Tokens per Sec:     3775, Lr: 0.000138\r\n",
      "2024-08-27 15:54:37,599 - INFO - joeynmt.training - Epoch 176, Step:    16690, Batch Loss:     0.078260, Batch Acc: 0.993885, Tokens per Sec:     3749, Lr: 0.000138\r\n",
      "2024-08-27 15:54:38,715 - INFO - joeynmt.training - Epoch 176, Step:    16695, Batch Loss:     0.084267, Batch Acc: 0.989316, Tokens per Sec:     3776, Lr: 0.000138\r\n",
      "2024-08-27 15:54:39,813 - INFO - joeynmt.training - Epoch 176, Step:    16700, Batch Loss:     0.075287, Batch Acc: 0.991742, Tokens per Sec:     3752, Lr: 0.000138\r\n",
      "2024-08-27 15:54:40,908 - INFO - joeynmt.training - Epoch 176, Step:    16705, Batch Loss:     0.082532, Batch Acc: 0.989981, Tokens per Sec:     3832, Lr: 0.000138\r\n",
      "2024-08-27 15:54:42,003 - INFO - joeynmt.training - Epoch 176, Step:    16710, Batch Loss:     0.079360, Batch Acc: 0.993780, Tokens per Sec:     3966, Lr: 0.000138\r\n",
      "2024-08-27 15:54:43,103 - INFO - joeynmt.training - Epoch 176, Step:    16715, Batch Loss:     0.080433, Batch Acc: 0.994208, Tokens per Sec:     3927, Lr: 0.000138\r\n",
      "2024-08-27 15:54:44,190 - INFO - joeynmt.training - Epoch 176, Step:    16720, Batch Loss:     0.071964, Batch Acc: 0.994651, Tokens per Sec:     3785, Lr: 0.000138\r\n",
      "2024-08-27 15:54:45,296 - INFO - joeynmt.training - Epoch 176, Step:    16725, Batch Loss:     0.080961, Batch Acc: 0.994362, Tokens per Sec:     3853, Lr: 0.000138\r\n",
      "2024-08-27 15:54:46,428 - INFO - joeynmt.training - Epoch 176, Step:    16730, Batch Loss:     0.068349, Batch Acc: 0.994441, Tokens per Sec:     3816, Lr: 0.000138\r\n",
      "2024-08-27 15:54:47,601 - INFO - joeynmt.training - Epoch 176, Step:    16735, Batch Loss:     0.065120, Batch Acc: 0.994172, Tokens per Sec:     3513, Lr: 0.000138\r\n",
      "2024-08-27 15:54:48,696 - INFO - joeynmt.training - Epoch 176, Step:    16740, Batch Loss:     0.084810, Batch Acc: 0.991069, Tokens per Sec:     3684, Lr: 0.000138\r\n",
      "2024-08-27 15:54:49,800 - INFO - joeynmt.training - Epoch 176, Step:    16745, Batch Loss:     0.073260, Batch Acc: 0.993435, Tokens per Sec:     3864, Lr: 0.000138\r\n",
      "2024-08-27 15:54:50,117 - INFO - joeynmt.training - Epoch 176, total training loss: 7.05, num. of seqs: 8065, num. of tokens: 80463, 21.2380[sec]\r\n",
      "2024-08-27 15:54:50,117 - INFO - joeynmt.training - EPOCH 177\r\n",
      "2024-08-27 15:54:50,989 - INFO - joeynmt.training - Epoch 177, Step:    16750, Batch Loss:     0.064708, Batch Acc: 0.993168, Tokens per Sec:     3712, Lr: 0.000138\r\n",
      "2024-08-27 15:54:52,104 - INFO - joeynmt.training - Epoch 177, Step:    16755, Batch Loss:     0.066887, Batch Acc: 0.995551, Tokens per Sec:     3832, Lr: 0.000138\r\n",
      "2024-08-27 15:54:53,203 - INFO - joeynmt.training - Epoch 177, Step:    16760, Batch Loss:     0.076714, Batch Acc: 0.993044, Tokens per Sec:     3928, Lr: 0.000138\r\n",
      "2024-08-27 15:54:54,276 - INFO - joeynmt.training - Epoch 177, Step:    16765, Batch Loss:     0.073672, Batch Acc: 0.995350, Tokens per Sec:     4009, Lr: 0.000138\r\n",
      "2024-08-27 15:54:55,359 - INFO - joeynmt.training - Epoch 177, Step:    16770, Batch Loss:     0.064785, Batch Acc: 0.994159, Tokens per Sec:     3957, Lr: 0.000138\r\n",
      "2024-08-27 15:54:56,464 - INFO - joeynmt.training - Epoch 177, Step:    16775, Batch Loss:     0.062533, Batch Acc: 0.994389, Tokens per Sec:     3710, Lr: 0.000138\r\n",
      "2024-08-27 15:54:57,693 - INFO - joeynmt.training - Epoch 177, Step:    16780, Batch Loss:     0.065280, Batch Acc: 0.991517, Tokens per Sec:     3265, Lr: 0.000138\r\n",
      "2024-08-27 15:54:58,794 - INFO - joeynmt.training - Epoch 177, Step:    16785, Batch Loss:     0.068975, Batch Acc: 0.989720, Tokens per Sec:     3889, Lr: 0.000138\r\n",
      "2024-08-27 15:54:59,875 - INFO - joeynmt.training - Epoch 177, Step:    16790, Batch Loss:     0.084476, Batch Acc: 0.992892, Tokens per Sec:     4034, Lr: 0.000138\r\n",
      "2024-08-27 15:55:00,980 - INFO - joeynmt.training - Epoch 177, Step:    16795, Batch Loss:     0.069611, Batch Acc: 0.991900, Tokens per Sec:     3914, Lr: 0.000138\r\n",
      "2024-08-27 15:55:02,079 - INFO - joeynmt.training - Epoch 177, Step:    16800, Batch Loss:     0.070157, Batch Acc: 0.993753, Tokens per Sec:     3935, Lr: 0.000138\r\n",
      "2024-08-27 15:55:03,159 - INFO - joeynmt.training - Epoch 177, Step:    16805, Batch Loss:     0.073597, Batch Acc: 0.991803, Tokens per Sec:     3844, Lr: 0.000138\r\n",
      "2024-08-27 15:55:04,252 - INFO - joeynmt.training - Epoch 177, Step:    16810, Batch Loss:     0.075846, Batch Acc: 0.992801, Tokens per Sec:     3812, Lr: 0.000138\r\n",
      "2024-08-27 15:55:05,360 - INFO - joeynmt.training - Epoch 177, Step:    16815, Batch Loss:     0.083672, Batch Acc: 0.993224, Tokens per Sec:     3734, Lr: 0.000138\r\n",
      "2024-08-27 15:55:06,437 - INFO - joeynmt.training - Epoch 177, Step:    16820, Batch Loss:     0.071662, Batch Acc: 0.993669, Tokens per Sec:     3963, Lr: 0.000138\r\n",
      "2024-08-27 15:55:07,564 - INFO - joeynmt.training - Epoch 177, Step:    16825, Batch Loss:     0.069109, Batch Acc: 0.994585, Tokens per Sec:     3442, Lr: 0.000138\r\n",
      "2024-08-27 15:55:08,658 - INFO - joeynmt.training - Epoch 177, Step:    16830, Batch Loss:     0.079614, Batch Acc: 0.990732, Tokens per Sec:     3849, Lr: 0.000138\r\n",
      "2024-08-27 15:55:09,782 - INFO - joeynmt.training - Epoch 177, Step:    16835, Batch Loss:     0.063895, Batch Acc: 0.993620, Tokens per Sec:     3908, Lr: 0.000138\r\n",
      "2024-08-27 15:55:10,888 - INFO - joeynmt.training - Epoch 177, Step:    16840, Batch Loss:     0.076764, Batch Acc: 0.994286, Tokens per Sec:     3799, Lr: 0.000138\r\n",
      "2024-08-27 15:55:11,266 - INFO - joeynmt.training - Epoch 177, total training loss: 6.99, num. of seqs: 8065, num. of tokens: 80463, 21.1319[sec]\r\n",
      "2024-08-27 15:55:11,266 - INFO - joeynmt.training - EPOCH 178\r\n",
      "2024-08-27 15:55:12,159 - INFO - joeynmt.training - Epoch 178, Step:    16845, Batch Loss:     0.068314, Batch Acc: 0.995217, Tokens per Sec:     4001, Lr: 0.000138\r\n",
      "2024-08-27 15:55:13,255 - INFO - joeynmt.training - Epoch 178, Step:    16850, Batch Loss:     0.062443, Batch Acc: 0.995078, Tokens per Sec:     3709, Lr: 0.000138\r\n",
      "2024-08-27 15:55:14,351 - INFO - joeynmt.training - Epoch 178, Step:    16855, Batch Loss:     0.076698, Batch Acc: 0.992058, Tokens per Sec:     3906, Lr: 0.000138\r\n",
      "2024-08-27 15:55:15,447 - INFO - joeynmt.training - Epoch 178, Step:    16860, Batch Loss:     0.080730, Batch Acc: 0.994488, Tokens per Sec:     3812, Lr: 0.000138\r\n",
      "2024-08-27 15:55:16,558 - INFO - joeynmt.training - Epoch 178, Step:    16865, Batch Loss:     0.080096, Batch Acc: 0.993789, Tokens per Sec:     3478, Lr: 0.000138\r\n",
      "2024-08-27 15:55:17,684 - INFO - joeynmt.training - Epoch 178, Step:    16870, Batch Loss:     0.059913, Batch Acc: 0.993743, Tokens per Sec:     3835, Lr: 0.000138\r\n",
      "2024-08-27 15:55:18,786 - INFO - joeynmt.training - Epoch 178, Step:    16875, Batch Loss:     0.074332, Batch Acc: 0.992612, Tokens per Sec:     3810, Lr: 0.000138\r\n",
      "2024-08-27 15:55:19,861 - INFO - joeynmt.training - Epoch 178, Step:    16880, Batch Loss:     0.066780, Batch Acc: 0.993858, Tokens per Sec:     3939, Lr: 0.000138\r\n",
      "2024-08-27 15:55:20,938 - INFO - joeynmt.training - Epoch 178, Step:    16885, Batch Loss:     0.076199, Batch Acc: 0.993536, Tokens per Sec:     4026, Lr: 0.000138\r\n",
      "2024-08-27 15:55:22,012 - INFO - joeynmt.training - Epoch 178, Step:    16890, Batch Loss:     0.076976, Batch Acc: 0.992539, Tokens per Sec:     3869, Lr: 0.000138\r\n",
      "2024-08-27 15:55:23,096 - INFO - joeynmt.training - Epoch 178, Step:    16895, Batch Loss:     0.070979, Batch Acc: 0.995221, Tokens per Sec:     3866, Lr: 0.000138\r\n",
      "2024-08-27 15:55:24,183 - INFO - joeynmt.training - Epoch 178, Step:    16900, Batch Loss:     0.075227, Batch Acc: 0.993757, Tokens per Sec:     3983, Lr: 0.000138\r\n",
      "2024-08-27 15:55:25,292 - INFO - joeynmt.training - Epoch 178, Step:    16905, Batch Loss:     0.072967, Batch Acc: 0.992968, Tokens per Sec:     3846, Lr: 0.000138\r\n",
      "2024-08-27 15:55:26,398 - INFO - joeynmt.training - Epoch 178, Step:    16910, Batch Loss:     0.067493, Batch Acc: 0.991930, Tokens per Sec:     3813, Lr: 0.000138\r\n",
      "2024-08-27 15:55:27,509 - INFO - joeynmt.training - Epoch 178, Step:    16915, Batch Loss:     0.071398, Batch Acc: 0.992850, Tokens per Sec:     3654, Lr: 0.000138\r\n",
      "2024-08-27 15:55:28,676 - INFO - joeynmt.training - Epoch 178, Step:    16920, Batch Loss:     0.087619, Batch Acc: 0.993024, Tokens per Sec:     3440, Lr: 0.000138\r\n",
      "2024-08-27 15:55:29,768 - INFO - joeynmt.training - Epoch 178, Step:    16925, Batch Loss:     0.071275, Batch Acc: 0.993775, Tokens per Sec:     3828, Lr: 0.000138\r\n",
      "2024-08-27 15:55:30,875 - INFO - joeynmt.training - Epoch 178, Step:    16930, Batch Loss:     0.076457, Batch Acc: 0.990282, Tokens per Sec:     3721, Lr: 0.000137\r\n",
      "2024-08-27 15:55:31,984 - INFO - joeynmt.training - Epoch 178, Step:    16935, Batch Loss:     0.072109, Batch Acc: 0.993954, Tokens per Sec:     3730, Lr: 0.000137\r\n",
      "2024-08-27 15:55:32,467 - INFO - joeynmt.training - Epoch 178, total training loss: 6.98, num. of seqs: 8065, num. of tokens: 80463, 21.1839[sec]\r\n",
      "2024-08-27 15:55:32,467 - INFO - joeynmt.training - EPOCH 179\r\n",
      "2024-08-27 15:55:33,130 - INFO - joeynmt.training - Epoch 179, Step:    16940, Batch Loss:     0.075410, Batch Acc: 0.993365, Tokens per Sec:     3889, Lr: 0.000137\r\n",
      "2024-08-27 15:55:34,222 - INFO - joeynmt.training - Epoch 179, Step:    16945, Batch Loss:     0.069020, Batch Acc: 0.992734, Tokens per Sec:     4034, Lr: 0.000137\r\n",
      "2024-08-27 15:55:35,328 - INFO - joeynmt.training - Epoch 179, Step:    16950, Batch Loss:     0.074917, Batch Acc: 0.995433, Tokens per Sec:     3962, Lr: 0.000137\r\n",
      "2024-08-27 15:55:36,428 - INFO - joeynmt.training - Epoch 179, Step:    16955, Batch Loss:     0.069156, Batch Acc: 0.995078, Tokens per Sec:     3696, Lr: 0.000137\r\n",
      "2024-08-27 15:55:37,536 - INFO - joeynmt.training - Epoch 179, Step:    16960, Batch Loss:     0.066246, Batch Acc: 0.992272, Tokens per Sec:     3742, Lr: 0.000137\r\n",
      "2024-08-27 15:55:38,627 - INFO - joeynmt.training - Epoch 179, Step:    16965, Batch Loss:     0.065081, Batch Acc: 0.993432, Tokens per Sec:     3769, Lr: 0.000137\r\n",
      "2024-08-27 15:55:39,728 - INFO - joeynmt.training - Epoch 179, Step:    16970, Batch Loss:     0.079309, Batch Acc: 0.992896, Tokens per Sec:     3838, Lr: 0.000137\r\n",
      "2024-08-27 15:55:40,834 - INFO - joeynmt.training - Epoch 179, Step:    16975, Batch Loss:     0.071227, Batch Acc: 0.994453, Tokens per Sec:     3915, Lr: 0.000137\r\n",
      "2024-08-27 15:55:41,930 - INFO - joeynmt.training - Epoch 179, Step:    16980, Batch Loss:     0.067393, Batch Acc: 0.993316, Tokens per Sec:     3825, Lr: 0.000137\r\n",
      "2024-08-27 15:55:43,007 - INFO - joeynmt.training - Epoch 179, Step:    16985, Batch Loss:     0.066398, Batch Acc: 0.992696, Tokens per Sec:     3941, Lr: 0.000137\r\n",
      "2024-08-27 15:55:44,128 - INFO - joeynmt.training - Epoch 179, Step:    16990, Batch Loss:     0.077675, Batch Acc: 0.993310, Tokens per Sec:     3870, Lr: 0.000137\r\n",
      "2024-08-27 15:55:45,210 - INFO - joeynmt.training - Epoch 179, Step:    16995, Batch Loss:     0.070897, Batch Acc: 0.992794, Tokens per Sec:     3850, Lr: 0.000137\r\n",
      "2024-08-27 15:55:46,297 - INFO - joeynmt.training - Epoch 179, Step:    17000, Batch Loss:     0.079875, Batch Acc: 0.991641, Tokens per Sec:     3853, Lr: 0.000137\r\n",
      "2024-08-27 15:55:46,298 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=17042\r\n",
      "2024-08-27 15:55:46,298 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:23<00:00, 62.89it/s]\r\n",
      "2024-08-27 15:56:09,515 - INFO - joeynmt.prediction - Generation took 23.2148[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44550.98 examples/s]\r\n",
      "2024-08-27 15:56:09,903 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:56:09,903 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.11, loss:   6.86, ppl: 954.07, acc:   0.29, 0.1720[sec]\r\n",
      "2024-08-27 15:56:09,905 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:56:10,057 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:56:10,057 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:56:10,057 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 15:56:10,353 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:56:10,499 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:56:10,500 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:56:10,500 - INFO - joeynmt.training - \tHypothesis: trois tables blanches\r\n",
      "2024-08-27 15:56:10,794 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:56:10,941 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:56:10,941 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:56:10,941 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 15:56:11,238 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:56:11,385 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:56:11,386 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:56:11,386 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 15:56:12,771 - INFO - joeynmt.training - Epoch 179, Step:    17005, Batch Loss:     0.080388, Batch Acc: 0.990748, Tokens per Sec:     3775, Lr: 0.000137\r\n",
      "2024-08-27 15:56:13,862 - INFO - joeynmt.training - Epoch 179, Step:    17010, Batch Loss:     0.072735, Batch Acc: 0.991679, Tokens per Sec:     3857, Lr: 0.000137\r\n",
      "2024-08-27 15:56:14,959 - INFO - joeynmt.training - Epoch 179, Step:    17015, Batch Loss:     0.071023, Batch Acc: 0.992670, Tokens per Sec:     3731, Lr: 0.000137\r\n",
      "2024-08-27 15:56:16,053 - INFO - joeynmt.training - Epoch 179, Step:    17020, Batch Loss:     0.078590, Batch Acc: 0.993271, Tokens per Sec:     3943, Lr: 0.000137\r\n",
      "2024-08-27 15:56:17,180 - INFO - joeynmt.training - Epoch 179, Step:    17025, Batch Loss:     0.072049, Batch Acc: 0.995196, Tokens per Sec:     3880, Lr: 0.000137\r\n",
      "2024-08-27 15:56:18,276 - INFO - joeynmt.training - Epoch 179, Step:    17030, Batch Loss:     0.075880, Batch Acc: 0.992993, Tokens per Sec:     4038, Lr: 0.000137\r\n",
      "2024-08-27 15:56:18,715 - INFO - joeynmt.training - Epoch 179, total training loss: 6.96, num. of seqs: 8065, num. of tokens: 80463, 20.8481[sec]\r\n",
      "2024-08-27 15:56:18,716 - INFO - joeynmt.training - EPOCH 180\r\n",
      "2024-08-27 15:56:19,372 - INFO - joeynmt.training - Epoch 180, Step:    17035, Batch Loss:     0.073500, Batch Acc: 0.992369, Tokens per Sec:     3809, Lr: 0.000137\r\n",
      "2024-08-27 15:56:20,485 - INFO - joeynmt.training - Epoch 180, Step:    17040, Batch Loss:     0.067703, Batch Acc: 0.994856, Tokens per Sec:     3847, Lr: 0.000137\r\n",
      "2024-08-27 15:56:21,560 - INFO - joeynmt.training - Epoch 180, Step:    17045, Batch Loss:     0.069903, Batch Acc: 0.992333, Tokens per Sec:     4004, Lr: 0.000137\r\n",
      "2024-08-27 15:56:22,654 - INFO - joeynmt.training - Epoch 180, Step:    17050, Batch Loss:     0.066192, Batch Acc: 0.995631, Tokens per Sec:     3771, Lr: 0.000137\r\n",
      "2024-08-27 15:56:23,756 - INFO - joeynmt.training - Epoch 180, Step:    17055, Batch Loss:     0.079743, Batch Acc: 0.992580, Tokens per Sec:     3792, Lr: 0.000137\r\n",
      "2024-08-27 15:56:24,845 - INFO - joeynmt.training - Epoch 180, Step:    17060, Batch Loss:     0.063853, Batch Acc: 0.993901, Tokens per Sec:     3769, Lr: 0.000137\r\n",
      "2024-08-27 15:56:25,928 - INFO - joeynmt.training - Epoch 180, Step:    17065, Batch Loss:     0.078418, Batch Acc: 0.993125, Tokens per Sec:     3895, Lr: 0.000137\r\n",
      "2024-08-27 15:56:27,066 - INFO - joeynmt.training - Epoch 180, Step:    17070, Batch Loss:     0.071804, Batch Acc: 0.993970, Tokens per Sec:     3792, Lr: 0.000137\r\n",
      "2024-08-27 15:56:28,160 - INFO - joeynmt.training - Epoch 180, Step:    17075, Batch Loss:     0.089829, Batch Acc: 0.992837, Tokens per Sec:     3960, Lr: 0.000137\r\n",
      "2024-08-27 15:56:29,253 - INFO - joeynmt.training - Epoch 180, Step:    17080, Batch Loss:     0.072469, Batch Acc: 0.991492, Tokens per Sec:     3979, Lr: 0.000137\r\n",
      "2024-08-27 15:56:30,354 - INFO - joeynmt.training - Epoch 180, Step:    17085, Batch Loss:     0.076663, Batch Acc: 0.993039, Tokens per Sec:     3788, Lr: 0.000137\r\n",
      "2024-08-27 15:56:31,693 - INFO - joeynmt.training - Epoch 180, Step:    17090, Batch Loss:     0.072612, Batch Acc: 0.992410, Tokens per Sec:     3150, Lr: 0.000137\r\n",
      "2024-08-27 15:56:32,798 - INFO - joeynmt.training - Epoch 180, Step:    17095, Batch Loss:     0.075235, Batch Acc: 0.992408, Tokens per Sec:     3816, Lr: 0.000137\r\n",
      "2024-08-27 15:56:33,911 - INFO - joeynmt.training - Epoch 180, Step:    17100, Batch Loss:     0.083933, Batch Acc: 0.992270, Tokens per Sec:     3837, Lr: 0.000137\r\n",
      "2024-08-27 15:56:34,999 - INFO - joeynmt.training - Epoch 180, Step:    17105, Batch Loss:     0.072061, Batch Acc: 0.993246, Tokens per Sec:     3949, Lr: 0.000137\r\n",
      "2024-08-27 15:56:36,091 - INFO - joeynmt.training - Epoch 180, Step:    17110, Batch Loss:     0.082590, Batch Acc: 0.993018, Tokens per Sec:     4198, Lr: 0.000137\r\n",
      "2024-08-27 15:56:37,215 - INFO - joeynmt.training - Epoch 180, Step:    17115, Batch Loss:     0.082430, Batch Acc: 0.991101, Tokens per Sec:     3703, Lr: 0.000137\r\n",
      "2024-08-27 15:56:38,303 - INFO - joeynmt.training - Epoch 180, Step:    17120, Batch Loss:     0.064252, Batch Acc: 0.994887, Tokens per Sec:     3955, Lr: 0.000137\r\n",
      "2024-08-27 15:56:39,384 - INFO - joeynmt.training - Epoch 180, Step:    17125, Batch Loss:     0.064830, Batch Acc: 0.992965, Tokens per Sec:     3817, Lr: 0.000137\r\n",
      "2024-08-27 15:56:39,808 - INFO - joeynmt.training - Epoch 180, total training loss: 7.02, num. of seqs: 8065, num. of tokens: 80463, 21.0769[sec]\r\n",
      "2024-08-27 15:56:39,809 - INFO - joeynmt.training - EPOCH 181\r\n",
      "2024-08-27 15:56:40,471 - INFO - joeynmt.training - Epoch 181, Step:    17130, Batch Loss:     0.062362, Batch Acc: 0.995710, Tokens per Sec:     3889, Lr: 0.000137\r\n",
      "2024-08-27 15:56:41,572 - INFO - joeynmt.training - Epoch 181, Step:    17135, Batch Loss:     0.077580, Batch Acc: 0.994960, Tokens per Sec:     3967, Lr: 0.000137\r\n",
      "2024-08-27 15:56:42,656 - INFO - joeynmt.training - Epoch 181, Step:    17140, Batch Loss:     0.067315, Batch Acc: 0.992428, Tokens per Sec:     4025, Lr: 0.000137\r\n",
      "2024-08-27 15:56:43,749 - INFO - joeynmt.training - Epoch 181, Step:    17145, Batch Loss:     0.065452, Batch Acc: 0.994160, Tokens per Sec:     3918, Lr: 0.000137\r\n",
      "2024-08-27 15:56:44,856 - INFO - joeynmt.training - Epoch 181, Step:    17150, Batch Loss:     0.071663, Batch Acc: 0.993945, Tokens per Sec:     3732, Lr: 0.000137\r\n",
      "2024-08-27 15:56:45,939 - INFO - joeynmt.training - Epoch 181, Step:    17155, Batch Loss:     0.074446, Batch Acc: 0.994692, Tokens per Sec:     3829, Lr: 0.000137\r\n",
      "2024-08-27 15:56:47,083 - INFO - joeynmt.training - Epoch 181, Step:    17160, Batch Loss:     0.092052, Batch Acc: 0.995171, Tokens per Sec:     3806, Lr: 0.000137\r\n",
      "2024-08-27 15:56:48,173 - INFO - joeynmt.training - Epoch 181, Step:    17165, Batch Loss:     0.067421, Batch Acc: 0.995241, Tokens per Sec:     3858, Lr: 0.000137\r\n",
      "2024-08-27 15:56:49,255 - INFO - joeynmt.training - Epoch 181, Step:    17170, Batch Loss:     0.067503, Batch Acc: 0.993492, Tokens per Sec:     3835, Lr: 0.000137\r\n",
      "2024-08-27 15:56:50,356 - INFO - joeynmt.training - Epoch 181, Step:    17175, Batch Loss:     0.076623, Batch Acc: 0.992419, Tokens per Sec:     3836, Lr: 0.000136\r\n",
      "2024-08-27 15:56:51,453 - INFO - joeynmt.training - Epoch 181, Step:    17180, Batch Loss:     0.072276, Batch Acc: 0.990559, Tokens per Sec:     3767, Lr: 0.000136\r\n",
      "2024-08-27 15:56:52,552 - INFO - joeynmt.training - Epoch 181, Step:    17185, Batch Loss:     0.073953, Batch Acc: 0.991467, Tokens per Sec:     3843, Lr: 0.000136\r\n",
      "2024-08-27 15:56:53,643 - INFO - joeynmt.training - Epoch 181, Step:    17190, Batch Loss:     0.068648, Batch Acc: 0.992726, Tokens per Sec:     3781, Lr: 0.000136\r\n",
      "2024-08-27 15:56:54,736 - INFO - joeynmt.training - Epoch 181, Step:    17195, Batch Loss:     0.076646, Batch Acc: 0.993908, Tokens per Sec:     3908, Lr: 0.000136\r\n",
      "2024-08-27 15:56:55,816 - INFO - joeynmt.training - Epoch 181, Step:    17200, Batch Loss:     0.077796, Batch Acc: 0.993853, Tokens per Sec:     3769, Lr: 0.000136\r\n",
      "2024-08-27 15:56:56,992 - INFO - joeynmt.training - Epoch 181, Step:    17205, Batch Loss:     0.073143, Batch Acc: 0.992421, Tokens per Sec:     3705, Lr: 0.000136\r\n",
      "2024-08-27 15:56:58,062 - INFO - joeynmt.training - Epoch 181, Step:    17210, Batch Loss:     0.064107, Batch Acc: 0.994748, Tokens per Sec:     3916, Lr: 0.000136\r\n",
      "2024-08-27 15:56:59,157 - INFO - joeynmt.training - Epoch 181, Step:    17215, Batch Loss:     0.075246, Batch Acc: 0.992833, Tokens per Sec:     3825, Lr: 0.000136\r\n",
      "2024-08-27 15:57:00,248 - INFO - joeynmt.training - Epoch 181, Step:    17220, Batch Loss:     0.074666, Batch Acc: 0.991566, Tokens per Sec:     3807, Lr: 0.000136\r\n",
      "2024-08-27 15:57:00,816 - INFO - joeynmt.training - Epoch 181, total training loss: 6.87, num. of seqs: 8065, num. of tokens: 80463, 20.9907[sec]\r\n",
      "2024-08-27 15:57:00,816 - INFO - joeynmt.training - EPOCH 182\r\n",
      "2024-08-27 15:57:01,501 - INFO - joeynmt.training - Epoch 182, Step:    17225, Batch Loss:     0.076645, Batch Acc: 0.991835, Tokens per Sec:     3783, Lr: 0.000136\r\n",
      "2024-08-27 15:57:02,688 - INFO - joeynmt.training - Epoch 182, Step:    17230, Batch Loss:     0.073160, Batch Acc: 0.995562, Tokens per Sec:     3609, Lr: 0.000136\r\n",
      "2024-08-27 15:57:03,772 - INFO - joeynmt.training - Epoch 182, Step:    17235, Batch Loss:     0.075289, Batch Acc: 0.994926, Tokens per Sec:     3821, Lr: 0.000136\r\n",
      "2024-08-27 15:57:04,974 - INFO - joeynmt.training - Epoch 182, Step:    17240, Batch Loss:     0.071944, Batch Acc: 0.992126, Tokens per Sec:     3488, Lr: 0.000136\r\n",
      "2024-08-27 15:57:06,045 - INFO - joeynmt.training - Epoch 182, Step:    17245, Batch Loss:     0.071919, Batch Acc: 0.993574, Tokens per Sec:     3928, Lr: 0.000136\r\n",
      "2024-08-27 15:57:07,166 - INFO - joeynmt.training - Epoch 182, Step:    17250, Batch Loss:     0.069951, Batch Acc: 0.992809, Tokens per Sec:     3848, Lr: 0.000136\r\n",
      "2024-08-27 15:57:08,261 - INFO - joeynmt.training - Epoch 182, Step:    17255, Batch Loss:     0.075134, Batch Acc: 0.994419, Tokens per Sec:     3765, Lr: 0.000136\r\n",
      "2024-08-27 15:57:09,370 - INFO - joeynmt.training - Epoch 182, Step:    17260, Batch Loss:     0.068746, Batch Acc: 0.992391, Tokens per Sec:     3913, Lr: 0.000136\r\n",
      "2024-08-27 15:57:10,475 - INFO - joeynmt.training - Epoch 182, Step:    17265, Batch Loss:     0.084335, Batch Acc: 0.991016, Tokens per Sec:     3933, Lr: 0.000136\r\n",
      "2024-08-27 15:57:11,569 - INFO - joeynmt.training - Epoch 182, Step:    17270, Batch Loss:     0.067097, Batch Acc: 0.994066, Tokens per Sec:     3853, Lr: 0.000136\r\n",
      "2024-08-27 15:57:12,672 - INFO - joeynmt.training - Epoch 182, Step:    17275, Batch Loss:     0.070227, Batch Acc: 0.994197, Tokens per Sec:     3910, Lr: 0.000136\r\n",
      "2024-08-27 15:57:13,774 - INFO - joeynmt.training - Epoch 182, Step:    17280, Batch Loss:     0.064620, Batch Acc: 0.994524, Tokens per Sec:     3976, Lr: 0.000136\r\n",
      "2024-08-27 15:57:14,861 - INFO - joeynmt.training - Epoch 182, Step:    17285, Batch Loss:     0.073478, Batch Acc: 0.993982, Tokens per Sec:     3674, Lr: 0.000136\r\n",
      "2024-08-27 15:57:15,951 - INFO - joeynmt.training - Epoch 182, Step:    17290, Batch Loss:     0.069748, Batch Acc: 0.994622, Tokens per Sec:     3756, Lr: 0.000136\r\n",
      "2024-08-27 15:57:17,070 - INFO - joeynmt.training - Epoch 182, Step:    17295, Batch Loss:     0.063175, Batch Acc: 0.990699, Tokens per Sec:     3748, Lr: 0.000136\r\n",
      "2024-08-27 15:57:18,160 - INFO - joeynmt.training - Epoch 182, Step:    17300, Batch Loss:     0.069212, Batch Acc: 0.994493, Tokens per Sec:     3667, Lr: 0.000136\r\n",
      "2024-08-27 15:57:19,240 - INFO - joeynmt.training - Epoch 182, Step:    17305, Batch Loss:     0.079116, Batch Acc: 0.993685, Tokens per Sec:     4111, Lr: 0.000136\r\n",
      "2024-08-27 15:57:20,325 - INFO - joeynmt.training - Epoch 182, Step:    17310, Batch Loss:     0.078001, Batch Acc: 0.992847, Tokens per Sec:     3865, Lr: 0.000136\r\n",
      "2024-08-27 15:57:21,401 - INFO - joeynmt.training - Epoch 182, Step:    17315, Batch Loss:     0.076266, Batch Acc: 0.993224, Tokens per Sec:     3845, Lr: 0.000136\r\n",
      "2024-08-27 15:57:21,947 - INFO - joeynmt.training - Epoch 182, total training loss: 6.89, num. of seqs: 8065, num. of tokens: 80463, 21.1119[sec]\r\n",
      "2024-08-27 15:57:21,947 - INFO - joeynmt.training - EPOCH 183\r\n",
      "2024-08-27 15:57:22,621 - INFO - joeynmt.training - Epoch 183, Step:    17320, Batch Loss:     0.067809, Batch Acc: 0.993399, Tokens per Sec:     4076, Lr: 0.000136\r\n",
      "2024-08-27 15:57:23,721 - INFO - joeynmt.training - Epoch 183, Step:    17325, Batch Loss:     0.074423, Batch Acc: 0.996456, Tokens per Sec:     3848, Lr: 0.000136\r\n",
      "2024-08-27 15:57:24,806 - INFO - joeynmt.training - Epoch 183, Step:    17330, Batch Loss:     0.087283, Batch Acc: 0.992374, Tokens per Sec:     3871, Lr: 0.000136\r\n",
      "2024-08-27 15:57:25,874 - INFO - joeynmt.training - Epoch 183, Step:    17335, Batch Loss:     0.084517, Batch Acc: 0.991992, Tokens per Sec:     3745, Lr: 0.000136\r\n",
      "2024-08-27 15:57:26,987 - INFO - joeynmt.training - Epoch 183, Step:    17340, Batch Loss:     0.062212, Batch Acc: 0.993948, Tokens per Sec:     3862, Lr: 0.000136\r\n",
      "2024-08-27 15:57:28,077 - INFO - joeynmt.training - Epoch 183, Step:    17345, Batch Loss:     0.076329, Batch Acc: 0.991891, Tokens per Sec:     3963, Lr: 0.000136\r\n",
      "2024-08-27 15:57:29,162 - INFO - joeynmt.training - Epoch 183, Step:    17350, Batch Loss:     0.062299, Batch Acc: 0.995071, Tokens per Sec:     3742, Lr: 0.000136\r\n",
      "2024-08-27 15:57:30,258 - INFO - joeynmt.training - Epoch 183, Step:    17355, Batch Loss:     0.065729, Batch Acc: 0.993562, Tokens per Sec:     3827, Lr: 0.000136\r\n",
      "2024-08-27 15:57:31,349 - INFO - joeynmt.training - Epoch 183, Step:    17360, Batch Loss:     0.075682, Batch Acc: 0.992726, Tokens per Sec:     3785, Lr: 0.000136\r\n",
      "2024-08-27 15:57:32,445 - INFO - joeynmt.training - Epoch 183, Step:    17365, Batch Loss:     0.072403, Batch Acc: 0.992211, Tokens per Sec:     3867, Lr: 0.000136\r\n",
      "2024-08-27 15:57:33,638 - INFO - joeynmt.training - Epoch 183, Step:    17370, Batch Loss:     0.065636, Batch Acc: 0.993057, Tokens per Sec:     3384, Lr: 0.000136\r\n",
      "2024-08-27 15:57:34,748 - INFO - joeynmt.training - Epoch 183, Step:    17375, Batch Loss:     0.075166, Batch Acc: 0.994494, Tokens per Sec:     3765, Lr: 0.000136\r\n",
      "2024-08-27 15:57:35,886 - INFO - joeynmt.training - Epoch 183, Step:    17380, Batch Loss:     0.063899, Batch Acc: 0.994480, Tokens per Sec:     3824, Lr: 0.000136\r\n",
      "2024-08-27 15:57:37,014 - INFO - joeynmt.training - Epoch 183, Step:    17385, Batch Loss:     0.075017, Batch Acc: 0.995517, Tokens per Sec:     3562, Lr: 0.000136\r\n",
      "2024-08-27 15:57:38,125 - INFO - joeynmt.training - Epoch 183, Step:    17390, Batch Loss:     0.083649, Batch Acc: 0.992148, Tokens per Sec:     3898, Lr: 0.000136\r\n",
      "2024-08-27 15:57:39,224 - INFO - joeynmt.training - Epoch 183, Step:    17395, Batch Loss:     0.065834, Batch Acc: 0.992809, Tokens per Sec:     3927, Lr: 0.000136\r\n",
      "2024-08-27 15:57:40,332 - INFO - joeynmt.training - Epoch 183, Step:    17400, Batch Loss:     0.082333, Batch Acc: 0.991972, Tokens per Sec:     3825, Lr: 0.000136\r\n",
      "2024-08-27 15:57:41,447 - INFO - joeynmt.training - Epoch 183, Step:    17405, Batch Loss:     0.070547, Batch Acc: 0.993357, Tokens per Sec:     3782, Lr: 0.000136\r\n",
      "2024-08-27 15:57:42,535 - INFO - joeynmt.training - Epoch 183, Step:    17410, Batch Loss:     0.067512, Batch Acc: 0.994830, Tokens per Sec:     3911, Lr: 0.000136\r\n",
      "2024-08-27 15:57:43,123 - INFO - joeynmt.training - Epoch 183, total training loss: 6.84, num. of seqs: 8065, num. of tokens: 80463, 21.1588[sec]\r\n",
      "2024-08-27 15:57:43,124 - INFO - joeynmt.training - EPOCH 184\r\n",
      "2024-08-27 15:57:43,782 - INFO - joeynmt.training - Epoch 184, Step:    17415, Batch Loss:     0.079242, Batch Acc: 0.994045, Tokens per Sec:     3848, Lr: 0.000136\r\n",
      "2024-08-27 15:57:44,866 - INFO - joeynmt.training - Epoch 184, Step:    17420, Batch Loss:     0.073994, Batch Acc: 0.994515, Tokens per Sec:     3872, Lr: 0.000136\r\n",
      "2024-08-27 15:57:45,945 - INFO - joeynmt.training - Epoch 184, Step:    17425, Batch Loss:     0.069464, Batch Acc: 0.993568, Tokens per Sec:     3892, Lr: 0.000136\r\n",
      "2024-08-27 15:57:47,089 - INFO - joeynmt.training - Epoch 184, Step:    17430, Batch Loss:     0.076210, Batch Acc: 0.991801, Tokens per Sec:     3734, Lr: 0.000135\r\n",
      "2024-08-27 15:57:48,191 - INFO - joeynmt.training - Epoch 184, Step:    17435, Batch Loss:     0.060038, Batch Acc: 0.995906, Tokens per Sec:     3770, Lr: 0.000135\r\n",
      "2024-08-27 15:57:49,276 - INFO - joeynmt.training - Epoch 184, Step:    17440, Batch Loss:     0.071777, Batch Acc: 0.992474, Tokens per Sec:     3802, Lr: 0.000135\r\n",
      "2024-08-27 15:57:50,371 - INFO - joeynmt.training - Epoch 184, Step:    17445, Batch Loss:     0.073047, Batch Acc: 0.993705, Tokens per Sec:     3773, Lr: 0.000135\r\n",
      "2024-08-27 15:57:51,472 - INFO - joeynmt.training - Epoch 184, Step:    17450, Batch Loss:     0.073457, Batch Acc: 0.994138, Tokens per Sec:     3719, Lr: 0.000135\r\n",
      "2024-08-27 15:57:52,567 - INFO - joeynmt.training - Epoch 184, Step:    17455, Batch Loss:     0.067431, Batch Acc: 0.992421, Tokens per Sec:     3978, Lr: 0.000135\r\n",
      "2024-08-27 15:57:53,648 - INFO - joeynmt.training - Epoch 184, Step:    17460, Batch Loss:     0.066061, Batch Acc: 0.995418, Tokens per Sec:     3838, Lr: 0.000135\r\n",
      "2024-08-27 15:57:54,759 - INFO - joeynmt.training - Epoch 184, Step:    17465, Batch Loss:     0.075954, Batch Acc: 0.992062, Tokens per Sec:     3972, Lr: 0.000135\r\n",
      "2024-08-27 15:57:55,855 - INFO - joeynmt.training - Epoch 184, Step:    17470, Batch Loss:     0.064726, Batch Acc: 0.992217, Tokens per Sec:     3638, Lr: 0.000135\r\n",
      "2024-08-27 15:57:56,995 - INFO - joeynmt.training - Epoch 184, Step:    17475, Batch Loss:     0.065475, Batch Acc: 0.991864, Tokens per Sec:     3560, Lr: 0.000135\r\n",
      "2024-08-27 15:57:58,079 - INFO - joeynmt.training - Epoch 184, Step:    17480, Batch Loss:     0.071926, Batch Acc: 0.992150, Tokens per Sec:     3880, Lr: 0.000135\r\n",
      "2024-08-27 15:57:59,165 - INFO - joeynmt.training - Epoch 184, Step:    17485, Batch Loss:     0.072440, Batch Acc: 0.991962, Tokens per Sec:     3897, Lr: 0.000135\r\n",
      "2024-08-27 15:58:00,254 - INFO - joeynmt.training - Epoch 184, Step:    17490, Batch Loss:     0.076723, Batch Acc: 0.994283, Tokens per Sec:     3856, Lr: 0.000135\r\n",
      "2024-08-27 15:58:01,349 - INFO - joeynmt.training - Epoch 184, Step:    17495, Batch Loss:     0.075488, Batch Acc: 0.994370, Tokens per Sec:     3899, Lr: 0.000135\r\n",
      "2024-08-27 15:58:02,435 - INFO - joeynmt.training - Epoch 184, Step:    17500, Batch Loss:     0.063250, Batch Acc: 0.993005, Tokens per Sec:     3951, Lr: 0.000135\r\n",
      "2024-08-27 15:58:02,436 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=17542\r\n",
      "2024-08-27 15:58:02,436 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 64.81it/s]\r\n",
      "2024-08-27 15:58:24,965 - INFO - joeynmt.prediction - Generation took 22.5271[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 42658.76 examples/s]\r\n",
      "2024-08-27 15:58:25,354 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 15:58:25,354 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.26, loss:   6.88, ppl: 970.74, acc:   0.28, 0.1741[sec]\r\n",
      "2024-08-27 15:58:25,355 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 15:58:25,502 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 15:58:25,502 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 15:58:25,503 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 15:58:25,795 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 15:58:25,941 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 15:58:25,941 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 15:58:25,942 - INFO - joeynmt.training - \tHypothesis: trois tables blanches\r\n",
      "2024-08-27 15:58:26,235 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 15:58:26,381 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 15:58:26,381 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 15:58:26,381 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 15:58:26,680 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 15:58:26,841 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 15:58:26,841 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 15:58:26,841 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 15:58:28,223 - INFO - joeynmt.training - Epoch 184, Step:    17505, Batch Loss:     0.064434, Batch Acc: 0.992987, Tokens per Sec:     3957, Lr: 0.000135\r\n",
      "2024-08-27 15:58:28,873 - INFO - joeynmt.training - Epoch 184, total training loss: 6.84, num. of seqs: 8065, num. of tokens: 80463, 21.0252[sec]\r\n",
      "2024-08-27 15:58:28,873 - INFO - joeynmt.training - EPOCH 185\r\n",
      "2024-08-27 15:58:29,317 - INFO - joeynmt.training - Epoch 185, Step:    17510, Batch Loss:     0.062324, Batch Acc: 0.995986, Tokens per Sec:     3965, Lr: 0.000135\r\n",
      "2024-08-27 15:58:30,404 - INFO - joeynmt.training - Epoch 185, Step:    17515, Batch Loss:     0.064742, Batch Acc: 0.993817, Tokens per Sec:     3868, Lr: 0.000135\r\n",
      "2024-08-27 15:58:31,500 - INFO - joeynmt.training - Epoch 185, Step:    17520, Batch Loss:     0.069454, Batch Acc: 0.995993, Tokens per Sec:     3875, Lr: 0.000135\r\n",
      "2024-08-27 15:58:32,578 - INFO - joeynmt.training - Epoch 185, Step:    17525, Batch Loss:     0.057841, Batch Acc: 0.993382, Tokens per Sec:     3786, Lr: 0.000135\r\n",
      "2024-08-27 15:58:33,665 - INFO - joeynmt.training - Epoch 185, Step:    17530, Batch Loss:     0.064127, Batch Acc: 0.993799, Tokens per Sec:     3860, Lr: 0.000135\r\n",
      "2024-08-27 15:58:34,744 - INFO - joeynmt.training - Epoch 185, Step:    17535, Batch Loss:     0.067225, Batch Acc: 0.993253, Tokens per Sec:     3850, Lr: 0.000135\r\n",
      "2024-08-27 15:58:35,934 - INFO - joeynmt.training - Epoch 185, Step:    17540, Batch Loss:     0.060697, Batch Acc: 0.994668, Tokens per Sec:     3468, Lr: 0.000135\r\n",
      "2024-08-27 15:58:37,079 - INFO - joeynmt.training - Epoch 185, Step:    17545, Batch Loss:     0.064827, Batch Acc: 0.993103, Tokens per Sec:     3675, Lr: 0.000135\r\n",
      "2024-08-27 15:58:38,212 - INFO - joeynmt.training - Epoch 185, Step:    17550, Batch Loss:     0.071952, Batch Acc: 0.992029, Tokens per Sec:     3877, Lr: 0.000135\r\n",
      "2024-08-27 15:58:39,295 - INFO - joeynmt.training - Epoch 185, Step:    17555, Batch Loss:     0.065868, Batch Acc: 0.993876, Tokens per Sec:     3774, Lr: 0.000135\r\n",
      "2024-08-27 15:58:40,405 - INFO - joeynmt.training - Epoch 185, Step:    17560, Batch Loss:     0.069781, Batch Acc: 0.995044, Tokens per Sec:     3817, Lr: 0.000135\r\n",
      "2024-08-27 15:58:41,502 - INFO - joeynmt.training - Epoch 185, Step:    17565, Batch Loss:     0.068319, Batch Acc: 0.994737, Tokens per Sec:     3639, Lr: 0.000135\r\n",
      "2024-08-27 15:58:42,601 - INFO - joeynmt.training - Epoch 185, Step:    17570, Batch Loss:     0.071490, Batch Acc: 0.992162, Tokens per Sec:     3951, Lr: 0.000135\r\n",
      "2024-08-27 15:58:43,704 - INFO - joeynmt.training - Epoch 185, Step:    17575, Batch Loss:     0.065462, Batch Acc: 0.994538, Tokens per Sec:     3988, Lr: 0.000135\r\n",
      "2024-08-27 15:58:44,791 - INFO - joeynmt.training - Epoch 185, Step:    17580, Batch Loss:     0.080957, Batch Acc: 0.992640, Tokens per Sec:     4002, Lr: 0.000135\r\n",
      "2024-08-27 15:58:45,876 - INFO - joeynmt.training - Epoch 185, Step:    17585, Batch Loss:     0.072828, Batch Acc: 0.995112, Tokens per Sec:     3960, Lr: 0.000135\r\n",
      "2024-08-27 15:58:47,034 - INFO - joeynmt.training - Epoch 185, Step:    17590, Batch Loss:     0.067111, Batch Acc: 0.992530, Tokens per Sec:     3587, Lr: 0.000135\r\n",
      "2024-08-27 15:58:48,140 - INFO - joeynmt.training - Epoch 185, Step:    17595, Batch Loss:     0.070231, Batch Acc: 0.995026, Tokens per Sec:     3818, Lr: 0.000135\r\n",
      "2024-08-27 15:58:49,223 - INFO - joeynmt.training - Epoch 185, Step:    17600, Batch Loss:     0.060746, Batch Acc: 0.994280, Tokens per Sec:     4040, Lr: 0.000135\r\n",
      "2024-08-27 15:58:49,930 - INFO - joeynmt.training - Epoch 185, total training loss: 6.64, num. of seqs: 8065, num. of tokens: 80463, 21.0405[sec]\r\n",
      "2024-08-27 15:58:49,931 - INFO - joeynmt.training - EPOCH 186\r\n",
      "2024-08-27 15:58:50,391 - INFO - joeynmt.training - Epoch 186, Step:    17605, Batch Loss:     0.073159, Batch Acc: 0.995322, Tokens per Sec:     3753, Lr: 0.000135\r\n",
      "2024-08-27 15:58:51,510 - INFO - joeynmt.training - Epoch 186, Step:    17610, Batch Loss:     0.073091, Batch Acc: 0.994909, Tokens per Sec:     3688, Lr: 0.000135\r\n",
      "2024-08-27 15:58:52,605 - INFO - joeynmt.training - Epoch 186, Step:    17615, Batch Loss:     0.073124, Batch Acc: 0.992553, Tokens per Sec:     3925, Lr: 0.000135\r\n",
      "2024-08-27 15:58:53,695 - INFO - joeynmt.training - Epoch 186, Step:    17620, Batch Loss:     0.068340, Batch Acc: 0.993908, Tokens per Sec:     3919, Lr: 0.000135\r\n",
      "2024-08-27 15:58:54,788 - INFO - joeynmt.training - Epoch 186, Step:    17625, Batch Loss:     0.066494, Batch Acc: 0.992380, Tokens per Sec:     3724, Lr: 0.000135\r\n",
      "2024-08-27 15:58:55,889 - INFO - joeynmt.training - Epoch 186, Step:    17630, Batch Loss:     0.070390, Batch Acc: 0.993650, Tokens per Sec:     3864, Lr: 0.000135\r\n",
      "2024-08-27 15:58:57,074 - INFO - joeynmt.training - Epoch 186, Step:    17635, Batch Loss:     0.068406, Batch Acc: 0.993004, Tokens per Sec:     3500, Lr: 0.000135\r\n",
      "2024-08-27 15:58:58,188 - INFO - joeynmt.training - Epoch 186, Step:    17640, Batch Loss:     0.076499, Batch Acc: 0.994946, Tokens per Sec:     3910, Lr: 0.000135\r\n",
      "2024-08-27 15:58:59,299 - INFO - joeynmt.training - Epoch 186, Step:    17645, Batch Loss:     0.061124, Batch Acc: 0.991868, Tokens per Sec:     3875, Lr: 0.000135\r\n",
      "2024-08-27 15:59:00,408 - INFO - joeynmt.training - Epoch 186, Step:    17650, Batch Loss:     0.070630, Batch Acc: 0.995042, Tokens per Sec:     3823, Lr: 0.000135\r\n",
      "2024-08-27 15:59:01,500 - INFO - joeynmt.training - Epoch 186, Step:    17655, Batch Loss:     0.071793, Batch Acc: 0.993741, Tokens per Sec:     3808, Lr: 0.000135\r\n",
      "2024-08-27 15:59:02,592 - INFO - joeynmt.training - Epoch 186, Step:    17660, Batch Loss:     0.077308, Batch Acc: 0.994100, Tokens per Sec:     3881, Lr: 0.000135\r\n",
      "2024-08-27 15:59:03,675 - INFO - joeynmt.training - Epoch 186, Step:    17665, Batch Loss:     0.077737, Batch Acc: 0.994963, Tokens per Sec:     3854, Lr: 0.000135\r\n",
      "2024-08-27 15:59:04,763 - INFO - joeynmt.training - Epoch 186, Step:    17670, Batch Loss:     0.067070, Batch Acc: 0.993208, Tokens per Sec:     3927, Lr: 0.000135\r\n",
      "2024-08-27 15:59:05,838 - INFO - joeynmt.training - Epoch 186, Step:    17675, Batch Loss:     0.071444, Batch Acc: 0.992109, Tokens per Sec:     3890, Lr: 0.000135\r\n",
      "2024-08-27 15:59:07,040 - INFO - joeynmt.training - Epoch 186, Step:    17680, Batch Loss:     0.075014, Batch Acc: 0.991785, Tokens per Sec:     3446, Lr: 0.000135\r\n",
      "2024-08-27 15:59:08,233 - INFO - joeynmt.training - Epoch 186, Step:    17685, Batch Loss:     0.079797, Batch Acc: 0.993872, Tokens per Sec:     3557, Lr: 0.000135\r\n",
      "2024-08-27 15:59:09,409 - INFO - joeynmt.training - Epoch 186, Step:    17690, Batch Loss:     0.064591, Batch Acc: 0.994793, Tokens per Sec:     3597, Lr: 0.000134\r\n",
      "2024-08-27 15:59:10,488 - INFO - joeynmt.training - Epoch 186, Step:    17695, Batch Loss:     0.071564, Batch Acc: 0.991530, Tokens per Sec:     3831, Lr: 0.000134\r\n",
      "2024-08-27 15:59:11,290 - INFO - joeynmt.training - Epoch 186, total training loss: 6.72, num. of seqs: 8065, num. of tokens: 80463, 21.3420[sec]\r\n",
      "2024-08-27 15:59:11,290 - INFO - joeynmt.training - EPOCH 187\r\n",
      "2024-08-27 15:59:11,739 - INFO - joeynmt.training - Epoch 187, Step:    17700, Batch Loss:     0.063689, Batch Acc: 0.993068, Tokens per Sec:     3898, Lr: 0.000134\r\n",
      "2024-08-27 15:59:12,832 - INFO - joeynmt.training - Epoch 187, Step:    17705, Batch Loss:     0.069809, Batch Acc: 0.994708, Tokens per Sec:     3805, Lr: 0.000134\r\n",
      "2024-08-27 15:59:13,916 - INFO - joeynmt.training - Epoch 187, Step:    17710, Batch Loss:     0.085118, Batch Acc: 0.990931, Tokens per Sec:     3766, Lr: 0.000134\r\n",
      "2024-08-27 15:59:15,022 - INFO - joeynmt.training - Epoch 187, Step:    17715, Batch Loss:     0.064939, Batch Acc: 0.994079, Tokens per Sec:     3819, Lr: 0.000134\r\n",
      "2024-08-27 15:59:16,125 - INFO - joeynmt.training - Epoch 187, Step:    17720, Batch Loss:     0.062677, Batch Acc: 0.994536, Tokens per Sec:     3986, Lr: 0.000134\r\n",
      "2024-08-27 15:59:17,251 - INFO - joeynmt.training - Epoch 187, Step:    17725, Batch Loss:     0.065801, Batch Acc: 0.992883, Tokens per Sec:     3747, Lr: 0.000134\r\n",
      "2024-08-27 15:59:18,328 - INFO - joeynmt.training - Epoch 187, Step:    17730, Batch Loss:     0.067405, Batch Acc: 0.994867, Tokens per Sec:     3800, Lr: 0.000134\r\n",
      "2024-08-27 15:59:19,425 - INFO - joeynmt.training - Epoch 187, Step:    17735, Batch Loss:     0.070879, Batch Acc: 0.992923, Tokens per Sec:     3867, Lr: 0.000134\r\n",
      "2024-08-27 15:59:20,507 - INFO - joeynmt.training - Epoch 187, Step:    17740, Batch Loss:     0.066128, Batch Acc: 0.993309, Tokens per Sec:     3871, Lr: 0.000134\r\n",
      "2024-08-27 15:59:21,597 - INFO - joeynmt.training - Epoch 187, Step:    17745, Batch Loss:     0.067376, Batch Acc: 0.993146, Tokens per Sec:     3884, Lr: 0.000134\r\n",
      "2024-08-27 15:59:22,694 - INFO - joeynmt.training - Epoch 187, Step:    17750, Batch Loss:     0.095535, Batch Acc: 0.990548, Tokens per Sec:     3861, Lr: 0.000134\r\n",
      "2024-08-27 15:59:23,768 - INFO - joeynmt.training - Epoch 187, Step:    17755, Batch Loss:     0.078391, Batch Acc: 0.992993, Tokens per Sec:     3857, Lr: 0.000134\r\n",
      "2024-08-27 15:59:24,862 - INFO - joeynmt.training - Epoch 187, Step:    17760, Batch Loss:     0.075184, Batch Acc: 0.993054, Tokens per Sec:     3949, Lr: 0.000134\r\n",
      "2024-08-27 15:59:25,936 - INFO - joeynmt.training - Epoch 187, Step:    17765, Batch Loss:     0.079311, Batch Acc: 0.992324, Tokens per Sec:     3884, Lr: 0.000134\r\n",
      "2024-08-27 15:59:27,077 - INFO - joeynmt.training - Epoch 187, Step:    17770, Batch Loss:     0.069470, Batch Acc: 0.993427, Tokens per Sec:     3871, Lr: 0.000134\r\n",
      "2024-08-27 15:59:28,170 - INFO - joeynmt.training - Epoch 187, Step:    17775, Batch Loss:     0.062875, Batch Acc: 0.996261, Tokens per Sec:     3915, Lr: 0.000134\r\n",
      "2024-08-27 15:59:29,269 - INFO - joeynmt.training - Epoch 187, Step:    17780, Batch Loss:     0.082442, Batch Acc: 0.991679, Tokens per Sec:     3829, Lr: 0.000134\r\n",
      "2024-08-27 15:59:30,364 - INFO - joeynmt.training - Epoch 187, Step:    17785, Batch Loss:     0.086032, Batch Acc: 0.993039, Tokens per Sec:     3940, Lr: 0.000134\r\n",
      "2024-08-27 15:59:31,454 - INFO - joeynmt.training - Epoch 187, Step:    17790, Batch Loss:     0.072897, Batch Acc: 0.991641, Tokens per Sec:     3842, Lr: 0.000134\r\n",
      "2024-08-27 15:59:32,149 - INFO - joeynmt.training - Epoch 187, total training loss: 6.80, num. of seqs: 8065, num. of tokens: 80463, 20.8413[sec]\r\n",
      "2024-08-27 15:59:32,149 - INFO - joeynmt.training - EPOCH 188\r\n",
      "2024-08-27 15:59:32,589 - INFO - joeynmt.training - Epoch 188, Step:    17795, Batch Loss:     0.063159, Batch Acc: 0.994512, Tokens per Sec:     3769, Lr: 0.000134\r\n",
      "2024-08-27 15:59:33,678 - INFO - joeynmt.training - Epoch 188, Step:    17800, Batch Loss:     0.079774, Batch Acc: 0.992748, Tokens per Sec:     3803, Lr: 0.000134\r\n",
      "2024-08-27 15:59:34,779 - INFO - joeynmt.training - Epoch 188, Step:    17805, Batch Loss:     0.068571, Batch Acc: 0.994065, Tokens per Sec:     3827, Lr: 0.000134\r\n",
      "2024-08-27 15:59:35,884 - INFO - joeynmt.training - Epoch 188, Step:    17810, Batch Loss:     0.062902, Batch Acc: 0.993995, Tokens per Sec:     4073, Lr: 0.000134\r\n",
      "2024-08-27 15:59:36,999 - INFO - joeynmt.training - Epoch 188, Step:    17815, Batch Loss:     0.068152, Batch Acc: 0.993034, Tokens per Sec:     3735, Lr: 0.000134\r\n",
      "2024-08-27 15:59:38,160 - INFO - joeynmt.training - Epoch 188, Step:    17820, Batch Loss:     0.070515, Batch Acc: 0.995442, Tokens per Sec:     3782, Lr: 0.000134\r\n",
      "2024-08-27 15:59:39,269 - INFO - joeynmt.training - Epoch 188, Step:    17825, Batch Loss:     0.061417, Batch Acc: 0.992603, Tokens per Sec:     3904, Lr: 0.000134\r\n",
      "2024-08-27 15:59:40,492 - INFO - joeynmt.training - Epoch 188, Step:    17830, Batch Loss:     0.069408, Batch Acc: 0.994652, Tokens per Sec:     3520, Lr: 0.000134\r\n",
      "2024-08-27 15:59:41,577 - INFO - joeynmt.training - Epoch 188, Step:    17835, Batch Loss:     0.071586, Batch Acc: 0.993475, Tokens per Sec:     3815, Lr: 0.000134\r\n",
      "2024-08-27 15:59:42,671 - INFO - joeynmt.training - Epoch 188, Step:    17840, Batch Loss:     0.066865, Batch Acc: 0.994826, Tokens per Sec:     4066, Lr: 0.000134\r\n",
      "2024-08-27 15:59:43,779 - INFO - joeynmt.training - Epoch 188, Step:    17845, Batch Loss:     0.069230, Batch Acc: 0.993041, Tokens per Sec:     3763, Lr: 0.000134\r\n",
      "2024-08-27 15:59:44,851 - INFO - joeynmt.training - Epoch 188, Step:    17850, Batch Loss:     0.070651, Batch Acc: 0.994301, Tokens per Sec:     3930, Lr: 0.000134\r\n",
      "2024-08-27 15:59:45,941 - INFO - joeynmt.training - Epoch 188, Step:    17855, Batch Loss:     0.066851, Batch Acc: 0.993606, Tokens per Sec:     3734, Lr: 0.000134\r\n",
      "2024-08-27 15:59:47,056 - INFO - joeynmt.training - Epoch 188, Step:    17860, Batch Loss:     0.083553, Batch Acc: 0.993130, Tokens per Sec:     3789, Lr: 0.000134\r\n",
      "2024-08-27 15:59:48,149 - INFO - joeynmt.training - Epoch 188, Step:    17865, Batch Loss:     0.066810, Batch Acc: 0.994202, Tokens per Sec:     3947, Lr: 0.000134\r\n",
      "2024-08-27 15:59:49,221 - INFO - joeynmt.training - Epoch 188, Step:    17870, Batch Loss:     0.068531, Batch Acc: 0.995309, Tokens per Sec:     3779, Lr: 0.000134\r\n",
      "2024-08-27 15:59:50,322 - INFO - joeynmt.training - Epoch 188, Step:    17875, Batch Loss:     0.067211, Batch Acc: 0.994292, Tokens per Sec:     3983, Lr: 0.000134\r\n",
      "2024-08-27 15:59:51,421 - INFO - joeynmt.training - Epoch 188, Step:    17880, Batch Loss:     0.063496, Batch Acc: 0.990754, Tokens per Sec:     3740, Lr: 0.000134\r\n",
      "2024-08-27 15:59:52,514 - INFO - joeynmt.training - Epoch 188, Step:    17885, Batch Loss:     0.071535, Batch Acc: 0.992896, Tokens per Sec:     3739, Lr: 0.000134\r\n",
      "2024-08-27 15:59:53,178 - INFO - joeynmt.training - Epoch 188, total training loss: 6.72, num. of seqs: 8065, num. of tokens: 80463, 21.0103[sec]\r\n",
      "2024-08-27 15:59:53,178 - INFO - joeynmt.training - EPOCH 189\r\n",
      "2024-08-27 15:59:53,631 - INFO - joeynmt.training - Epoch 189, Step:    17890, Batch Loss:     0.069230, Batch Acc: 0.990050, Tokens per Sec:     3577, Lr: 0.000134\r\n",
      "2024-08-27 15:59:54,752 - INFO - joeynmt.training - Epoch 189, Step:    17895, Batch Loss:     0.070112, Batch Acc: 0.992223, Tokens per Sec:     4132, Lr: 0.000134\r\n",
      "2024-08-27 15:59:55,863 - INFO - joeynmt.training - Epoch 189, Step:    17900, Batch Loss:     0.069663, Batch Acc: 0.995325, Tokens per Sec:     3856, Lr: 0.000134\r\n",
      "2024-08-27 15:59:57,032 - INFO - joeynmt.training - Epoch 189, Step:    17905, Batch Loss:     0.068372, Batch Acc: 0.996278, Tokens per Sec:     3677, Lr: 0.000134\r\n",
      "2024-08-27 15:59:58,140 - INFO - joeynmt.training - Epoch 189, Step:    17910, Batch Loss:     0.060501, Batch Acc: 0.993853, Tokens per Sec:     3822, Lr: 0.000134\r\n",
      "2024-08-27 15:59:59,250 - INFO - joeynmt.training - Epoch 189, Step:    17915, Batch Loss:     0.059073, Batch Acc: 0.996933, Tokens per Sec:     3819, Lr: 0.000134\r\n",
      "2024-08-27 16:00:00,351 - INFO - joeynmt.training - Epoch 189, Step:    17920, Batch Loss:     0.067219, Batch Acc: 0.991100, Tokens per Sec:     3676, Lr: 0.000134\r\n",
      "2024-08-27 16:00:01,452 - INFO - joeynmt.training - Epoch 189, Step:    17925, Batch Loss:     0.064913, Batch Acc: 0.994369, Tokens per Sec:     3873, Lr: 0.000134\r\n",
      "2024-08-27 16:00:02,585 - INFO - joeynmt.training - Epoch 189, Step:    17930, Batch Loss:     0.067534, Batch Acc: 0.993242, Tokens per Sec:     3922, Lr: 0.000134\r\n",
      "2024-08-27 16:00:03,717 - INFO - joeynmt.training - Epoch 189, Step:    17935, Batch Loss:     0.062456, Batch Acc: 0.993100, Tokens per Sec:     3842, Lr: 0.000134\r\n",
      "2024-08-27 16:00:04,813 - INFO - joeynmt.training - Epoch 189, Step:    17940, Batch Loss:     0.070842, Batch Acc: 0.995038, Tokens per Sec:     3865, Lr: 0.000134\r\n",
      "2024-08-27 16:00:05,899 - INFO - joeynmt.training - Epoch 189, Step:    17945, Batch Loss:     0.069431, Batch Acc: 0.994164, Tokens per Sec:     3948, Lr: 0.000134\r\n",
      "2024-08-27 16:00:07,023 - INFO - joeynmt.training - Epoch 189, Step:    17950, Batch Loss:     0.072452, Batch Acc: 0.993620, Tokens per Sec:     3626, Lr: 0.000134\r\n",
      "2024-08-27 16:00:08,108 - INFO - joeynmt.training - Epoch 189, Step:    17955, Batch Loss:     0.073208, Batch Acc: 0.993712, Tokens per Sec:     3814, Lr: 0.000134\r\n",
      "2024-08-27 16:00:09,224 - INFO - joeynmt.training - Epoch 189, Step:    17960, Batch Loss:     0.072892, Batch Acc: 0.992081, Tokens per Sec:     3736, Lr: 0.000133\r\n",
      "2024-08-27 16:00:10,462 - INFO - joeynmt.training - Epoch 189, Step:    17965, Batch Loss:     0.061172, Batch Acc: 0.993191, Tokens per Sec:     3562, Lr: 0.000133\r\n",
      "2024-08-27 16:00:11,554 - INFO - joeynmt.training - Epoch 189, Step:    17970, Batch Loss:     0.059691, Batch Acc: 0.993260, Tokens per Sec:     3806, Lr: 0.000133\r\n",
      "2024-08-27 16:00:12,677 - INFO - joeynmt.training - Epoch 189, Step:    17975, Batch Loss:     0.078646, Batch Acc: 0.992182, Tokens per Sec:     3876, Lr: 0.000133\r\n",
      "2024-08-27 16:00:13,777 - INFO - joeynmt.training - Epoch 189, Step:    17980, Batch Loss:     0.065891, Batch Acc: 0.993759, Tokens per Sec:     3934, Lr: 0.000133\r\n",
      "2024-08-27 16:00:14,361 - INFO - joeynmt.training - Epoch 189, total training loss: 6.61, num. of seqs: 8065, num. of tokens: 80463, 21.1659[sec]\r\n",
      "2024-08-27 16:00:14,361 - INFO - joeynmt.training - EPOCH 190\r\n",
      "2024-08-27 16:00:15,028 - INFO - joeynmt.training - Epoch 190, Step:    17985, Batch Loss:     0.074966, Batch Acc: 0.991123, Tokens per Sec:     3912, Lr: 0.000133\r\n",
      "2024-08-27 16:00:16,116 - INFO - joeynmt.training - Epoch 190, Step:    17990, Batch Loss:     0.068755, Batch Acc: 0.993969, Tokens per Sec:     3810, Lr: 0.000133\r\n",
      "2024-08-27 16:00:17,253 - INFO - joeynmt.training - Epoch 190, Step:    17995, Batch Loss:     0.066807, Batch Acc: 0.994833, Tokens per Sec:     3748, Lr: 0.000133\r\n",
      "2024-08-27 16:00:18,341 - INFO - joeynmt.training - Epoch 190, Step:    18000, Batch Loss:     0.072049, Batch Acc: 0.996402, Tokens per Sec:     3834, Lr: 0.000133\r\n",
      "2024-08-27 16:00:18,342 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=18042\r\n",
      "2024-08-27 16:00:18,342 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 64.39it/s]\r\n",
      "2024-08-27 16:00:41,020 - INFO - joeynmt.prediction - Generation took 22.6756[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44233.18 examples/s]\r\n",
      "2024-08-27 16:00:41,418 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:00:41,418 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.15, loss:   6.83, ppl: 929.33, acc:   0.28, 0.1770[sec]\r\n",
      "2024-08-27 16:00:41,420 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:00:41,570 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:00:41,570 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:00:41,570 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 16:00:41,867 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:00:42,015 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:00:42,015 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:00:42,015 - INFO - joeynmt.training - \tHypothesis: trois maisons\r\n",
      "2024-08-27 16:00:42,309 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:00:42,455 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:00:42,455 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:00:42,455 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 16:00:42,748 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:00:42,895 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:00:42,895 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:00:42,895 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 16:00:44,266 - INFO - joeynmt.training - Epoch 190, Step:    18005, Batch Loss:     0.065750, Batch Acc: 0.995475, Tokens per Sec:     3898, Lr: 0.000133\r\n",
      "2024-08-27 16:00:45,354 - INFO - joeynmt.training - Epoch 190, Step:    18010, Batch Loss:     0.068620, Batch Acc: 0.993505, Tokens per Sec:     4105, Lr: 0.000133\r\n",
      "2024-08-27 16:00:46,425 - INFO - joeynmt.training - Epoch 190, Step:    18015, Batch Loss:     0.077661, Batch Acc: 0.993202, Tokens per Sec:     3712, Lr: 0.000133\r\n",
      "2024-08-27 16:00:47,560 - INFO - joeynmt.training - Epoch 190, Step:    18020, Batch Loss:     0.073359, Batch Acc: 0.994069, Tokens per Sec:     3865, Lr: 0.000133\r\n",
      "2024-08-27 16:00:48,664 - INFO - joeynmt.training - Epoch 190, Step:    18025, Batch Loss:     0.076186, Batch Acc: 0.994112, Tokens per Sec:     4005, Lr: 0.000133\r\n",
      "2024-08-27 16:00:49,753 - INFO - joeynmt.training - Epoch 190, Step:    18030, Batch Loss:     0.069122, Batch Acc: 0.993308, Tokens per Sec:     3842, Lr: 0.000133\r\n",
      "2024-08-27 16:00:50,848 - INFO - joeynmt.training - Epoch 190, Step:    18035, Batch Loss:     0.061108, Batch Acc: 0.995584, Tokens per Sec:     3935, Lr: 0.000133\r\n",
      "2024-08-27 16:00:51,918 - INFO - joeynmt.training - Epoch 190, Step:    18040, Batch Loss:     0.065805, Batch Acc: 0.992306, Tokens per Sec:     3644, Lr: 0.000133\r\n",
      "2024-08-27 16:00:53,003 - INFO - joeynmt.training - Epoch 190, Step:    18045, Batch Loss:     0.066143, Batch Acc: 0.993271, Tokens per Sec:     3837, Lr: 0.000133\r\n",
      "2024-08-27 16:00:54,064 - INFO - joeynmt.training - Epoch 190, Step:    18050, Batch Loss:     0.071658, Batch Acc: 0.993765, Tokens per Sec:     3933, Lr: 0.000133\r\n",
      "2024-08-27 16:00:55,150 - INFO - joeynmt.training - Epoch 190, Step:    18055, Batch Loss:     0.072642, Batch Acc: 0.992295, Tokens per Sec:     3829, Lr: 0.000133\r\n",
      "2024-08-27 16:00:56,279 - INFO - joeynmt.training - Epoch 190, Step:    18060, Batch Loss:     0.071903, Batch Acc: 0.995193, Tokens per Sec:     3872, Lr: 0.000133\r\n",
      "2024-08-27 16:00:57,421 - INFO - joeynmt.training - Epoch 190, Step:    18065, Batch Loss:     0.078979, Batch Acc: 0.993908, Tokens per Sec:     3882, Lr: 0.000133\r\n",
      "2024-08-27 16:00:58,498 - INFO - joeynmt.training - Epoch 190, Step:    18070, Batch Loss:     0.066782, Batch Acc: 0.993137, Tokens per Sec:     3792, Lr: 0.000133\r\n",
      "2024-08-27 16:00:59,573 - INFO - joeynmt.training - Epoch 190, Step:    18075, Batch Loss:     0.071245, Batch Acc: 0.994055, Tokens per Sec:     3913, Lr: 0.000133\r\n",
      "2024-08-27 16:01:00,058 - INFO - joeynmt.training - Epoch 190, total training loss: 6.62, num. of seqs: 8065, num. of tokens: 80463, 20.8322[sec]\r\n",
      "2024-08-27 16:01:00,058 - INFO - joeynmt.training - EPOCH 191\r\n",
      "2024-08-27 16:01:00,726 - INFO - joeynmt.training - Epoch 191, Step:    18080, Batch Loss:     0.066520, Batch Acc: 0.993978, Tokens per Sec:     3753, Lr: 0.000133\r\n",
      "2024-08-27 16:01:01,821 - INFO - joeynmt.training - Epoch 191, Step:    18085, Batch Loss:     0.065447, Batch Acc: 0.992844, Tokens per Sec:     3959, Lr: 0.000133\r\n",
      "2024-08-27 16:01:02,914 - INFO - joeynmt.training - Epoch 191, Step:    18090, Batch Loss:     0.064801, Batch Acc: 0.994353, Tokens per Sec:     3728, Lr: 0.000133\r\n",
      "2024-08-27 16:01:04,009 - INFO - joeynmt.training - Epoch 191, Step:    18095, Batch Loss:     0.078980, Batch Acc: 0.992713, Tokens per Sec:     3887, Lr: 0.000133\r\n",
      "2024-08-27 16:01:05,101 - INFO - joeynmt.training - Epoch 191, Step:    18100, Batch Loss:     0.073687, Batch Acc: 0.992111, Tokens per Sec:     3950, Lr: 0.000133\r\n",
      "2024-08-27 16:01:06,196 - INFO - joeynmt.training - Epoch 191, Step:    18105, Batch Loss:     0.064839, Batch Acc: 0.993526, Tokens per Sec:     3953, Lr: 0.000133\r\n",
      "2024-08-27 16:01:07,315 - INFO - joeynmt.training - Epoch 191, Step:    18110, Batch Loss:     0.061341, Batch Acc: 0.993310, Tokens per Sec:     3609, Lr: 0.000133\r\n",
      "2024-08-27 16:01:08,433 - INFO - joeynmt.training - Epoch 191, Step:    18115, Batch Loss:     0.065951, Batch Acc: 0.994834, Tokens per Sec:     3639, Lr: 0.000133\r\n",
      "2024-08-27 16:01:09,572 - INFO - joeynmt.training - Epoch 191, Step:    18120, Batch Loss:     0.077120, Batch Acc: 0.993629, Tokens per Sec:     3722, Lr: 0.000133\r\n",
      "2024-08-27 16:01:10,687 - INFO - joeynmt.training - Epoch 191, Step:    18125, Batch Loss:     0.074272, Batch Acc: 0.993377, Tokens per Sec:     3797, Lr: 0.000133\r\n",
      "2024-08-27 16:01:11,883 - INFO - joeynmt.training - Epoch 191, Step:    18130, Batch Loss:     0.078737, Batch Acc: 0.992637, Tokens per Sec:     3633, Lr: 0.000133\r\n",
      "2024-08-27 16:01:13,111 - INFO - joeynmt.training - Epoch 191, Step:    18135, Batch Loss:     0.068922, Batch Acc: 0.993717, Tokens per Sec:     3504, Lr: 0.000133\r\n",
      "2024-08-27 16:01:14,233 - INFO - joeynmt.training - Epoch 191, Step:    18140, Batch Loss:     0.064702, Batch Acc: 0.994421, Tokens per Sec:     3837, Lr: 0.000133\r\n",
      "2024-08-27 16:01:15,325 - INFO - joeynmt.training - Epoch 191, Step:    18145, Batch Loss:     0.072840, Batch Acc: 0.994666, Tokens per Sec:     3951, Lr: 0.000133\r\n",
      "2024-08-27 16:01:16,421 - INFO - joeynmt.training - Epoch 191, Step:    18150, Batch Loss:     0.064110, Batch Acc: 0.994456, Tokens per Sec:     3787, Lr: 0.000133\r\n",
      "2024-08-27 16:01:17,516 - INFO - joeynmt.training - Epoch 191, Step:    18155, Batch Loss:     0.073718, Batch Acc: 0.994900, Tokens per Sec:     3763, Lr: 0.000133\r\n",
      "2024-08-27 16:01:18,594 - INFO - joeynmt.training - Epoch 191, Step:    18160, Batch Loss:     0.077248, Batch Acc: 0.996095, Tokens per Sec:     3801, Lr: 0.000133\r\n",
      "2024-08-27 16:01:19,681 - INFO - joeynmt.training - Epoch 191, Step:    18165, Batch Loss:     0.064442, Batch Acc: 0.992785, Tokens per Sec:     3829, Lr: 0.000133\r\n",
      "2024-08-27 16:01:20,760 - INFO - joeynmt.training - Epoch 191, Step:    18170, Batch Loss:     0.063576, Batch Acc: 0.994206, Tokens per Sec:     3841, Lr: 0.000133\r\n",
      "2024-08-27 16:01:21,353 - INFO - joeynmt.training - Epoch 191, total training loss: 6.63, num. of seqs: 8065, num. of tokens: 80463, 21.2770[sec]\r\n",
      "2024-08-27 16:01:21,353 - INFO - joeynmt.training - EPOCH 192\r\n",
      "2024-08-27 16:01:22,008 - INFO - joeynmt.training - Epoch 192, Step:    18175, Batch Loss:     0.072275, Batch Acc: 0.995094, Tokens per Sec:     4076, Lr: 0.000133\r\n",
      "2024-08-27 16:01:23,108 - INFO - joeynmt.training - Epoch 192, Step:    18180, Batch Loss:     0.067416, Batch Acc: 0.994019, Tokens per Sec:     3953, Lr: 0.000133\r\n",
      "2024-08-27 16:01:24,187 - INFO - joeynmt.training - Epoch 192, Step:    18185, Batch Loss:     0.070194, Batch Acc: 0.994461, Tokens per Sec:     3850, Lr: 0.000133\r\n",
      "2024-08-27 16:01:25,272 - INFO - joeynmt.training - Epoch 192, Step:    18190, Batch Loss:     0.064726, Batch Acc: 0.994683, Tokens per Sec:     3817, Lr: 0.000133\r\n",
      "2024-08-27 16:01:26,369 - INFO - joeynmt.training - Epoch 192, Step:    18195, Batch Loss:     0.065365, Batch Acc: 0.996338, Tokens per Sec:     3986, Lr: 0.000133\r\n",
      "2024-08-27 16:01:27,495 - INFO - joeynmt.training - Epoch 192, Step:    18200, Batch Loss:     0.070396, Batch Acc: 0.995063, Tokens per Sec:     3960, Lr: 0.000133\r\n",
      "2024-08-27 16:01:28,595 - INFO - joeynmt.training - Epoch 192, Step:    18205, Batch Loss:     0.060694, Batch Acc: 0.995198, Tokens per Sec:     3788, Lr: 0.000133\r\n",
      "2024-08-27 16:01:29,679 - INFO - joeynmt.training - Epoch 192, Step:    18210, Batch Loss:     0.071608, Batch Acc: 0.992843, Tokens per Sec:     3738, Lr: 0.000133\r\n",
      "2024-08-27 16:01:30,767 - INFO - joeynmt.training - Epoch 192, Step:    18215, Batch Loss:     0.080172, Batch Acc: 0.995124, Tokens per Sec:     3775, Lr: 0.000133\r\n",
      "2024-08-27 16:01:31,866 - INFO - joeynmt.training - Epoch 192, Step:    18220, Batch Loss:     0.073876, Batch Acc: 0.993154, Tokens per Sec:     3858, Lr: 0.000133\r\n",
      "2024-08-27 16:01:32,951 - INFO - joeynmt.training - Epoch 192, Step:    18225, Batch Loss:     0.075624, Batch Acc: 0.993298, Tokens per Sec:     3852, Lr: 0.000133\r\n",
      "2024-08-27 16:01:34,051 - INFO - joeynmt.training - Epoch 192, Step:    18230, Batch Loss:     0.061106, Batch Acc: 0.995773, Tokens per Sec:     3875, Lr: 0.000132\r\n",
      "2024-08-27 16:01:35,150 - INFO - joeynmt.training - Epoch 192, Step:    18235, Batch Loss:     0.060842, Batch Acc: 0.994279, Tokens per Sec:     3816, Lr: 0.000132\r\n",
      "2024-08-27 16:01:36,251 - INFO - joeynmt.training - Epoch 192, Step:    18240, Batch Loss:     0.062477, Batch Acc: 0.994348, Tokens per Sec:     3860, Lr: 0.000132\r\n",
      "2024-08-27 16:01:37,375 - INFO - joeynmt.training - Epoch 192, Step:    18245, Batch Loss:     0.064006, Batch Acc: 0.994067, Tokens per Sec:     3602, Lr: 0.000132\r\n",
      "2024-08-27 16:01:38,458 - INFO - joeynmt.training - Epoch 192, Step:    18250, Batch Loss:     0.066571, Batch Acc: 0.995166, Tokens per Sec:     3823, Lr: 0.000132\r\n",
      "2024-08-27 16:01:39,549 - INFO - joeynmt.training - Epoch 192, Step:    18255, Batch Loss:     0.066262, Batch Acc: 0.994232, Tokens per Sec:     3814, Lr: 0.000132\r\n",
      "2024-08-27 16:01:40,642 - INFO - joeynmt.training - Epoch 192, Step:    18260, Batch Loss:     0.073676, Batch Acc: 0.995711, Tokens per Sec:     4055, Lr: 0.000132\r\n",
      "2024-08-27 16:01:41,757 - INFO - joeynmt.training - Epoch 192, Step:    18265, Batch Loss:     0.059993, Batch Acc: 0.992170, Tokens per Sec:     3900, Lr: 0.000132\r\n",
      "2024-08-27 16:01:42,213 - INFO - joeynmt.training - Epoch 192, total training loss: 6.40, num. of seqs: 8065, num. of tokens: 80463, 20.8421[sec]\r\n",
      "2024-08-27 16:01:42,213 - INFO - joeynmt.training - EPOCH 193\r\n",
      "2024-08-27 16:01:42,921 - INFO - joeynmt.training - Epoch 193, Step:    18270, Batch Loss:     0.064288, Batch Acc: 0.992381, Tokens per Sec:     3724, Lr: 0.000132\r\n",
      "2024-08-27 16:01:44,047 - INFO - joeynmt.training - Epoch 193, Step:    18275, Batch Loss:     0.064927, Batch Acc: 0.994019, Tokens per Sec:     3714, Lr: 0.000132\r\n",
      "2024-08-27 16:01:45,148 - INFO - joeynmt.training - Epoch 193, Step:    18280, Batch Loss:     0.072924, Batch Acc: 0.995612, Tokens per Sec:     3728, Lr: 0.000132\r\n",
      "2024-08-27 16:01:46,218 - INFO - joeynmt.training - Epoch 193, Step:    18285, Batch Loss:     0.063776, Batch Acc: 0.995554, Tokens per Sec:     3786, Lr: 0.000132\r\n",
      "2024-08-27 16:01:47,352 - INFO - joeynmt.training - Epoch 193, Step:    18290, Batch Loss:     0.065513, Batch Acc: 0.994037, Tokens per Sec:     3848, Lr: 0.000132\r\n",
      "2024-08-27 16:01:48,451 - INFO - joeynmt.training - Epoch 193, Step:    18295, Batch Loss:     0.065240, Batch Acc: 0.993199, Tokens per Sec:     4018, Lr: 0.000132\r\n",
      "2024-08-27 16:01:49,530 - INFO - joeynmt.training - Epoch 193, Step:    18300, Batch Loss:     0.072849, Batch Acc: 0.991561, Tokens per Sec:     3737, Lr: 0.000132\r\n",
      "2024-08-27 16:01:50,620 - INFO - joeynmt.training - Epoch 193, Step:    18305, Batch Loss:     0.064016, Batch Acc: 0.995294, Tokens per Sec:     3901, Lr: 0.000132\r\n",
      "2024-08-27 16:01:51,702 - INFO - joeynmt.training - Epoch 193, Step:    18310, Batch Loss:     0.079586, Batch Acc: 0.992302, Tokens per Sec:     3844, Lr: 0.000132\r\n",
      "2024-08-27 16:01:52,789 - INFO - joeynmt.training - Epoch 193, Step:    18315, Batch Loss:     0.071362, Batch Acc: 0.994420, Tokens per Sec:     3959, Lr: 0.000132\r\n",
      "2024-08-27 16:01:53,865 - INFO - joeynmt.training - Epoch 193, Step:    18320, Batch Loss:     0.073044, Batch Acc: 0.994283, Tokens per Sec:     3901, Lr: 0.000132\r\n",
      "2024-08-27 16:01:54,969 - INFO - joeynmt.training - Epoch 193, Step:    18325, Batch Loss:     0.072791, Batch Acc: 0.993669, Tokens per Sec:     3725, Lr: 0.000132\r\n",
      "2024-08-27 16:01:56,089 - INFO - joeynmt.training - Epoch 193, Step:    18330, Batch Loss:     0.065995, Batch Acc: 0.993031, Tokens per Sec:     3590, Lr: 0.000132\r\n",
      "2024-08-27 16:01:57,253 - INFO - joeynmt.training - Epoch 193, Step:    18335, Batch Loss:     0.060652, Batch Acc: 0.993565, Tokens per Sec:     3740, Lr: 0.000132\r\n",
      "2024-08-27 16:01:58,364 - INFO - joeynmt.training - Epoch 193, Step:    18340, Batch Loss:     0.062544, Batch Acc: 0.995274, Tokens per Sec:     3812, Lr: 0.000132\r\n",
      "2024-08-27 16:01:59,459 - INFO - joeynmt.training - Epoch 193, Step:    18345, Batch Loss:     0.071307, Batch Acc: 0.993033, Tokens per Sec:     3933, Lr: 0.000132\r\n",
      "2024-08-27 16:02:00,601 - INFO - joeynmt.training - Epoch 193, Step:    18350, Batch Loss:     0.073736, Batch Acc: 0.990305, Tokens per Sec:     3705, Lr: 0.000132\r\n",
      "2024-08-27 16:02:01,717 - INFO - joeynmt.training - Epoch 193, Step:    18355, Batch Loss:     0.068275, Batch Acc: 0.993676, Tokens per Sec:     3688, Lr: 0.000132\r\n",
      "2024-08-27 16:02:02,825 - INFO - joeynmt.training - Epoch 193, Step:    18360, Batch Loss:     0.061920, Batch Acc: 0.994019, Tokens per Sec:     3925, Lr: 0.000132\r\n",
      "2024-08-27 16:02:03,369 - INFO - joeynmt.training - Epoch 193, total training loss: 6.49, num. of seqs: 8065, num. of tokens: 80463, 21.1396[sec]\r\n",
      "2024-08-27 16:02:03,370 - INFO - joeynmt.training - EPOCH 194\r\n",
      "2024-08-27 16:02:04,032 - INFO - joeynmt.training - Epoch 194, Step:    18365, Batch Loss:     0.063902, Batch Acc: 0.992675, Tokens per Sec:     3941, Lr: 0.000132\r\n",
      "2024-08-27 16:02:05,133 - INFO - joeynmt.training - Epoch 194, Step:    18370, Batch Loss:     0.081030, Batch Acc: 0.993765, Tokens per Sec:     3790, Lr: 0.000132\r\n",
      "2024-08-27 16:02:06,235 - INFO - joeynmt.training - Epoch 194, Step:    18375, Batch Loss:     0.058888, Batch Acc: 0.995405, Tokens per Sec:     3756, Lr: 0.000132\r\n",
      "2024-08-27 16:02:07,362 - INFO - joeynmt.training - Epoch 194, Step:    18380, Batch Loss:     0.073188, Batch Acc: 0.994251, Tokens per Sec:     3704, Lr: 0.000132\r\n",
      "2024-08-27 16:02:08,453 - INFO - joeynmt.training - Epoch 194, Step:    18385, Batch Loss:     0.057622, Batch Acc: 0.994678, Tokens per Sec:     3795, Lr: 0.000132\r\n",
      "2024-08-27 16:02:09,534 - INFO - joeynmt.training - Epoch 194, Step:    18390, Batch Loss:     0.068435, Batch Acc: 0.994817, Tokens per Sec:     3751, Lr: 0.000132\r\n",
      "2024-08-27 16:02:10,617 - INFO - joeynmt.training - Epoch 194, Step:    18395, Batch Loss:     0.076667, Batch Acc: 0.995503, Tokens per Sec:     3697, Lr: 0.000132\r\n",
      "2024-08-27 16:02:11,708 - INFO - joeynmt.training - Epoch 194, Step:    18400, Batch Loss:     0.066064, Batch Acc: 0.993807, Tokens per Sec:     3852, Lr: 0.000132\r\n",
      "2024-08-27 16:02:12,799 - INFO - joeynmt.training - Epoch 194, Step:    18405, Batch Loss:     0.074324, Batch Acc: 0.995011, Tokens per Sec:     3858, Lr: 0.000132\r\n",
      "2024-08-27 16:02:13,950 - INFO - joeynmt.training - Epoch 194, Step:    18410, Batch Loss:     0.059583, Batch Acc: 0.996582, Tokens per Sec:     3817, Lr: 0.000132\r\n",
      "2024-08-27 16:02:15,122 - INFO - joeynmt.training - Epoch 194, Step:    18415, Batch Loss:     0.075963, Batch Acc: 0.992543, Tokens per Sec:     3664, Lr: 0.000132\r\n",
      "2024-08-27 16:02:16,246 - INFO - joeynmt.training - Epoch 194, Step:    18420, Batch Loss:     0.062216, Batch Acc: 0.995778, Tokens per Sec:     3792, Lr: 0.000132\r\n",
      "2024-08-27 16:02:17,368 - INFO - joeynmt.training - Epoch 194, Step:    18425, Batch Loss:     0.080784, Batch Acc: 0.993407, Tokens per Sec:     3788, Lr: 0.000132\r\n",
      "2024-08-27 16:02:18,481 - INFO - joeynmt.training - Epoch 194, Step:    18430, Batch Loss:     0.065572, Batch Acc: 0.994495, Tokens per Sec:     3757, Lr: 0.000132\r\n",
      "2024-08-27 16:02:19,589 - INFO - joeynmt.training - Epoch 194, Step:    18435, Batch Loss:     0.067852, Batch Acc: 0.992120, Tokens per Sec:     3781, Lr: 0.000132\r\n",
      "2024-08-27 16:02:20,686 - INFO - joeynmt.training - Epoch 194, Step:    18440, Batch Loss:     0.059356, Batch Acc: 0.995823, Tokens per Sec:     3933, Lr: 0.000132\r\n",
      "2024-08-27 16:02:21,800 - INFO - joeynmt.training - Epoch 194, Step:    18445, Batch Loss:     0.064137, Batch Acc: 0.993985, Tokens per Sec:     3733, Lr: 0.000132\r\n",
      "2024-08-27 16:02:22,927 - INFO - joeynmt.training - Epoch 194, Step:    18450, Batch Loss:     0.065415, Batch Acc: 0.994444, Tokens per Sec:     3676, Lr: 0.000132\r\n",
      "2024-08-27 16:02:24,022 - INFO - joeynmt.training - Epoch 194, Step:    18455, Batch Loss:     0.071317, Batch Acc: 0.994672, Tokens per Sec:     3771, Lr: 0.000132\r\n",
      "2024-08-27 16:02:24,733 - INFO - joeynmt.training - Epoch 194, total training loss: 6.53, num. of seqs: 8065, num. of tokens: 80463, 21.3460[sec]\r\n",
      "2024-08-27 16:02:24,733 - INFO - joeynmt.training - EPOCH 195\r\n",
      "2024-08-27 16:02:25,167 - INFO - joeynmt.training - Epoch 195, Step:    18460, Batch Loss:     0.064146, Batch Acc: 0.995404, Tokens per Sec:     3543, Lr: 0.000132\r\n",
      "2024-08-27 16:02:26,241 - INFO - joeynmt.training - Epoch 195, Step:    18465, Batch Loss:     0.065748, Batch Acc: 0.995553, Tokens per Sec:     3980, Lr: 0.000132\r\n",
      "2024-08-27 16:02:27,357 - INFO - joeynmt.training - Epoch 195, Step:    18470, Batch Loss:     0.062535, Batch Acc: 0.995627, Tokens per Sec:     3899, Lr: 0.000132\r\n",
      "2024-08-27 16:02:28,444 - INFO - joeynmt.training - Epoch 195, Step:    18475, Batch Loss:     0.064755, Batch Acc: 0.995239, Tokens per Sec:     3865, Lr: 0.000132\r\n",
      "2024-08-27 16:02:29,537 - INFO - joeynmt.training - Epoch 195, Step:    18480, Batch Loss:     0.067423, Batch Acc: 0.995112, Tokens per Sec:     4122, Lr: 0.000132\r\n",
      "2024-08-27 16:02:30,616 - INFO - joeynmt.training - Epoch 195, Step:    18485, Batch Loss:     0.060167, Batch Acc: 0.994055, Tokens per Sec:     3898, Lr: 0.000132\r\n",
      "2024-08-27 16:02:31,709 - INFO - joeynmt.training - Epoch 195, Step:    18490, Batch Loss:     0.070900, Batch Acc: 0.993355, Tokens per Sec:     4132, Lr: 0.000132\r\n",
      "2024-08-27 16:02:32,824 - INFO - joeynmt.training - Epoch 195, Step:    18495, Batch Loss:     0.065936, Batch Acc: 0.994493, Tokens per Sec:     3913, Lr: 0.000132\r\n",
      "2024-08-27 16:02:33,924 - INFO - joeynmt.training - Epoch 195, Step:    18500, Batch Loss:     0.071780, Batch Acc: 0.994561, Tokens per Sec:     3681, Lr: 0.000132\r\n",
      "2024-08-27 16:02:33,924 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=18542\r\n",
      "2024-08-27 16:02:33,925 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 63.52it/s]\r\n",
      "2024-08-27 16:02:56,911 - INFO - joeynmt.prediction - Generation took 22.9843[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44265.87 examples/s]\r\n",
      "2024-08-27 16:02:57,294 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:02:57,295 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.39, loss:   6.84, ppl: 934.57, acc:   0.28, 0.1714[sec]\r\n",
      "2024-08-27 16:02:57,612 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/18500.ckpt.\r\n",
      "2024-08-27 16:02:57,613 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/14000.ckpt\r\n",
      "2024-08-27 16:02:57,640 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:02:57,788 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:02:57,788 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:02:57,788 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 16:02:58,081 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:02:58,228 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:02:58,229 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:02:58,229 - INFO - joeynmt.training - \tHypothesis: trois tables blanches\r\n",
      "2024-08-27 16:02:58,521 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:02:58,668 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:02:58,668 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:02:58,669 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 16:02:58,962 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:02:59,108 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:02:59,109 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:02:59,109 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 16:03:00,506 - INFO - joeynmt.training - Epoch 195, Step:    18505, Batch Loss:     0.056956, Batch Acc: 0.994277, Tokens per Sec:     3970, Lr: 0.000132\r\n",
      "2024-08-27 16:03:01,623 - INFO - joeynmt.training - Epoch 195, Step:    18510, Batch Loss:     0.064547, Batch Acc: 0.994707, Tokens per Sec:     3890, Lr: 0.000131\r\n",
      "2024-08-27 16:03:02,724 - INFO - joeynmt.training - Epoch 195, Step:    18515, Batch Loss:     0.073613, Batch Acc: 0.994321, Tokens per Sec:     3680, Lr: 0.000131\r\n",
      "2024-08-27 16:03:03,837 - INFO - joeynmt.training - Epoch 195, Step:    18520, Batch Loss:     0.061135, Batch Acc: 0.994690, Tokens per Sec:     3556, Lr: 0.000131\r\n",
      "2024-08-27 16:03:04,951 - INFO - joeynmt.training - Epoch 195, Step:    18525, Batch Loss:     0.067602, Batch Acc: 0.993237, Tokens per Sec:     3853, Lr: 0.000131\r\n",
      "2024-08-27 16:03:06,061 - INFO - joeynmt.training - Epoch 195, Step:    18530, Batch Loss:     0.069329, Batch Acc: 0.994831, Tokens per Sec:     3837, Lr: 0.000131\r\n",
      "2024-08-27 16:03:07,191 - INFO - joeynmt.training - Epoch 195, Step:    18535, Batch Loss:     0.063831, Batch Acc: 0.995357, Tokens per Sec:     3624, Lr: 0.000131\r\n",
      "2024-08-27 16:03:08,291 - INFO - joeynmt.training - Epoch 195, Step:    18540, Batch Loss:     0.070750, Batch Acc: 0.992235, Tokens per Sec:     3748, Lr: 0.000131\r\n",
      "2024-08-27 16:03:09,385 - INFO - joeynmt.training - Epoch 195, Step:    18545, Batch Loss:     0.067124, Batch Acc: 0.992574, Tokens per Sec:     3693, Lr: 0.000131\r\n",
      "2024-08-27 16:03:10,463 - INFO - joeynmt.training - Epoch 195, Step:    18550, Batch Loss:     0.062842, Batch Acc: 0.995885, Tokens per Sec:     3837, Lr: 0.000131\r\n",
      "2024-08-27 16:03:11,201 - INFO - joeynmt.training - Epoch 195, total training loss: 6.39, num. of seqs: 8065, num. of tokens: 80463, 20.9696[sec]\r\n",
      "2024-08-27 16:03:11,201 - INFO - joeynmt.training - EPOCH 196\r\n",
      "2024-08-27 16:03:11,635 - INFO - joeynmt.training - Epoch 196, Step:    18555, Batch Loss:     0.056503, Batch Acc: 0.997558, Tokens per Sec:     3814, Lr: 0.000131\r\n",
      "2024-08-27 16:03:12,714 - INFO - joeynmt.training - Epoch 196, Step:    18560, Batch Loss:     0.071938, Batch Acc: 0.992989, Tokens per Sec:     3968, Lr: 0.000131\r\n",
      "2024-08-27 16:03:13,812 - INFO - joeynmt.training - Epoch 196, Step:    18565, Batch Loss:     0.066136, Batch Acc: 0.994778, Tokens per Sec:     3840, Lr: 0.000131\r\n",
      "2024-08-27 16:03:14,919 - INFO - joeynmt.training - Epoch 196, Step:    18570, Batch Loss:     0.059337, Batch Acc: 0.992951, Tokens per Sec:     3974, Lr: 0.000131\r\n",
      "2024-08-27 16:03:16,013 - INFO - joeynmt.training - Epoch 196, Step:    18575, Batch Loss:     0.062396, Batch Acc: 0.996206, Tokens per Sec:     3615, Lr: 0.000131\r\n",
      "2024-08-27 16:03:17,268 - INFO - joeynmt.training - Epoch 196, Step:    18580, Batch Loss:     0.067645, Batch Acc: 0.994106, Tokens per Sec:     3246, Lr: 0.000131\r\n",
      "2024-08-27 16:03:18,446 - INFO - joeynmt.training - Epoch 196, Step:    18585, Batch Loss:     0.058183, Batch Acc: 0.994843, Tokens per Sec:     3624, Lr: 0.000131\r\n",
      "2024-08-27 16:03:19,531 - INFO - joeynmt.training - Epoch 196, Step:    18590, Batch Loss:     0.067451, Batch Acc: 0.996462, Tokens per Sec:     3912, Lr: 0.000131\r\n",
      "2024-08-27 16:03:20,648 - INFO - joeynmt.training - Epoch 196, Step:    18595, Batch Loss:     0.069410, Batch Acc: 0.994320, Tokens per Sec:     3783, Lr: 0.000131\r\n",
      "2024-08-27 16:03:21,746 - INFO - joeynmt.training - Epoch 196, Step:    18600, Batch Loss:     0.068476, Batch Acc: 0.993836, Tokens per Sec:     3845, Lr: 0.000131\r\n",
      "2024-08-27 16:03:22,836 - INFO - joeynmt.training - Epoch 196, Step:    18605, Batch Loss:     0.070812, Batch Acc: 0.993519, Tokens per Sec:     3965, Lr: 0.000131\r\n",
      "2024-08-27 16:03:23,916 - INFO - joeynmt.training - Epoch 196, Step:    18610, Batch Loss:     0.067110, Batch Acc: 0.995087, Tokens per Sec:     3959, Lr: 0.000131\r\n",
      "2024-08-27 16:03:24,989 - INFO - joeynmt.training - Epoch 196, Step:    18615, Batch Loss:     0.067237, Batch Acc: 0.994408, Tokens per Sec:     3835, Lr: 0.000131\r\n",
      "2024-08-27 16:03:26,085 - INFO - joeynmt.training - Epoch 196, Step:    18620, Batch Loss:     0.075573, Batch Acc: 0.994629, Tokens per Sec:     3911, Lr: 0.000131\r\n",
      "2024-08-27 16:03:27,197 - INFO - joeynmt.training - Epoch 196, Step:    18625, Batch Loss:     0.074498, Batch Acc: 0.995000, Tokens per Sec:     3780, Lr: 0.000131\r\n",
      "2024-08-27 16:03:28,334 - INFO - joeynmt.training - Epoch 196, Step:    18630, Batch Loss:     0.067685, Batch Acc: 0.992794, Tokens per Sec:     3786, Lr: 0.000131\r\n",
      "2024-08-27 16:03:29,425 - INFO - joeynmt.training - Epoch 196, Step:    18635, Batch Loss:     0.073034, Batch Acc: 0.993014, Tokens per Sec:     3938, Lr: 0.000131\r\n",
      "2024-08-27 16:03:30,526 - INFO - joeynmt.training - Epoch 196, Step:    18640, Batch Loss:     0.064788, Batch Acc: 0.994183, Tokens per Sec:     3751, Lr: 0.000131\r\n",
      "2024-08-27 16:03:31,629 - INFO - joeynmt.training - Epoch 196, Step:    18645, Batch Loss:     0.069940, Batch Acc: 0.994514, Tokens per Sec:     3636, Lr: 0.000131\r\n",
      "2024-08-27 16:03:32,433 - INFO - joeynmt.training - Epoch 196, total training loss: 6.39, num. of seqs: 8065, num. of tokens: 80463, 21.2154[sec]\r\n",
      "2024-08-27 16:03:32,434 - INFO - joeynmt.training - EPOCH 197\r\n",
      "2024-08-27 16:03:32,899 - INFO - joeynmt.training - Epoch 197, Step:    18650, Batch Loss:     0.067130, Batch Acc: 0.994752, Tokens per Sec:     3734, Lr: 0.000131\r\n",
      "2024-08-27 16:03:34,010 - INFO - joeynmt.training - Epoch 197, Step:    18655, Batch Loss:     0.071187, Batch Acc: 0.995369, Tokens per Sec:     3696, Lr: 0.000131\r\n",
      "2024-08-27 16:03:35,111 - INFO - joeynmt.training - Epoch 197, Step:    18660, Batch Loss:     0.065333, Batch Acc: 0.994090, Tokens per Sec:     3997, Lr: 0.000131\r\n",
      "2024-08-27 16:03:36,212 - INFO - joeynmt.training - Epoch 197, Step:    18665, Batch Loss:     0.061715, Batch Acc: 0.995940, Tokens per Sec:     3808, Lr: 0.000131\r\n",
      "2024-08-27 16:03:37,341 - INFO - joeynmt.training - Epoch 197, Step:    18670, Batch Loss:     0.065669, Batch Acc: 0.993795, Tokens per Sec:     3712, Lr: 0.000131\r\n",
      "2024-08-27 16:03:38,447 - INFO - joeynmt.training - Epoch 197, Step:    18675, Batch Loss:     0.060979, Batch Acc: 0.994754, Tokens per Sec:     3794, Lr: 0.000131\r\n",
      "2024-08-27 16:03:39,549 - INFO - joeynmt.training - Epoch 197, Step:    18680, Batch Loss:     0.072453, Batch Acc: 0.997194, Tokens per Sec:     3882, Lr: 0.000131\r\n",
      "2024-08-27 16:03:40,644 - INFO - joeynmt.training - Epoch 197, Step:    18685, Batch Loss:     0.072895, Batch Acc: 0.994753, Tokens per Sec:     3835, Lr: 0.000131\r\n",
      "2024-08-27 16:03:41,733 - INFO - joeynmt.training - Epoch 197, Step:    18690, Batch Loss:     0.068599, Batch Acc: 0.992887, Tokens per Sec:     3746, Lr: 0.000131\r\n",
      "2024-08-27 16:03:42,823 - INFO - joeynmt.training - Epoch 197, Step:    18695, Batch Loss:     0.059688, Batch Acc: 0.991801, Tokens per Sec:     3805, Lr: 0.000131\r\n",
      "2024-08-27 16:03:43,914 - INFO - joeynmt.training - Epoch 197, Step:    18700, Batch Loss:     0.066610, Batch Acc: 0.995393, Tokens per Sec:     3979, Lr: 0.000131\r\n",
      "2024-08-27 16:03:45,021 - INFO - joeynmt.training - Epoch 197, Step:    18705, Batch Loss:     0.065166, Batch Acc: 0.993382, Tokens per Sec:     3962, Lr: 0.000131\r\n",
      "2024-08-27 16:03:46,115 - INFO - joeynmt.training - Epoch 197, Step:    18710, Batch Loss:     0.067386, Batch Acc: 0.993516, Tokens per Sec:     3808, Lr: 0.000131\r\n",
      "2024-08-27 16:03:47,266 - INFO - joeynmt.training - Epoch 197, Step:    18715, Batch Loss:     0.060963, Batch Acc: 0.993877, Tokens per Sec:     3548, Lr: 0.000131\r\n",
      "2024-08-27 16:03:48,506 - INFO - joeynmt.training - Epoch 197, Step:    18720, Batch Loss:     0.074385, Batch Acc: 0.995246, Tokens per Sec:     3394, Lr: 0.000131\r\n",
      "2024-08-27 16:03:49,790 - INFO - joeynmt.training - Epoch 197, Step:    18725, Batch Loss:     0.065644, Batch Acc: 0.992459, Tokens per Sec:     3412, Lr: 0.000131\r\n",
      "2024-08-27 16:03:50,909 - INFO - joeynmt.training - Epoch 197, Step:    18730, Batch Loss:     0.064757, Batch Acc: 0.994174, Tokens per Sec:     3991, Lr: 0.000131\r\n",
      "2024-08-27 16:03:52,009 - INFO - joeynmt.training - Epoch 197, Step:    18735, Batch Loss:     0.054272, Batch Acc: 0.996011, Tokens per Sec:     3649, Lr: 0.000131\r\n",
      "2024-08-27 16:03:53,136 - INFO - joeynmt.training - Epoch 197, Step:    18740, Batch Loss:     0.064049, Batch Acc: 0.995977, Tokens per Sec:     3751, Lr: 0.000131\r\n",
      "2024-08-27 16:03:53,885 - INFO - joeynmt.training - Epoch 197, total training loss: 6.41, num. of seqs: 8065, num. of tokens: 80463, 21.4327[sec]\r\n",
      "2024-08-27 16:03:53,885 - INFO - joeynmt.training - EPOCH 198\r\n",
      "2024-08-27 16:03:54,330 - INFO - joeynmt.training - Epoch 198, Step:    18745, Batch Loss:     0.068849, Batch Acc: 0.995302, Tokens per Sec:     3869, Lr: 0.000131\r\n",
      "2024-08-27 16:03:55,408 - INFO - joeynmt.training - Epoch 198, Step:    18750, Batch Loss:     0.065403, Batch Acc: 0.995421, Tokens per Sec:     3850, Lr: 0.000131\r\n",
      "2024-08-27 16:03:56,523 - INFO - joeynmt.training - Epoch 198, Step:    18755, Batch Loss:     0.070050, Batch Acc: 0.995176, Tokens per Sec:     3906, Lr: 0.000131\r\n",
      "2024-08-27 16:03:57,651 - INFO - joeynmt.training - Epoch 198, Step:    18760, Batch Loss:     0.061767, Batch Acc: 0.994496, Tokens per Sec:     3709, Lr: 0.000131\r\n",
      "2024-08-27 16:03:58,731 - INFO - joeynmt.training - Epoch 198, Step:    18765, Batch Loss:     0.060720, Batch Acc: 0.994393, Tokens per Sec:     3635, Lr: 0.000131\r\n",
      "2024-08-27 16:03:59,838 - INFO - joeynmt.training - Epoch 198, Step:    18770, Batch Loss:     0.060271, Batch Acc: 0.994645, Tokens per Sec:     3885, Lr: 0.000131\r\n",
      "2024-08-27 16:04:00,928 - INFO - joeynmt.training - Epoch 198, Step:    18775, Batch Loss:     0.061511, Batch Acc: 0.994969, Tokens per Sec:     3648, Lr: 0.000131\r\n",
      "2024-08-27 16:04:02,030 - INFO - joeynmt.training - Epoch 198, Step:    18780, Batch Loss:     0.063006, Batch Acc: 0.993046, Tokens per Sec:     3786, Lr: 0.000131\r\n",
      "2024-08-27 16:04:03,144 - INFO - joeynmt.training - Epoch 198, Step:    18785, Batch Loss:     0.066719, Batch Acc: 0.993777, Tokens per Sec:     3756, Lr: 0.000131\r\n",
      "2024-08-27 16:04:04,251 - INFO - joeynmt.training - Epoch 198, Step:    18790, Batch Loss:     0.063730, Batch Acc: 0.995473, Tokens per Sec:     3793, Lr: 0.000131\r\n",
      "2024-08-27 16:04:05,343 - INFO - joeynmt.training - Epoch 198, Step:    18795, Batch Loss:     0.066847, Batch Acc: 0.994592, Tokens per Sec:     3729, Lr: 0.000130\r\n",
      "2024-08-27 16:04:06,426 - INFO - joeynmt.training - Epoch 198, Step:    18800, Batch Loss:     0.061776, Batch Acc: 0.995517, Tokens per Sec:     3913, Lr: 0.000130\r\n",
      "2024-08-27 16:04:07,563 - INFO - joeynmt.training - Epoch 198, Step:    18805, Batch Loss:     0.067055, Batch Acc: 0.994622, Tokens per Sec:     3928, Lr: 0.000130\r\n",
      "2024-08-27 16:04:08,664 - INFO - joeynmt.training - Epoch 198, Step:    18810, Batch Loss:     0.061409, Batch Acc: 0.996460, Tokens per Sec:     3850, Lr: 0.000130\r\n",
      "2024-08-27 16:04:09,757 - INFO - joeynmt.training - Epoch 198, Step:    18815, Batch Loss:     0.071038, Batch Acc: 0.993267, Tokens per Sec:     3946, Lr: 0.000130\r\n",
      "2024-08-27 16:04:10,856 - INFO - joeynmt.training - Epoch 198, Step:    18820, Batch Loss:     0.069962, Batch Acc: 0.994041, Tokens per Sec:     3818, Lr: 0.000130\r\n",
      "2024-08-27 16:04:11,946 - INFO - joeynmt.training - Epoch 198, Step:    18825, Batch Loss:     0.069794, Batch Acc: 0.994148, Tokens per Sec:     3609, Lr: 0.000130\r\n",
      "2024-08-27 16:04:13,032 - INFO - joeynmt.training - Epoch 198, Step:    18830, Batch Loss:     0.059391, Batch Acc: 0.994447, Tokens per Sec:     3982, Lr: 0.000130\r\n",
      "2024-08-27 16:04:14,114 - INFO - joeynmt.training - Epoch 198, Step:    18835, Batch Loss:     0.072553, Batch Acc: 0.995149, Tokens per Sec:     4002, Lr: 0.000130\r\n",
      "2024-08-27 16:04:14,977 - INFO - joeynmt.training - Epoch 198, total training loss: 6.39, num. of seqs: 8065, num. of tokens: 80463, 21.0740[sec]\r\n",
      "2024-08-27 16:04:14,977 - INFO - joeynmt.training - EPOCH 199\r\n",
      "2024-08-27 16:04:15,212 - INFO - joeynmt.training - Epoch 199, Step:    18840, Batch Loss:     0.069853, Batch Acc: 0.994675, Tokens per Sec:     4069, Lr: 0.000130\r\n",
      "2024-08-27 16:04:16,296 - INFO - joeynmt.training - Epoch 199, Step:    18845, Batch Loss:     0.064230, Batch Acc: 0.995781, Tokens per Sec:     3720, Lr: 0.000130\r\n",
      "2024-08-27 16:04:17,417 - INFO - joeynmt.training - Epoch 199, Step:    18850, Batch Loss:     0.064930, Batch Acc: 0.995314, Tokens per Sec:     3808, Lr: 0.000130\r\n",
      "2024-08-27 16:04:18,503 - INFO - joeynmt.training - Epoch 199, Step:    18855, Batch Loss:     0.056645, Batch Acc: 0.997101, Tokens per Sec:     3814, Lr: 0.000130\r\n",
      "2024-08-27 16:04:19,716 - INFO - joeynmt.training - Epoch 199, Step:    18860, Batch Loss:     0.084739, Batch Acc: 0.993070, Tokens per Sec:     3570, Lr: 0.000130\r\n",
      "2024-08-27 16:04:20,872 - INFO - joeynmt.training - Epoch 199, Step:    18865, Batch Loss:     0.071175, Batch Acc: 0.993062, Tokens per Sec:     3869, Lr: 0.000130\r\n",
      "2024-08-27 16:04:21,970 - INFO - joeynmt.training - Epoch 199, Step:    18870, Batch Loss:     0.071611, Batch Acc: 0.994202, Tokens per Sec:     3927, Lr: 0.000130\r\n",
      "2024-08-27 16:04:23,051 - INFO - joeynmt.training - Epoch 199, Step:    18875, Batch Loss:     0.078723, Batch Acc: 0.995081, Tokens per Sec:     3954, Lr: 0.000130\r\n",
      "2024-08-27 16:04:24,126 - INFO - joeynmt.training - Epoch 199, Step:    18880, Batch Loss:     0.064072, Batch Acc: 0.993977, Tokens per Sec:     3862, Lr: 0.000130\r\n",
      "2024-08-27 16:04:25,234 - INFO - joeynmt.training - Epoch 199, Step:    18885, Batch Loss:     0.064481, Batch Acc: 0.993576, Tokens per Sec:     3796, Lr: 0.000130\r\n",
      "2024-08-27 16:04:26,371 - INFO - joeynmt.training - Epoch 199, Step:    18890, Batch Loss:     0.069532, Batch Acc: 0.993577, Tokens per Sec:     3975, Lr: 0.000130\r\n",
      "2024-08-27 16:04:27,523 - INFO - joeynmt.training - Epoch 199, Step:    18895, Batch Loss:     0.059900, Batch Acc: 0.995137, Tokens per Sec:     3749, Lr: 0.000130\r\n",
      "2024-08-27 16:04:28,628 - INFO - joeynmt.training - Epoch 199, Step:    18900, Batch Loss:     0.070427, Batch Acc: 0.994714, Tokens per Sec:     3770, Lr: 0.000130\r\n",
      "2024-08-27 16:04:29,735 - INFO - joeynmt.training - Epoch 199, Step:    18905, Batch Loss:     0.062233, Batch Acc: 0.993944, Tokens per Sec:     3732, Lr: 0.000130\r\n",
      "2024-08-27 16:04:30,817 - INFO - joeynmt.training - Epoch 199, Step:    18910, Batch Loss:     0.058774, Batch Acc: 0.994526, Tokens per Sec:     3716, Lr: 0.000130\r\n",
      "2024-08-27 16:04:31,900 - INFO - joeynmt.training - Epoch 199, Step:    18915, Batch Loss:     0.064054, Batch Acc: 0.994021, Tokens per Sec:     3863, Lr: 0.000130\r\n",
      "2024-08-27 16:04:32,998 - INFO - joeynmt.training - Epoch 199, Step:    18920, Batch Loss:     0.077287, Batch Acc: 0.993445, Tokens per Sec:     3754, Lr: 0.000130\r\n",
      "2024-08-27 16:04:34,086 - INFO - joeynmt.training - Epoch 199, Step:    18925, Batch Loss:     0.070294, Batch Acc: 0.995501, Tokens per Sec:     3883, Lr: 0.000130\r\n",
      "2024-08-27 16:04:35,179 - INFO - joeynmt.training - Epoch 199, Step:    18930, Batch Loss:     0.063025, Batch Acc: 0.994940, Tokens per Sec:     3801, Lr: 0.000130\r\n",
      "2024-08-27 16:04:36,135 - INFO - joeynmt.training - Epoch 199, total training loss: 6.40, num. of seqs: 8065, num. of tokens: 80463, 21.1412[sec]\r\n",
      "2024-08-27 16:04:36,136 - INFO - joeynmt.training - EPOCH 200\r\n",
      "2024-08-27 16:04:36,364 - INFO - joeynmt.training - Epoch 200, Step:    18935, Batch Loss:     0.088307, Batch Acc: 0.985552, Tokens per Sec:     4324, Lr: 0.000130\r\n",
      "2024-08-27 16:04:37,486 - INFO - joeynmt.training - Epoch 200, Step:    18940, Batch Loss:     0.072461, Batch Acc: 0.994609, Tokens per Sec:     3803, Lr: 0.000130\r\n",
      "2024-08-27 16:04:38,600 - INFO - joeynmt.training - Epoch 200, Step:    18945, Batch Loss:     0.076728, Batch Acc: 0.994217, Tokens per Sec:     4040, Lr: 0.000130\r\n",
      "2024-08-27 16:04:39,687 - INFO - joeynmt.training - Epoch 200, Step:    18950, Batch Loss:     0.062986, Batch Acc: 0.995503, Tokens per Sec:     3887, Lr: 0.000130\r\n",
      "2024-08-27 16:04:40,798 - INFO - joeynmt.training - Epoch 200, Step:    18955, Batch Loss:     0.071681, Batch Acc: 0.992935, Tokens per Sec:     3824, Lr: 0.000130\r\n",
      "2024-08-27 16:04:41,900 - INFO - joeynmt.training - Epoch 200, Step:    18960, Batch Loss:     0.066091, Batch Acc: 0.996567, Tokens per Sec:     3968, Lr: 0.000130\r\n",
      "2024-08-27 16:04:43,006 - INFO - joeynmt.training - Epoch 200, Step:    18965, Batch Loss:     0.061293, Batch Acc: 0.992976, Tokens per Sec:     3865, Lr: 0.000130\r\n",
      "2024-08-27 16:04:44,086 - INFO - joeynmt.training - Epoch 200, Step:    18970, Batch Loss:     0.061492, Batch Acc: 0.995028, Tokens per Sec:     3914, Lr: 0.000130\r\n",
      "2024-08-27 16:04:45,181 - INFO - joeynmt.training - Epoch 200, Step:    18975, Batch Loss:     0.060718, Batch Acc: 0.996557, Tokens per Sec:     3713, Lr: 0.000130\r\n",
      "2024-08-27 16:04:46,264 - INFO - joeynmt.training - Epoch 200, Step:    18980, Batch Loss:     0.078753, Batch Acc: 0.993742, Tokens per Sec:     3840, Lr: 0.000130\r\n",
      "2024-08-27 16:04:47,422 - INFO - joeynmt.training - Epoch 200, Step:    18985, Batch Loss:     0.067203, Batch Acc: 0.993603, Tokens per Sec:     3782, Lr: 0.000130\r\n",
      "2024-08-27 16:04:48,519 - INFO - joeynmt.training - Epoch 200, Step:    18990, Batch Loss:     0.063501, Batch Acc: 0.995067, Tokens per Sec:     3885, Lr: 0.000130\r\n",
      "2024-08-27 16:04:49,609 - INFO - joeynmt.training - Epoch 200, Step:    18995, Batch Loss:     0.069194, Batch Acc: 0.994772, Tokens per Sec:     3687, Lr: 0.000130\r\n",
      "2024-08-27 16:04:50,817 - INFO - joeynmt.training - Epoch 200, Step:    19000, Batch Loss:     0.068673, Batch Acc: 0.993777, Tokens per Sec:     3461, Lr: 0.000130\r\n",
      "2024-08-27 16:04:50,818 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=19042\r\n",
      "2024-08-27 16:04:50,818 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 63.99it/s]\r\n",
      "2024-08-27 16:05:13,637 - INFO - joeynmt.prediction - Generation took 22.8175[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 43942.09 examples/s]\r\n",
      "2024-08-27 16:05:14,022 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:05:14,022 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   4.96, loss:   6.81, ppl: 909.89, acc:   0.28, 0.1676[sec]\r\n",
      "2024-08-27 16:05:14,024 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:05:14,170 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:05:14,171 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:05:14,171 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 16:05:14,465 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:05:14,610 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:05:14,610 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:05:14,611 - INFO - joeynmt.training - \tHypothesis: trois éléphants\r\n",
      "2024-08-27 16:05:14,902 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:05:15,048 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:05:15,048 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:05:15,048 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 16:05:15,340 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:05:15,486 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:05:15,486 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:05:15,486 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 16:05:16,898 - INFO - joeynmt.training - Epoch 200, Step:    19005, Batch Loss:     0.058053, Batch Acc: 0.994737, Tokens per Sec:     3909, Lr: 0.000130\r\n",
      "2024-08-27 16:05:17,975 - INFO - joeynmt.training - Epoch 200, Step:    19010, Batch Loss:     0.067643, Batch Acc: 0.995688, Tokens per Sec:     3877, Lr: 0.000130\r\n",
      "2024-08-27 16:05:19,050 - INFO - joeynmt.training - Epoch 200, Step:    19015, Batch Loss:     0.063237, Batch Acc: 0.991654, Tokens per Sec:     3681, Lr: 0.000130\r\n",
      "2024-08-27 16:05:20,142 - INFO - joeynmt.training - Epoch 200, Step:    19020, Batch Loss:     0.080443, Batch Acc: 0.991591, Tokens per Sec:     3924, Lr: 0.000130\r\n",
      "2024-08-27 16:05:21,223 - INFO - joeynmt.training - Epoch 200, Step:    19025, Batch Loss:     0.052039, Batch Acc: 0.995909, Tokens per Sec:     3847, Lr: 0.000130\r\n",
      "2024-08-27 16:05:22,221 - INFO - joeynmt.training - Epoch 200, total training loss: 6.36, num. of seqs: 8065, num. of tokens: 80463, 21.1048[sec]\r\n",
      "2024-08-27 16:05:22,221 - INFO - joeynmt.training - EPOCH 201\r\n",
      "2024-08-27 16:05:22,459 - INFO - joeynmt.training - Epoch 201, Step:    19030, Batch Loss:     0.063530, Batch Acc: 0.994426, Tokens per Sec:     3822, Lr: 0.000130\r\n",
      "2024-08-27 16:05:23,587 - INFO - joeynmt.training - Epoch 201, Step:    19035, Batch Loss:     0.075775, Batch Acc: 0.992508, Tokens per Sec:     3673, Lr: 0.000130\r\n",
      "2024-08-27 16:05:24,692 - INFO - joeynmt.training - Epoch 201, Step:    19040, Batch Loss:     0.066393, Batch Acc: 0.997110, Tokens per Sec:     4074, Lr: 0.000130\r\n",
      "2024-08-27 16:05:25,780 - INFO - joeynmt.training - Epoch 201, Step:    19045, Batch Loss:     0.067508, Batch Acc: 0.993703, Tokens per Sec:     3796, Lr: 0.000130\r\n",
      "2024-08-27 16:05:26,902 - INFO - joeynmt.training - Epoch 201, Step:    19050, Batch Loss:     0.074635, Batch Acc: 0.994541, Tokens per Sec:     3758, Lr: 0.000130\r\n",
      "2024-08-27 16:05:27,988 - INFO - joeynmt.training - Epoch 201, Step:    19055, Batch Loss:     0.065615, Batch Acc: 0.995525, Tokens per Sec:     3912, Lr: 0.000130\r\n",
      "2024-08-27 16:05:29,076 - INFO - joeynmt.training - Epoch 201, Step:    19060, Batch Loss:     0.066584, Batch Acc: 0.994209, Tokens per Sec:     3971, Lr: 0.000130\r\n",
      "2024-08-27 16:05:30,172 - INFO - joeynmt.training - Epoch 201, Step:    19065, Batch Loss:     0.068733, Batch Acc: 0.994936, Tokens per Sec:     3785, Lr: 0.000130\r\n",
      "2024-08-27 16:05:31,291 - INFO - joeynmt.training - Epoch 201, Step:    19070, Batch Loss:     0.067422, Batch Acc: 0.996293, Tokens per Sec:     3858, Lr: 0.000130\r\n",
      "2024-08-27 16:05:32,422 - INFO - joeynmt.training - Epoch 201, Step:    19075, Batch Loss:     0.061719, Batch Acc: 0.995425, Tokens per Sec:     3870, Lr: 0.000130\r\n",
      "2024-08-27 16:05:33,500 - INFO - joeynmt.training - Epoch 201, Step:    19080, Batch Loss:     0.069673, Batch Acc: 0.995223, Tokens per Sec:     3692, Lr: 0.000130\r\n",
      "2024-08-27 16:05:34,611 - INFO - joeynmt.training - Epoch 201, Step:    19085, Batch Loss:     0.058779, Batch Acc: 0.996032, Tokens per Sec:     3858, Lr: 0.000129\r\n",
      "2024-08-27 16:05:35,706 - INFO - joeynmt.training - Epoch 201, Step:    19090, Batch Loss:     0.075396, Batch Acc: 0.996020, Tokens per Sec:     3901, Lr: 0.000129\r\n",
      "2024-08-27 16:05:36,844 - INFO - joeynmt.training - Epoch 201, Step:    19095, Batch Loss:     0.064255, Batch Acc: 0.994079, Tokens per Sec:     3713, Lr: 0.000129\r\n",
      "2024-08-27 16:05:37,933 - INFO - joeynmt.training - Epoch 201, Step:    19100, Batch Loss:     0.070570, Batch Acc: 0.995092, Tokens per Sec:     3934, Lr: 0.000129\r\n",
      "2024-08-27 16:05:39,038 - INFO - joeynmt.training - Epoch 201, Step:    19105, Batch Loss:     0.073654, Batch Acc: 0.995147, Tokens per Sec:     3919, Lr: 0.000129\r\n",
      "2024-08-27 16:05:40,135 - INFO - joeynmt.training - Epoch 201, Step:    19110, Batch Loss:     0.062796, Batch Acc: 0.993961, Tokens per Sec:     3776, Lr: 0.000129\r\n",
      "2024-08-27 16:05:41,234 - INFO - joeynmt.training - Epoch 201, Step:    19115, Batch Loss:     0.064351, Batch Acc: 0.995050, Tokens per Sec:     3863, Lr: 0.000129\r\n",
      "2024-08-27 16:05:42,347 - INFO - joeynmt.training - Epoch 201, Step:    19120, Batch Loss:     0.058536, Batch Acc: 0.996788, Tokens per Sec:     3918, Lr: 0.000129\r\n",
      "2024-08-27 16:05:43,139 - INFO - joeynmt.training - Epoch 201, total training loss: 6.20, num. of seqs: 8065, num. of tokens: 80463, 20.9015[sec]\r\n",
      "2024-08-27 16:05:43,139 - INFO - joeynmt.training - EPOCH 202\r\n",
      "2024-08-27 16:05:43,575 - INFO - joeynmt.training - Epoch 202, Step:    19125, Batch Loss:     0.066056, Batch Acc: 0.993793, Tokens per Sec:     3744, Lr: 0.000129\r\n",
      "2024-08-27 16:05:44,660 - INFO - joeynmt.training - Epoch 202, Step:    19130, Batch Loss:     0.068718, Batch Acc: 0.996848, Tokens per Sec:     3802, Lr: 0.000129\r\n",
      "2024-08-27 16:05:45,746 - INFO - joeynmt.training - Epoch 202, Step:    19135, Batch Loss:     0.062447, Batch Acc: 0.992908, Tokens per Sec:     4031, Lr: 0.000129\r\n",
      "2024-08-27 16:05:46,871 - INFO - joeynmt.training - Epoch 202, Step:    19140, Batch Loss:     0.059004, Batch Acc: 0.995342, Tokens per Sec:     3816, Lr: 0.000129\r\n",
      "2024-08-27 16:05:47,990 - INFO - joeynmt.training - Epoch 202, Step:    19145, Batch Loss:     0.060377, Batch Acc: 0.994854, Tokens per Sec:     3823, Lr: 0.000129\r\n",
      "2024-08-27 16:05:49,082 - INFO - joeynmt.training - Epoch 202, Step:    19150, Batch Loss:     0.063166, Batch Acc: 0.994747, Tokens per Sec:     3840, Lr: 0.000129\r\n",
      "2024-08-27 16:05:50,169 - INFO - joeynmt.training - Epoch 202, Step:    19155, Batch Loss:     0.059152, Batch Acc: 0.994997, Tokens per Sec:     3680, Lr: 0.000129\r\n",
      "2024-08-27 16:05:51,273 - INFO - joeynmt.training - Epoch 202, Step:    19160, Batch Loss:     0.065047, Batch Acc: 0.996912, Tokens per Sec:     3814, Lr: 0.000129\r\n",
      "2024-08-27 16:05:52,393 - INFO - joeynmt.training - Epoch 202, Step:    19165, Batch Loss:     0.064275, Batch Acc: 0.993817, Tokens per Sec:     3902, Lr: 0.000129\r\n",
      "2024-08-27 16:05:53,607 - INFO - joeynmt.training - Epoch 202, Step:    19170, Batch Loss:     0.054605, Batch Acc: 0.994330, Tokens per Sec:     3490, Lr: 0.000129\r\n",
      "2024-08-27 16:05:54,955 - INFO - joeynmt.training - Epoch 202, Step:    19175, Batch Loss:     0.066042, Batch Acc: 0.995256, Tokens per Sec:     2972, Lr: 0.000129\r\n",
      "2024-08-27 16:05:56,093 - INFO - joeynmt.training - Epoch 202, Step:    19180, Batch Loss:     0.065822, Batch Acc: 0.995814, Tokens per Sec:     3783, Lr: 0.000129\r\n",
      "2024-08-27 16:05:57,245 - INFO - joeynmt.training - Epoch 202, Step:    19185, Batch Loss:     0.072386, Batch Acc: 0.994792, Tokens per Sec:     3834, Lr: 0.000129\r\n",
      "2024-08-27 16:05:58,344 - INFO - joeynmt.training - Epoch 202, Step:    19190, Batch Loss:     0.061898, Batch Acc: 0.995553, Tokens per Sec:     3685, Lr: 0.000129\r\n",
      "2024-08-27 16:05:59,420 - INFO - joeynmt.training - Epoch 202, Step:    19195, Batch Loss:     0.068594, Batch Acc: 0.994764, Tokens per Sec:     3733, Lr: 0.000129\r\n",
      "2024-08-27 16:06:00,521 - INFO - joeynmt.training - Epoch 202, Step:    19200, Batch Loss:     0.063774, Batch Acc: 0.995290, Tokens per Sec:     3859, Lr: 0.000129\r\n",
      "2024-08-27 16:06:01,606 - INFO - joeynmt.training - Epoch 202, Step:    19205, Batch Loss:     0.079126, Batch Acc: 0.991722, Tokens per Sec:     3898, Lr: 0.000129\r\n",
      "2024-08-27 16:06:02,681 - INFO - joeynmt.training - Epoch 202, Step:    19210, Batch Loss:     0.069099, Batch Acc: 0.994913, Tokens per Sec:     3841, Lr: 0.000129\r\n",
      "2024-08-27 16:06:03,760 - INFO - joeynmt.training - Epoch 202, Step:    19215, Batch Loss:     0.070084, Batch Acc: 0.994447, Tokens per Sec:     3842, Lr: 0.000129\r\n",
      "2024-08-27 16:06:04,569 - INFO - joeynmt.training - Epoch 202, total training loss: 6.21, num. of seqs: 8065, num. of tokens: 80463, 21.4110[sec]\r\n",
      "2024-08-27 16:06:04,570 - INFO - joeynmt.training - EPOCH 203\r\n",
      "2024-08-27 16:06:05,010 - INFO - joeynmt.training - Epoch 203, Step:    19220, Batch Loss:     0.078206, Batch Acc: 0.993197, Tokens per Sec:     3709, Lr: 0.000129\r\n",
      "2024-08-27 16:06:06,104 - INFO - joeynmt.training - Epoch 203, Step:    19225, Batch Loss:     0.067481, Batch Acc: 0.994112, Tokens per Sec:     3726, Lr: 0.000129\r\n",
      "2024-08-27 16:06:07,237 - INFO - joeynmt.training - Epoch 203, Step:    19230, Batch Loss:     0.070317, Batch Acc: 0.996061, Tokens per Sec:     3813, Lr: 0.000129\r\n",
      "2024-08-27 16:06:08,326 - INFO - joeynmt.training - Epoch 203, Step:    19235, Batch Loss:     0.065632, Batch Acc: 0.993833, Tokens per Sec:     3874, Lr: 0.000129\r\n",
      "2024-08-27 16:06:09,413 - INFO - joeynmt.training - Epoch 203, Step:    19240, Batch Loss:     0.064011, Batch Acc: 0.995436, Tokens per Sec:     3832, Lr: 0.000129\r\n",
      "2024-08-27 16:06:10,518 - INFO - joeynmt.training - Epoch 203, Step:    19245, Batch Loss:     0.075021, Batch Acc: 0.993102, Tokens per Sec:     3807, Lr: 0.000129\r\n",
      "2024-08-27 16:06:11,635 - INFO - joeynmt.training - Epoch 203, Step:    19250, Batch Loss:     0.065615, Batch Acc: 0.994947, Tokens per Sec:     3899, Lr: 0.000129\r\n",
      "2024-08-27 16:06:12,730 - INFO - joeynmt.training - Epoch 203, Step:    19255, Batch Loss:     0.065814, Batch Acc: 0.994422, Tokens per Sec:     3768, Lr: 0.000129\r\n",
      "2024-08-27 16:06:13,833 - INFO - joeynmt.training - Epoch 203, Step:    19260, Batch Loss:     0.066922, Batch Acc: 0.994622, Tokens per Sec:     3881, Lr: 0.000129\r\n",
      "2024-08-27 16:06:14,918 - INFO - joeynmt.training - Epoch 203, Step:    19265, Batch Loss:     0.063306, Batch Acc: 0.993584, Tokens per Sec:     3879, Lr: 0.000129\r\n",
      "2024-08-27 16:06:16,002 - INFO - joeynmt.training - Epoch 203, Step:    19270, Batch Loss:     0.073414, Batch Acc: 0.994442, Tokens per Sec:     3986, Lr: 0.000129\r\n",
      "2024-08-27 16:06:17,146 - INFO - joeynmt.training - Epoch 203, Step:    19275, Batch Loss:     0.065679, Batch Acc: 0.994881, Tokens per Sec:     3929, Lr: 0.000129\r\n",
      "2024-08-27 16:06:18,241 - INFO - joeynmt.training - Epoch 203, Step:    19280, Batch Loss:     0.062590, Batch Acc: 0.996432, Tokens per Sec:     3844, Lr: 0.000129\r\n",
      "2024-08-27 16:06:19,355 - INFO - joeynmt.training - Epoch 203, Step:    19285, Batch Loss:     0.056040, Batch Acc: 0.996566, Tokens per Sec:     3662, Lr: 0.000129\r\n",
      "2024-08-27 16:06:20,441 - INFO - joeynmt.training - Epoch 203, Step:    19290, Batch Loss:     0.066968, Batch Acc: 0.992845, Tokens per Sec:     3733, Lr: 0.000129\r\n",
      "2024-08-27 16:06:21,526 - INFO - joeynmt.training - Epoch 203, Step:    19295, Batch Loss:     0.067598, Batch Acc: 0.995969, Tokens per Sec:     3891, Lr: 0.000129\r\n",
      "2024-08-27 16:06:22,613 - INFO - joeynmt.training - Epoch 203, Step:    19300, Batch Loss:     0.072054, Batch Acc: 0.995760, Tokens per Sec:     3908, Lr: 0.000129\r\n",
      "2024-08-27 16:06:23,696 - INFO - joeynmt.training - Epoch 203, Step:    19305, Batch Loss:     0.062656, Batch Acc: 0.994456, Tokens per Sec:     3832, Lr: 0.000129\r\n",
      "2024-08-27 16:06:24,907 - INFO - joeynmt.training - Epoch 203, Step:    19310, Batch Loss:     0.057391, Batch Acc: 0.993452, Tokens per Sec:     3535, Lr: 0.000129\r\n",
      "2024-08-27 16:06:25,825 - INFO - joeynmt.training - Epoch 203, total training loss: 6.29, num. of seqs: 8065, num. of tokens: 80463, 21.2382[sec]\r\n",
      "2024-08-27 16:06:25,826 - INFO - joeynmt.training - EPOCH 204\r\n",
      "2024-08-27 16:06:26,263 - INFO - joeynmt.training - Epoch 204, Step:    19315, Batch Loss:     0.059216, Batch Acc: 0.995821, Tokens per Sec:     3866, Lr: 0.000129\r\n",
      "2024-08-27 16:06:27,371 - INFO - joeynmt.training - Epoch 204, Step:    19320, Batch Loss:     0.052934, Batch Acc: 0.995591, Tokens per Sec:     3686, Lr: 0.000129\r\n",
      "2024-08-27 16:06:28,446 - INFO - joeynmt.training - Epoch 204, Step:    19325, Batch Loss:     0.062519, Batch Acc: 0.996406, Tokens per Sec:     3887, Lr: 0.000129\r\n",
      "2024-08-27 16:06:29,531 - INFO - joeynmt.training - Epoch 204, Step:    19330, Batch Loss:     0.070758, Batch Acc: 0.995602, Tokens per Sec:     3984, Lr: 0.000129\r\n",
      "2024-08-27 16:06:30,612 - INFO - joeynmt.training - Epoch 204, Step:    19335, Batch Loss:     0.063460, Batch Acc: 0.993033, Tokens per Sec:     3984, Lr: 0.000129\r\n",
      "2024-08-27 16:06:31,714 - INFO - joeynmt.training - Epoch 204, Step:    19340, Batch Loss:     0.064803, Batch Acc: 0.995659, Tokens per Sec:     3976, Lr: 0.000129\r\n",
      "2024-08-27 16:06:32,813 - INFO - joeynmt.training - Epoch 204, Step:    19345, Batch Loss:     0.057960, Batch Acc: 0.994159, Tokens per Sec:     3898, Lr: 0.000129\r\n",
      "2024-08-27 16:06:33,913 - INFO - joeynmt.training - Epoch 204, Step:    19350, Batch Loss:     0.066814, Batch Acc: 0.994353, Tokens per Sec:     3865, Lr: 0.000129\r\n",
      "2024-08-27 16:06:35,013 - INFO - joeynmt.training - Epoch 204, Step:    19355, Batch Loss:     0.076509, Batch Acc: 0.992257, Tokens per Sec:     3876, Lr: 0.000129\r\n",
      "2024-08-27 16:06:36,099 - INFO - joeynmt.training - Epoch 204, Step:    19360, Batch Loss:     0.061433, Batch Acc: 0.992381, Tokens per Sec:     3751, Lr: 0.000129\r\n",
      "2024-08-27 16:06:37,225 - INFO - joeynmt.training - Epoch 204, Step:    19365, Batch Loss:     0.059954, Batch Acc: 0.994884, Tokens per Sec:     3647, Lr: 0.000129\r\n",
      "2024-08-27 16:06:38,349 - INFO - joeynmt.training - Epoch 204, Step:    19370, Batch Loss:     0.071516, Batch Acc: 0.993619, Tokens per Sec:     3767, Lr: 0.000129\r\n",
      "2024-08-27 16:06:39,443 - INFO - joeynmt.training - Epoch 204, Step:    19375, Batch Loss:     0.071567, Batch Acc: 0.994126, Tokens per Sec:     3892, Lr: 0.000129\r\n",
      "2024-08-27 16:06:40,518 - INFO - joeynmt.training - Epoch 204, Step:    19380, Batch Loss:     0.064233, Batch Acc: 0.993689, Tokens per Sec:     3836, Lr: 0.000128\r\n",
      "2024-08-27 16:06:41,605 - INFO - joeynmt.training - Epoch 204, Step:    19385, Batch Loss:     0.060972, Batch Acc: 0.994042, Tokens per Sec:     3709, Lr: 0.000128\r\n",
      "2024-08-27 16:06:42,703 - INFO - joeynmt.training - Epoch 204, Step:    19390, Batch Loss:     0.063675, Batch Acc: 0.995307, Tokens per Sec:     3883, Lr: 0.000128\r\n",
      "2024-08-27 16:06:43,816 - INFO - joeynmt.training - Epoch 204, Step:    19395, Batch Loss:     0.078468, Batch Acc: 0.994302, Tokens per Sec:     3786, Lr: 0.000128\r\n",
      "2024-08-27 16:06:44,914 - INFO - joeynmt.training - Epoch 204, Step:    19400, Batch Loss:     0.062967, Batch Acc: 0.995520, Tokens per Sec:     3663, Lr: 0.000128\r\n",
      "2024-08-27 16:06:45,994 - INFO - joeynmt.training - Epoch 204, Step:    19405, Batch Loss:     0.082865, Batch Acc: 0.993320, Tokens per Sec:     4019, Lr: 0.000128\r\n",
      "2024-08-27 16:06:46,834 - INFO - joeynmt.training - Epoch 204, total training loss: 6.31, num. of seqs: 8065, num. of tokens: 80463, 20.9914[sec]\r\n",
      "2024-08-27 16:06:46,835 - INFO - joeynmt.training - EPOCH 205\r\n",
      "2024-08-27 16:06:47,274 - INFO - joeynmt.training - Epoch 205, Step:    19410, Batch Loss:     0.067161, Batch Acc: 0.993687, Tokens per Sec:     3639, Lr: 0.000128\r\n",
      "2024-08-27 16:06:48,357 - INFO - joeynmt.training - Epoch 205, Step:    19415, Batch Loss:     0.057953, Batch Acc: 0.995220, Tokens per Sec:     3867, Lr: 0.000128\r\n",
      "2024-08-27 16:06:49,451 - INFO - joeynmt.training - Epoch 205, Step:    19420, Batch Loss:     0.075727, Batch Acc: 0.993426, Tokens per Sec:     3894, Lr: 0.000128\r\n",
      "2024-08-27 16:06:50,531 - INFO - joeynmt.training - Epoch 205, Step:    19425, Batch Loss:     0.064030, Batch Acc: 0.995476, Tokens per Sec:     3893, Lr: 0.000128\r\n",
      "2024-08-27 16:06:51,629 - INFO - joeynmt.training - Epoch 205, Step:    19430, Batch Loss:     0.067830, Batch Acc: 0.993824, Tokens per Sec:     3983, Lr: 0.000128\r\n",
      "2024-08-27 16:06:52,711 - INFO - joeynmt.training - Epoch 205, Step:    19435, Batch Loss:     0.057658, Batch Acc: 0.997554, Tokens per Sec:     3780, Lr: 0.000128\r\n",
      "2024-08-27 16:06:53,813 - INFO - joeynmt.training - Epoch 205, Step:    19440, Batch Loss:     0.067374, Batch Acc: 0.994958, Tokens per Sec:     3963, Lr: 0.000128\r\n",
      "2024-08-27 16:06:54,937 - INFO - joeynmt.training - Epoch 205, Step:    19445, Batch Loss:     0.061270, Batch Acc: 0.997005, Tokens per Sec:     3865, Lr: 0.000128\r\n",
      "2024-08-27 16:06:56,133 - INFO - joeynmt.training - Epoch 205, Step:    19450, Batch Loss:     0.060617, Batch Acc: 0.994526, Tokens per Sec:     3516, Lr: 0.000128\r\n",
      "2024-08-27 16:06:57,439 - INFO - joeynmt.training - Epoch 205, Step:    19455, Batch Loss:     0.064236, Batch Acc: 0.992652, Tokens per Sec:     3230, Lr: 0.000128\r\n",
      "2024-08-27 16:06:58,544 - INFO - joeynmt.training - Epoch 205, Step:    19460, Batch Loss:     0.067944, Batch Acc: 0.993902, Tokens per Sec:     3862, Lr: 0.000128\r\n",
      "2024-08-27 16:06:59,646 - INFO - joeynmt.training - Epoch 205, Step:    19465, Batch Loss:     0.076328, Batch Acc: 0.995283, Tokens per Sec:     4042, Lr: 0.000128\r\n",
      "2024-08-27 16:07:00,757 - INFO - joeynmt.training - Epoch 205, Step:    19470, Batch Loss:     0.081574, Batch Acc: 0.993005, Tokens per Sec:     3735, Lr: 0.000128\r\n",
      "2024-08-27 16:07:01,882 - INFO - joeynmt.training - Epoch 205, Step:    19475, Batch Loss:     0.080418, Batch Acc: 0.993581, Tokens per Sec:     3882, Lr: 0.000128\r\n",
      "2024-08-27 16:07:02,965 - INFO - joeynmt.training - Epoch 205, Step:    19480, Batch Loss:     0.058591, Batch Acc: 0.995428, Tokens per Sec:     3840, Lr: 0.000128\r\n",
      "2024-08-27 16:07:04,053 - INFO - joeynmt.training - Epoch 205, Step:    19485, Batch Loss:     0.070263, Batch Acc: 0.994187, Tokens per Sec:     3797, Lr: 0.000128\r\n",
      "2024-08-27 16:07:05,152 - INFO - joeynmt.training - Epoch 205, Step:    19490, Batch Loss:     0.063044, Batch Acc: 0.993485, Tokens per Sec:     3912, Lr: 0.000128\r\n",
      "2024-08-27 16:07:06,238 - INFO - joeynmt.training - Epoch 205, Step:    19495, Batch Loss:     0.068586, Batch Acc: 0.995272, Tokens per Sec:     3897, Lr: 0.000128\r\n",
      "2024-08-27 16:07:07,363 - INFO - joeynmt.training - Epoch 205, Step:    19500, Batch Loss:     0.064598, Batch Acc: 0.994258, Tokens per Sec:     3874, Lr: 0.000128\r\n",
      "2024-08-27 16:07:07,364 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=19542\r\n",
      "2024-08-27 16:07:07,364 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 65.75it/s]\r\n",
      "2024-08-27 16:07:29,572 - INFO - joeynmt.prediction - Generation took 22.2058[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44740.62 examples/s]\r\n",
      "2024-08-27 16:07:29,952 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:07:29,953 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.31, loss:   6.81, ppl: 911.35, acc:   0.29, 0.1673[sec]\r\n",
      "2024-08-27 16:07:29,954 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:07:30,101 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:07:30,102 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:07:30,102 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 16:07:30,396 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:07:30,543 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:07:30,543 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:07:30,544 - INFO - joeynmt.training - \tHypothesis: trois tables blanches\r\n",
      "2024-08-27 16:07:30,834 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:07:30,980 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:07:30,980 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:07:30,980 - INFO - joeynmt.training - \tHypothesis: le service passagers n'est pas en activité actuellement\r\n",
      "2024-08-27 16:07:31,274 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:07:31,420 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:07:31,421 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:07:31,421 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 16:07:32,367 - INFO - joeynmt.training - Epoch 205, total training loss: 6.18, num. of seqs: 8065, num. of tokens: 80463, 21.1641[sec]\r\n",
      "2024-08-27 16:07:32,367 - INFO - joeynmt.training - EPOCH 206\r\n",
      "2024-08-27 16:07:32,800 - INFO - joeynmt.training - Epoch 206, Step:    19505, Batch Loss:     0.071573, Batch Acc: 0.993193, Tokens per Sec:     3762, Lr: 0.000128\r\n",
      "2024-08-27 16:07:33,911 - INFO - joeynmt.training - Epoch 206, Step:    19510, Batch Loss:     0.069139, Batch Acc: 0.995860, Tokens per Sec:     3916, Lr: 0.000128\r\n",
      "2024-08-27 16:07:35,028 - INFO - joeynmt.training - Epoch 206, Step:    19515, Batch Loss:     0.059368, Batch Acc: 0.994234, Tokens per Sec:     3729, Lr: 0.000128\r\n",
      "2024-08-27 16:07:36,128 - INFO - joeynmt.training - Epoch 206, Step:    19520, Batch Loss:     0.068732, Batch Acc: 0.995344, Tokens per Sec:     3713, Lr: 0.000128\r\n",
      "2024-08-27 16:07:37,280 - INFO - joeynmt.training - Epoch 206, Step:    19525, Batch Loss:     0.066065, Batch Acc: 0.995239, Tokens per Sec:     3647, Lr: 0.000128\r\n",
      "2024-08-27 16:07:38,380 - INFO - joeynmt.training - Epoch 206, Step:    19530, Batch Loss:     0.060161, Batch Acc: 0.996128, Tokens per Sec:     3994, Lr: 0.000128\r\n",
      "2024-08-27 16:07:39,488 - INFO - joeynmt.training - Epoch 206, Step:    19535, Batch Loss:     0.063743, Batch Acc: 0.994605, Tokens per Sec:     3684, Lr: 0.000128\r\n",
      "2024-08-27 16:07:40,566 - INFO - joeynmt.training - Epoch 206, Step:    19540, Batch Loss:     0.072042, Batch Acc: 0.995436, Tokens per Sec:     3659, Lr: 0.000128\r\n",
      "2024-08-27 16:07:41,687 - INFO - joeynmt.training - Epoch 206, Step:    19545, Batch Loss:     0.058202, Batch Acc: 0.996586, Tokens per Sec:     3660, Lr: 0.000128\r\n",
      "2024-08-27 16:07:42,800 - INFO - joeynmt.training - Epoch 206, Step:    19550, Batch Loss:     0.062437, Batch Acc: 0.992593, Tokens per Sec:     3885, Lr: 0.000128\r\n",
      "2024-08-27 16:07:43,905 - INFO - joeynmt.training - Epoch 206, Step:    19555, Batch Loss:     0.064864, Batch Acc: 0.995362, Tokens per Sec:     3905, Lr: 0.000128\r\n",
      "2024-08-27 16:07:44,977 - INFO - joeynmt.training - Epoch 206, Step:    19560, Batch Loss:     0.059211, Batch Acc: 0.995240, Tokens per Sec:     3923, Lr: 0.000128\r\n",
      "2024-08-27 16:07:46,051 - INFO - joeynmt.training - Epoch 206, Step:    19565, Batch Loss:     0.067249, Batch Acc: 0.993440, Tokens per Sec:     3974, Lr: 0.000128\r\n",
      "2024-08-27 16:07:47,162 - INFO - joeynmt.training - Epoch 206, Step:    19570, Batch Loss:     0.060401, Batch Acc: 0.994683, Tokens per Sec:     3727, Lr: 0.000128\r\n",
      "2024-08-27 16:07:48,267 - INFO - joeynmt.training - Epoch 206, Step:    19575, Batch Loss:     0.063852, Batch Acc: 0.993484, Tokens per Sec:     3891, Lr: 0.000128\r\n",
      "2024-08-27 16:07:49,349 - INFO - joeynmt.training - Epoch 206, Step:    19580, Batch Loss:     0.067815, Batch Acc: 0.993490, Tokens per Sec:     3979, Lr: 0.000128\r\n",
      "2024-08-27 16:07:50,432 - INFO - joeynmt.training - Epoch 206, Step:    19585, Batch Loss:     0.074111, Batch Acc: 0.995481, Tokens per Sec:     4088, Lr: 0.000128\r\n",
      "2024-08-27 16:07:51,515 - INFO - joeynmt.training - Epoch 206, Step:    19590, Batch Loss:     0.066489, Batch Acc: 0.994670, Tokens per Sec:     3639, Lr: 0.000128\r\n",
      "2024-08-27 16:07:52,582 - INFO - joeynmt.training - Epoch 206, Step:    19595, Batch Loss:     0.064101, Batch Acc: 0.993139, Tokens per Sec:     3827, Lr: 0.000128\r\n",
      "2024-08-27 16:07:53,446 - INFO - joeynmt.training - Epoch 206, total training loss: 6.25, num. of seqs: 8065, num. of tokens: 80463, 21.0628[sec]\r\n",
      "2024-08-27 16:07:53,446 - INFO - joeynmt.training - EPOCH 207\r\n",
      "2024-08-27 16:07:53,674 - INFO - joeynmt.training - Epoch 207, Step:    19600, Batch Loss:     0.072084, Batch Acc: 0.992135, Tokens per Sec:     3963, Lr: 0.000128\r\n",
      "2024-08-27 16:07:54,761 - INFO - joeynmt.training - Epoch 207, Step:    19605, Batch Loss:     0.062773, Batch Acc: 0.995315, Tokens per Sec:     3930, Lr: 0.000128\r\n",
      "2024-08-27 16:07:55,853 - INFO - joeynmt.training - Epoch 207, Step:    19610, Batch Loss:     0.072478, Batch Acc: 0.993968, Tokens per Sec:     3950, Lr: 0.000128\r\n",
      "2024-08-27 16:07:57,005 - INFO - joeynmt.training - Epoch 207, Step:    19615, Batch Loss:     0.065395, Batch Acc: 0.994764, Tokens per Sec:     3648, Lr: 0.000128\r\n",
      "2024-08-27 16:07:58,150 - INFO - joeynmt.training - Epoch 207, Step:    19620, Batch Loss:     0.065817, Batch Acc: 0.994910, Tokens per Sec:     3777, Lr: 0.000128\r\n",
      "2024-08-27 16:07:59,573 - INFO - joeynmt.training - Epoch 207, Step:    19625, Batch Loss:     0.061490, Batch Acc: 0.994069, Tokens per Sec:     2966, Lr: 0.000128\r\n",
      "2024-08-27 16:08:00,666 - INFO - joeynmt.training - Epoch 207, Step:    19630, Batch Loss:     0.068470, Batch Acc: 0.991820, Tokens per Sec:     3691, Lr: 0.000128\r\n",
      "2024-08-27 16:08:01,763 - INFO - joeynmt.training - Epoch 207, Step:    19635, Batch Loss:     0.070428, Batch Acc: 0.994194, Tokens per Sec:     3929, Lr: 0.000128\r\n",
      "2024-08-27 16:08:02,852 - INFO - joeynmt.training - Epoch 207, Step:    19640, Batch Loss:     0.075888, Batch Acc: 0.993220, Tokens per Sec:     3795, Lr: 0.000128\r\n",
      "2024-08-27 16:08:03,956 - INFO - joeynmt.training - Epoch 207, Step:    19645, Batch Loss:     0.062398, Batch Acc: 0.995806, Tokens per Sec:     3674, Lr: 0.000128\r\n",
      "2024-08-27 16:08:05,061 - INFO - joeynmt.training - Epoch 207, Step:    19650, Batch Loss:     0.063654, Batch Acc: 0.996417, Tokens per Sec:     4043, Lr: 0.000128\r\n",
      "2024-08-27 16:08:06,161 - INFO - joeynmt.training - Epoch 207, Step:    19655, Batch Loss:     0.066866, Batch Acc: 0.994954, Tokens per Sec:     3967, Lr: 0.000128\r\n",
      "2024-08-27 16:08:07,287 - INFO - joeynmt.training - Epoch 207, Step:    19660, Batch Loss:     0.063948, Batch Acc: 0.993466, Tokens per Sec:     3672, Lr: 0.000128\r\n",
      "2024-08-27 16:08:08,378 - INFO - joeynmt.training - Epoch 207, Step:    19665, Batch Loss:     0.058204, Batch Acc: 0.992808, Tokens per Sec:     3696, Lr: 0.000128\r\n",
      "2024-08-27 16:08:09,482 - INFO - joeynmt.training - Epoch 207, Step:    19670, Batch Loss:     0.070033, Batch Acc: 0.991649, Tokens per Sec:     3801, Lr: 0.000128\r\n",
      "2024-08-27 16:08:10,608 - INFO - joeynmt.training - Epoch 207, Step:    19675, Batch Loss:     0.081355, Batch Acc: 0.992645, Tokens per Sec:     3867, Lr: 0.000128\r\n",
      "2024-08-27 16:08:11,706 - INFO - joeynmt.training - Epoch 207, Step:    19680, Batch Loss:     0.055940, Batch Acc: 0.997390, Tokens per Sec:     3838, Lr: 0.000128\r\n",
      "2024-08-27 16:08:12,801 - INFO - joeynmt.training - Epoch 207, Step:    19685, Batch Loss:     0.069009, Batch Acc: 0.994715, Tokens per Sec:     3806, Lr: 0.000127\r\n",
      "2024-08-27 16:08:13,902 - INFO - joeynmt.training - Epoch 207, Step:    19690, Batch Loss:     0.060666, Batch Acc: 0.994467, Tokens per Sec:     3777, Lr: 0.000127\r\n",
      "2024-08-27 16:08:14,902 - INFO - joeynmt.training - Epoch 207, total training loss: 6.31, num. of seqs: 8065, num. of tokens: 80463, 21.4394[sec]\r\n",
      "2024-08-27 16:08:14,902 - INFO - joeynmt.training - EPOCH 208\r\n",
      "2024-08-27 16:08:15,132 - INFO - joeynmt.training - Epoch 208, Step:    19695, Batch Loss:     0.080229, Batch Acc: 0.992974, Tokens per Sec:     3793, Lr: 0.000127\r\n",
      "2024-08-27 16:08:16,250 - INFO - joeynmt.training - Epoch 208, Step:    19700, Batch Loss:     0.069167, Batch Acc: 0.995161, Tokens per Sec:     3882, Lr: 0.000127\r\n",
      "2024-08-27 16:08:17,403 - INFO - joeynmt.training - Epoch 208, Step:    19705, Batch Loss:     0.056763, Batch Acc: 0.994487, Tokens per Sec:     3777, Lr: 0.000127\r\n",
      "2024-08-27 16:08:18,522 - INFO - joeynmt.training - Epoch 208, Step:    19710, Batch Loss:     0.065852, Batch Acc: 0.992416, Tokens per Sec:     3892, Lr: 0.000127\r\n",
      "2024-08-27 16:08:19,647 - INFO - joeynmt.training - Epoch 208, Step:    19715, Batch Loss:     0.071231, Batch Acc: 0.993263, Tokens per Sec:     3696, Lr: 0.000127\r\n",
      "2024-08-27 16:08:20,741 - INFO - joeynmt.training - Epoch 208, Step:    19720, Batch Loss:     0.065675, Batch Acc: 0.995320, Tokens per Sec:     3714, Lr: 0.000127\r\n",
      "2024-08-27 16:08:21,843 - INFO - joeynmt.training - Epoch 208, Step:    19725, Batch Loss:     0.062629, Batch Acc: 0.996176, Tokens per Sec:     4036, Lr: 0.000127\r\n",
      "2024-08-27 16:08:22,957 - INFO - joeynmt.training - Epoch 208, Step:    19730, Batch Loss:     0.059378, Batch Acc: 0.993760, Tokens per Sec:     3744, Lr: 0.000127\r\n",
      "2024-08-27 16:08:24,064 - INFO - joeynmt.training - Epoch 208, Step:    19735, Batch Loss:     0.079481, Batch Acc: 0.992984, Tokens per Sec:     3865, Lr: 0.000127\r\n",
      "2024-08-27 16:08:25,156 - INFO - joeynmt.training - Epoch 208, Step:    19740, Batch Loss:     0.057945, Batch Acc: 0.995433, Tokens per Sec:     3810, Lr: 0.000127\r\n",
      "2024-08-27 16:08:26,258 - INFO - joeynmt.training - Epoch 208, Step:    19745, Batch Loss:     0.062158, Batch Acc: 0.993764, Tokens per Sec:     3934, Lr: 0.000127\r\n",
      "2024-08-27 16:08:27,391 - INFO - joeynmt.training - Epoch 208, Step:    19750, Batch Loss:     0.060008, Batch Acc: 0.997893, Tokens per Sec:     3773, Lr: 0.000127\r\n",
      "2024-08-27 16:08:28,465 - INFO - joeynmt.training - Epoch 208, Step:    19755, Batch Loss:     0.061526, Batch Acc: 0.996280, Tokens per Sec:     4006, Lr: 0.000127\r\n",
      "2024-08-27 16:08:29,609 - INFO - joeynmt.training - Epoch 208, Step:    19760, Batch Loss:     0.056420, Batch Acc: 0.994454, Tokens per Sec:     3626, Lr: 0.000127\r\n",
      "2024-08-27 16:08:30,981 - INFO - joeynmt.training - Epoch 208, Step:    19765, Batch Loss:     0.067258, Batch Acc: 0.995508, Tokens per Sec:     2923, Lr: 0.000127\r\n",
      "2024-08-27 16:08:32,080 - INFO - joeynmt.training - Epoch 208, Step:    19770, Batch Loss:     0.070931, Batch Acc: 0.994100, Tokens per Sec:     3859, Lr: 0.000127\r\n",
      "2024-08-27 16:08:33,182 - INFO - joeynmt.training - Epoch 208, Step:    19775, Batch Loss:     0.070587, Batch Acc: 0.992184, Tokens per Sec:     3718, Lr: 0.000127\r\n",
      "2024-08-27 16:08:34,274 - INFO - joeynmt.training - Epoch 208, Step:    19780, Batch Loss:     0.073780, Batch Acc: 0.993056, Tokens per Sec:     3693, Lr: 0.000127\r\n",
      "2024-08-27 16:08:35,361 - INFO - joeynmt.training - Epoch 208, Step:    19785, Batch Loss:     0.071954, Batch Acc: 0.994533, Tokens per Sec:     3874, Lr: 0.000127\r\n",
      "2024-08-27 16:08:36,287 - INFO - joeynmt.training - Epoch 208, total training loss: 6.16, num. of seqs: 8065, num. of tokens: 80463, 21.3672[sec]\r\n",
      "2024-08-27 16:08:36,288 - INFO - joeynmt.training - EPOCH 209\r\n",
      "2024-08-27 16:08:36,512 - INFO - joeynmt.training - Epoch 209, Step:    19790, Batch Loss:     0.062114, Batch Acc: 0.997429, Tokens per Sec:     3536, Lr: 0.000127\r\n",
      "2024-08-27 16:08:37,645 - INFO - joeynmt.training - Epoch 209, Step:    19795, Batch Loss:     0.065519, Batch Acc: 0.994322, Tokens per Sec:     3731, Lr: 0.000127\r\n",
      "2024-08-27 16:08:38,739 - INFO - joeynmt.training - Epoch 209, Step:    19800, Batch Loss:     0.065124, Batch Acc: 0.994463, Tokens per Sec:     3472, Lr: 0.000127\r\n",
      "2024-08-27 16:08:39,849 - INFO - joeynmt.training - Epoch 209, Step:    19805, Batch Loss:     0.060147, Batch Acc: 0.993553, Tokens per Sec:     3914, Lr: 0.000127\r\n",
      "2024-08-27 16:08:40,948 - INFO - joeynmt.training - Epoch 209, Step:    19810, Batch Loss:     0.071671, Batch Acc: 0.991947, Tokens per Sec:     3957, Lr: 0.000127\r\n",
      "2024-08-27 16:08:42,050 - INFO - joeynmt.training - Epoch 209, Step:    19815, Batch Loss:     0.060905, Batch Acc: 0.994345, Tokens per Sec:     3855, Lr: 0.000127\r\n",
      "2024-08-27 16:08:43,143 - INFO - joeynmt.training - Epoch 209, Step:    19820, Batch Loss:     0.063809, Batch Acc: 0.997912, Tokens per Sec:     3945, Lr: 0.000127\r\n",
      "2024-08-27 16:08:44,235 - INFO - joeynmt.training - Epoch 209, Step:    19825, Batch Loss:     0.070298, Batch Acc: 0.995553, Tokens per Sec:     3710, Lr: 0.000127\r\n",
      "2024-08-27 16:08:45,311 - INFO - joeynmt.training - Epoch 209, Step:    19830, Batch Loss:     0.062767, Batch Acc: 0.995875, Tokens per Sec:     3832, Lr: 0.000127\r\n",
      "2024-08-27 16:08:46,404 - INFO - joeynmt.training - Epoch 209, Step:    19835, Batch Loss:     0.063572, Batch Acc: 0.995184, Tokens per Sec:     3803, Lr: 0.000127\r\n",
      "2024-08-27 16:08:47,533 - INFO - joeynmt.training - Epoch 209, Step:    19840, Batch Loss:     0.058057, Batch Acc: 0.994538, Tokens per Sec:     3894, Lr: 0.000127\r\n",
      "2024-08-27 16:08:48,661 - INFO - joeynmt.training - Epoch 209, Step:    19845, Batch Loss:     0.062649, Batch Acc: 0.994425, Tokens per Sec:     3979, Lr: 0.000127\r\n",
      "2024-08-27 16:08:49,734 - INFO - joeynmt.training - Epoch 209, Step:    19850, Batch Loss:     0.057796, Batch Acc: 0.992405, Tokens per Sec:     3683, Lr: 0.000127\r\n",
      "2024-08-27 16:08:50,835 - INFO - joeynmt.training - Epoch 209, Step:    19855, Batch Loss:     0.069001, Batch Acc: 0.994496, Tokens per Sec:     3799, Lr: 0.000127\r\n",
      "2024-08-27 16:08:51,927 - INFO - joeynmt.training - Epoch 209, Step:    19860, Batch Loss:     0.063313, Batch Acc: 0.995168, Tokens per Sec:     3981, Lr: 0.000127\r\n",
      "2024-08-27 16:08:53,008 - INFO - joeynmt.training - Epoch 209, Step:    19865, Batch Loss:     0.062177, Batch Acc: 0.993914, Tokens per Sec:     3952, Lr: 0.000127\r\n",
      "2024-08-27 16:08:54,110 - INFO - joeynmt.training - Epoch 209, Step:    19870, Batch Loss:     0.056078, Batch Acc: 0.994160, Tokens per Sec:     4042, Lr: 0.000127\r\n",
      "2024-08-27 16:08:55,194 - INFO - joeynmt.training - Epoch 209, Step:    19875, Batch Loss:     0.068781, Batch Acc: 0.994423, Tokens per Sec:     3809, Lr: 0.000127\r\n",
      "2024-08-27 16:08:56,283 - INFO - joeynmt.training - Epoch 209, Step:    19880, Batch Loss:     0.073681, Batch Acc: 0.994956, Tokens per Sec:     3823, Lr: 0.000127\r\n",
      "2024-08-27 16:08:57,308 - INFO - joeynmt.training - Epoch 209, total training loss: 6.13, num. of seqs: 8065, num. of tokens: 80463, 21.0041[sec]\r\n",
      "2024-08-27 16:08:57,309 - INFO - joeynmt.training - EPOCH 210\r\n",
      "2024-08-27 16:08:57,540 - INFO - joeynmt.training - Epoch 210, Step:    19885, Batch Loss:     0.057167, Batch Acc: 0.996633, Tokens per Sec:     3926, Lr: 0.000127\r\n",
      "2024-08-27 16:08:58,651 - INFO - joeynmt.training - Epoch 210, Step:    19890, Batch Loss:     0.070965, Batch Acc: 0.994370, Tokens per Sec:     3838, Lr: 0.000127\r\n",
      "2024-08-27 16:08:59,737 - INFO - joeynmt.training - Epoch 210, Step:    19895, Batch Loss:     0.085309, Batch Acc: 0.993368, Tokens per Sec:     3749, Lr: 0.000127\r\n",
      "2024-08-27 16:09:00,876 - INFO - joeynmt.training - Epoch 210, Step:    19900, Batch Loss:     0.057320, Batch Acc: 0.996352, Tokens per Sec:     3612, Lr: 0.000127\r\n",
      "2024-08-27 16:09:02,090 - INFO - joeynmt.training - Epoch 210, Step:    19905, Batch Loss:     0.066849, Batch Acc: 0.995450, Tokens per Sec:     3260, Lr: 0.000127\r\n",
      "2024-08-27 16:09:03,170 - INFO - joeynmt.training - Epoch 210, Step:    19910, Batch Loss:     0.059623, Batch Acc: 0.993654, Tokens per Sec:     3797, Lr: 0.000127\r\n",
      "2024-08-27 16:09:04,249 - INFO - joeynmt.training - Epoch 210, Step:    19915, Batch Loss:     0.073235, Batch Acc: 0.994382, Tokens per Sec:     3796, Lr: 0.000127\r\n",
      "2024-08-27 16:09:05,323 - INFO - joeynmt.training - Epoch 210, Step:    19920, Batch Loss:     0.062707, Batch Acc: 0.993804, Tokens per Sec:     3910, Lr: 0.000127\r\n",
      "2024-08-27 16:09:06,400 - INFO - joeynmt.training - Epoch 210, Step:    19925, Batch Loss:     0.061479, Batch Acc: 0.996970, Tokens per Sec:     3985, Lr: 0.000127\r\n",
      "2024-08-27 16:09:07,506 - INFO - joeynmt.training - Epoch 210, Step:    19930, Batch Loss:     0.060115, Batch Acc: 0.994782, Tokens per Sec:     3814, Lr: 0.000127\r\n",
      "2024-08-27 16:09:08,613 - INFO - joeynmt.training - Epoch 210, Step:    19935, Batch Loss:     0.062127, Batch Acc: 0.996232, Tokens per Sec:     3837, Lr: 0.000127\r\n",
      "2024-08-27 16:09:09,703 - INFO - joeynmt.training - Epoch 210, Step:    19940, Batch Loss:     0.059837, Batch Acc: 0.995140, Tokens per Sec:     3969, Lr: 0.000127\r\n",
      "2024-08-27 16:09:10,792 - INFO - joeynmt.training - Epoch 210, Step:    19945, Batch Loss:     0.057748, Batch Acc: 0.995187, Tokens per Sec:     3818, Lr: 0.000127\r\n",
      "2024-08-27 16:09:11,885 - INFO - joeynmt.training - Epoch 210, Step:    19950, Batch Loss:     0.072999, Batch Acc: 0.993658, Tokens per Sec:     3897, Lr: 0.000127\r\n",
      "2024-08-27 16:09:12,999 - INFO - joeynmt.training - Epoch 210, Step:    19955, Batch Loss:     0.068664, Batch Acc: 0.994553, Tokens per Sec:     3956, Lr: 0.000127\r\n",
      "2024-08-27 16:09:14,089 - INFO - joeynmt.training - Epoch 210, Step:    19960, Batch Loss:     0.062964, Batch Acc: 0.996244, Tokens per Sec:     3910, Lr: 0.000127\r\n",
      "2024-08-27 16:09:15,183 - INFO - joeynmt.training - Epoch 210, Step:    19965, Batch Loss:     0.079814, Batch Acc: 0.992368, Tokens per Sec:     3837, Lr: 0.000127\r\n",
      "2024-08-27 16:09:16,260 - INFO - joeynmt.training - Epoch 210, Step:    19970, Batch Loss:     0.051833, Batch Acc: 0.996549, Tokens per Sec:     3770, Lr: 0.000127\r\n",
      "2024-08-27 16:09:17,391 - INFO - joeynmt.training - Epoch 210, Step:    19975, Batch Loss:     0.073182, Batch Acc: 0.993652, Tokens per Sec:     3763, Lr: 0.000127\r\n",
      "2024-08-27 16:09:18,498 - INFO - joeynmt.training - Epoch 210, Step:    19980, Batch Loss:     0.073652, Batch Acc: 0.995641, Tokens per Sec:     3731, Lr: 0.000127\r\n",
      "2024-08-27 16:09:18,499 - INFO - joeynmt.training - Epoch 210, total training loss: 6.16, num. of seqs: 8065, num. of tokens: 80463, 21.1738[sec]\r\n",
      "2024-08-27 16:09:18,499 - INFO - joeynmt.training - EPOCH 211\r\n",
      "2024-08-27 16:09:19,603 - INFO - joeynmt.training - Epoch 211, Step:    19985, Batch Loss:     0.068875, Batch Acc: 0.995168, Tokens per Sec:     3948, Lr: 0.000127\r\n",
      "2024-08-27 16:09:20,723 - INFO - joeynmt.training - Epoch 211, Step:    19990, Batch Loss:     0.054455, Batch Acc: 0.994049, Tokens per Sec:     3906, Lr: 0.000127\r\n",
      "2024-08-27 16:09:21,828 - INFO - joeynmt.training - Epoch 211, Step:    19995, Batch Loss:     0.060163, Batch Acc: 0.994660, Tokens per Sec:     3899, Lr: 0.000127\r\n",
      "2024-08-27 16:09:22,954 - INFO - joeynmt.training - Epoch 211, Step:    20000, Batch Loss:     0.049288, Batch Acc: 0.996369, Tokens per Sec:     3671, Lr: 0.000126\r\n",
      "2024-08-27 16:09:22,955 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=20042\r\n",
      "2024-08-27 16:09:22,955 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 65.14it/s]\r\n",
      "2024-08-27 16:09:45,372 - INFO - joeynmt.prediction - Generation took 22.4148[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44407.97 examples/s]\r\n",
      "2024-08-27 16:09:45,754 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:09:45,754 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.61, loss:   6.80, ppl: 897.24, acc:   0.28, 0.1691[sec]\r\n",
      "2024-08-27 16:09:46,062 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/20000.ckpt.\r\n",
      "2024-08-27 16:09:46,063 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/14500.ckpt\r\n",
      "2024-08-27 16:09:46,087 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:09:46,235 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:09:46,236 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:09:46,236 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 16:09:46,530 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:09:46,681 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:09:46,682 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:09:46,682 - INFO - joeynmt.training - \tHypothesis: trois tables blanches\r\n",
      "2024-08-27 16:09:46,984 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:09:47,131 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:09:47,131 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:09:47,131 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 16:09:47,423 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:09:47,568 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:09:47,569 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:09:47,569 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 16:09:48,973 - INFO - joeynmt.training - Epoch 211, Step:    20005, Batch Loss:     0.058978, Batch Acc: 0.994743, Tokens per Sec:     3641, Lr: 0.000126\r\n",
      "2024-08-27 16:09:50,078 - INFO - joeynmt.training - Epoch 211, Step:    20010, Batch Loss:     0.072513, Batch Acc: 0.995537, Tokens per Sec:     3855, Lr: 0.000126\r\n",
      "2024-08-27 16:09:51,183 - INFO - joeynmt.training - Epoch 211, Step:    20015, Batch Loss:     0.065771, Batch Acc: 0.994872, Tokens per Sec:     3887, Lr: 0.000126\r\n",
      "2024-08-27 16:09:52,288 - INFO - joeynmt.training - Epoch 211, Step:    20020, Batch Loss:     0.065411, Batch Acc: 0.994530, Tokens per Sec:     3808, Lr: 0.000126\r\n",
      "2024-08-27 16:09:53,410 - INFO - joeynmt.training - Epoch 211, Step:    20025, Batch Loss:     0.065033, Batch Acc: 0.994611, Tokens per Sec:     3807, Lr: 0.000126\r\n",
      "2024-08-27 16:09:54,518 - INFO - joeynmt.training - Epoch 211, Step:    20030, Batch Loss:     0.058125, Batch Acc: 0.995602, Tokens per Sec:     3694, Lr: 0.000126\r\n",
      "2024-08-27 16:09:55,625 - INFO - joeynmt.training - Epoch 211, Step:    20035, Batch Loss:     0.062571, Batch Acc: 0.994115, Tokens per Sec:     3839, Lr: 0.000126\r\n",
      "2024-08-27 16:09:56,783 - INFO - joeynmt.training - Epoch 211, Step:    20040, Batch Loss:     0.064114, Batch Acc: 0.993937, Tokens per Sec:     3705, Lr: 0.000126\r\n",
      "2024-08-27 16:09:57,891 - INFO - joeynmt.training - Epoch 211, Step:    20045, Batch Loss:     0.066544, Batch Acc: 0.995839, Tokens per Sec:     3692, Lr: 0.000126\r\n",
      "2024-08-27 16:09:58,981 - INFO - joeynmt.training - Epoch 211, Step:    20050, Batch Loss:     0.071102, Batch Acc: 0.994498, Tokens per Sec:     3835, Lr: 0.000126\r\n",
      "2024-08-27 16:10:00,094 - INFO - joeynmt.training - Epoch 211, Step:    20055, Batch Loss:     0.069172, Batch Acc: 0.993931, Tokens per Sec:     4000, Lr: 0.000126\r\n",
      "2024-08-27 16:10:01,213 - INFO - joeynmt.training - Epoch 211, Step:    20060, Batch Loss:     0.060469, Batch Acc: 0.995066, Tokens per Sec:     3989, Lr: 0.000126\r\n",
      "2024-08-27 16:10:02,302 - INFO - joeynmt.training - Epoch 211, Step:    20065, Batch Loss:     0.059458, Batch Acc: 0.995129, Tokens per Sec:     3961, Lr: 0.000126\r\n",
      "2024-08-27 16:10:03,406 - INFO - joeynmt.training - Epoch 211, Step:    20070, Batch Loss:     0.061385, Batch Acc: 0.995487, Tokens per Sec:     3817, Lr: 0.000126\r\n",
      "2024-08-27 16:10:04,831 - INFO - joeynmt.training - Epoch 211, Step:    20075, Batch Loss:     0.073369, Batch Acc: 0.994208, Tokens per Sec:     2788, Lr: 0.000126\r\n",
      "2024-08-27 16:10:04,831 - INFO - joeynmt.training - Epoch 211, total training loss: 6.05, num. of seqs: 8065, num. of tokens: 80463, 21.3942[sec]\r\n",
      "2024-08-27 16:10:04,832 - INFO - joeynmt.training - EPOCH 212\r\n",
      "2024-08-27 16:10:05,926 - INFO - joeynmt.training - Epoch 212, Step:    20080, Batch Loss:     0.069718, Batch Acc: 0.993966, Tokens per Sec:     3796, Lr: 0.000126\r\n",
      "2024-08-27 16:10:07,051 - INFO - joeynmt.training - Epoch 212, Step:    20085, Batch Loss:     0.065612, Batch Acc: 0.995557, Tokens per Sec:     4006, Lr: 0.000126\r\n",
      "2024-08-27 16:10:08,142 - INFO - joeynmt.training - Epoch 212, Step:    20090, Batch Loss:     0.061135, Batch Acc: 0.996215, Tokens per Sec:     3874, Lr: 0.000126\r\n",
      "2024-08-27 16:10:09,217 - INFO - joeynmt.training - Epoch 212, Step:    20095, Batch Loss:     0.056706, Batch Acc: 0.995540, Tokens per Sec:     3757, Lr: 0.000126\r\n",
      "2024-08-27 16:10:10,311 - INFO - joeynmt.training - Epoch 212, Step:    20100, Batch Loss:     0.061590, Batch Acc: 0.996740, Tokens per Sec:     3927, Lr: 0.000126\r\n",
      "2024-08-27 16:10:11,412 - INFO - joeynmt.training - Epoch 212, Step:    20105, Batch Loss:     0.060789, Batch Acc: 0.996880, Tokens per Sec:     3790, Lr: 0.000126\r\n",
      "2024-08-27 16:10:12,501 - INFO - joeynmt.training - Epoch 212, Step:    20110, Batch Loss:     0.058328, Batch Acc: 0.995191, Tokens per Sec:     3820, Lr: 0.000126\r\n",
      "2024-08-27 16:10:13,608 - INFO - joeynmt.training - Epoch 212, Step:    20115, Batch Loss:     0.056018, Batch Acc: 0.996107, Tokens per Sec:     3949, Lr: 0.000126\r\n",
      "2024-08-27 16:10:14,718 - INFO - joeynmt.training - Epoch 212, Step:    20120, Batch Loss:     0.064903, Batch Acc: 0.992272, Tokens per Sec:     3849, Lr: 0.000126\r\n",
      "2024-08-27 16:10:15,810 - INFO - joeynmt.training - Epoch 212, Step:    20125, Batch Loss:     0.055242, Batch Acc: 0.995405, Tokens per Sec:     3788, Lr: 0.000126\r\n",
      "2024-08-27 16:10:16,932 - INFO - joeynmt.training - Epoch 212, Step:    20130, Batch Loss:     0.063048, Batch Acc: 0.994267, Tokens per Sec:     3889, Lr: 0.000126\r\n",
      "2024-08-27 16:10:18,030 - INFO - joeynmt.training - Epoch 212, Step:    20135, Batch Loss:     0.061307, Batch Acc: 0.994271, Tokens per Sec:     3818, Lr: 0.000126\r\n",
      "2024-08-27 16:10:19,121 - INFO - joeynmt.training - Epoch 212, Step:    20140, Batch Loss:     0.057439, Batch Acc: 0.995886, Tokens per Sec:     3788, Lr: 0.000126\r\n",
      "2024-08-27 16:10:20,209 - INFO - joeynmt.training - Epoch 212, Step:    20145, Batch Loss:     0.061223, Batch Acc: 0.994550, Tokens per Sec:     4052, Lr: 0.000126\r\n",
      "2024-08-27 16:10:21,326 - INFO - joeynmt.training - Epoch 212, Step:    20150, Batch Loss:     0.057999, Batch Acc: 0.994726, Tokens per Sec:     3906, Lr: 0.000126\r\n",
      "2024-08-27 16:10:22,452 - INFO - joeynmt.training - Epoch 212, Step:    20155, Batch Loss:     0.058766, Batch Acc: 0.995729, Tokens per Sec:     3954, Lr: 0.000126\r\n",
      "2024-08-27 16:10:23,559 - INFO - joeynmt.training - Epoch 212, Step:    20160, Batch Loss:     0.066735, Batch Acc: 0.993898, Tokens per Sec:     3850, Lr: 0.000126\r\n",
      "2024-08-27 16:10:24,680 - INFO - joeynmt.training - Epoch 212, Step:    20165, Batch Loss:     0.072023, Batch Acc: 0.994801, Tokens per Sec:     3948, Lr: 0.000126\r\n",
      "2024-08-27 16:10:25,662 - INFO - joeynmt.training - Epoch 212, total training loss: 5.93, num. of seqs: 8065, num. of tokens: 80463, 20.8155[sec]\r\n",
      "2024-08-27 16:10:25,662 - INFO - joeynmt.training - EPOCH 213\r\n",
      "2024-08-27 16:10:25,889 - INFO - joeynmt.training - Epoch 213, Step:    20170, Batch Loss:     0.063338, Batch Acc: 0.996433, Tokens per Sec:     3799, Lr: 0.000126\r\n",
      "2024-08-27 16:10:27,002 - INFO - joeynmt.training - Epoch 213, Step:    20175, Batch Loss:     0.056058, Batch Acc: 0.995486, Tokens per Sec:     3783, Lr: 0.000126\r\n",
      "2024-08-27 16:10:28,086 - INFO - joeynmt.training - Epoch 213, Step:    20180, Batch Loss:     0.057151, Batch Acc: 0.996308, Tokens per Sec:     4000, Lr: 0.000126\r\n",
      "2024-08-27 16:10:29,176 - INFO - joeynmt.training - Epoch 213, Step:    20185, Batch Loss:     0.073919, Batch Acc: 0.994352, Tokens per Sec:     3737, Lr: 0.000126\r\n",
      "2024-08-27 16:10:30,273 - INFO - joeynmt.training - Epoch 213, Step:    20190, Batch Loss:     0.069690, Batch Acc: 0.993472, Tokens per Sec:     3913, Lr: 0.000126\r\n",
      "2024-08-27 16:10:31,359 - INFO - joeynmt.training - Epoch 213, Step:    20195, Batch Loss:     0.056611, Batch Acc: 0.995731, Tokens per Sec:     3884, Lr: 0.000126\r\n",
      "2024-08-27 16:10:32,446 - INFO - joeynmt.training - Epoch 213, Step:    20200, Batch Loss:     0.067091, Batch Acc: 0.994261, Tokens per Sec:     4011, Lr: 0.000126\r\n",
      "2024-08-27 16:10:33,511 - INFO - joeynmt.training - Epoch 213, Step:    20205, Batch Loss:     0.059419, Batch Acc: 0.995714, Tokens per Sec:     3726, Lr: 0.000126\r\n",
      "2024-08-27 16:10:34,593 - INFO - joeynmt.training - Epoch 213, Step:    20210, Batch Loss:     0.072990, Batch Acc: 0.994435, Tokens per Sec:     3822, Lr: 0.000126\r\n",
      "2024-08-27 16:10:35,803 - INFO - joeynmt.training - Epoch 213, Step:    20215, Batch Loss:     0.059743, Batch Acc: 0.996199, Tokens per Sec:     3481, Lr: 0.000126\r\n",
      "2024-08-27 16:10:36,938 - INFO - joeynmt.training - Epoch 213, Step:    20220, Batch Loss:     0.076512, Batch Acc: 0.992959, Tokens per Sec:     3885, Lr: 0.000126\r\n",
      "2024-08-27 16:10:38,050 - INFO - joeynmt.training - Epoch 213, Step:    20225, Batch Loss:     0.069394, Batch Acc: 0.994208, Tokens per Sec:     3729, Lr: 0.000126\r\n",
      "2024-08-27 16:10:39,156 - INFO - joeynmt.training - Epoch 213, Step:    20230, Batch Loss:     0.065472, Batch Acc: 0.995959, Tokens per Sec:     3804, Lr: 0.000126\r\n",
      "2024-08-27 16:10:40,260 - INFO - joeynmt.training - Epoch 213, Step:    20235, Batch Loss:     0.064631, Batch Acc: 0.995021, Tokens per Sec:     3825, Lr: 0.000126\r\n",
      "2024-08-27 16:10:41,333 - INFO - joeynmt.training - Epoch 213, Step:    20240, Batch Loss:     0.057533, Batch Acc: 0.997727, Tokens per Sec:     3693, Lr: 0.000126\r\n",
      "2024-08-27 16:10:42,466 - INFO - joeynmt.training - Epoch 213, Step:    20245, Batch Loss:     0.056913, Batch Acc: 0.995475, Tokens per Sec:     3904, Lr: 0.000126\r\n",
      "2024-08-27 16:10:43,577 - INFO - joeynmt.training - Epoch 213, Step:    20250, Batch Loss:     0.075229, Batch Acc: 0.992946, Tokens per Sec:     3831, Lr: 0.000126\r\n",
      "2024-08-27 16:10:44,663 - INFO - joeynmt.training - Epoch 213, Step:    20255, Batch Loss:     0.059796, Batch Acc: 0.993988, Tokens per Sec:     3983, Lr: 0.000126\r\n",
      "2024-08-27 16:10:45,776 - INFO - joeynmt.training - Epoch 213, Step:    20260, Batch Loss:     0.064710, Batch Acc: 0.994696, Tokens per Sec:     3899, Lr: 0.000126\r\n",
      "2024-08-27 16:10:46,778 - INFO - joeynmt.training - Epoch 213, total training loss: 6.06, num. of seqs: 8065, num. of tokens: 80463, 21.0972[sec]\r\n",
      "2024-08-27 16:10:46,778 - INFO - joeynmt.training - EPOCH 214\r\n",
      "2024-08-27 16:10:47,000 - INFO - joeynmt.training - Epoch 214, Step:    20265, Batch Loss:     0.065767, Batch Acc: 0.988251, Tokens per Sec:     3527, Lr: 0.000126\r\n",
      "2024-08-27 16:10:48,074 - INFO - joeynmt.training - Epoch 214, Step:    20270, Batch Loss:     0.054463, Batch Acc: 0.996861, Tokens per Sec:     3858, Lr: 0.000126\r\n",
      "2024-08-27 16:10:49,160 - INFO - joeynmt.training - Epoch 214, Step:    20275, Batch Loss:     0.063693, Batch Acc: 0.994220, Tokens per Sec:     3825, Lr: 0.000126\r\n",
      "2024-08-27 16:10:50,245 - INFO - joeynmt.training - Epoch 214, Step:    20280, Batch Loss:     0.055959, Batch Acc: 0.996376, Tokens per Sec:     3817, Lr: 0.000126\r\n",
      "2024-08-27 16:10:51,354 - INFO - joeynmt.training - Epoch 214, Step:    20285, Batch Loss:     0.066626, Batch Acc: 0.995082, Tokens per Sec:     3854, Lr: 0.000126\r\n",
      "2024-08-27 16:10:52,453 - INFO - joeynmt.training - Epoch 214, Step:    20290, Batch Loss:     0.059673, Batch Acc: 0.996471, Tokens per Sec:     3871, Lr: 0.000126\r\n",
      "2024-08-27 16:10:53,550 - INFO - joeynmt.training - Epoch 214, Step:    20295, Batch Loss:     0.056580, Batch Acc: 0.995364, Tokens per Sec:     3935, Lr: 0.000126\r\n",
      "2024-08-27 16:10:54,637 - INFO - joeynmt.training - Epoch 214, Step:    20300, Batch Loss:     0.071651, Batch Acc: 0.993206, Tokens per Sec:     4199, Lr: 0.000126\r\n",
      "2024-08-27 16:10:55,715 - INFO - joeynmt.training - Epoch 214, Step:    20305, Batch Loss:     0.055590, Batch Acc: 0.995023, Tokens per Sec:     3919, Lr: 0.000126\r\n",
      "2024-08-27 16:10:56,875 - INFO - joeynmt.training - Epoch 214, Step:    20310, Batch Loss:     0.073585, Batch Acc: 0.995064, Tokens per Sec:     3843, Lr: 0.000126\r\n",
      "2024-08-27 16:10:57,983 - INFO - joeynmt.training - Epoch 214, Step:    20315, Batch Loss:     0.068144, Batch Acc: 0.995679, Tokens per Sec:     3763, Lr: 0.000126\r\n",
      "2024-08-27 16:10:59,099 - INFO - joeynmt.training - Epoch 214, Step:    20320, Batch Loss:     0.052981, Batch Acc: 0.996934, Tokens per Sec:     3802, Lr: 0.000125\r\n",
      "2024-08-27 16:11:00,187 - INFO - joeynmt.training - Epoch 214, Step:    20325, Batch Loss:     0.060042, Batch Acc: 0.997453, Tokens per Sec:     3609, Lr: 0.000125\r\n",
      "2024-08-27 16:11:01,298 - INFO - joeynmt.training - Epoch 214, Step:    20330, Batch Loss:     0.061547, Batch Acc: 0.993373, Tokens per Sec:     3943, Lr: 0.000125\r\n",
      "2024-08-27 16:11:02,405 - INFO - joeynmt.training - Epoch 214, Step:    20335, Batch Loss:     0.055927, Batch Acc: 0.997464, Tokens per Sec:     3921, Lr: 0.000125\r\n",
      "2024-08-27 16:11:03,494 - INFO - joeynmt.training - Epoch 214, Step:    20340, Batch Loss:     0.075467, Batch Acc: 0.993171, Tokens per Sec:     3768, Lr: 0.000125\r\n",
      "2024-08-27 16:11:04,576 - INFO - joeynmt.training - Epoch 214, Step:    20345, Batch Loss:     0.057100, Batch Acc: 0.994152, Tokens per Sec:     3794, Lr: 0.000125\r\n",
      "2024-08-27 16:11:05,658 - INFO - joeynmt.training - Epoch 214, Step:    20350, Batch Loss:     0.065239, Batch Acc: 0.995183, Tokens per Sec:     3647, Lr: 0.000125\r\n",
      "2024-08-27 16:11:07,063 - INFO - joeynmt.training - Epoch 214, Step:    20355, Batch Loss:     0.068531, Batch Acc: 0.994235, Tokens per Sec:     2965, Lr: 0.000125\r\n",
      "2024-08-27 16:11:08,179 - INFO - joeynmt.training - Epoch 214, Step:    20360, Batch Loss:     0.055415, Batch Acc: 0.996088, Tokens per Sec:     3440, Lr: 0.000125\r\n",
      "2024-08-27 16:11:08,179 - INFO - joeynmt.training - Epoch 214, total training loss: 5.98, num. of seqs: 8065, num. of tokens: 80463, 21.3825[sec]\r\n",
      "2024-08-27 16:11:08,179 - INFO - joeynmt.training - EPOCH 215\r\n",
      "2024-08-27 16:11:09,263 - INFO - joeynmt.training - Epoch 215, Step:    20365, Batch Loss:     0.052975, Batch Acc: 0.995131, Tokens per Sec:     3803, Lr: 0.000125\r\n",
      "2024-08-27 16:11:10,353 - INFO - joeynmt.training - Epoch 215, Step:    20370, Batch Loss:     0.061725, Batch Acc: 0.995422, Tokens per Sec:     4009, Lr: 0.000125\r\n",
      "2024-08-27 16:11:11,446 - INFO - joeynmt.training - Epoch 215, Step:    20375, Batch Loss:     0.066613, Batch Acc: 0.995246, Tokens per Sec:     3855, Lr: 0.000125\r\n",
      "2024-08-27 16:11:12,508 - INFO - joeynmt.training - Epoch 215, Step:    20380, Batch Loss:     0.054409, Batch Acc: 0.996441, Tokens per Sec:     3969, Lr: 0.000125\r\n",
      "2024-08-27 16:11:13,603 - INFO - joeynmt.training - Epoch 215, Step:    20385, Batch Loss:     0.056637, Batch Acc: 0.997198, Tokens per Sec:     3916, Lr: 0.000125\r\n",
      "2024-08-27 16:11:14,691 - INFO - joeynmt.training - Epoch 215, Step:    20390, Batch Loss:     0.064685, Batch Acc: 0.995560, Tokens per Sec:     4141, Lr: 0.000125\r\n",
      "2024-08-27 16:11:15,796 - INFO - joeynmt.training - Epoch 215, Step:    20395, Batch Loss:     0.060080, Batch Acc: 0.995937, Tokens per Sec:     3788, Lr: 0.000125\r\n",
      "2024-08-27 16:11:16,940 - INFO - joeynmt.training - Epoch 215, Step:    20400, Batch Loss:     0.070373, Batch Acc: 0.993887, Tokens per Sec:     3721, Lr: 0.000125\r\n",
      "2024-08-27 16:11:18,023 - INFO - joeynmt.training - Epoch 215, Step:    20405, Batch Loss:     0.054454, Batch Acc: 0.995369, Tokens per Sec:     3791, Lr: 0.000125\r\n",
      "2024-08-27 16:11:19,104 - INFO - joeynmt.training - Epoch 215, Step:    20410, Batch Loss:     0.060852, Batch Acc: 0.995401, Tokens per Sec:     3625, Lr: 0.000125\r\n",
      "2024-08-27 16:11:20,204 - INFO - joeynmt.training - Epoch 215, Step:    20415, Batch Loss:     0.054637, Batch Acc: 0.997400, Tokens per Sec:     3849, Lr: 0.000125\r\n",
      "2024-08-27 16:11:21,325 - INFO - joeynmt.training - Epoch 215, Step:    20420, Batch Loss:     0.058222, Batch Acc: 0.995313, Tokens per Sec:     3999, Lr: 0.000125\r\n",
      "2024-08-27 16:11:22,445 - INFO - joeynmt.training - Epoch 215, Step:    20425, Batch Loss:     0.067444, Batch Acc: 0.993735, Tokens per Sec:     3850, Lr: 0.000125\r\n",
      "2024-08-27 16:11:23,553 - INFO - joeynmt.training - Epoch 215, Step:    20430, Batch Loss:     0.071047, Batch Acc: 0.993000, Tokens per Sec:     3742, Lr: 0.000125\r\n",
      "2024-08-27 16:11:24,666 - INFO - joeynmt.training - Epoch 215, Step:    20435, Batch Loss:     0.063178, Batch Acc: 0.995352, Tokens per Sec:     3674, Lr: 0.000125\r\n",
      "2024-08-27 16:11:25,769 - INFO - joeynmt.training - Epoch 215, Step:    20440, Batch Loss:     0.057445, Batch Acc: 0.995208, Tokens per Sec:     3788, Lr: 0.000125\r\n",
      "2024-08-27 16:11:26,911 - INFO - joeynmt.training - Epoch 215, Step:    20445, Batch Loss:     0.061168, Batch Acc: 0.994593, Tokens per Sec:     3888, Lr: 0.000125\r\n",
      "2024-08-27 16:11:28,011 - INFO - joeynmt.training - Epoch 215, Step:    20450, Batch Loss:     0.061246, Batch Acc: 0.994542, Tokens per Sec:     4000, Lr: 0.000125\r\n",
      "2024-08-27 16:11:29,106 - INFO - joeynmt.training - Epoch 215, Step:    20455, Batch Loss:     0.064071, Batch Acc: 0.994089, Tokens per Sec:     3712, Lr: 0.000125\r\n",
      "2024-08-27 16:11:29,106 - INFO - joeynmt.training - Epoch 215, total training loss: 5.83, num. of seqs: 8065, num. of tokens: 80463, 20.9098[sec]\r\n",
      "2024-08-27 16:11:29,106 - INFO - joeynmt.training - EPOCH 216\r\n",
      "2024-08-27 16:11:30,246 - INFO - joeynmt.training - Epoch 216, Step:    20460, Batch Loss:     0.054741, Batch Acc: 0.994168, Tokens per Sec:     3773, Lr: 0.000125\r\n",
      "2024-08-27 16:11:31,341 - INFO - joeynmt.training - Epoch 216, Step:    20465, Batch Loss:     0.063130, Batch Acc: 0.996627, Tokens per Sec:     3795, Lr: 0.000125\r\n",
      "2024-08-27 16:11:32,445 - INFO - joeynmt.training - Epoch 216, Step:    20470, Batch Loss:     0.053046, Batch Acc: 0.997148, Tokens per Sec:     3810, Lr: 0.000125\r\n",
      "2024-08-27 16:11:33,557 - INFO - joeynmt.training - Epoch 216, Step:    20475, Batch Loss:     0.057246, Batch Acc: 0.995814, Tokens per Sec:     3872, Lr: 0.000125\r\n",
      "2024-08-27 16:11:34,654 - INFO - joeynmt.training - Epoch 216, Step:    20480, Batch Loss:     0.054585, Batch Acc: 0.996739, Tokens per Sec:     3916, Lr: 0.000125\r\n",
      "2024-08-27 16:11:35,739 - INFO - joeynmt.training - Epoch 216, Step:    20485, Batch Loss:     0.061388, Batch Acc: 0.995980, Tokens per Sec:     3898, Lr: 0.000125\r\n",
      "2024-08-27 16:11:36,857 - INFO - joeynmt.training - Epoch 216, Step:    20490, Batch Loss:     0.056842, Batch Acc: 0.993207, Tokens per Sec:     3955, Lr: 0.000125\r\n",
      "2024-08-27 16:11:38,122 - INFO - joeynmt.training - Epoch 216, Step:    20495, Batch Loss:     0.080851, Batch Acc: 0.993738, Tokens per Sec:     3282, Lr: 0.000125\r\n",
      "2024-08-27 16:11:39,331 - INFO - joeynmt.training - Epoch 216, Step:    20500, Batch Loss:     0.059261, Batch Acc: 0.995296, Tokens per Sec:     3520, Lr: 0.000125\r\n",
      "2024-08-27 16:11:39,332 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=20542\r\n",
      "2024-08-27 16:11:39,332 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 65.70it/s]\r\n",
      "2024-08-27 16:12:01,556 - INFO - joeynmt.prediction - Generation took 22.2221[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44756.20 examples/s]\r\n",
      "2024-08-27 16:12:01,939 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:12:01,939 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.30, loss:   6.76, ppl: 866.31, acc:   0.29, 0.1687[sec]\r\n",
      "2024-08-27 16:12:01,941 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:12:02,089 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:12:02,090 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:12:02,090 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 16:12:02,382 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:12:02,528 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:12:02,528 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:12:02,528 - INFO - joeynmt.training - \tHypothesis: trois tables blanches\r\n",
      "2024-08-27 16:12:02,821 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:12:02,966 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:12:02,966 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:12:02,966 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 16:12:03,259 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:12:03,406 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:12:03,406 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:12:03,406 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 16:12:04,833 - INFO - joeynmt.training - Epoch 216, Step:    20505, Batch Loss:     0.061209, Batch Acc: 0.994494, Tokens per Sec:     3683, Lr: 0.000125\r\n",
      "2024-08-27 16:12:05,941 - INFO - joeynmt.training - Epoch 216, Step:    20510, Batch Loss:     0.058117, Batch Acc: 0.995778, Tokens per Sec:     3637, Lr: 0.000125\r\n",
      "2024-08-27 16:12:07,069 - INFO - joeynmt.training - Epoch 216, Step:    20515, Batch Loss:     0.055682, Batch Acc: 0.995334, Tokens per Sec:     3800, Lr: 0.000125\r\n",
      "2024-08-27 16:12:08,153 - INFO - joeynmt.training - Epoch 216, Step:    20520, Batch Loss:     0.057126, Batch Acc: 0.995241, Tokens per Sec:     4074, Lr: 0.000125\r\n",
      "2024-08-27 16:12:09,366 - INFO - joeynmt.training - Epoch 216, Step:    20525, Batch Loss:     0.053299, Batch Acc: 0.995440, Tokens per Sec:     3437, Lr: 0.000125\r\n",
      "2024-08-27 16:12:10,454 - INFO - joeynmt.training - Epoch 216, Step:    20530, Batch Loss:     0.063305, Batch Acc: 0.994961, Tokens per Sec:     3653, Lr: 0.000125\r\n",
      "2024-08-27 16:12:11,535 - INFO - joeynmt.training - Epoch 216, Step:    20535, Batch Loss:     0.057423, Batch Acc: 0.993611, Tokens per Sec:     3911, Lr: 0.000125\r\n",
      "2024-08-27 16:12:12,630 - INFO - joeynmt.training - Epoch 216, Step:    20540, Batch Loss:     0.064542, Batch Acc: 0.995374, Tokens per Sec:     3950, Lr: 0.000125\r\n",
      "2024-08-27 16:12:13,718 - INFO - joeynmt.training - Epoch 216, Step:    20545, Batch Loss:     0.071612, Batch Acc: 0.995486, Tokens per Sec:     3872, Lr: 0.000125\r\n",
      "2024-08-27 16:12:14,804 - INFO - joeynmt.training - Epoch 216, Step:    20550, Batch Loss:     0.065332, Batch Acc: 0.992694, Tokens per Sec:     4034, Lr: 0.000125\r\n",
      "2024-08-27 16:12:14,805 - INFO - joeynmt.training - Epoch 216, total training loss: 5.93, num. of seqs: 8065, num. of tokens: 80463, 21.3144[sec]\r\n",
      "2024-08-27 16:12:14,805 - INFO - joeynmt.training - EPOCH 217\r\n",
      "2024-08-27 16:12:15,910 - INFO - joeynmt.training - Epoch 217, Step:    20555, Batch Loss:     0.061091, Batch Acc: 0.995576, Tokens per Sec:     3902, Lr: 0.000125\r\n",
      "2024-08-27 16:12:17,023 - INFO - joeynmt.training - Epoch 217, Step:    20560, Batch Loss:     0.061312, Batch Acc: 0.994162, Tokens per Sec:     3540, Lr: 0.000125\r\n",
      "2024-08-27 16:12:18,113 - INFO - joeynmt.training - Epoch 217, Step:    20565, Batch Loss:     0.061435, Batch Acc: 0.994037, Tokens per Sec:     3696, Lr: 0.000125\r\n",
      "2024-08-27 16:12:19,206 - INFO - joeynmt.training - Epoch 217, Step:    20570, Batch Loss:     0.061869, Batch Acc: 0.995398, Tokens per Sec:     3779, Lr: 0.000125\r\n",
      "2024-08-27 16:12:20,303 - INFO - joeynmt.training - Epoch 217, Step:    20575, Batch Loss:     0.061483, Batch Acc: 0.995219, Tokens per Sec:     3816, Lr: 0.000125\r\n",
      "2024-08-27 16:12:21,397 - INFO - joeynmt.training - Epoch 217, Step:    20580, Batch Loss:     0.057808, Batch Acc: 0.996196, Tokens per Sec:     3847, Lr: 0.000125\r\n",
      "2024-08-27 16:12:22,527 - INFO - joeynmt.training - Epoch 217, Step:    20585, Batch Loss:     0.063363, Batch Acc: 0.996138, Tokens per Sec:     3897, Lr: 0.000125\r\n",
      "2024-08-27 16:12:23,633 - INFO - joeynmt.training - Epoch 217, Step:    20590, Batch Loss:     0.063064, Batch Acc: 0.994387, Tokens per Sec:     3872, Lr: 0.000125\r\n",
      "2024-08-27 16:12:24,727 - INFO - joeynmt.training - Epoch 217, Step:    20595, Batch Loss:     0.054009, Batch Acc: 0.994242, Tokens per Sec:     3969, Lr: 0.000125\r\n",
      "2024-08-27 16:12:25,806 - INFO - joeynmt.training - Epoch 217, Step:    20600, Batch Loss:     0.064644, Batch Acc: 0.994725, Tokens per Sec:     3868, Lr: 0.000125\r\n",
      "2024-08-27 16:12:26,921 - INFO - joeynmt.training - Epoch 217, Step:    20605, Batch Loss:     0.055333, Batch Acc: 0.995796, Tokens per Sec:     3845, Lr: 0.000125\r\n",
      "2024-08-27 16:12:28,016 - INFO - joeynmt.training - Epoch 217, Step:    20610, Batch Loss:     0.068766, Batch Acc: 0.994863, Tokens per Sec:     3733, Lr: 0.000125\r\n",
      "2024-08-27 16:12:29,117 - INFO - joeynmt.training - Epoch 217, Step:    20615, Batch Loss:     0.062073, Batch Acc: 0.995803, Tokens per Sec:     3901, Lr: 0.000125\r\n",
      "2024-08-27 16:12:30,211 - INFO - joeynmt.training - Epoch 217, Step:    20620, Batch Loss:     0.074558, Batch Acc: 0.995976, Tokens per Sec:     3634, Lr: 0.000125\r\n",
      "2024-08-27 16:12:31,292 - INFO - joeynmt.training - Epoch 217, Step:    20625, Batch Loss:     0.063475, Batch Acc: 0.994587, Tokens per Sec:     3764, Lr: 0.000125\r\n",
      "2024-08-27 16:12:32,385 - INFO - joeynmt.training - Epoch 217, Step:    20630, Batch Loss:     0.057765, Batch Acc: 0.994542, Tokens per Sec:     3856, Lr: 0.000125\r\n",
      "2024-08-27 16:12:33,466 - INFO - joeynmt.training - Epoch 217, Step:    20635, Batch Loss:     0.066573, Batch Acc: 0.993057, Tokens per Sec:     3870, Lr: 0.000125\r\n",
      "2024-08-27 16:12:34,561 - INFO - joeynmt.training - Epoch 217, Step:    20640, Batch Loss:     0.062467, Batch Acc: 0.994713, Tokens per Sec:     3974, Lr: 0.000125\r\n",
      "2024-08-27 16:12:35,641 - INFO - joeynmt.training - Epoch 217, Step:    20645, Batch Loss:     0.063482, Batch Acc: 0.995422, Tokens per Sec:     3846, Lr: 0.000124\r\n",
      "2024-08-27 16:12:35,908 - INFO - joeynmt.training - Epoch 217, total training loss: 5.99, num. of seqs: 8065, num. of tokens: 80463, 21.0856[sec]\r\n",
      "2024-08-27 16:12:35,908 - INFO - joeynmt.training - EPOCH 218\r\n",
      "2024-08-27 16:12:36,821 - INFO - joeynmt.training - Epoch 218, Step:    20650, Batch Loss:     0.055853, Batch Acc: 0.996744, Tokens per Sec:     3715, Lr: 0.000124\r\n",
      "2024-08-27 16:12:37,909 - INFO - joeynmt.training - Epoch 218, Step:    20655, Batch Loss:     0.057128, Batch Acc: 0.996503, Tokens per Sec:     3945, Lr: 0.000124\r\n",
      "2024-08-27 16:12:38,993 - INFO - joeynmt.training - Epoch 218, Step:    20660, Batch Loss:     0.055372, Batch Acc: 0.996945, Tokens per Sec:     3930, Lr: 0.000124\r\n",
      "2024-08-27 16:12:40,128 - INFO - joeynmt.training - Epoch 218, Step:    20665, Batch Loss:     0.057515, Batch Acc: 0.995174, Tokens per Sec:     3656, Lr: 0.000124\r\n",
      "2024-08-27 16:12:41,281 - INFO - joeynmt.training - Epoch 218, Step:    20670, Batch Loss:     0.060413, Batch Acc: 0.994146, Tokens per Sec:     3556, Lr: 0.000124\r\n",
      "2024-08-27 16:12:42,369 - INFO - joeynmt.training - Epoch 218, Step:    20675, Batch Loss:     0.062747, Batch Acc: 0.996633, Tokens per Sec:     4097, Lr: 0.000124\r\n",
      "2024-08-27 16:12:43,466 - INFO - joeynmt.training - Epoch 218, Step:    20680, Batch Loss:     0.056301, Batch Acc: 0.994960, Tokens per Sec:     3982, Lr: 0.000124\r\n",
      "2024-08-27 16:12:44,577 - INFO - joeynmt.training - Epoch 218, Step:    20685, Batch Loss:     0.060607, Batch Acc: 0.995164, Tokens per Sec:     3912, Lr: 0.000124\r\n",
      "2024-08-27 16:12:45,702 - INFO - joeynmt.training - Epoch 218, Step:    20690, Batch Loss:     0.067266, Batch Acc: 0.994378, Tokens per Sec:     3955, Lr: 0.000124\r\n",
      "2024-08-27 16:12:46,857 - INFO - joeynmt.training - Epoch 218, Step:    20695, Batch Loss:     0.060924, Batch Acc: 0.995345, Tokens per Sec:     3720, Lr: 0.000124\r\n",
      "2024-08-27 16:12:47,977 - INFO - joeynmt.training - Epoch 218, Step:    20700, Batch Loss:     0.058695, Batch Acc: 0.995819, Tokens per Sec:     3845, Lr: 0.000124\r\n",
      "2024-08-27 16:12:49,093 - INFO - joeynmt.training - Epoch 218, Step:    20705, Batch Loss:     0.085677, Batch Acc: 0.994042, Tokens per Sec:     3765, Lr: 0.000124\r\n",
      "2024-08-27 16:12:50,190 - INFO - joeynmt.training - Epoch 218, Step:    20710, Batch Loss:     0.064761, Batch Acc: 0.996325, Tokens per Sec:     3722, Lr: 0.000124\r\n",
      "2024-08-27 16:12:51,284 - INFO - joeynmt.training - Epoch 218, Step:    20715, Batch Loss:     0.058132, Batch Acc: 0.993010, Tokens per Sec:     3663, Lr: 0.000124\r\n",
      "2024-08-27 16:12:52,382 - INFO - joeynmt.training - Epoch 218, Step:    20720, Batch Loss:     0.061542, Batch Acc: 0.994185, Tokens per Sec:     3920, Lr: 0.000124\r\n",
      "2024-08-27 16:12:53,461 - INFO - joeynmt.training - Epoch 218, Step:    20725, Batch Loss:     0.068717, Batch Acc: 0.995054, Tokens per Sec:     3934, Lr: 0.000124\r\n",
      "2024-08-27 16:12:54,520 - INFO - joeynmt.training - Epoch 218, Step:    20730, Batch Loss:     0.058179, Batch Acc: 0.995420, Tokens per Sec:     3715, Lr: 0.000124\r\n",
      "2024-08-27 16:12:55,589 - INFO - joeynmt.training - Epoch 218, Step:    20735, Batch Loss:     0.061568, Batch Acc: 0.994478, Tokens per Sec:     3899, Lr: 0.000124\r\n",
      "2024-08-27 16:12:56,715 - INFO - joeynmt.training - Epoch 218, Step:    20740, Batch Loss:     0.073552, Batch Acc: 0.994232, Tokens per Sec:     3697, Lr: 0.000124\r\n",
      "2024-08-27 16:12:56,991 - INFO - joeynmt.training - Epoch 218, total training loss: 5.85, num. of seqs: 8065, num. of tokens: 80463, 21.0660[sec]\r\n",
      "2024-08-27 16:12:56,991 - INFO - joeynmt.training - EPOCH 219\r\n",
      "2024-08-27 16:12:57,867 - INFO - joeynmt.training - Epoch 219, Step:    20745, Batch Loss:     0.058230, Batch Acc: 0.994389, Tokens per Sec:     3888, Lr: 0.000124\r\n",
      "2024-08-27 16:12:58,963 - INFO - joeynmt.training - Epoch 219, Step:    20750, Batch Loss:     0.059740, Batch Acc: 0.994842, Tokens per Sec:     4068, Lr: 0.000124\r\n",
      "2024-08-27 16:13:00,031 - INFO - joeynmt.training - Epoch 219, Step:    20755, Batch Loss:     0.063862, Batch Acc: 0.995657, Tokens per Sec:     3667, Lr: 0.000124\r\n",
      "2024-08-27 16:13:01,116 - INFO - joeynmt.training - Epoch 219, Step:    20760, Batch Loss:     0.053257, Batch Acc: 0.997629, Tokens per Sec:     3890, Lr: 0.000124\r\n",
      "2024-08-27 16:13:02,204 - INFO - joeynmt.training - Epoch 219, Step:    20765, Batch Loss:     0.061013, Batch Acc: 0.994353, Tokens per Sec:     3746, Lr: 0.000124\r\n",
      "2024-08-27 16:13:03,324 - INFO - joeynmt.training - Epoch 219, Step:    20770, Batch Loss:     0.061845, Batch Acc: 0.994645, Tokens per Sec:     3838, Lr: 0.000124\r\n",
      "2024-08-27 16:13:04,436 - INFO - joeynmt.training - Epoch 219, Step:    20775, Batch Loss:     0.061121, Batch Acc: 0.994793, Tokens per Sec:     3630, Lr: 0.000124\r\n",
      "2024-08-27 16:13:05,530 - INFO - joeynmt.training - Epoch 219, Step:    20780, Batch Loss:     0.058450, Batch Acc: 0.994531, Tokens per Sec:     4015, Lr: 0.000124\r\n",
      "2024-08-27 16:13:06,630 - INFO - joeynmt.training - Epoch 219, Step:    20785, Batch Loss:     0.060589, Batch Acc: 0.993365, Tokens per Sec:     3839, Lr: 0.000124\r\n",
      "2024-08-27 16:13:07,755 - INFO - joeynmt.training - Epoch 219, Step:    20790, Batch Loss:     0.062152, Batch Acc: 0.996168, Tokens per Sec:     3711, Lr: 0.000124\r\n",
      "2024-08-27 16:13:08,868 - INFO - joeynmt.training - Epoch 219, Step:    20795, Batch Loss:     0.063559, Batch Acc: 0.994917, Tokens per Sec:     3892, Lr: 0.000124\r\n",
      "2024-08-27 16:13:09,961 - INFO - joeynmt.training - Epoch 219, Step:    20800, Batch Loss:     0.076267, Batch Acc: 0.994521, Tokens per Sec:     3677, Lr: 0.000124\r\n",
      "2024-08-27 16:13:11,094 - INFO - joeynmt.training - Epoch 219, Step:    20805, Batch Loss:     0.056079, Batch Acc: 0.994093, Tokens per Sec:     3739, Lr: 0.000124\r\n",
      "2024-08-27 16:13:12,331 - INFO - joeynmt.training - Epoch 219, Step:    20810, Batch Loss:     0.055025, Batch Acc: 0.996047, Tokens per Sec:     3478, Lr: 0.000124\r\n",
      "2024-08-27 16:13:13,414 - INFO - joeynmt.training - Epoch 219, Step:    20815, Batch Loss:     0.054943, Batch Acc: 0.996689, Tokens per Sec:     3905, Lr: 0.000124\r\n",
      "2024-08-27 16:13:14,515 - INFO - joeynmt.training - Epoch 219, Step:    20820, Batch Loss:     0.055400, Batch Acc: 0.995993, Tokens per Sec:     3855, Lr: 0.000124\r\n",
      "2024-08-27 16:13:15,590 - INFO - joeynmt.training - Epoch 219, Step:    20825, Batch Loss:     0.056636, Batch Acc: 0.994353, Tokens per Sec:     3958, Lr: 0.000124\r\n",
      "2024-08-27 16:13:16,676 - INFO - joeynmt.training - Epoch 219, Step:    20830, Batch Loss:     0.059452, Batch Acc: 0.994760, Tokens per Sec:     3693, Lr: 0.000124\r\n",
      "2024-08-27 16:13:17,798 - INFO - joeynmt.training - Epoch 219, Step:    20835, Batch Loss:     0.060808, Batch Acc: 0.994764, Tokens per Sec:     3919, Lr: 0.000124\r\n",
      "2024-08-27 16:13:18,173 - INFO - joeynmt.training - Epoch 219, total training loss: 5.92, num. of seqs: 8065, num. of tokens: 80463, 21.1637[sec]\r\n",
      "2024-08-27 16:13:18,174 - INFO - joeynmt.training - EPOCH 220\r\n",
      "2024-08-27 16:13:19,048 - INFO - joeynmt.training - Epoch 220, Step:    20840, Batch Loss:     0.054377, Batch Acc: 0.996257, Tokens per Sec:     3994, Lr: 0.000124\r\n",
      "2024-08-27 16:13:20,129 - INFO - joeynmt.training - Epoch 220, Step:    20845, Batch Loss:     0.061035, Batch Acc: 0.995158, Tokens per Sec:     4204, Lr: 0.000124\r\n",
      "2024-08-27 16:13:21,214 - INFO - joeynmt.training - Epoch 220, Step:    20850, Batch Loss:     0.066476, Batch Acc: 0.994331, Tokens per Sec:     3744, Lr: 0.000124\r\n",
      "2024-08-27 16:13:22,303 - INFO - joeynmt.training - Epoch 220, Step:    20855, Batch Loss:     0.063863, Batch Acc: 0.996614, Tokens per Sec:     3799, Lr: 0.000124\r\n",
      "2024-08-27 16:13:23,402 - INFO - joeynmt.training - Epoch 220, Step:    20860, Batch Loss:     0.054166, Batch Acc: 0.997638, Tokens per Sec:     3852, Lr: 0.000124\r\n",
      "2024-08-27 16:13:24,509 - INFO - joeynmt.training - Epoch 220, Step:    20865, Batch Loss:     0.053969, Batch Acc: 0.996137, Tokens per Sec:     3745, Lr: 0.000124\r\n",
      "2024-08-27 16:13:25,620 - INFO - joeynmt.training - Epoch 220, Step:    20870, Batch Loss:     0.063420, Batch Acc: 0.994337, Tokens per Sec:     3977, Lr: 0.000124\r\n",
      "2024-08-27 16:13:26,763 - INFO - joeynmt.training - Epoch 220, Step:    20875, Batch Loss:     0.067806, Batch Acc: 0.995126, Tokens per Sec:     3772, Lr: 0.000124\r\n",
      "2024-08-27 16:13:27,865 - INFO - joeynmt.training - Epoch 220, Step:    20880, Batch Loss:     0.052348, Batch Acc: 0.995843, Tokens per Sec:     3714, Lr: 0.000124\r\n",
      "2024-08-27 16:13:28,948 - INFO - joeynmt.training - Epoch 220, Step:    20885, Batch Loss:     0.055496, Batch Acc: 0.994139, Tokens per Sec:     3782, Lr: 0.000124\r\n",
      "2024-08-27 16:13:30,027 - INFO - joeynmt.training - Epoch 220, Step:    20890, Batch Loss:     0.062564, Batch Acc: 0.992982, Tokens per Sec:     3831, Lr: 0.000124\r\n",
      "2024-08-27 16:13:31,118 - INFO - joeynmt.training - Epoch 220, Step:    20895, Batch Loss:     0.066948, Batch Acc: 0.996187, Tokens per Sec:     3851, Lr: 0.000124\r\n",
      "2024-08-27 16:13:32,210 - INFO - joeynmt.training - Epoch 220, Step:    20900, Batch Loss:     0.065850, Batch Acc: 0.996444, Tokens per Sec:     3865, Lr: 0.000124\r\n",
      "2024-08-27 16:13:33,311 - INFO - joeynmt.training - Epoch 220, Step:    20905, Batch Loss:     0.065420, Batch Acc: 0.994685, Tokens per Sec:     3933, Lr: 0.000124\r\n",
      "2024-08-27 16:13:34,403 - INFO - joeynmt.training - Epoch 220, Step:    20910, Batch Loss:     0.066616, Batch Acc: 0.993963, Tokens per Sec:     3945, Lr: 0.000124\r\n",
      "2024-08-27 16:13:35,485 - INFO - joeynmt.training - Epoch 220, Step:    20915, Batch Loss:     0.072332, Batch Acc: 0.995829, Tokens per Sec:     3769, Lr: 0.000124\r\n",
      "2024-08-27 16:13:36,572 - INFO - joeynmt.training - Epoch 220, Step:    20920, Batch Loss:     0.052463, Batch Acc: 0.995396, Tokens per Sec:     3799, Lr: 0.000124\r\n",
      "2024-08-27 16:13:37,682 - INFO - joeynmt.training - Epoch 220, Step:    20925, Batch Loss:     0.063284, Batch Acc: 0.996183, Tokens per Sec:     3778, Lr: 0.000124\r\n",
      "2024-08-27 16:13:38,772 - INFO - joeynmt.training - Epoch 220, Step:    20930, Batch Loss:     0.058502, Batch Acc: 0.997259, Tokens per Sec:     4020, Lr: 0.000124\r\n",
      "2024-08-27 16:13:39,040 - INFO - joeynmt.training - Epoch 220, total training loss: 5.78, num. of seqs: 8065, num. of tokens: 80463, 20.8487[sec]\r\n",
      "2024-08-27 16:13:39,040 - INFO - joeynmt.training - EPOCH 221\r\n",
      "2024-08-27 16:13:39,911 - INFO - joeynmt.training - Epoch 221, Step:    20935, Batch Loss:     0.066063, Batch Acc: 0.994722, Tokens per Sec:     3717, Lr: 0.000124\r\n",
      "2024-08-27 16:13:41,001 - INFO - joeynmt.training - Epoch 221, Step:    20940, Batch Loss:     0.052429, Batch Acc: 0.996307, Tokens per Sec:     3977, Lr: 0.000124\r\n",
      "2024-08-27 16:13:42,123 - INFO - joeynmt.training - Epoch 221, Step:    20945, Batch Loss:     0.059747, Batch Acc: 0.996313, Tokens per Sec:     3868, Lr: 0.000124\r\n",
      "2024-08-27 16:13:43,272 - INFO - joeynmt.training - Epoch 221, Step:    20950, Batch Loss:     0.067781, Batch Acc: 0.995468, Tokens per Sec:     3653, Lr: 0.000124\r\n",
      "2024-08-27 16:13:44,383 - INFO - joeynmt.training - Epoch 221, Step:    20955, Batch Loss:     0.056893, Batch Acc: 0.996414, Tokens per Sec:     3767, Lr: 0.000124\r\n",
      "2024-08-27 16:13:45,479 - INFO - joeynmt.training - Epoch 221, Step:    20960, Batch Loss:     0.067155, Batch Acc: 0.994435, Tokens per Sec:     3772, Lr: 0.000124\r\n",
      "2024-08-27 16:13:46,579 - INFO - joeynmt.training - Epoch 221, Step:    20965, Batch Loss:     0.056810, Batch Acc: 0.993973, Tokens per Sec:     3772, Lr: 0.000124\r\n",
      "2024-08-27 16:13:47,705 - INFO - joeynmt.training - Epoch 221, Step:    20970, Batch Loss:     0.066398, Batch Acc: 0.993729, Tokens per Sec:     3687, Lr: 0.000124\r\n",
      "2024-08-27 16:13:48,820 - INFO - joeynmt.training - Epoch 221, Step:    20975, Batch Loss:     0.060544, Batch Acc: 0.995287, Tokens per Sec:     3808, Lr: 0.000124\r\n",
      "2024-08-27 16:13:49,935 - INFO - joeynmt.training - Epoch 221, Step:    20980, Batch Loss:     0.057449, Batch Acc: 0.996358, Tokens per Sec:     3942, Lr: 0.000124\r\n",
      "2024-08-27 16:13:51,022 - INFO - joeynmt.training - Epoch 221, Step:    20985, Batch Loss:     0.054913, Batch Acc: 0.993708, Tokens per Sec:     3805, Lr: 0.000123\r\n",
      "2024-08-27 16:13:52,123 - INFO - joeynmt.training - Epoch 221, Step:    20990, Batch Loss:     0.066880, Batch Acc: 0.996317, Tokens per Sec:     3946, Lr: 0.000123\r\n",
      "2024-08-27 16:13:53,217 - INFO - joeynmt.training - Epoch 221, Step:    20995, Batch Loss:     0.061617, Batch Acc: 0.994378, Tokens per Sec:     3906, Lr: 0.000123\r\n",
      "2024-08-27 16:13:54,306 - INFO - joeynmt.training - Epoch 221, Step:    21000, Batch Loss:     0.062700, Batch Acc: 0.994346, Tokens per Sec:     3738, Lr: 0.000123\r\n",
      "2024-08-27 16:13:54,307 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=21042\r\n",
      "2024-08-27 16:13:54,307 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 63.92it/s]\r\n",
      "2024-08-27 16:14:17,151 - INFO - joeynmt.prediction - Generation took 22.8425[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 43324.66 examples/s]\r\n",
      "2024-08-27 16:14:17,537 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:14:17,537 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.37, loss:   6.75, ppl: 851.41, acc:   0.29, 0.1702[sec]\r\n",
      "2024-08-27 16:14:17,538 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:14:17,686 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:14:17,686 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:14:17,687 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 16:14:17,980 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:14:18,127 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:14:18,127 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:14:18,127 - INFO - joeynmt.training - \tHypothesis: trois tables blanches\r\n",
      "2024-08-27 16:14:18,420 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:14:18,566 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:14:18,566 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:14:18,566 - INFO - joeynmt.training - \tHypothesis: les autres pièces devraient être proportionnées\r\n",
      "2024-08-27 16:14:18,859 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:14:19,004 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:14:19,005 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:14:19,005 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 16:14:20,423 - INFO - joeynmt.training - Epoch 221, Step:    21005, Batch Loss:     0.053535, Batch Acc: 0.996234, Tokens per Sec:     3780, Lr: 0.000123\r\n",
      "2024-08-27 16:14:21,505 - INFO - joeynmt.training - Epoch 221, Step:    21010, Batch Loss:     0.060721, Batch Acc: 0.995685, Tokens per Sec:     3646, Lr: 0.000123\r\n",
      "2024-08-27 16:14:22,598 - INFO - joeynmt.training - Epoch 221, Step:    21015, Batch Loss:     0.071763, Batch Acc: 0.993538, Tokens per Sec:     3822, Lr: 0.000123\r\n",
      "2024-08-27 16:14:23,720 - INFO - joeynmt.training - Epoch 221, Step:    21020, Batch Loss:     0.050512, Batch Acc: 0.994064, Tokens per Sec:     3908, Lr: 0.000123\r\n",
      "2024-08-27 16:14:24,831 - INFO - joeynmt.training - Epoch 221, Step:    21025, Batch Loss:     0.058007, Batch Acc: 0.994894, Tokens per Sec:     3878, Lr: 0.000123\r\n",
      "2024-08-27 16:14:25,200 - INFO - joeynmt.training - Epoch 221, total training loss: 5.87, num. of seqs: 8065, num. of tokens: 80463, 21.1490[sec]\r\n",
      "2024-08-27 16:14:25,200 - INFO - joeynmt.training - EPOCH 222\r\n",
      "2024-08-27 16:14:26,066 - INFO - joeynmt.training - Epoch 222, Step:    21030, Batch Loss:     0.058386, Batch Acc: 0.996778, Tokens per Sec:     3603, Lr: 0.000123\r\n",
      "2024-08-27 16:14:27,206 - INFO - joeynmt.training - Epoch 222, Step:    21035, Batch Loss:     0.060999, Batch Acc: 0.997213, Tokens per Sec:     3778, Lr: 0.000123\r\n",
      "2024-08-27 16:14:28,315 - INFO - joeynmt.training - Epoch 222, Step:    21040, Batch Loss:     0.056365, Batch Acc: 0.995808, Tokens per Sec:     3874, Lr: 0.000123\r\n",
      "2024-08-27 16:14:29,424 - INFO - joeynmt.training - Epoch 222, Step:    21045, Batch Loss:     0.052207, Batch Acc: 0.997146, Tokens per Sec:     3795, Lr: 0.000123\r\n",
      "2024-08-27 16:14:30,511 - INFO - joeynmt.training - Epoch 222, Step:    21050, Batch Loss:     0.061878, Batch Acc: 0.997153, Tokens per Sec:     3877, Lr: 0.000123\r\n",
      "2024-08-27 16:14:31,603 - INFO - joeynmt.training - Epoch 222, Step:    21055, Batch Loss:     0.065419, Batch Acc: 0.994109, Tokens per Sec:     3735, Lr: 0.000123\r\n",
      "2024-08-27 16:14:32,699 - INFO - joeynmt.training - Epoch 222, Step:    21060, Batch Loss:     0.064466, Batch Acc: 0.993074, Tokens per Sec:     4087, Lr: 0.000123\r\n",
      "2024-08-27 16:14:33,793 - INFO - joeynmt.training - Epoch 222, Step:    21065, Batch Loss:     0.056534, Batch Acc: 0.995843, Tokens per Sec:     3959, Lr: 0.000123\r\n",
      "2024-08-27 16:14:34,902 - INFO - joeynmt.training - Epoch 222, Step:    21070, Batch Loss:     0.074009, Batch Acc: 0.996823, Tokens per Sec:     3976, Lr: 0.000123\r\n",
      "2024-08-27 16:14:35,989 - INFO - joeynmt.training - Epoch 222, Step:    21075, Batch Loss:     0.074962, Batch Acc: 0.994451, Tokens per Sec:     3815, Lr: 0.000123\r\n",
      "2024-08-27 16:14:37,114 - INFO - joeynmt.training - Epoch 222, Step:    21080, Batch Loss:     0.060537, Batch Acc: 0.994824, Tokens per Sec:     3780, Lr: 0.000123\r\n",
      "2024-08-27 16:14:38,211 - INFO - joeynmt.training - Epoch 222, Step:    21085, Batch Loss:     0.056342, Batch Acc: 0.996106, Tokens per Sec:     3750, Lr: 0.000123\r\n",
      "2024-08-27 16:14:39,301 - INFO - joeynmt.training - Epoch 222, Step:    21090, Batch Loss:     0.059852, Batch Acc: 0.995973, Tokens per Sec:     3876, Lr: 0.000123\r\n",
      "2024-08-27 16:14:40,389 - INFO - joeynmt.training - Epoch 222, Step:    21095, Batch Loss:     0.066000, Batch Acc: 0.994466, Tokens per Sec:     3821, Lr: 0.000123\r\n",
      "2024-08-27 16:14:41,482 - INFO - joeynmt.training - Epoch 222, Step:    21100, Batch Loss:     0.063630, Batch Acc: 0.995337, Tokens per Sec:     3928, Lr: 0.000123\r\n",
      "2024-08-27 16:14:42,563 - INFO - joeynmt.training - Epoch 222, Step:    21105, Batch Loss:     0.056220, Batch Acc: 0.995904, Tokens per Sec:     3842, Lr: 0.000123\r\n",
      "2024-08-27 16:14:43,650 - INFO - joeynmt.training - Epoch 222, Step:    21110, Batch Loss:     0.058894, Batch Acc: 0.993640, Tokens per Sec:     3762, Lr: 0.000123\r\n",
      "2024-08-27 16:14:44,745 - INFO - joeynmt.training - Epoch 222, Step:    21115, Batch Loss:     0.068798, Batch Acc: 0.993787, Tokens per Sec:     3825, Lr: 0.000123\r\n",
      "2024-08-27 16:14:45,934 - INFO - joeynmt.training - Epoch 222, Step:    21120, Batch Loss:     0.069493, Batch Acc: 0.993629, Tokens per Sec:     3434, Lr: 0.000123\r\n",
      "2024-08-27 16:14:46,309 - INFO - joeynmt.training - Epoch 222, total training loss: 5.82, num. of seqs: 8065, num. of tokens: 80463, 21.0915[sec]\r\n",
      "2024-08-27 16:14:46,309 - INFO - joeynmt.training - EPOCH 223\r\n",
      "2024-08-27 16:14:47,209 - INFO - joeynmt.training - Epoch 223, Step:    21125, Batch Loss:     0.060553, Batch Acc: 0.995791, Tokens per Sec:     3711, Lr: 0.000123\r\n",
      "2024-08-27 16:14:48,308 - INFO - joeynmt.training - Epoch 223, Step:    21130, Batch Loss:     0.059422, Batch Acc: 0.993269, Tokens per Sec:     3789, Lr: 0.000123\r\n",
      "2024-08-27 16:14:49,416 - INFO - joeynmt.training - Epoch 223, Step:    21135, Batch Loss:     0.060843, Batch Acc: 0.995133, Tokens per Sec:     3713, Lr: 0.000123\r\n",
      "2024-08-27 16:14:50,524 - INFO - joeynmt.training - Epoch 223, Step:    21140, Batch Loss:     0.057203, Batch Acc: 0.996667, Tokens per Sec:     3794, Lr: 0.000123\r\n",
      "2024-08-27 16:14:51,604 - INFO - joeynmt.training - Epoch 223, Step:    21145, Batch Loss:     0.056213, Batch Acc: 0.995782, Tokens per Sec:     3732, Lr: 0.000123\r\n",
      "2024-08-27 16:14:52,684 - INFO - joeynmt.training - Epoch 223, Step:    21150, Batch Loss:     0.049817, Batch Acc: 0.996394, Tokens per Sec:     3857, Lr: 0.000123\r\n",
      "2024-08-27 16:14:53,767 - INFO - joeynmt.training - Epoch 223, Step:    21155, Batch Loss:     0.054057, Batch Acc: 0.996133, Tokens per Sec:     4062, Lr: 0.000123\r\n",
      "2024-08-27 16:14:54,864 - INFO - joeynmt.training - Epoch 223, Step:    21160, Batch Loss:     0.060767, Batch Acc: 0.994464, Tokens per Sec:     3953, Lr: 0.000123\r\n",
      "2024-08-27 16:14:55,957 - INFO - joeynmt.training - Epoch 223, Step:    21165, Batch Loss:     0.060810, Batch Acc: 0.996660, Tokens per Sec:     3836, Lr: 0.000123\r\n",
      "2024-08-27 16:14:57,101 - INFO - joeynmt.training - Epoch 223, Step:    21170, Batch Loss:     0.064099, Batch Acc: 0.993043, Tokens per Sec:     3769, Lr: 0.000123\r\n",
      "2024-08-27 16:14:58,189 - INFO - joeynmt.training - Epoch 223, Step:    21175, Batch Loss:     0.062572, Batch Acc: 0.997372, Tokens per Sec:     3851, Lr: 0.000123\r\n",
      "2024-08-27 16:14:59,276 - INFO - joeynmt.training - Epoch 223, Step:    21180, Batch Loss:     0.063048, Batch Acc: 0.994858, Tokens per Sec:     3759, Lr: 0.000123\r\n",
      "2024-08-27 16:15:00,348 - INFO - joeynmt.training - Epoch 223, Step:    21185, Batch Loss:     0.067064, Batch Acc: 0.993162, Tokens per Sec:     3958, Lr: 0.000123\r\n",
      "2024-08-27 16:15:01,435 - INFO - joeynmt.training - Epoch 223, Step:    21190, Batch Loss:     0.054165, Batch Acc: 0.997233, Tokens per Sec:     3995, Lr: 0.000123\r\n",
      "2024-08-27 16:15:02,529 - INFO - joeynmt.training - Epoch 223, Step:    21195, Batch Loss:     0.064016, Batch Acc: 0.994503, Tokens per Sec:     3827, Lr: 0.000123\r\n",
      "2024-08-27 16:15:03,604 - INFO - joeynmt.training - Epoch 223, Step:    21200, Batch Loss:     0.061813, Batch Acc: 0.994089, Tokens per Sec:     3780, Lr: 0.000123\r\n",
      "2024-08-27 16:15:04,683 - INFO - joeynmt.training - Epoch 223, Step:    21205, Batch Loss:     0.064773, Batch Acc: 0.995025, Tokens per Sec:     3914, Lr: 0.000123\r\n",
      "2024-08-27 16:15:05,768 - INFO - joeynmt.training - Epoch 223, Step:    21210, Batch Loss:     0.064081, Batch Acc: 0.996423, Tokens per Sec:     3866, Lr: 0.000123\r\n",
      "2024-08-27 16:15:06,888 - INFO - joeynmt.training - Epoch 223, Step:    21215, Batch Loss:     0.059410, Batch Acc: 0.994967, Tokens per Sec:     3905, Lr: 0.000123\r\n",
      "2024-08-27 16:15:07,214 - INFO - joeynmt.training - Epoch 223, total training loss: 5.84, num. of seqs: 8065, num. of tokens: 80463, 20.8870[sec]\r\n",
      "2024-08-27 16:15:07,214 - INFO - joeynmt.training - EPOCH 224\r\n",
      "2024-08-27 16:15:08,102 - INFO - joeynmt.training - Epoch 224, Step:    21220, Batch Loss:     0.054489, Batch Acc: 0.998027, Tokens per Sec:     4013, Lr: 0.000123\r\n",
      "2024-08-27 16:15:09,208 - INFO - joeynmt.training - Epoch 224, Step:    21225, Batch Loss:     0.057680, Batch Acc: 0.994403, Tokens per Sec:     3718, Lr: 0.000123\r\n",
      "2024-08-27 16:15:10,311 - INFO - joeynmt.training - Epoch 224, Step:    21230, Batch Loss:     0.055332, Batch Acc: 0.995977, Tokens per Sec:     3833, Lr: 0.000123\r\n",
      "2024-08-27 16:15:11,414 - INFO - joeynmt.training - Epoch 224, Step:    21235, Batch Loss:     0.059637, Batch Acc: 0.993365, Tokens per Sec:     3830, Lr: 0.000123\r\n",
      "2024-08-27 16:15:12,519 - INFO - joeynmt.training - Epoch 224, Step:    21240, Batch Loss:     0.071253, Batch Acc: 0.995782, Tokens per Sec:     3863, Lr: 0.000123\r\n",
      "2024-08-27 16:15:13,624 - INFO - joeynmt.training - Epoch 224, Step:    21245, Batch Loss:     0.069338, Batch Acc: 0.997378, Tokens per Sec:     3800, Lr: 0.000123\r\n",
      "2024-08-27 16:15:14,699 - INFO - joeynmt.training - Epoch 224, Step:    21250, Batch Loss:     0.062924, Batch Acc: 0.995668, Tokens per Sec:     3653, Lr: 0.000123\r\n",
      "2024-08-27 16:15:15,805 - INFO - joeynmt.training - Epoch 224, Step:    21255, Batch Loss:     0.057993, Batch Acc: 0.996643, Tokens per Sec:     3771, Lr: 0.000123\r\n",
      "2024-08-27 16:15:17,072 - INFO - joeynmt.training - Epoch 224, Step:    21260, Batch Loss:     0.056708, Batch Acc: 0.995776, Tokens per Sec:     3552, Lr: 0.000123\r\n",
      "2024-08-27 16:15:18,170 - INFO - joeynmt.training - Epoch 224, Step:    21265, Batch Loss:     0.055254, Batch Acc: 0.996265, Tokens per Sec:     3660, Lr: 0.000123\r\n",
      "2024-08-27 16:15:19,242 - INFO - joeynmt.training - Epoch 224, Step:    21270, Batch Loss:     0.060168, Batch Acc: 0.994894, Tokens per Sec:     3840, Lr: 0.000123\r\n",
      "2024-08-27 16:15:20,322 - INFO - joeynmt.training - Epoch 224, Step:    21275, Batch Loss:     0.054870, Batch Acc: 0.994371, Tokens per Sec:     3949, Lr: 0.000123\r\n",
      "2024-08-27 16:15:21,414 - INFO - joeynmt.training - Epoch 224, Step:    21280, Batch Loss:     0.054153, Batch Acc: 0.994620, Tokens per Sec:     3918, Lr: 0.000123\r\n",
      "2024-08-27 16:15:22,508 - INFO - joeynmt.training - Epoch 224, Step:    21285, Batch Loss:     0.070085, Batch Acc: 0.995214, Tokens per Sec:     4013, Lr: 0.000123\r\n",
      "2024-08-27 16:15:23,625 - INFO - joeynmt.training - Epoch 224, Step:    21290, Batch Loss:     0.072542, Batch Acc: 0.994462, Tokens per Sec:     3723, Lr: 0.000123\r\n",
      "2024-08-27 16:15:24,693 - INFO - joeynmt.training - Epoch 224, Step:    21295, Batch Loss:     0.060792, Batch Acc: 0.996453, Tokens per Sec:     3697, Lr: 0.000123\r\n",
      "2024-08-27 16:15:25,790 - INFO - joeynmt.training - Epoch 224, Step:    21300, Batch Loss:     0.057123, Batch Acc: 0.994067, Tokens per Sec:     3841, Lr: 0.000123\r\n",
      "2024-08-27 16:15:26,941 - INFO - joeynmt.training - Epoch 224, Step:    21305, Batch Loss:     0.056395, Batch Acc: 0.995391, Tokens per Sec:     3772, Lr: 0.000123\r\n",
      "2024-08-27 16:15:28,042 - INFO - joeynmt.training - Epoch 224, Step:    21310, Batch Loss:     0.064074, Batch Acc: 0.996113, Tokens per Sec:     3978, Lr: 0.000123\r\n",
      "2024-08-27 16:15:28,407 - INFO - joeynmt.training - Epoch 224, total training loss: 5.76, num. of seqs: 8065, num. of tokens: 80463, 21.1764[sec]\r\n",
      "2024-08-27 16:15:28,408 - INFO - joeynmt.training - EPOCH 225\r\n",
      "2024-08-27 16:15:29,284 - INFO - joeynmt.training - Epoch 225, Step:    21315, Batch Loss:     0.054675, Batch Acc: 0.994590, Tokens per Sec:     3816, Lr: 0.000123\r\n",
      "2024-08-27 16:15:30,394 - INFO - joeynmt.training - Epoch 225, Step:    21320, Batch Loss:     0.058286, Batch Acc: 0.995392, Tokens per Sec:     3912, Lr: 0.000123\r\n",
      "2024-08-27 16:15:31,497 - INFO - joeynmt.training - Epoch 225, Step:    21325, Batch Loss:     0.062499, Batch Acc: 0.996994, Tokens per Sec:     3925, Lr: 0.000122\r\n",
      "2024-08-27 16:15:32,580 - INFO - joeynmt.training - Epoch 225, Step:    21330, Batch Loss:     0.057144, Batch Acc: 0.996441, Tokens per Sec:     3635, Lr: 0.000122\r\n",
      "2024-08-27 16:15:33,668 - INFO - joeynmt.training - Epoch 225, Step:    21335, Batch Loss:     0.051736, Batch Acc: 0.996036, Tokens per Sec:     3941, Lr: 0.000122\r\n",
      "2024-08-27 16:15:34,756 - INFO - joeynmt.training - Epoch 225, Step:    21340, Batch Loss:     0.062976, Batch Acc: 0.995509, Tokens per Sec:     3893, Lr: 0.000122\r\n",
      "2024-08-27 16:15:35,844 - INFO - joeynmt.training - Epoch 225, Step:    21345, Batch Loss:     0.058634, Batch Acc: 0.996256, Tokens per Sec:     3929, Lr: 0.000122\r\n",
      "2024-08-27 16:15:36,970 - INFO - joeynmt.training - Epoch 225, Step:    21350, Batch Loss:     0.053335, Batch Acc: 0.995905, Tokens per Sec:     3690, Lr: 0.000122\r\n",
      "2024-08-27 16:15:38,076 - INFO - joeynmt.training - Epoch 225, Step:    21355, Batch Loss:     0.069786, Batch Acc: 0.994048, Tokens per Sec:     3802, Lr: 0.000122\r\n",
      "2024-08-27 16:15:39,161 - INFO - joeynmt.training - Epoch 225, Step:    21360, Batch Loss:     0.056618, Batch Acc: 0.995525, Tokens per Sec:     3708, Lr: 0.000122\r\n",
      "2024-08-27 16:15:40,247 - INFO - joeynmt.training - Epoch 225, Step:    21365, Batch Loss:     0.053413, Batch Acc: 0.996758, Tokens per Sec:     3695, Lr: 0.000122\r\n",
      "2024-08-27 16:15:41,357 - INFO - joeynmt.training - Epoch 225, Step:    21370, Batch Loss:     0.058346, Batch Acc: 0.995231, Tokens per Sec:     3782, Lr: 0.000122\r\n",
      "2024-08-27 16:15:42,475 - INFO - joeynmt.training - Epoch 225, Step:    21375, Batch Loss:     0.056808, Batch Acc: 0.995637, Tokens per Sec:     3898, Lr: 0.000122\r\n",
      "2024-08-27 16:15:43,577 - INFO - joeynmt.training - Epoch 225, Step:    21380, Batch Loss:     0.065405, Batch Acc: 0.992516, Tokens per Sec:     3882, Lr: 0.000122\r\n",
      "2024-08-27 16:15:44,674 - INFO - joeynmt.training - Epoch 225, Step:    21385, Batch Loss:     0.054515, Batch Acc: 0.995586, Tokens per Sec:     3719, Lr: 0.000122\r\n",
      "2024-08-27 16:15:45,778 - INFO - joeynmt.training - Epoch 225, Step:    21390, Batch Loss:     0.061616, Batch Acc: 0.995424, Tokens per Sec:     3962, Lr: 0.000122\r\n",
      "2024-08-27 16:15:47,077 - INFO - joeynmt.training - Epoch 225, Step:    21395, Batch Loss:     0.056854, Batch Acc: 0.995753, Tokens per Sec:     3265, Lr: 0.000122\r\n",
      "2024-08-27 16:15:48,307 - INFO - joeynmt.training - Epoch 225, Step:    21400, Batch Loss:     0.065596, Batch Acc: 0.996126, Tokens per Sec:     3360, Lr: 0.000122\r\n",
      "2024-08-27 16:15:49,451 - INFO - joeynmt.training - Epoch 225, Step:    21405, Batch Loss:     0.059529, Batch Acc: 0.994941, Tokens per Sec:     3632, Lr: 0.000122\r\n",
      "2024-08-27 16:15:49,887 - INFO - joeynmt.training - Epoch 225, total training loss: 5.69, num. of seqs: 8065, num. of tokens: 80463, 21.4616[sec]\r\n",
      "2024-08-27 16:15:49,887 - INFO - joeynmt.training - EPOCH 226\r\n",
      "2024-08-27 16:15:50,558 - INFO - joeynmt.training - Epoch 226, Step:    21410, Batch Loss:     0.066166, Batch Acc: 0.995008, Tokens per Sec:     3910, Lr: 0.000122\r\n",
      "2024-08-27 16:15:51,638 - INFO - joeynmt.training - Epoch 226, Step:    21415, Batch Loss:     0.074675, Batch Acc: 0.995953, Tokens per Sec:     3892, Lr: 0.000122\r\n",
      "2024-08-27 16:15:52,736 - INFO - joeynmt.training - Epoch 226, Step:    21420, Batch Loss:     0.065388, Batch Acc: 0.995190, Tokens per Sec:     3790, Lr: 0.000122\r\n",
      "2024-08-27 16:15:53,823 - INFO - joeynmt.training - Epoch 226, Step:    21425, Batch Loss:     0.059602, Batch Acc: 0.995632, Tokens per Sec:     3583, Lr: 0.000122\r\n",
      "2024-08-27 16:15:54,920 - INFO - joeynmt.training - Epoch 226, Step:    21430, Batch Loss:     0.054064, Batch Acc: 0.994718, Tokens per Sec:     3796, Lr: 0.000122\r\n",
      "2024-08-27 16:15:56,020 - INFO - joeynmt.training - Epoch 226, Step:    21435, Batch Loss:     0.066749, Batch Acc: 0.997437, Tokens per Sec:     3906, Lr: 0.000122\r\n",
      "2024-08-27 16:15:57,147 - INFO - joeynmt.training - Epoch 226, Step:    21440, Batch Loss:     0.057823, Batch Acc: 0.996564, Tokens per Sec:     3876, Lr: 0.000122\r\n",
      "2024-08-27 16:15:58,199 - INFO - joeynmt.training - Epoch 226, Step:    21445, Batch Loss:     0.063726, Batch Acc: 0.995708, Tokens per Sec:     3768, Lr: 0.000122\r\n",
      "2024-08-27 16:15:59,250 - INFO - joeynmt.training - Epoch 226, Step:    21450, Batch Loss:     0.049222, Batch Acc: 0.995666, Tokens per Sec:     3955, Lr: 0.000122\r\n",
      "2024-08-27 16:16:00,346 - INFO - joeynmt.training - Epoch 226, Step:    21455, Batch Loss:     0.063884, Batch Acc: 0.996146, Tokens per Sec:     3789, Lr: 0.000122\r\n",
      "2024-08-27 16:16:01,448 - INFO - joeynmt.training - Epoch 226, Step:    21460, Batch Loss:     0.053583, Batch Acc: 0.995812, Tokens per Sec:     3902, Lr: 0.000122\r\n",
      "2024-08-27 16:16:02,542 - INFO - joeynmt.training - Epoch 226, Step:    21465, Batch Loss:     0.057217, Batch Acc: 0.993721, Tokens per Sec:     3787, Lr: 0.000122\r\n",
      "2024-08-27 16:16:03,638 - INFO - joeynmt.training - Epoch 226, Step:    21470, Batch Loss:     0.069215, Batch Acc: 0.993197, Tokens per Sec:     3758, Lr: 0.000122\r\n",
      "2024-08-27 16:16:04,725 - INFO - joeynmt.training - Epoch 226, Step:    21475, Batch Loss:     0.059054, Batch Acc: 0.993908, Tokens per Sec:     3780, Lr: 0.000122\r\n",
      "2024-08-27 16:16:05,800 - INFO - joeynmt.training - Epoch 226, Step:    21480, Batch Loss:     0.062237, Batch Acc: 0.996979, Tokens per Sec:     4004, Lr: 0.000122\r\n",
      "2024-08-27 16:16:06,932 - INFO - joeynmt.training - Epoch 226, Step:    21485, Batch Loss:     0.064169, Batch Acc: 0.994820, Tokens per Sec:     3752, Lr: 0.000122\r\n",
      "2024-08-27 16:16:08,041 - INFO - joeynmt.training - Epoch 226, Step:    21490, Batch Loss:     0.053827, Batch Acc: 0.995590, Tokens per Sec:     3888, Lr: 0.000122\r\n",
      "2024-08-27 16:16:09,142 - INFO - joeynmt.training - Epoch 226, Step:    21495, Batch Loss:     0.052680, Batch Acc: 0.997624, Tokens per Sec:     3825, Lr: 0.000122\r\n",
      "2024-08-27 16:16:10,247 - INFO - joeynmt.training - Epoch 226, Step:    21500, Batch Loss:     0.061419, Batch Acc: 0.996452, Tokens per Sec:     3830, Lr: 0.000122\r\n",
      "2024-08-27 16:16:10,248 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=21542\r\n",
      "2024-08-27 16:16:10,248 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 65.51it/s]\r\n",
      "2024-08-27 16:16:32,538 - INFO - joeynmt.prediction - Generation took 22.2885[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 41382.64 examples/s]\r\n",
      "2024-08-27 16:16:32,928 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:16:32,929 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.51, loss:   6.79, ppl: 889.36, acc:   0.28, 0.1748[sec]\r\n",
      "2024-08-27 16:16:33,242 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/21500.ckpt.\r\n",
      "2024-08-27 16:16:33,243 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/18500.ckpt\r\n",
      "2024-08-27 16:16:33,268 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:16:33,416 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:16:33,416 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:16:33,416 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 16:16:33,707 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:16:33,852 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:16:33,853 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:16:33,853 - INFO - joeynmt.training - \tHypothesis: trois éléphants\r\n",
      "2024-08-27 16:16:34,145 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:16:34,290 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:16:34,290 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:16:34,291 - INFO - joeynmt.training - \tHypothesis: le service passagers n'est pas en activité actuellement\r\n",
      "2024-08-27 16:16:34,583 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:16:34,727 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:16:34,728 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:16:34,728 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 16:16:35,709 - INFO - joeynmt.training - Epoch 226, total training loss: 5.79, num. of seqs: 8065, num. of tokens: 80463, 21.0317[sec]\r\n",
      "2024-08-27 16:16:35,709 - INFO - joeynmt.training - EPOCH 227\r\n",
      "2024-08-27 16:16:36,143 - INFO - joeynmt.training - Epoch 227, Step:    21505, Batch Loss:     0.062252, Batch Acc: 0.992202, Tokens per Sec:     3884, Lr: 0.000122\r\n",
      "2024-08-27 16:16:37,245 - INFO - joeynmt.training - Epoch 227, Step:    21510, Batch Loss:     0.055120, Batch Acc: 0.995807, Tokens per Sec:     3895, Lr: 0.000122\r\n",
      "2024-08-27 16:16:38,325 - INFO - joeynmt.training - Epoch 227, Step:    21515, Batch Loss:     0.048779, Batch Acc: 0.997061, Tokens per Sec:     4098, Lr: 0.000122\r\n",
      "2024-08-27 16:16:39,425 - INFO - joeynmt.training - Epoch 227, Step:    21520, Batch Loss:     0.056246, Batch Acc: 0.995287, Tokens per Sec:     3861, Lr: 0.000122\r\n",
      "2024-08-27 16:16:40,566 - INFO - joeynmt.training - Epoch 227, Step:    21525, Batch Loss:     0.064247, Batch Acc: 0.996508, Tokens per Sec:     3767, Lr: 0.000122\r\n",
      "2024-08-27 16:16:41,658 - INFO - joeynmt.training - Epoch 227, Step:    21530, Batch Loss:     0.062915, Batch Acc: 0.995383, Tokens per Sec:     3771, Lr: 0.000122\r\n",
      "2024-08-27 16:16:42,763 - INFO - joeynmt.training - Epoch 227, Step:    21535, Batch Loss:     0.053818, Batch Acc: 0.996818, Tokens per Sec:     3701, Lr: 0.000122\r\n",
      "2024-08-27 16:16:43,863 - INFO - joeynmt.training - Epoch 227, Step:    21540, Batch Loss:     0.057971, Batch Acc: 0.995387, Tokens per Sec:     3745, Lr: 0.000122\r\n",
      "2024-08-27 16:16:44,951 - INFO - joeynmt.training - Epoch 227, Step:    21545, Batch Loss:     0.072513, Batch Acc: 0.995531, Tokens per Sec:     3705, Lr: 0.000122\r\n",
      "2024-08-27 16:16:46,028 - INFO - joeynmt.training - Epoch 227, Step:    21550, Batch Loss:     0.054477, Batch Acc: 0.995005, Tokens per Sec:     3906, Lr: 0.000122\r\n",
      "2024-08-27 16:16:47,152 - INFO - joeynmt.training - Epoch 227, Step:    21555, Batch Loss:     0.063033, Batch Acc: 0.993347, Tokens per Sec:     3880, Lr: 0.000122\r\n",
      "2024-08-27 16:16:48,225 - INFO - joeynmt.training - Epoch 227, Step:    21560, Batch Loss:     0.062188, Batch Acc: 0.996249, Tokens per Sec:     3977, Lr: 0.000122\r\n",
      "2024-08-27 16:16:49,332 - INFO - joeynmt.training - Epoch 227, Step:    21565, Batch Loss:     0.063126, Batch Acc: 0.995923, Tokens per Sec:     3768, Lr: 0.000122\r\n",
      "2024-08-27 16:16:50,517 - INFO - joeynmt.training - Epoch 227, Step:    21570, Batch Loss:     0.060103, Batch Acc: 0.996232, Tokens per Sec:     3586, Lr: 0.000122\r\n",
      "2024-08-27 16:16:51,622 - INFO - joeynmt.training - Epoch 227, Step:    21575, Batch Loss:     0.061639, Batch Acc: 0.996616, Tokens per Sec:     3748, Lr: 0.000122\r\n",
      "2024-08-27 16:16:52,693 - INFO - joeynmt.training - Epoch 227, Step:    21580, Batch Loss:     0.063508, Batch Acc: 0.995921, Tokens per Sec:     3893, Lr: 0.000122\r\n",
      "2024-08-27 16:16:53,755 - INFO - joeynmt.training - Epoch 227, Step:    21585, Batch Loss:     0.058879, Batch Acc: 0.993968, Tokens per Sec:     4061, Lr: 0.000122\r\n",
      "2024-08-27 16:16:54,831 - INFO - joeynmt.training - Epoch 227, Step:    21590, Batch Loss:     0.062667, Batch Acc: 0.993184, Tokens per Sec:     3956, Lr: 0.000122\r\n",
      "2024-08-27 16:16:55,913 - INFO - joeynmt.training - Epoch 227, Step:    21595, Batch Loss:     0.060916, Batch Acc: 0.996443, Tokens per Sec:     4162, Lr: 0.000122\r\n",
      "2024-08-27 16:16:56,648 - INFO - joeynmt.training - Epoch 227, total training loss: 5.66, num. of seqs: 8065, num. of tokens: 80463, 20.9220[sec]\r\n",
      "2024-08-27 16:16:56,648 - INFO - joeynmt.training - EPOCH 228\r\n",
      "2024-08-27 16:16:57,096 - INFO - joeynmt.training - Epoch 228, Step:    21600, Batch Loss:     0.060730, Batch Acc: 0.996906, Tokens per Sec:     3647, Lr: 0.000122\r\n",
      "2024-08-27 16:16:58,172 - INFO - joeynmt.training - Epoch 228, Step:    21605, Batch Loss:     0.063538, Batch Acc: 0.995625, Tokens per Sec:     4035, Lr: 0.000122\r\n",
      "2024-08-27 16:16:59,237 - INFO - joeynmt.training - Epoch 228, Step:    21610, Batch Loss:     0.068326, Batch Acc: 0.995212, Tokens per Sec:     3928, Lr: 0.000122\r\n",
      "2024-08-27 16:17:00,293 - INFO - joeynmt.training - Epoch 228, Step:    21615, Batch Loss:     0.063864, Batch Acc: 0.996325, Tokens per Sec:     4122, Lr: 0.000122\r\n",
      "2024-08-27 16:17:01,411 - INFO - joeynmt.training - Epoch 228, Step:    21620, Batch Loss:     0.058900, Batch Acc: 0.997389, Tokens per Sec:     3771, Lr: 0.000122\r\n",
      "2024-08-27 16:17:02,487 - INFO - joeynmt.training - Epoch 228, Step:    21625, Batch Loss:     0.065474, Batch Acc: 0.995445, Tokens per Sec:     3881, Lr: 0.000122\r\n",
      "2024-08-27 16:17:03,558 - INFO - joeynmt.training - Epoch 228, Step:    21630, Batch Loss:     0.055401, Batch Acc: 0.995366, Tokens per Sec:     4030, Lr: 0.000122\r\n",
      "2024-08-27 16:17:04,633 - INFO - joeynmt.training - Epoch 228, Step:    21635, Batch Loss:     0.061989, Batch Acc: 0.996482, Tokens per Sec:     3705, Lr: 0.000122\r\n",
      "2024-08-27 16:17:05,702 - INFO - joeynmt.training - Epoch 228, Step:    21640, Batch Loss:     0.057856, Batch Acc: 0.995773, Tokens per Sec:     3987, Lr: 0.000122\r\n",
      "2024-08-27 16:17:06,837 - INFO - joeynmt.training - Epoch 228, Step:    21645, Batch Loss:     0.055928, Batch Acc: 0.995410, Tokens per Sec:     3839, Lr: 0.000122\r\n",
      "2024-08-27 16:17:07,905 - INFO - joeynmt.training - Epoch 228, Step:    21650, Batch Loss:     0.054213, Batch Acc: 0.996924, Tokens per Sec:     3962, Lr: 0.000122\r\n",
      "2024-08-27 16:17:08,970 - INFO - joeynmt.training - Epoch 228, Step:    21655, Batch Loss:     0.076600, Batch Acc: 0.993553, Tokens per Sec:     3787, Lr: 0.000122\r\n",
      "2024-08-27 16:17:10,041 - INFO - joeynmt.training - Epoch 228, Step:    21660, Batch Loss:     0.053874, Batch Acc: 0.997914, Tokens per Sec:     4033, Lr: 0.000122\r\n",
      "2024-08-27 16:17:11,112 - INFO - joeynmt.training - Epoch 228, Step:    21665, Batch Loss:     0.059760, Batch Acc: 0.995441, Tokens per Sec:     3686, Lr: 0.000122\r\n",
      "2024-08-27 16:17:12,198 - INFO - joeynmt.training - Epoch 228, Step:    21670, Batch Loss:     0.055132, Batch Acc: 0.995550, Tokens per Sec:     3935, Lr: 0.000122\r\n",
      "2024-08-27 16:17:13,262 - INFO - joeynmt.training - Epoch 228, Step:    21675, Batch Loss:     0.061840, Batch Acc: 0.996053, Tokens per Sec:     4051, Lr: 0.000122\r\n",
      "2024-08-27 16:17:14,316 - INFO - joeynmt.training - Epoch 228, Step:    21680, Batch Loss:     0.066820, Batch Acc: 0.995194, Tokens per Sec:     3752, Lr: 0.000121\r\n",
      "2024-08-27 16:17:15,372 - INFO - joeynmt.training - Epoch 228, Step:    21685, Batch Loss:     0.062623, Batch Acc: 0.995690, Tokens per Sec:     3955, Lr: 0.000121\r\n",
      "2024-08-27 16:17:16,432 - INFO - joeynmt.training - Epoch 228, Step:    21690, Batch Loss:     0.059199, Batch Acc: 0.995988, Tokens per Sec:     4002, Lr: 0.000121\r\n",
      "2024-08-27 16:17:17,329 - INFO - joeynmt.training - Epoch 228, total training loss: 5.72, num. of seqs: 8065, num. of tokens: 80463, 20.6650[sec]\r\n",
      "2024-08-27 16:17:17,330 - INFO - joeynmt.training - EPOCH 229\r\n",
      "2024-08-27 16:17:17,553 - INFO - joeynmt.training - Epoch 229, Step:    21695, Batch Loss:     0.065995, Batch Acc: 0.996359, Tokens per Sec:     3748, Lr: 0.000121\r\n",
      "2024-08-27 16:17:18,639 - INFO - joeynmt.training - Epoch 229, Step:    21700, Batch Loss:     0.052093, Batch Acc: 0.994698, Tokens per Sec:     3999, Lr: 0.000121\r\n",
      "2024-08-27 16:17:19,713 - INFO - joeynmt.training - Epoch 229, Step:    21705, Batch Loss:     0.052048, Batch Acc: 0.997157, Tokens per Sec:     3932, Lr: 0.000121\r\n",
      "2024-08-27 16:17:20,819 - INFO - joeynmt.training - Epoch 229, Step:    21710, Batch Loss:     0.049779, Batch Acc: 0.996366, Tokens per Sec:     3733, Lr: 0.000121\r\n",
      "2024-08-27 16:17:22,002 - INFO - joeynmt.training - Epoch 229, Step:    21715, Batch Loss:     0.054733, Batch Acc: 0.995236, Tokens per Sec:     3552, Lr: 0.000121\r\n",
      "2024-08-27 16:17:23,099 - INFO - joeynmt.training - Epoch 229, Step:    21720, Batch Loss:     0.054834, Batch Acc: 0.996682, Tokens per Sec:     3849, Lr: 0.000121\r\n",
      "2024-08-27 16:17:24,179 - INFO - joeynmt.training - Epoch 229, Step:    21725, Batch Loss:     0.065001, Batch Acc: 0.995853, Tokens per Sec:     3798, Lr: 0.000121\r\n",
      "2024-08-27 16:17:25,242 - INFO - joeynmt.training - Epoch 229, Step:    21730, Batch Loss:     0.055898, Batch Acc: 0.996934, Tokens per Sec:     3991, Lr: 0.000121\r\n",
      "2024-08-27 16:17:26,303 - INFO - joeynmt.training - Epoch 229, Step:    21735, Batch Loss:     0.058196, Batch Acc: 0.996397, Tokens per Sec:     3925, Lr: 0.000121\r\n",
      "2024-08-27 16:17:27,426 - INFO - joeynmt.training - Epoch 229, Step:    21740, Batch Loss:     0.057548, Batch Acc: 0.993945, Tokens per Sec:     3828, Lr: 0.000121\r\n",
      "2024-08-27 16:17:28,505 - INFO - joeynmt.training - Epoch 229, Step:    21745, Batch Loss:     0.056027, Batch Acc: 0.994812, Tokens per Sec:     3574, Lr: 0.000121\r\n",
      "2024-08-27 16:17:29,588 - INFO - joeynmt.training - Epoch 229, Step:    21750, Batch Loss:     0.058653, Batch Acc: 0.995943, Tokens per Sec:     3873, Lr: 0.000121\r\n",
      "2024-08-27 16:17:30,660 - INFO - joeynmt.training - Epoch 229, Step:    21755, Batch Loss:     0.053772, Batch Acc: 0.995252, Tokens per Sec:     3931, Lr: 0.000121\r\n",
      "2024-08-27 16:17:31,728 - INFO - joeynmt.training - Epoch 229, Step:    21760, Batch Loss:     0.063765, Batch Acc: 0.995867, Tokens per Sec:     3854, Lr: 0.000121\r\n",
      "2024-08-27 16:17:32,800 - INFO - joeynmt.training - Epoch 229, Step:    21765, Batch Loss:     0.052687, Batch Acc: 0.995455, Tokens per Sec:     3899, Lr: 0.000121\r\n",
      "2024-08-27 16:17:33,864 - INFO - joeynmt.training - Epoch 229, Step:    21770, Batch Loss:     0.059171, Batch Acc: 0.996132, Tokens per Sec:     3891, Lr: 0.000121\r\n",
      "2024-08-27 16:17:34,964 - INFO - joeynmt.training - Epoch 229, Step:    21775, Batch Loss:     0.059004, Batch Acc: 0.995813, Tokens per Sec:     3911, Lr: 0.000121\r\n",
      "2024-08-27 16:17:36,032 - INFO - joeynmt.training - Epoch 229, Step:    21780, Batch Loss:     0.059356, Batch Acc: 0.996193, Tokens per Sec:     4183, Lr: 0.000121\r\n",
      "2024-08-27 16:17:37,118 - INFO - joeynmt.training - Epoch 229, Step:    21785, Batch Loss:     0.061826, Batch Acc: 0.995916, Tokens per Sec:     3610, Lr: 0.000121\r\n",
      "2024-08-27 16:17:38,185 - INFO - joeynmt.training - Epoch 229, Step:    21790, Batch Loss:     0.061403, Batch Acc: 0.992645, Tokens per Sec:     3955, Lr: 0.000121\r\n",
      "2024-08-27 16:17:38,234 - INFO - joeynmt.training - Epoch 229, total training loss: 5.76, num. of seqs: 8065, num. of tokens: 80463, 20.8872[sec]\r\n",
      "2024-08-27 16:17:38,235 - INFO - joeynmt.training - EPOCH 230\r\n",
      "2024-08-27 16:17:39,315 - INFO - joeynmt.training - Epoch 230, Step:    21795, Batch Loss:     0.058535, Batch Acc: 0.995065, Tokens per Sec:     3954, Lr: 0.000121\r\n",
      "2024-08-27 16:17:40,391 - INFO - joeynmt.training - Epoch 230, Step:    21800, Batch Loss:     0.065323, Batch Acc: 0.995597, Tokens per Sec:     4012, Lr: 0.000121\r\n",
      "2024-08-27 16:17:41,455 - INFO - joeynmt.training - Epoch 230, Step:    21805, Batch Loss:     0.058806, Batch Acc: 0.996040, Tokens per Sec:     4038, Lr: 0.000121\r\n",
      "2024-08-27 16:17:42,527 - INFO - joeynmt.training - Epoch 230, Step:    21810, Batch Loss:     0.056415, Batch Acc: 0.995651, Tokens per Sec:     4078, Lr: 0.000121\r\n",
      "2024-08-27 16:17:43,603 - INFO - joeynmt.training - Epoch 230, Step:    21815, Batch Loss:     0.059592, Batch Acc: 0.995610, Tokens per Sec:     3603, Lr: 0.000121\r\n",
      "2024-08-27 16:17:44,718 - INFO - joeynmt.training - Epoch 230, Step:    21820, Batch Loss:     0.056175, Batch Acc: 0.995768, Tokens per Sec:     3817, Lr: 0.000121\r\n",
      "2024-08-27 16:17:45,782 - INFO - joeynmt.training - Epoch 230, Step:    21825, Batch Loss:     0.056827, Batch Acc: 0.995640, Tokens per Sec:     4096, Lr: 0.000121\r\n",
      "2024-08-27 16:17:46,871 - INFO - joeynmt.training - Epoch 230, Step:    21830, Batch Loss:     0.052228, Batch Acc: 0.995557, Tokens per Sec:     3723, Lr: 0.000121\r\n",
      "2024-08-27 16:17:47,968 - INFO - joeynmt.training - Epoch 230, Step:    21835, Batch Loss:     0.058717, Batch Acc: 0.995533, Tokens per Sec:     3880, Lr: 0.000121\r\n",
      "2024-08-27 16:17:49,055 - INFO - joeynmt.training - Epoch 230, Step:    21840, Batch Loss:     0.061929, Batch Acc: 0.995798, Tokens per Sec:     3944, Lr: 0.000121\r\n",
      "2024-08-27 16:17:50,143 - INFO - joeynmt.training - Epoch 230, Step:    21845, Batch Loss:     0.056669, Batch Acc: 0.996819, Tokens per Sec:     4048, Lr: 0.000121\r\n",
      "2024-08-27 16:17:51,204 - INFO - joeynmt.training - Epoch 230, Step:    21850, Batch Loss:     0.054204, Batch Acc: 0.996152, Tokens per Sec:     3919, Lr: 0.000121\r\n",
      "2024-08-27 16:17:52,438 - INFO - joeynmt.training - Epoch 230, Step:    21855, Batch Loss:     0.070535, Batch Acc: 0.995092, Tokens per Sec:     3304, Lr: 0.000121\r\n",
      "2024-08-27 16:17:53,711 - INFO - joeynmt.training - Epoch 230, Step:    21860, Batch Loss:     0.067112, Batch Acc: 0.995073, Tokens per Sec:     3192, Lr: 0.000121\r\n",
      "2024-08-27 16:17:54,808 - INFO - joeynmt.training - Epoch 230, Step:    21865, Batch Loss:     0.063284, Batch Acc: 0.996415, Tokens per Sec:     3814, Lr: 0.000121\r\n",
      "2024-08-27 16:17:55,901 - INFO - joeynmt.training - Epoch 230, Step:    21870, Batch Loss:     0.058637, Batch Acc: 0.996436, Tokens per Sec:     3855, Lr: 0.000121\r\n",
      "2024-08-27 16:17:57,063 - INFO - joeynmt.training - Epoch 230, Step:    21875, Batch Loss:     0.058566, Batch Acc: 0.994096, Tokens per Sec:     3938, Lr: 0.000121\r\n",
      "2024-08-27 16:17:58,130 - INFO - joeynmt.training - Epoch 230, Step:    21880, Batch Loss:     0.067572, Batch Acc: 0.995224, Tokens per Sec:     3928, Lr: 0.000121\r\n",
      "2024-08-27 16:17:59,204 - INFO - joeynmt.training - Epoch 230, Step:    21885, Batch Loss:     0.058254, Batch Acc: 0.998314, Tokens per Sec:     3870, Lr: 0.000121\r\n",
      "2024-08-27 16:17:59,254 - INFO - joeynmt.training - Epoch 230, total training loss: 5.58, num. of seqs: 8065, num. of tokens: 80463, 21.0022[sec]\r\n",
      "2024-08-27 16:17:59,254 - INFO - joeynmt.training - EPOCH 231\r\n",
      "2024-08-27 16:18:00,337 - INFO - joeynmt.training - Epoch 231, Step:    21890, Batch Loss:     0.054914, Batch Acc: 0.994478, Tokens per Sec:     3860, Lr: 0.000121\r\n",
      "2024-08-27 16:18:01,418 - INFO - joeynmt.training - Epoch 231, Step:    21895, Batch Loss:     0.055303, Batch Acc: 0.996305, Tokens per Sec:     4010, Lr: 0.000121\r\n",
      "2024-08-27 16:18:02,514 - INFO - joeynmt.training - Epoch 231, Step:    21900, Batch Loss:     0.055765, Batch Acc: 0.995603, Tokens per Sec:     3736, Lr: 0.000121\r\n",
      "2024-08-27 16:18:03,606 - INFO - joeynmt.training - Epoch 231, Step:    21905, Batch Loss:     0.063125, Batch Acc: 0.995324, Tokens per Sec:     3920, Lr: 0.000121\r\n",
      "2024-08-27 16:18:04,698 - INFO - joeynmt.training - Epoch 231, Step:    21910, Batch Loss:     0.059235, Batch Acc: 0.994744, Tokens per Sec:     4010, Lr: 0.000121\r\n",
      "2024-08-27 16:18:05,757 - INFO - joeynmt.training - Epoch 231, Step:    21915, Batch Loss:     0.054817, Batch Acc: 0.995654, Tokens per Sec:     3696, Lr: 0.000121\r\n",
      "2024-08-27 16:18:06,861 - INFO - joeynmt.training - Epoch 231, Step:    21920, Batch Loss:     0.050352, Batch Acc: 0.997611, Tokens per Sec:     3792, Lr: 0.000121\r\n",
      "2024-08-27 16:18:07,929 - INFO - joeynmt.training - Epoch 231, Step:    21925, Batch Loss:     0.056702, Batch Acc: 0.995551, Tokens per Sec:     3793, Lr: 0.000121\r\n",
      "2024-08-27 16:18:09,004 - INFO - joeynmt.training - Epoch 231, Step:    21930, Batch Loss:     0.052474, Batch Acc: 0.995227, Tokens per Sec:     3705, Lr: 0.000121\r\n",
      "2024-08-27 16:18:10,062 - INFO - joeynmt.training - Epoch 231, Step:    21935, Batch Loss:     0.061060, Batch Acc: 0.994896, Tokens per Sec:     4074, Lr: 0.000121\r\n",
      "2024-08-27 16:18:11,122 - INFO - joeynmt.training - Epoch 231, Step:    21940, Batch Loss:     0.059892, Batch Acc: 0.996021, Tokens per Sec:     4032, Lr: 0.000121\r\n",
      "2024-08-27 16:18:12,197 - INFO - joeynmt.training - Epoch 231, Step:    21945, Batch Loss:     0.056099, Batch Acc: 0.995459, Tokens per Sec:     3896, Lr: 0.000121\r\n",
      "2024-08-27 16:18:13,265 - INFO - joeynmt.training - Epoch 231, Step:    21950, Batch Loss:     0.056863, Batch Acc: 0.996473, Tokens per Sec:     3986, Lr: 0.000121\r\n",
      "2024-08-27 16:18:14,335 - INFO - joeynmt.training - Epoch 231, Step:    21955, Batch Loss:     0.052425, Batch Acc: 0.996483, Tokens per Sec:     3987, Lr: 0.000121\r\n",
      "2024-08-27 16:18:15,403 - INFO - joeynmt.training - Epoch 231, Step:    21960, Batch Loss:     0.058794, Batch Acc: 0.993514, Tokens per Sec:     3899, Lr: 0.000121\r\n",
      "2024-08-27 16:18:16,467 - INFO - joeynmt.training - Epoch 231, Step:    21965, Batch Loss:     0.049000, Batch Acc: 0.995397, Tokens per Sec:     4086, Lr: 0.000121\r\n",
      "2024-08-27 16:18:17,570 - INFO - joeynmt.training - Epoch 231, Step:    21970, Batch Loss:     0.058418, Batch Acc: 0.994435, Tokens per Sec:     3911, Lr: 0.000121\r\n",
      "2024-08-27 16:18:18,633 - INFO - joeynmt.training - Epoch 231, Step:    21975, Batch Loss:     0.055216, Batch Acc: 0.995626, Tokens per Sec:     4090, Lr: 0.000121\r\n",
      "2024-08-27 16:18:19,714 - INFO - joeynmt.training - Epoch 231, Step:    21980, Batch Loss:     0.060114, Batch Acc: 0.995913, Tokens per Sec:     3851, Lr: 0.000121\r\n",
      "2024-08-27 16:18:19,818 - INFO - joeynmt.training - Epoch 231, total training loss: 5.63, num. of seqs: 8065, num. of tokens: 80463, 20.5480[sec]\r\n",
      "2024-08-27 16:18:19,818 - INFO - joeynmt.training - EPOCH 232\r\n",
      "2024-08-27 16:18:20,906 - INFO - joeynmt.training - Epoch 232, Step:    21985, Batch Loss:     0.058508, Batch Acc: 0.995905, Tokens per Sec:     3830, Lr: 0.000121\r\n",
      "2024-08-27 16:18:21,990 - INFO - joeynmt.training - Epoch 232, Step:    21990, Batch Loss:     0.060844, Batch Acc: 0.996305, Tokens per Sec:     3746, Lr: 0.000121\r\n",
      "2024-08-27 16:18:23,069 - INFO - joeynmt.training - Epoch 232, Step:    21995, Batch Loss:     0.058476, Batch Acc: 0.997640, Tokens per Sec:     3929, Lr: 0.000121\r\n",
      "2024-08-27 16:18:24,285 - INFO - joeynmt.training - Epoch 232, Step:    22000, Batch Loss:     0.057947, Batch Acc: 0.995290, Tokens per Sec:     3493, Lr: 0.000121\r\n",
      "2024-08-27 16:18:24,286 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=22042\r\n",
      "2024-08-27 16:18:24,286 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 66.35it/s]\r\n",
      "2024-08-27 16:18:46,292 - INFO - joeynmt.prediction - Generation took 22.0039[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44342.86 examples/s]\r\n",
      "2024-08-27 16:18:46,680 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:18:46,680 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.77, loss:   6.73, ppl: 836.42, acc:   0.29, 0.1726[sec]\r\n",
      "2024-08-27 16:18:46,681 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\r\n",
      "2024-08-27 16:18:47,006 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/22000.ckpt.\r\n",
      "2024-08-27 16:18:47,007 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/21500.ckpt\r\n",
      "2024-08-27 16:18:47,032 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:18:47,179 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:18:47,180 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:18:47,180 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 16:18:47,528 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:18:47,676 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:18:47,676 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:18:47,676 - INFO - joeynmt.training - \tHypothesis: trois point d'avance\r\n",
      "2024-08-27 16:18:47,997 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:18:48,146 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:18:48,146 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:18:48,146 - INFO - joeynmt.training - \tHypothesis: cependant l'ancienne lentille et l'équipement d'éclairage restent en place\r\n",
      "2024-08-27 16:18:48,461 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:18:48,609 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:18:48,609 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:18:48,609 - INFO - joeynmt.training - \tHypothesis: un bout\r\n",
      "2024-08-27 16:18:50,014 - INFO - joeynmt.training - Epoch 232, Step:    22005, Batch Loss:     0.057559, Batch Acc: 0.995625, Tokens per Sec:     3964, Lr: 0.000121\r\n",
      "2024-08-27 16:18:51,110 - INFO - joeynmt.training - Epoch 232, Step:    22010, Batch Loss:     0.052100, Batch Acc: 0.994991, Tokens per Sec:     4008, Lr: 0.000121\r\n",
      "2024-08-27 16:18:52,188 - INFO - joeynmt.training - Epoch 232, Step:    22015, Batch Loss:     0.057010, Batch Acc: 0.994251, Tokens per Sec:     3876, Lr: 0.000121\r\n",
      "2024-08-27 16:18:53,245 - INFO - joeynmt.training - Epoch 232, Step:    22020, Batch Loss:     0.060697, Batch Acc: 0.994815, Tokens per Sec:     4016, Lr: 0.000121\r\n",
      "2024-08-27 16:18:54,324 - INFO - joeynmt.training - Epoch 232, Step:    22025, Batch Loss:     0.059187, Batch Acc: 0.995417, Tokens per Sec:     3847, Lr: 0.000121\r\n",
      "2024-08-27 16:18:55,567 - INFO - joeynmt.training - Epoch 232, Step:    22030, Batch Loss:     0.061837, Batch Acc: 0.994442, Tokens per Sec:     3475, Lr: 0.000121\r\n",
      "2024-08-27 16:18:56,727 - INFO - joeynmt.training - Epoch 232, Step:    22035, Batch Loss:     0.066536, Batch Acc: 0.993596, Tokens per Sec:     3638, Lr: 0.000121\r\n",
      "2024-08-27 16:18:57,803 - INFO - joeynmt.training - Epoch 232, Step:    22040, Batch Loss:     0.056350, Batch Acc: 0.996107, Tokens per Sec:     4061, Lr: 0.000120\r\n",
      "2024-08-27 16:18:58,886 - INFO - joeynmt.training - Epoch 232, Step:    22045, Batch Loss:     0.057470, Batch Acc: 0.996307, Tokens per Sec:     3752, Lr: 0.000120\r\n",
      "2024-08-27 16:19:00,000 - INFO - joeynmt.training - Epoch 232, Step:    22050, Batch Loss:     0.052282, Batch Acc: 0.993866, Tokens per Sec:     3808, Lr: 0.000120\r\n",
      "2024-08-27 16:19:01,112 - INFO - joeynmt.training - Epoch 232, Step:    22055, Batch Loss:     0.065757, Batch Acc: 0.994457, Tokens per Sec:     3897, Lr: 0.000120\r\n",
      "2024-08-27 16:19:02,215 - INFO - joeynmt.training - Epoch 232, Step:    22060, Batch Loss:     0.059465, Batch Acc: 0.993565, Tokens per Sec:     3949, Lr: 0.000120\r\n",
      "2024-08-27 16:19:03,338 - INFO - joeynmt.training - Epoch 232, Step:    22065, Batch Loss:     0.057453, Batch Acc: 0.995086, Tokens per Sec:     3624, Lr: 0.000120\r\n",
      "2024-08-27 16:19:04,469 - INFO - joeynmt.training - Epoch 232, Step:    22070, Batch Loss:     0.063246, Batch Acc: 0.996386, Tokens per Sec:     3673, Lr: 0.000120\r\n",
      "2024-08-27 16:19:05,641 - INFO - joeynmt.training - Epoch 232, Step:    22075, Batch Loss:     0.063196, Batch Acc: 0.995345, Tokens per Sec:     3667, Lr: 0.000120\r\n",
      "2024-08-27 16:19:05,688 - INFO - joeynmt.training - Epoch 232, total training loss: 5.64, num. of seqs: 8065, num. of tokens: 80463, 21.2199[sec]\r\n",
      "2024-08-27 16:19:05,688 - INFO - joeynmt.training - EPOCH 233\r\n",
      "2024-08-27 16:19:06,824 - INFO - joeynmt.training - Epoch 233, Step:    22080, Batch Loss:     0.054387, Batch Acc: 0.996072, Tokens per Sec:     3597, Lr: 0.000120\r\n",
      "2024-08-27 16:19:07,918 - INFO - joeynmt.training - Epoch 233, Step:    22085, Batch Loss:     0.060130, Batch Acc: 0.995280, Tokens per Sec:     3682, Lr: 0.000120\r\n",
      "2024-08-27 16:19:09,039 - INFO - joeynmt.training - Epoch 233, Step:    22090, Batch Loss:     0.063770, Batch Acc: 0.995485, Tokens per Sec:     3560, Lr: 0.000120\r\n",
      "2024-08-27 16:19:10,189 - INFO - joeynmt.training - Epoch 233, Step:    22095, Batch Loss:     0.064162, Batch Acc: 0.996547, Tokens per Sec:     3779, Lr: 0.000120\r\n",
      "2024-08-27 16:19:11,310 - INFO - joeynmt.training - Epoch 233, Step:    22100, Batch Loss:     0.060832, Batch Acc: 0.995986, Tokens per Sec:     3780, Lr: 0.000120\r\n",
      "2024-08-27 16:19:12,459 - INFO - joeynmt.training - Epoch 233, Step:    22105, Batch Loss:     0.061950, Batch Acc: 0.993775, Tokens per Sec:     3638, Lr: 0.000120\r\n",
      "2024-08-27 16:19:13,830 - INFO - joeynmt.training - Epoch 233, Step:    22110, Batch Loss:     0.061518, Batch Acc: 0.995114, Tokens per Sec:     3135, Lr: 0.000120\r\n",
      "2024-08-27 16:19:14,964 - INFO - joeynmt.training - Epoch 233, Step:    22115, Batch Loss:     0.062321, Batch Acc: 0.997277, Tokens per Sec:     3567, Lr: 0.000120\r\n",
      "2024-08-27 16:19:16,171 - INFO - joeynmt.training - Epoch 233, Step:    22120, Batch Loss:     0.065431, Batch Acc: 0.996161, Tokens per Sec:     3454, Lr: 0.000120\r\n",
      "2024-08-27 16:19:17,311 - INFO - joeynmt.training - Epoch 233, Step:    22125, Batch Loss:     0.057187, Batch Acc: 0.996639, Tokens per Sec:     3656, Lr: 0.000120\r\n",
      "2024-08-27 16:19:18,397 - INFO - joeynmt.training - Epoch 233, Step:    22130, Batch Loss:     0.052658, Batch Acc: 0.996535, Tokens per Sec:     3988, Lr: 0.000120\r\n",
      "2024-08-27 16:19:19,476 - INFO - joeynmt.training - Epoch 233, Step:    22135, Batch Loss:     0.066065, Batch Acc: 0.994799, Tokens per Sec:     3925, Lr: 0.000120\r\n",
      "2024-08-27 16:19:20,554 - INFO - joeynmt.training - Epoch 233, Step:    22140, Batch Loss:     0.060781, Batch Acc: 0.994923, Tokens per Sec:     3836, Lr: 0.000120\r\n",
      "2024-08-27 16:19:21,695 - INFO - joeynmt.training - Epoch 233, Step:    22145, Batch Loss:     0.060266, Batch Acc: 0.995434, Tokens per Sec:     3651, Lr: 0.000120\r\n",
      "2024-08-27 16:19:22,797 - INFO - joeynmt.training - Epoch 233, Step:    22150, Batch Loss:     0.066240, Batch Acc: 0.994151, Tokens per Sec:     3880, Lr: 0.000120\r\n",
      "2024-08-27 16:19:23,888 - INFO - joeynmt.training - Epoch 233, Step:    22155, Batch Loss:     0.060228, Batch Acc: 0.995683, Tokens per Sec:     3825, Lr: 0.000120\r\n",
      "2024-08-27 16:19:24,990 - INFO - joeynmt.training - Epoch 233, Step:    22160, Batch Loss:     0.054249, Batch Acc: 0.996779, Tokens per Sec:     3945, Lr: 0.000120\r\n",
      "2024-08-27 16:19:26,110 - INFO - joeynmt.training - Epoch 233, Step:    22165, Batch Loss:     0.058403, Batch Acc: 0.993888, Tokens per Sec:     3802, Lr: 0.000120\r\n",
      "2024-08-27 16:19:27,557 - INFO - joeynmt.training - Epoch 233, Step:    22170, Batch Loss:     0.051365, Batch Acc: 0.995039, Tokens per Sec:     2926, Lr: 0.000120\r\n",
      "2024-08-27 16:19:27,777 - INFO - joeynmt.training - Epoch 233, total training loss: 5.67, num. of seqs: 8065, num. of tokens: 80463, 22.0729[sec]\r\n",
      "2024-08-27 16:19:27,778 - INFO - joeynmt.training - EPOCH 234\r\n",
      "2024-08-27 16:19:28,660 - INFO - joeynmt.training - Epoch 234, Step:    22175, Batch Loss:     0.051532, Batch Acc: 0.994603, Tokens per Sec:     3793, Lr: 0.000120\r\n",
      "2024-08-27 16:19:29,759 - INFO - joeynmt.training - Epoch 234, Step:    22180, Batch Loss:     0.055550, Batch Acc: 0.997145, Tokens per Sec:     3829, Lr: 0.000120\r\n",
      "2024-08-27 16:19:30,859 - INFO - joeynmt.training - Epoch 234, Step:    22185, Batch Loss:     0.056605, Batch Acc: 0.995459, Tokens per Sec:     3806, Lr: 0.000120\r\n",
      "2024-08-27 16:19:31,980 - INFO - joeynmt.training - Epoch 234, Step:    22190, Batch Loss:     0.057976, Batch Acc: 0.996622, Tokens per Sec:     3700, Lr: 0.000120\r\n",
      "2024-08-27 16:19:33,096 - INFO - joeynmt.training - Epoch 234, Step:    22195, Batch Loss:     0.058028, Batch Acc: 0.997156, Tokens per Sec:     3783, Lr: 0.000120\r\n",
      "2024-08-27 16:19:34,210 - INFO - joeynmt.training - Epoch 234, Step:    22200, Batch Loss:     0.059573, Batch Acc: 0.996973, Tokens per Sec:     3560, Lr: 0.000120\r\n",
      "2024-08-27 16:19:35,343 - INFO - joeynmt.training - Epoch 234, Step:    22205, Batch Loss:     0.052538, Batch Acc: 0.993641, Tokens per Sec:     3752, Lr: 0.000120\r\n",
      "2024-08-27 16:19:36,476 - INFO - joeynmt.training - Epoch 234, Step:    22210, Batch Loss:     0.057887, Batch Acc: 0.995556, Tokens per Sec:     3975, Lr: 0.000120\r\n",
      "2024-08-27 16:19:37,611 - INFO - joeynmt.training - Epoch 234, Step:    22215, Batch Loss:     0.051151, Batch Acc: 0.995958, Tokens per Sec:     3707, Lr: 0.000120\r\n",
      "2024-08-27 16:19:38,676 - INFO - joeynmt.training - Epoch 234, Step:    22220, Batch Loss:     0.073368, Batch Acc: 0.995239, Tokens per Sec:     3948, Lr: 0.000120\r\n",
      "2024-08-27 16:19:39,740 - INFO - joeynmt.training - Epoch 234, Step:    22225, Batch Loss:     0.050977, Batch Acc: 0.997497, Tokens per Sec:     3758, Lr: 0.000120\r\n",
      "2024-08-27 16:19:40,806 - INFO - joeynmt.training - Epoch 234, Step:    22230, Batch Loss:     0.060012, Batch Acc: 0.995416, Tokens per Sec:     4094, Lr: 0.000120\r\n",
      "2024-08-27 16:19:41,900 - INFO - joeynmt.training - Epoch 234, Step:    22235, Batch Loss:     0.057250, Batch Acc: 0.993715, Tokens per Sec:     3931, Lr: 0.000120\r\n",
      "2024-08-27 16:19:42,988 - INFO - joeynmt.training - Epoch 234, Step:    22240, Batch Loss:     0.063953, Batch Acc: 0.996008, Tokens per Sec:     3686, Lr: 0.000120\r\n",
      "2024-08-27 16:19:44,074 - INFO - joeynmt.training - Epoch 234, Step:    22245, Batch Loss:     0.054679, Batch Acc: 0.996046, Tokens per Sec:     3959, Lr: 0.000120\r\n",
      "2024-08-27 16:19:45,136 - INFO - joeynmt.training - Epoch 234, Step:    22250, Batch Loss:     0.049978, Batch Acc: 0.995653, Tokens per Sec:     4119, Lr: 0.000120\r\n",
      "2024-08-27 16:19:46,202 - INFO - joeynmt.training - Epoch 234, Step:    22255, Batch Loss:     0.048560, Batch Acc: 0.995997, Tokens per Sec:     3987, Lr: 0.000120\r\n",
      "2024-08-27 16:19:47,315 - INFO - joeynmt.training - Epoch 234, Step:    22260, Batch Loss:     0.060039, Batch Acc: 0.996038, Tokens per Sec:     3857, Lr: 0.000120\r\n",
      "2024-08-27 16:19:48,385 - INFO - joeynmt.training - Epoch 234, Step:    22265, Batch Loss:     0.053785, Batch Acc: 0.995527, Tokens per Sec:     3973, Lr: 0.000120\r\n",
      "2024-08-27 16:19:48,696 - INFO - joeynmt.training - Epoch 234, total training loss: 5.51, num. of seqs: 8065, num. of tokens: 80463, 20.9006[sec]\r\n",
      "2024-08-27 16:19:48,696 - INFO - joeynmt.training - EPOCH 235\r\n",
      "2024-08-27 16:19:49,567 - INFO - joeynmt.training - Epoch 235, Step:    22270, Batch Loss:     0.051305, Batch Acc: 0.997362, Tokens per Sec:     3938, Lr: 0.000120\r\n",
      "2024-08-27 16:19:50,644 - INFO - joeynmt.training - Epoch 235, Step:    22275, Batch Loss:     0.052080, Batch Acc: 0.996831, Tokens per Sec:     3808, Lr: 0.000120\r\n",
      "2024-08-27 16:19:51,723 - INFO - joeynmt.training - Epoch 235, Step:    22280, Batch Loss:     0.065147, Batch Acc: 0.994746, Tokens per Sec:     3885, Lr: 0.000120\r\n",
      "2024-08-27 16:19:52,806 - INFO - joeynmt.training - Epoch 235, Step:    22285, Batch Loss:     0.053915, Batch Acc: 0.997125, Tokens per Sec:     3856, Lr: 0.000120\r\n",
      "2024-08-27 16:19:53,921 - INFO - joeynmt.training - Epoch 235, Step:    22290, Batch Loss:     0.054507, Batch Acc: 0.996617, Tokens per Sec:     3713, Lr: 0.000120\r\n",
      "2024-08-27 16:19:55,028 - INFO - joeynmt.training - Epoch 235, Step:    22295, Batch Loss:     0.055587, Batch Acc: 0.995335, Tokens per Sec:     3876, Lr: 0.000120\r\n",
      "2024-08-27 16:19:56,116 - INFO - joeynmt.training - Epoch 235, Step:    22300, Batch Loss:     0.057725, Batch Acc: 0.994080, Tokens per Sec:     3882, Lr: 0.000120\r\n",
      "2024-08-27 16:19:57,238 - INFO - joeynmt.training - Epoch 235, Step:    22305, Batch Loss:     0.058781, Batch Acc: 0.995483, Tokens per Sec:     3554, Lr: 0.000120\r\n",
      "2024-08-27 16:19:58,494 - INFO - joeynmt.training - Epoch 235, Step:    22310, Batch Loss:     0.054737, Batch Acc: 0.996667, Tokens per Sec:     3348, Lr: 0.000120\r\n",
      "2024-08-27 16:19:59,573 - INFO - joeynmt.training - Epoch 235, Step:    22315, Batch Loss:     0.053650, Batch Acc: 0.996045, Tokens per Sec:     3984, Lr: 0.000120\r\n",
      "2024-08-27 16:20:00,648 - INFO - joeynmt.training - Epoch 235, Step:    22320, Batch Loss:     0.061126, Batch Acc: 0.996173, Tokens per Sec:     3895, Lr: 0.000120\r\n",
      "2024-08-27 16:20:01,734 - INFO - joeynmt.training - Epoch 235, Step:    22325, Batch Loss:     0.056359, Batch Acc: 0.995911, Tokens per Sec:     3828, Lr: 0.000120\r\n",
      "2024-08-27 16:20:02,819 - INFO - joeynmt.training - Epoch 235, Step:    22330, Batch Loss:     0.048290, Batch Acc: 0.996409, Tokens per Sec:     4108, Lr: 0.000120\r\n",
      "2024-08-27 16:20:03,957 - INFO - joeynmt.training - Epoch 235, Step:    22335, Batch Loss:     0.063558, Batch Acc: 0.996578, Tokens per Sec:     3853, Lr: 0.000120\r\n",
      "2024-08-27 16:20:05,062 - INFO - joeynmt.training - Epoch 235, Step:    22340, Batch Loss:     0.056057, Batch Acc: 0.996982, Tokens per Sec:     3903, Lr: 0.000120\r\n",
      "2024-08-27 16:20:06,149 - INFO - joeynmt.training - Epoch 235, Step:    22345, Batch Loss:     0.056202, Batch Acc: 0.995491, Tokens per Sec:     3675, Lr: 0.000120\r\n",
      "2024-08-27 16:20:07,274 - INFO - joeynmt.training - Epoch 235, Step:    22350, Batch Loss:     0.059043, Batch Acc: 0.996388, Tokens per Sec:     3695, Lr: 0.000120\r\n",
      "2024-08-27 16:20:08,347 - INFO - joeynmt.training - Epoch 235, Step:    22355, Batch Loss:     0.061850, Batch Acc: 0.992572, Tokens per Sec:     3767, Lr: 0.000120\r\n",
      "2024-08-27 16:20:09,434 - INFO - joeynmt.training - Epoch 235, Step:    22360, Batch Loss:     0.056054, Batch Acc: 0.997051, Tokens per Sec:     4059, Lr: 0.000120\r\n",
      "2024-08-27 16:20:09,819 - INFO - joeynmt.training - Epoch 235, total training loss: 5.47, num. of seqs: 8065, num. of tokens: 80463, 21.1058[sec]\r\n",
      "2024-08-27 16:20:09,819 - INFO - joeynmt.training - EPOCH 236\r\n",
      "2024-08-27 16:20:10,696 - INFO - joeynmt.training - Epoch 236, Step:    22365, Batch Loss:     0.047698, Batch Acc: 0.997574, Tokens per Sec:     3779, Lr: 0.000120\r\n",
      "2024-08-27 16:20:11,764 - INFO - joeynmt.training - Epoch 236, Step:    22370, Batch Loss:     0.052811, Batch Acc: 0.996114, Tokens per Sec:     3618, Lr: 0.000120\r\n",
      "2024-08-27 16:20:12,853 - INFO - joeynmt.training - Epoch 236, Step:    22375, Batch Loss:     0.057901, Batch Acc: 0.994971, Tokens per Sec:     4019, Lr: 0.000120\r\n",
      "2024-08-27 16:20:13,943 - INFO - joeynmt.training - Epoch 236, Step:    22380, Batch Loss:     0.056579, Batch Acc: 0.997560, Tokens per Sec:     3763, Lr: 0.000120\r\n",
      "2024-08-27 16:20:15,037 - INFO - joeynmt.training - Epoch 236, Step:    22385, Batch Loss:     0.062326, Batch Acc: 0.994434, Tokens per Sec:     3780, Lr: 0.000120\r\n",
      "2024-08-27 16:20:16,158 - INFO - joeynmt.training - Epoch 236, Step:    22390, Batch Loss:     0.063082, Batch Acc: 0.995830, Tokens per Sec:     3854, Lr: 0.000120\r\n",
      "2024-08-27 16:20:17,549 - INFO - joeynmt.training - Epoch 236, Step:    22395, Batch Loss:     0.057637, Batch Acc: 0.994857, Tokens per Sec:     3078, Lr: 0.000120\r\n",
      "2024-08-27 16:20:18,653 - INFO - joeynmt.training - Epoch 236, Step:    22400, Batch Loss:     0.066986, Batch Acc: 0.995756, Tokens per Sec:     3844, Lr: 0.000120\r\n",
      "2024-08-27 16:20:19,778 - INFO - joeynmt.training - Epoch 236, Step:    22405, Batch Loss:     0.058410, Batch Acc: 0.996931, Tokens per Sec:     3766, Lr: 0.000120\r\n",
      "2024-08-27 16:20:20,903 - INFO - joeynmt.training - Epoch 236, Step:    22410, Batch Loss:     0.055196, Batch Acc: 0.994670, Tokens per Sec:     3839, Lr: 0.000119\r\n",
      "2024-08-27 16:20:22,003 - INFO - joeynmt.training - Epoch 236, Step:    22415, Batch Loss:     0.054876, Batch Acc: 0.996725, Tokens per Sec:     3890, Lr: 0.000119\r\n",
      "2024-08-27 16:20:23,101 - INFO - joeynmt.training - Epoch 236, Step:    22420, Batch Loss:     0.052352, Batch Acc: 0.993898, Tokens per Sec:     3880, Lr: 0.000119\r\n",
      "2024-08-27 16:20:24,199 - INFO - joeynmt.training - Epoch 236, Step:    22425, Batch Loss:     0.059505, Batch Acc: 0.995131, Tokens per Sec:     3933, Lr: 0.000119\r\n",
      "2024-08-27 16:20:25,294 - INFO - joeynmt.training - Epoch 236, Step:    22430, Batch Loss:     0.059912, Batch Acc: 0.996185, Tokens per Sec:     3592, Lr: 0.000119\r\n",
      "2024-08-27 16:20:26,400 - INFO - joeynmt.training - Epoch 236, Step:    22435, Batch Loss:     0.066974, Batch Acc: 0.994802, Tokens per Sec:     3828, Lr: 0.000119\r\n",
      "2024-08-27 16:20:27,543 - INFO - joeynmt.training - Epoch 236, Step:    22440, Batch Loss:     0.065261, Batch Acc: 0.995866, Tokens per Sec:     4025, Lr: 0.000119\r\n",
      "2024-08-27 16:20:28,654 - INFO - joeynmt.training - Epoch 236, Step:    22445, Batch Loss:     0.064734, Batch Acc: 0.995410, Tokens per Sec:     3923, Lr: 0.000119\r\n",
      "2024-08-27 16:20:30,071 - INFO - joeynmt.training - Epoch 236, Step:    22450, Batch Loss:     0.056010, Batch Acc: 0.995809, Tokens per Sec:     3033, Lr: 0.000119\r\n",
      "2024-08-27 16:20:31,182 - INFO - joeynmt.training - Epoch 236, Step:    22455, Batch Loss:     0.061168, Batch Acc: 0.996872, Tokens per Sec:     3743, Lr: 0.000119\r\n",
      "2024-08-27 16:20:31,454 - INFO - joeynmt.training - Epoch 236, total training loss: 5.54, num. of seqs: 8065, num. of tokens: 80463, 21.6166[sec]\r\n",
      "2024-08-27 16:20:31,454 - INFO - joeynmt.training - EPOCH 237\r\n",
      "2024-08-27 16:20:32,340 - INFO - joeynmt.training - Epoch 237, Step:    22460, Batch Loss:     0.057238, Batch Acc: 0.993038, Tokens per Sec:     3583, Lr: 0.000119\r\n",
      "2024-08-27 16:20:33,444 - INFO - joeynmt.training - Epoch 237, Step:    22465, Batch Loss:     0.056576, Batch Acc: 0.996486, Tokens per Sec:     3870, Lr: 0.000119\r\n",
      "2024-08-27 16:20:34,551 - INFO - joeynmt.training - Epoch 237, Step:    22470, Batch Loss:     0.055398, Batch Acc: 0.995588, Tokens per Sec:     3892, Lr: 0.000119\r\n",
      "2024-08-27 16:20:35,714 - INFO - joeynmt.training - Epoch 237, Step:    22475, Batch Loss:     0.058482, Batch Acc: 0.996741, Tokens per Sec:     3433, Lr: 0.000119\r\n",
      "2024-08-27 16:20:36,877 - INFO - joeynmt.training - Epoch 237, Step:    22480, Batch Loss:     0.052770, Batch Acc: 0.996728, Tokens per Sec:     3682, Lr: 0.000119\r\n",
      "2024-08-27 16:20:37,995 - INFO - joeynmt.training - Epoch 237, Step:    22485, Batch Loss:     0.060923, Batch Acc: 0.995921, Tokens per Sec:     3731, Lr: 0.000119\r\n",
      "2024-08-27 16:20:39,107 - INFO - joeynmt.training - Epoch 237, Step:    22490, Batch Loss:     0.058361, Batch Acc: 0.995272, Tokens per Sec:     3807, Lr: 0.000119\r\n",
      "2024-08-27 16:20:40,222 - INFO - joeynmt.training - Epoch 237, Step:    22495, Batch Loss:     0.056836, Batch Acc: 0.996471, Tokens per Sec:     3811, Lr: 0.000119\r\n",
      "2024-08-27 16:20:41,301 - INFO - joeynmt.training - Epoch 237, Step:    22500, Batch Loss:     0.054361, Batch Acc: 0.996530, Tokens per Sec:     4010, Lr: 0.000119\r\n",
      "2024-08-27 16:20:41,302 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=22542\r\n",
      "2024-08-27 16:20:41,302 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:23<00:00, 62.74it/s]\r\n",
      "2024-08-27 16:21:04,575 - INFO - joeynmt.prediction - Generation took 23.2701[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44609.61 examples/s]\r\n",
      "2024-08-27 16:21:04,959 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:21:04,959 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.42, loss:   6.72, ppl: 825.54, acc:   0.29, 0.1689[sec]\r\n",
      "2024-08-27 16:21:04,961 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:21:05,110 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:21:05,110 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:21:05,111 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 16:21:05,403 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:21:05,550 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:21:05,550 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:21:05,550 - INFO - joeynmt.training - \tHypothesis: trois éléphants\r\n",
      "2024-08-27 16:21:05,844 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:21:05,992 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:21:05,992 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:21:05,992 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 16:21:06,285 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:21:06,431 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:21:06,431 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:21:06,431 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 16:21:08,153 - INFO - joeynmt.training - Epoch 237, Step:    22505, Batch Loss:     0.048944, Batch Acc: 0.997602, Tokens per Sec:     3807, Lr: 0.000119\r\n",
      "2024-08-27 16:21:09,245 - INFO - joeynmt.training - Epoch 237, Step:    22510, Batch Loss:     0.055718, Batch Acc: 0.996545, Tokens per Sec:     3978, Lr: 0.000119\r\n",
      "2024-08-27 16:21:10,331 - INFO - joeynmt.training - Epoch 237, Step:    22515, Batch Loss:     0.066539, Batch Acc: 0.993793, Tokens per Sec:     4006, Lr: 0.000119\r\n",
      "2024-08-27 16:21:11,416 - INFO - joeynmt.training - Epoch 237, Step:    22520, Batch Loss:     0.056087, Batch Acc: 0.995817, Tokens per Sec:     3971, Lr: 0.000119\r\n",
      "2024-08-27 16:21:12,484 - INFO - joeynmt.training - Epoch 237, Step:    22525, Batch Loss:     0.060484, Batch Acc: 0.994978, Tokens per Sec:     3918, Lr: 0.000119\r\n",
      "2024-08-27 16:21:13,561 - INFO - joeynmt.training - Epoch 237, Step:    22530, Batch Loss:     0.064709, Batch Acc: 0.994918, Tokens per Sec:     3838, Lr: 0.000119\r\n",
      "2024-08-27 16:21:14,642 - INFO - joeynmt.training - Epoch 237, Step:    22535, Batch Loss:     0.052650, Batch Acc: 0.997205, Tokens per Sec:     3974, Lr: 0.000119\r\n",
      "2024-08-27 16:21:15,703 - INFO - joeynmt.training - Epoch 237, Step:    22540, Batch Loss:     0.063424, Batch Acc: 0.993691, Tokens per Sec:     3886, Lr: 0.000119\r\n",
      "2024-08-27 16:21:16,803 - INFO - joeynmt.training - Epoch 237, Step:    22545, Batch Loss:     0.063042, Batch Acc: 0.995515, Tokens per Sec:     3854, Lr: 0.000119\r\n",
      "2024-08-27 16:21:17,869 - INFO - joeynmt.training - Epoch 237, Step:    22550, Batch Loss:     0.059904, Batch Acc: 0.995955, Tokens per Sec:     3947, Lr: 0.000119\r\n",
      "2024-08-27 16:21:18,180 - INFO - joeynmt.training - Epoch 237, total training loss: 5.53, num. of seqs: 8065, num. of tokens: 80463, 20.9531[sec]\r\n",
      "2024-08-27 16:21:18,180 - INFO - joeynmt.training - EPOCH 238\r\n",
      "2024-08-27 16:21:19,026 - INFO - joeynmt.training - Epoch 238, Step:    22555, Batch Loss:     0.055100, Batch Acc: 0.995778, Tokens per Sec:     4223, Lr: 0.000119\r\n",
      "2024-08-27 16:21:20,102 - INFO - joeynmt.training - Epoch 238, Step:    22560, Batch Loss:     0.052329, Batch Acc: 0.995327, Tokens per Sec:     3779, Lr: 0.000119\r\n",
      "2024-08-27 16:21:21,174 - INFO - joeynmt.training - Epoch 238, Step:    22565, Batch Loss:     0.065430, Batch Acc: 0.996112, Tokens per Sec:     3842, Lr: 0.000119\r\n",
      "2024-08-27 16:21:22,259 - INFO - joeynmt.training - Epoch 238, Step:    22570, Batch Loss:     0.060479, Batch Acc: 0.993741, Tokens per Sec:     3832, Lr: 0.000119\r\n",
      "2024-08-27 16:21:23,336 - INFO - joeynmt.training - Epoch 238, Step:    22575, Batch Loss:     0.062701, Batch Acc: 0.996490, Tokens per Sec:     3970, Lr: 0.000119\r\n",
      "2024-08-27 16:21:24,430 - INFO - joeynmt.training - Epoch 238, Step:    22580, Batch Loss:     0.064013, Batch Acc: 0.995884, Tokens per Sec:     4001, Lr: 0.000119\r\n",
      "2024-08-27 16:21:25,507 - INFO - joeynmt.training - Epoch 238, Step:    22585, Batch Loss:     0.051819, Batch Acc: 0.997325, Tokens per Sec:     3819, Lr: 0.000119\r\n",
      "2024-08-27 16:21:26,582 - INFO - joeynmt.training - Epoch 238, Step:    22590, Batch Loss:     0.063310, Batch Acc: 0.996223, Tokens per Sec:     3943, Lr: 0.000119\r\n",
      "2024-08-27 16:21:27,685 - INFO - joeynmt.training - Epoch 238, Step:    22595, Batch Loss:     0.065492, Batch Acc: 0.996552, Tokens per Sec:     3946, Lr: 0.000119\r\n",
      "2024-08-27 16:21:28,778 - INFO - joeynmt.training - Epoch 238, Step:    22600, Batch Loss:     0.053481, Batch Acc: 0.996276, Tokens per Sec:     3932, Lr: 0.000119\r\n",
      "2024-08-27 16:21:29,860 - INFO - joeynmt.training - Epoch 238, Step:    22605, Batch Loss:     0.070959, Batch Acc: 0.994310, Tokens per Sec:     4067, Lr: 0.000119\r\n",
      "2024-08-27 16:21:30,922 - INFO - joeynmt.training - Epoch 238, Step:    22610, Batch Loss:     0.061574, Batch Acc: 0.996453, Tokens per Sec:     3984, Lr: 0.000119\r\n",
      "2024-08-27 16:21:32,051 - INFO - joeynmt.training - Epoch 238, Step:    22615, Batch Loss:     0.078184, Batch Acc: 0.992329, Tokens per Sec:     3812, Lr: 0.000119\r\n",
      "2024-08-27 16:21:33,431 - INFO - joeynmt.training - Epoch 238, Step:    22620, Batch Loss:     0.064052, Batch Acc: 0.994275, Tokens per Sec:     3166, Lr: 0.000119\r\n",
      "2024-08-27 16:21:34,516 - INFO - joeynmt.training - Epoch 238, Step:    22625, Batch Loss:     0.059465, Batch Acc: 0.996946, Tokens per Sec:     3621, Lr: 0.000119\r\n",
      "2024-08-27 16:21:35,597 - INFO - joeynmt.training - Epoch 238, Step:    22630, Batch Loss:     0.056248, Batch Acc: 0.996130, Tokens per Sec:     3826, Lr: 0.000119\r\n",
      "2024-08-27 16:21:36,666 - INFO - joeynmt.training - Epoch 238, Step:    22635, Batch Loss:     0.058024, Batch Acc: 0.996403, Tokens per Sec:     3905, Lr: 0.000119\r\n",
      "2024-08-27 16:21:37,819 - INFO - joeynmt.training - Epoch 238, Step:    22640, Batch Loss:     0.070619, Batch Acc: 0.995455, Tokens per Sec:     3628, Lr: 0.000119\r\n",
      "2024-08-27 16:21:38,941 - INFO - joeynmt.training - Epoch 238, Step:    22645, Batch Loss:     0.054441, Batch Acc: 0.994983, Tokens per Sec:     3732, Lr: 0.000119\r\n",
      "2024-08-27 16:21:39,259 - INFO - joeynmt.training - Epoch 238, total training loss: 5.57, num. of seqs: 8065, num. of tokens: 80463, 21.0626[sec]\r\n",
      "2024-08-27 16:21:39,260 - INFO - joeynmt.training - EPOCH 239\r\n",
      "2024-08-27 16:21:40,146 - INFO - joeynmt.training - Epoch 239, Step:    22650, Batch Loss:     0.062930, Batch Acc: 0.997316, Tokens per Sec:     3802, Lr: 0.000119\r\n",
      "2024-08-27 16:21:41,258 - INFO - joeynmt.training - Epoch 239, Step:    22655, Batch Loss:     0.056145, Batch Acc: 0.995989, Tokens per Sec:     4037, Lr: 0.000119\r\n",
      "2024-08-27 16:21:42,412 - INFO - joeynmt.training - Epoch 239, Step:    22660, Batch Loss:     0.047821, Batch Acc: 0.998090, Tokens per Sec:     3634, Lr: 0.000119\r\n",
      "2024-08-27 16:21:43,510 - INFO - joeynmt.training - Epoch 239, Step:    22665, Batch Loss:     0.053405, Batch Acc: 0.993419, Tokens per Sec:     3738, Lr: 0.000119\r\n",
      "2024-08-27 16:21:44,643 - INFO - joeynmt.training - Epoch 239, Step:    22670, Batch Loss:     0.059295, Batch Acc: 0.994819, Tokens per Sec:     3748, Lr: 0.000119\r\n",
      "2024-08-27 16:21:45,760 - INFO - joeynmt.training - Epoch 239, Step:    22675, Batch Loss:     0.054884, Batch Acc: 0.994978, Tokens per Sec:     3748, Lr: 0.000119\r\n",
      "2024-08-27 16:21:46,904 - INFO - joeynmt.training - Epoch 239, Step:    22680, Batch Loss:     0.054208, Batch Acc: 0.994452, Tokens per Sec:     3625, Lr: 0.000119\r\n",
      "2024-08-27 16:21:48,016 - INFO - joeynmt.training - Epoch 239, Step:    22685, Batch Loss:     0.056227, Batch Acc: 0.994753, Tokens per Sec:     3601, Lr: 0.000119\r\n",
      "2024-08-27 16:21:49,114 - INFO - joeynmt.training - Epoch 239, Step:    22690, Batch Loss:     0.064263, Batch Acc: 0.994546, Tokens per Sec:     3844, Lr: 0.000119\r\n",
      "2024-08-27 16:21:50,269 - INFO - joeynmt.training - Epoch 239, Step:    22695, Batch Loss:     0.065539, Batch Acc: 0.996395, Tokens per Sec:     3605, Lr: 0.000119\r\n",
      "2024-08-27 16:21:51,384 - INFO - joeynmt.training - Epoch 239, Step:    22700, Batch Loss:     0.067543, Batch Acc: 0.995514, Tokens per Sec:     3801, Lr: 0.000119\r\n",
      "2024-08-27 16:21:52,467 - INFO - joeynmt.training - Epoch 239, Step:    22705, Batch Loss:     0.061018, Batch Acc: 0.995052, Tokens per Sec:     3921, Lr: 0.000119\r\n",
      "2024-08-27 16:21:53,562 - INFO - joeynmt.training - Epoch 239, Step:    22710, Batch Loss:     0.060491, Batch Acc: 0.995353, Tokens per Sec:     3930, Lr: 0.000119\r\n",
      "2024-08-27 16:21:54,647 - INFO - joeynmt.training - Epoch 239, Step:    22715, Batch Loss:     0.061683, Batch Acc: 0.995788, Tokens per Sec:     3942, Lr: 0.000119\r\n",
      "2024-08-27 16:21:55,753 - INFO - joeynmt.training - Epoch 239, Step:    22720, Batch Loss:     0.056010, Batch Acc: 0.996471, Tokens per Sec:     3844, Lr: 0.000119\r\n",
      "2024-08-27 16:21:56,880 - INFO - joeynmt.training - Epoch 239, Step:    22725, Batch Loss:     0.065005, Batch Acc: 0.994517, Tokens per Sec:     3724, Lr: 0.000119\r\n",
      "2024-08-27 16:21:57,966 - INFO - joeynmt.training - Epoch 239, Step:    22730, Batch Loss:     0.061884, Batch Acc: 0.995073, Tokens per Sec:     3741, Lr: 0.000119\r\n",
      "2024-08-27 16:21:59,078 - INFO - joeynmt.training - Epoch 239, Step:    22735, Batch Loss:     0.059663, Batch Acc: 0.997035, Tokens per Sec:     3947, Lr: 0.000119\r\n",
      "2024-08-27 16:22:00,190 - INFO - joeynmt.training - Epoch 239, Step:    22740, Batch Loss:     0.066108, Batch Acc: 0.994947, Tokens per Sec:     3739, Lr: 0.000119\r\n",
      "2024-08-27 16:22:00,517 - INFO - joeynmt.training - Epoch 239, total training loss: 5.54, num. of seqs: 8065, num. of tokens: 80463, 21.2405[sec]\r\n",
      "2024-08-27 16:22:00,517 - INFO - joeynmt.training - EPOCH 240\r\n",
      "2024-08-27 16:22:01,427 - INFO - joeynmt.training - Epoch 240, Step:    22745, Batch Loss:     0.058545, Batch Acc: 0.997356, Tokens per Sec:     3760, Lr: 0.000119\r\n",
      "2024-08-27 16:22:02,503 - INFO - joeynmt.training - Epoch 240, Step:    22750, Batch Loss:     0.057567, Batch Acc: 0.996642, Tokens per Sec:     3877, Lr: 0.000119\r\n",
      "2024-08-27 16:22:03,577 - INFO - joeynmt.training - Epoch 240, Step:    22755, Batch Loss:     0.058091, Batch Acc: 0.996320, Tokens per Sec:     4049, Lr: 0.000119\r\n",
      "2024-08-27 16:22:04,670 - INFO - joeynmt.training - Epoch 240, Step:    22760, Batch Loss:     0.061437, Batch Acc: 0.995682, Tokens per Sec:     4242, Lr: 0.000119\r\n",
      "2024-08-27 16:22:05,747 - INFO - joeynmt.training - Epoch 240, Step:    22765, Batch Loss:     0.062345, Batch Acc: 0.997844, Tokens per Sec:     3878, Lr: 0.000119\r\n",
      "2024-08-27 16:22:06,861 - INFO - joeynmt.training - Epoch 240, Step:    22770, Batch Loss:     0.057667, Batch Acc: 0.995660, Tokens per Sec:     3932, Lr: 0.000119\r\n",
      "2024-08-27 16:22:07,969 - INFO - joeynmt.training - Epoch 240, Step:    22775, Batch Loss:     0.065639, Batch Acc: 0.996163, Tokens per Sec:     3766, Lr: 0.000119\r\n",
      "2024-08-27 16:22:09,062 - INFO - joeynmt.training - Epoch 240, Step:    22780, Batch Loss:     0.053428, Batch Acc: 0.996803, Tokens per Sec:     4007, Lr: 0.000119\r\n",
      "2024-08-27 16:22:10,123 - INFO - joeynmt.training - Epoch 240, Step:    22785, Batch Loss:     0.060274, Batch Acc: 0.995126, Tokens per Sec:     3871, Lr: 0.000119\r\n",
      "2024-08-27 16:22:11,190 - INFO - joeynmt.training - Epoch 240, Step:    22790, Batch Loss:     0.054971, Batch Acc: 0.995413, Tokens per Sec:     3883, Lr: 0.000118\r\n",
      "2024-08-27 16:22:12,240 - INFO - joeynmt.training - Epoch 240, Step:    22795, Batch Loss:     0.052460, Batch Acc: 0.996391, Tokens per Sec:     3960, Lr: 0.000118\r\n",
      "2024-08-27 16:22:13,308 - INFO - joeynmt.training - Epoch 240, Step:    22800, Batch Loss:     0.060796, Batch Acc: 0.996376, Tokens per Sec:     3879, Lr: 0.000118\r\n",
      "2024-08-27 16:22:14,375 - INFO - joeynmt.training - Epoch 240, Step:    22805, Batch Loss:     0.057301, Batch Acc: 0.996758, Tokens per Sec:     4048, Lr: 0.000118\r\n",
      "2024-08-27 16:22:15,447 - INFO - joeynmt.training - Epoch 240, Step:    22810, Batch Loss:     0.069805, Batch Acc: 0.995211, Tokens per Sec:     3900, Lr: 0.000118\r\n",
      "2024-08-27 16:22:16,518 - INFO - joeynmt.training - Epoch 240, Step:    22815, Batch Loss:     0.062680, Batch Acc: 0.996383, Tokens per Sec:     4131, Lr: 0.000118\r\n",
      "2024-08-27 16:22:17,594 - INFO - joeynmt.training - Epoch 240, Step:    22820, Batch Loss:     0.066564, Batch Acc: 0.995069, Tokens per Sec:     3772, Lr: 0.000118\r\n",
      "2024-08-27 16:22:18,662 - INFO - joeynmt.training - Epoch 240, Step:    22825, Batch Loss:     0.060023, Batch Acc: 0.995763, Tokens per Sec:     3983, Lr: 0.000118\r\n",
      "2024-08-27 16:22:19,723 - INFO - joeynmt.training - Epoch 240, Step:    22830, Batch Loss:     0.051131, Batch Acc: 0.996906, Tokens per Sec:     3960, Lr: 0.000118\r\n",
      "2024-08-27 16:22:20,787 - INFO - joeynmt.training - Epoch 240, Step:    22835, Batch Loss:     0.056096, Batch Acc: 0.995276, Tokens per Sec:     3984, Lr: 0.000118\r\n",
      "2024-08-27 16:22:20,940 - INFO - joeynmt.training - Epoch 240, total training loss: 5.41, num. of seqs: 8065, num. of tokens: 80463, 20.4065[sec]\r\n",
      "2024-08-27 16:22:20,941 - INFO - joeynmt.training - EPOCH 241\r\n",
      "2024-08-27 16:22:22,023 - INFO - joeynmt.training - Epoch 241, Step:    22840, Batch Loss:     0.061335, Batch Acc: 0.995207, Tokens per Sec:     3870, Lr: 0.000118\r\n",
      "2024-08-27 16:22:23,101 - INFO - joeynmt.training - Epoch 241, Step:    22845, Batch Loss:     0.052200, Batch Acc: 0.997060, Tokens per Sec:     4105, Lr: 0.000118\r\n",
      "2024-08-27 16:22:24,165 - INFO - joeynmt.training - Epoch 241, Step:    22850, Batch Loss:     0.060527, Batch Acc: 0.996622, Tokens per Sec:     3896, Lr: 0.000118\r\n",
      "2024-08-27 16:22:25,243 - INFO - joeynmt.training - Epoch 241, Step:    22855, Batch Loss:     0.051158, Batch Acc: 0.995237, Tokens per Sec:     4092, Lr: 0.000118\r\n",
      "2024-08-27 16:22:26,323 - INFO - joeynmt.training - Epoch 241, Step:    22860, Batch Loss:     0.050368, Batch Acc: 0.995903, Tokens per Sec:     4071, Lr: 0.000118\r\n",
      "2024-08-27 16:22:27,427 - INFO - joeynmt.training - Epoch 241, Step:    22865, Batch Loss:     0.052601, Batch Acc: 0.994539, Tokens per Sec:     3819, Lr: 0.000118\r\n",
      "2024-08-27 16:22:28,491 - INFO - joeynmt.training - Epoch 241, Step:    22870, Batch Loss:     0.052536, Batch Acc: 0.995251, Tokens per Sec:     3958, Lr: 0.000118\r\n",
      "2024-08-27 16:22:29,539 - INFO - joeynmt.training - Epoch 241, Step:    22875, Batch Loss:     0.052968, Batch Acc: 0.995753, Tokens per Sec:     3824, Lr: 0.000118\r\n",
      "2024-08-27 16:22:30,603 - INFO - joeynmt.training - Epoch 241, Step:    22880, Batch Loss:     0.056014, Batch Acc: 0.996162, Tokens per Sec:     3920, Lr: 0.000118\r\n",
      "2024-08-27 16:22:31,679 - INFO - joeynmt.training - Epoch 241, Step:    22885, Batch Loss:     0.064492, Batch Acc: 0.995967, Tokens per Sec:     3918, Lr: 0.000118\r\n",
      "2024-08-27 16:22:32,761 - INFO - joeynmt.training - Epoch 241, Step:    22890, Batch Loss:     0.059054, Batch Acc: 0.995942, Tokens per Sec:     3875, Lr: 0.000118\r\n",
      "2024-08-27 16:22:33,831 - INFO - joeynmt.training - Epoch 241, Step:    22895, Batch Loss:     0.053063, Batch Acc: 0.995735, Tokens per Sec:     3729, Lr: 0.000118\r\n",
      "2024-08-27 16:22:34,886 - INFO - joeynmt.training - Epoch 241, Step:    22900, Batch Loss:     0.061731, Batch Acc: 0.996337, Tokens per Sec:     3885, Lr: 0.000118\r\n",
      "2024-08-27 16:22:35,969 - INFO - joeynmt.training - Epoch 241, Step:    22905, Batch Loss:     0.065133, Batch Acc: 0.995023, Tokens per Sec:     3895, Lr: 0.000118\r\n",
      "2024-08-27 16:22:37,072 - INFO - joeynmt.training - Epoch 241, Step:    22910, Batch Loss:     0.056152, Batch Acc: 0.995437, Tokens per Sec:     3780, Lr: 0.000118\r\n",
      "2024-08-27 16:22:38,145 - INFO - joeynmt.training - Epoch 241, Step:    22915, Batch Loss:     0.062747, Batch Acc: 0.994877, Tokens per Sec:     4001, Lr: 0.000118\r\n",
      "2024-08-27 16:22:39,215 - INFO - joeynmt.training - Epoch 241, Step:    22920, Batch Loss:     0.057457, Batch Acc: 0.995500, Tokens per Sec:     3948, Lr: 0.000118\r\n",
      "2024-08-27 16:22:40,304 - INFO - joeynmt.training - Epoch 241, Step:    22925, Batch Loss:     0.061557, Batch Acc: 0.995947, Tokens per Sec:     3628, Lr: 0.000118\r\n",
      "2024-08-27 16:22:41,392 - INFO - joeynmt.training - Epoch 241, Step:    22930, Batch Loss:     0.054163, Batch Acc: 0.996037, Tokens per Sec:     3947, Lr: 0.000118\r\n",
      "2024-08-27 16:22:41,550 - INFO - joeynmt.training - Epoch 241, total training loss: 5.49, num. of seqs: 8065, num. of tokens: 80463, 20.5933[sec]\r\n",
      "2024-08-27 16:22:41,551 - INFO - joeynmt.training - EPOCH 242\r\n",
      "2024-08-27 16:22:42,650 - INFO - joeynmt.training - Epoch 242, Step:    22935, Batch Loss:     0.060577, Batch Acc: 0.995596, Tokens per Sec:     4148, Lr: 0.000118\r\n",
      "2024-08-27 16:22:43,737 - INFO - joeynmt.training - Epoch 242, Step:    22940, Batch Loss:     0.058352, Batch Acc: 0.997222, Tokens per Sec:     3973, Lr: 0.000118\r\n",
      "2024-08-27 16:22:44,830 - INFO - joeynmt.training - Epoch 242, Step:    22945, Batch Loss:     0.053428, Batch Acc: 0.996068, Tokens per Sec:     3957, Lr: 0.000118\r\n",
      "2024-08-27 16:22:45,886 - INFO - joeynmt.training - Epoch 242, Step:    22950, Batch Loss:     0.053544, Batch Acc: 0.996640, Tokens per Sec:     3951, Lr: 0.000118\r\n",
      "2024-08-27 16:22:46,982 - INFO - joeynmt.training - Epoch 242, Step:    22955, Batch Loss:     0.050366, Batch Acc: 0.996009, Tokens per Sec:     3660, Lr: 0.000118\r\n",
      "2024-08-27 16:22:48,055 - INFO - joeynmt.training - Epoch 242, Step:    22960, Batch Loss:     0.045890, Batch Acc: 0.997417, Tokens per Sec:     3968, Lr: 0.000118\r\n",
      "2024-08-27 16:22:49,120 - INFO - joeynmt.training - Epoch 242, Step:    22965, Batch Loss:     0.056286, Batch Acc: 0.997347, Tokens per Sec:     3898, Lr: 0.000118\r\n",
      "2024-08-27 16:22:50,182 - INFO - joeynmt.training - Epoch 242, Step:    22970, Batch Loss:     0.058768, Batch Acc: 0.995872, Tokens per Sec:     3880, Lr: 0.000118\r\n",
      "2024-08-27 16:22:51,230 - INFO - joeynmt.training - Epoch 242, Step:    22975, Batch Loss:     0.057772, Batch Acc: 0.993545, Tokens per Sec:     3847, Lr: 0.000118\r\n",
      "2024-08-27 16:22:52,302 - INFO - joeynmt.training - Epoch 242, Step:    22980, Batch Loss:     0.053879, Batch Acc: 0.997482, Tokens per Sec:     3707, Lr: 0.000118\r\n",
      "2024-08-27 16:22:53,375 - INFO - joeynmt.training - Epoch 242, Step:    22985, Batch Loss:     0.057646, Batch Acc: 0.995122, Tokens per Sec:     3824, Lr: 0.000118\r\n",
      "2024-08-27 16:22:54,455 - INFO - joeynmt.training - Epoch 242, Step:    22990, Batch Loss:     0.050547, Batch Acc: 0.996807, Tokens per Sec:     4059, Lr: 0.000118\r\n",
      "2024-08-27 16:22:55,518 - INFO - joeynmt.training - Epoch 242, Step:    22995, Batch Loss:     0.060402, Batch Acc: 0.994334, Tokens per Sec:     3989, Lr: 0.000118\r\n",
      "2024-08-27 16:22:56,633 - INFO - joeynmt.training - Epoch 242, Step:    23000, Batch Loss:     0.059840, Batch Acc: 0.997673, Tokens per Sec:     3859, Lr: 0.000118\r\n",
      "2024-08-27 16:22:56,633 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=23042\r\n",
      "2024-08-27 16:22:56,634 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:23<00:00, 63.46it/s]\r\n",
      "2024-08-27 16:23:19,643 - INFO - joeynmt.prediction - Generation took 23.0072[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44887.42 examples/s]\r\n",
      "2024-08-27 16:23:20,021 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:23:20,022 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.13, loss:   6.71, ppl: 816.49, acc:   0.29, 0.1646[sec]\r\n",
      "2024-08-27 16:23:20,023 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:23:20,169 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:23:20,169 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:23:20,170 - INFO - joeynmt.training - \tHypothesis: comment vas tu hippolyte\r\n",
      "2024-08-27 16:23:20,460 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:23:20,606 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:23:20,606 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:23:20,607 - INFO - joeynmt.training - \tHypothesis: trois éléphants\r\n",
      "2024-08-27 16:23:20,895 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:23:21,039 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:23:21,039 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:23:21,039 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 16:23:21,328 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:23:21,474 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:23:21,474 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:23:21,474 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 16:23:22,853 - INFO - joeynmt.training - Epoch 242, Step:    23005, Batch Loss:     0.049042, Batch Acc: 0.997063, Tokens per Sec:     3753, Lr: 0.000118\r\n",
      "2024-08-27 16:23:23,926 - INFO - joeynmt.training - Epoch 242, Step:    23010, Batch Loss:     0.053407, Batch Acc: 0.995327, Tokens per Sec:     3992, Lr: 0.000118\r\n",
      "2024-08-27 16:23:25,001 - INFO - joeynmt.training - Epoch 242, Step:    23015, Batch Loss:     0.062383, Batch Acc: 0.996411, Tokens per Sec:     3891, Lr: 0.000118\r\n",
      "2024-08-27 16:23:26,068 - INFO - joeynmt.training - Epoch 242, Step:    23020, Batch Loss:     0.049228, Batch Acc: 0.994785, Tokens per Sec:     3775, Lr: 0.000118\r\n",
      "2024-08-27 16:23:27,171 - INFO - joeynmt.training - Epoch 242, Step:    23025, Batch Loss:     0.054803, Batch Acc: 0.996192, Tokens per Sec:     3811, Lr: 0.000118\r\n",
      "2024-08-27 16:23:27,385 - INFO - joeynmt.training - Epoch 242, total training loss: 5.40, num. of seqs: 8065, num. of tokens: 80463, 20.6866[sec]\r\n",
      "2024-08-27 16:23:27,385 - INFO - joeynmt.training - EPOCH 243\r\n",
      "2024-08-27 16:23:28,248 - INFO - joeynmt.training - Epoch 243, Step:    23030, Batch Loss:     0.057382, Batch Acc: 0.996552, Tokens per Sec:     4049, Lr: 0.000118\r\n",
      "2024-08-27 16:23:29,329 - INFO - joeynmt.training - Epoch 243, Step:    23035, Batch Loss:     0.071583, Batch Acc: 0.995596, Tokens per Sec:     3996, Lr: 0.000118\r\n",
      "2024-08-27 16:23:30,392 - INFO - joeynmt.training - Epoch 243, Step:    23040, Batch Loss:     0.055386, Batch Acc: 0.995221, Tokens per Sec:     3938, Lr: 0.000118\r\n",
      "2024-08-27 16:23:31,454 - INFO - joeynmt.training - Epoch 243, Step:    23045, Batch Loss:     0.053067, Batch Acc: 0.996491, Tokens per Sec:     4027, Lr: 0.000118\r\n",
      "2024-08-27 16:23:32,527 - INFO - joeynmt.training - Epoch 243, Step:    23050, Batch Loss:     0.060561, Batch Acc: 0.995720, Tokens per Sec:     3923, Lr: 0.000118\r\n",
      "2024-08-27 16:23:33,592 - INFO - joeynmt.training - Epoch 243, Step:    23055, Batch Loss:     0.060018, Batch Acc: 0.995510, Tokens per Sec:     3978, Lr: 0.000118\r\n",
      "2024-08-27 16:23:34,656 - INFO - joeynmt.training - Epoch 243, Step:    23060, Batch Loss:     0.053558, Batch Acc: 0.994310, Tokens per Sec:     4132, Lr: 0.000118\r\n",
      "2024-08-27 16:23:35,737 - INFO - joeynmt.training - Epoch 243, Step:    23065, Batch Loss:     0.065267, Batch Acc: 0.995945, Tokens per Sec:     4336, Lr: 0.000118\r\n",
      "2024-08-27 16:23:36,838 - INFO - joeynmt.training - Epoch 243, Step:    23070, Batch Loss:     0.056156, Batch Acc: 0.997703, Tokens per Sec:     3560, Lr: 0.000118\r\n",
      "2024-08-27 16:23:37,932 - INFO - joeynmt.training - Epoch 243, Step:    23075, Batch Loss:     0.056642, Batch Acc: 0.996404, Tokens per Sec:     4068, Lr: 0.000118\r\n",
      "2024-08-27 16:23:39,000 - INFO - joeynmt.training - Epoch 243, Step:    23080, Batch Loss:     0.059775, Batch Acc: 0.996235, Tokens per Sec:     3735, Lr: 0.000118\r\n",
      "2024-08-27 16:23:40,043 - INFO - joeynmt.training - Epoch 243, Step:    23085, Batch Loss:     0.060692, Batch Acc: 0.996578, Tokens per Sec:     3922, Lr: 0.000118\r\n",
      "2024-08-27 16:23:41,104 - INFO - joeynmt.training - Epoch 243, Step:    23090, Batch Loss:     0.055826, Batch Acc: 0.997108, Tokens per Sec:     3913, Lr: 0.000118\r\n",
      "2024-08-27 16:23:42,184 - INFO - joeynmt.training - Epoch 243, Step:    23095, Batch Loss:     0.055764, Batch Acc: 0.997290, Tokens per Sec:     3763, Lr: 0.000118\r\n",
      "2024-08-27 16:23:43,282 - INFO - joeynmt.training - Epoch 243, Step:    23100, Batch Loss:     0.052223, Batch Acc: 0.995930, Tokens per Sec:     4028, Lr: 0.000118\r\n",
      "2024-08-27 16:23:44,362 - INFO - joeynmt.training - Epoch 243, Step:    23105, Batch Loss:     0.059143, Batch Acc: 0.994622, Tokens per Sec:     3792, Lr: 0.000118\r\n",
      "2024-08-27 16:23:45,414 - INFO - joeynmt.training - Epoch 243, Step:    23110, Batch Loss:     0.057106, Batch Acc: 0.997027, Tokens per Sec:     3840, Lr: 0.000118\r\n",
      "2024-08-27 16:23:46,479 - INFO - joeynmt.training - Epoch 243, Step:    23115, Batch Loss:     0.063519, Batch Acc: 0.995737, Tokens per Sec:     3967, Lr: 0.000118\r\n",
      "2024-08-27 16:23:47,572 - INFO - joeynmt.training - Epoch 243, Step:    23120, Batch Loss:     0.055764, Batch Acc: 0.996295, Tokens per Sec:     3952, Lr: 0.000118\r\n",
      "2024-08-27 16:23:47,830 - INFO - joeynmt.training - Epoch 243, total training loss: 5.34, num. of seqs: 8065, num. of tokens: 80463, 20.4288[sec]\r\n",
      "2024-08-27 16:23:47,830 - INFO - joeynmt.training - EPOCH 244\r\n",
      "2024-08-27 16:23:48,699 - INFO - joeynmt.training - Epoch 244, Step:    23125, Batch Loss:     0.058707, Batch Acc: 0.994715, Tokens per Sec:     3939, Lr: 0.000118\r\n",
      "2024-08-27 16:23:49,804 - INFO - joeynmt.training - Epoch 244, Step:    23130, Batch Loss:     0.056280, Batch Acc: 0.995366, Tokens per Sec:     3907, Lr: 0.000118\r\n",
      "2024-08-27 16:23:50,874 - INFO - joeynmt.training - Epoch 244, Step:    23135, Batch Loss:     0.058542, Batch Acc: 0.996179, Tokens per Sec:     4163, Lr: 0.000118\r\n",
      "2024-08-27 16:23:51,953 - INFO - joeynmt.training - Epoch 244, Step:    23140, Batch Loss:     0.049188, Batch Acc: 0.995210, Tokens per Sec:     4065, Lr: 0.000118\r\n",
      "2024-08-27 16:23:53,013 - INFO - joeynmt.training - Epoch 244, Step:    23145, Batch Loss:     0.057987, Batch Acc: 0.995177, Tokens per Sec:     3916, Lr: 0.000118\r\n",
      "2024-08-27 16:23:54,070 - INFO - joeynmt.training - Epoch 244, Step:    23150, Batch Loss:     0.059311, Batch Acc: 0.994802, Tokens per Sec:     4007, Lr: 0.000118\r\n",
      "2024-08-27 16:23:55,131 - INFO - joeynmt.training - Epoch 244, Step:    23155, Batch Loss:     0.052782, Batch Acc: 0.996739, Tokens per Sec:     4046, Lr: 0.000118\r\n",
      "2024-08-27 16:23:56,216 - INFO - joeynmt.training - Epoch 244, Step:    23160, Batch Loss:     0.054230, Batch Acc: 0.997298, Tokens per Sec:     3755, Lr: 0.000118\r\n",
      "2024-08-27 16:23:57,340 - INFO - joeynmt.training - Epoch 244, Step:    23165, Batch Loss:     0.047640, Batch Acc: 0.996919, Tokens per Sec:     3757, Lr: 0.000118\r\n",
      "2024-08-27 16:23:58,412 - INFO - joeynmt.training - Epoch 244, Step:    23170, Batch Loss:     0.053774, Batch Acc: 0.995391, Tokens per Sec:     4049, Lr: 0.000118\r\n",
      "2024-08-27 16:23:59,488 - INFO - joeynmt.training - Epoch 244, Step:    23175, Batch Loss:     0.055793, Batch Acc: 0.995851, Tokens per Sec:     3811, Lr: 0.000118\r\n",
      "2024-08-27 16:24:00,562 - INFO - joeynmt.training - Epoch 244, Step:    23180, Batch Loss:     0.054602, Batch Acc: 0.996061, Tokens per Sec:     4021, Lr: 0.000117\r\n",
      "2024-08-27 16:24:01,639 - INFO - joeynmt.training - Epoch 244, Step:    23185, Batch Loss:     0.059856, Batch Acc: 0.995739, Tokens per Sec:     3924, Lr: 0.000117\r\n",
      "2024-08-27 16:24:02,717 - INFO - joeynmt.training - Epoch 244, Step:    23190, Batch Loss:     0.047637, Batch Acc: 0.997495, Tokens per Sec:     4074, Lr: 0.000117\r\n",
      "2024-08-27 16:24:03,775 - INFO - joeynmt.training - Epoch 244, Step:    23195, Batch Loss:     0.060142, Batch Acc: 0.996800, Tokens per Sec:     3846, Lr: 0.000117\r\n",
      "2024-08-27 16:24:04,853 - INFO - joeynmt.training - Epoch 244, Step:    23200, Batch Loss:     0.062082, Batch Acc: 0.994975, Tokens per Sec:     4060, Lr: 0.000117\r\n",
      "2024-08-27 16:24:05,930 - INFO - joeynmt.training - Epoch 244, Step:    23205, Batch Loss:     0.046622, Batch Acc: 0.996243, Tokens per Sec:     3958, Lr: 0.000117\r\n",
      "2024-08-27 16:24:07,028 - INFO - joeynmt.training - Epoch 244, Step:    23210, Batch Loss:     0.058955, Batch Acc: 0.995375, Tokens per Sec:     3745, Lr: 0.000117\r\n",
      "2024-08-27 16:24:08,102 - INFO - joeynmt.training - Epoch 244, Step:    23215, Batch Loss:     0.060546, Batch Acc: 0.993318, Tokens per Sec:     4042, Lr: 0.000117\r\n",
      "2024-08-27 16:24:08,273 - INFO - joeynmt.training - Epoch 244, total training loss: 5.30, num. of seqs: 8065, num. of tokens: 80463, 20.4266[sec]\r\n",
      "2024-08-27 16:24:08,273 - INFO - joeynmt.training - EPOCH 245\r\n",
      "2024-08-27 16:24:09,384 - INFO - joeynmt.training - Epoch 245, Step:    23220, Batch Loss:     0.054259, Batch Acc: 0.995366, Tokens per Sec:     3712, Lr: 0.000117\r\n",
      "2024-08-27 16:24:10,443 - INFO - joeynmt.training - Epoch 245, Step:    23225, Batch Loss:     0.051274, Batch Acc: 0.997367, Tokens per Sec:     3946, Lr: 0.000117\r\n",
      "2024-08-27 16:24:11,517 - INFO - joeynmt.training - Epoch 245, Step:    23230, Batch Loss:     0.057028, Batch Acc: 0.995988, Tokens per Sec:     4179, Lr: 0.000117\r\n",
      "2024-08-27 16:24:12,584 - INFO - joeynmt.training - Epoch 245, Step:    23235, Batch Loss:     0.059146, Batch Acc: 0.993768, Tokens per Sec:     3913, Lr: 0.000117\r\n",
      "2024-08-27 16:24:13,667 - INFO - joeynmt.training - Epoch 245, Step:    23240, Batch Loss:     0.057881, Batch Acc: 0.996679, Tokens per Sec:     3894, Lr: 0.000117\r\n",
      "2024-08-27 16:24:14,736 - INFO - joeynmt.training - Epoch 245, Step:    23245, Batch Loss:     0.049825, Batch Acc: 0.995945, Tokens per Sec:     3695, Lr: 0.000117\r\n",
      "2024-08-27 16:24:15,792 - INFO - joeynmt.training - Epoch 245, Step:    23250, Batch Loss:     0.053982, Batch Acc: 0.995226, Tokens per Sec:     3971, Lr: 0.000117\r\n",
      "2024-08-27 16:24:16,900 - INFO - joeynmt.training - Epoch 245, Step:    23255, Batch Loss:     0.054808, Batch Acc: 0.996970, Tokens per Sec:     3576, Lr: 0.000117\r\n",
      "2024-08-27 16:24:17,975 - INFO - joeynmt.training - Epoch 245, Step:    23260, Batch Loss:     0.054019, Batch Acc: 0.995912, Tokens per Sec:     3871, Lr: 0.000117\r\n",
      "2024-08-27 16:24:19,045 - INFO - joeynmt.training - Epoch 245, Step:    23265, Batch Loss:     0.054773, Batch Acc: 0.997652, Tokens per Sec:     3983, Lr: 0.000117\r\n",
      "2024-08-27 16:24:20,114 - INFO - joeynmt.training - Epoch 245, Step:    23270, Batch Loss:     0.055361, Batch Acc: 0.995955, Tokens per Sec:     3935, Lr: 0.000117\r\n",
      "2024-08-27 16:24:21,188 - INFO - joeynmt.training - Epoch 245, Step:    23275, Batch Loss:     0.046622, Batch Acc: 0.997137, Tokens per Sec:     3906, Lr: 0.000117\r\n",
      "2024-08-27 16:24:22,251 - INFO - joeynmt.training - Epoch 245, Step:    23280, Batch Loss:     0.051433, Batch Acc: 0.997850, Tokens per Sec:     3939, Lr: 0.000117\r\n",
      "2024-08-27 16:24:23,326 - INFO - joeynmt.training - Epoch 245, Step:    23285, Batch Loss:     0.050840, Batch Acc: 0.996718, Tokens per Sec:     3970, Lr: 0.000117\r\n",
      "2024-08-27 16:24:24,405 - INFO - joeynmt.training - Epoch 245, Step:    23290, Batch Loss:     0.051793, Batch Acc: 0.995356, Tokens per Sec:     3996, Lr: 0.000117\r\n",
      "2024-08-27 16:24:25,462 - INFO - joeynmt.training - Epoch 245, Step:    23295, Batch Loss:     0.056008, Batch Acc: 0.995501, Tokens per Sec:     3996, Lr: 0.000117\r\n",
      "2024-08-27 16:24:26,509 - INFO - joeynmt.training - Epoch 245, Step:    23300, Batch Loss:     0.050749, Batch Acc: 0.995584, Tokens per Sec:     3897, Lr: 0.000117\r\n",
      "2024-08-27 16:24:27,608 - INFO - joeynmt.training - Epoch 245, Step:    23305, Batch Loss:     0.057134, Batch Acc: 0.995360, Tokens per Sec:     3923, Lr: 0.000117\r\n",
      "2024-08-27 16:24:28,690 - INFO - joeynmt.training - Epoch 245, Step:    23310, Batch Loss:     0.051881, Batch Acc: 0.996393, Tokens per Sec:     3847, Lr: 0.000117\r\n",
      "2024-08-27 16:24:28,953 - INFO - joeynmt.training - Epoch 245, total training loss: 5.36, num. of seqs: 8065, num. of tokens: 80463, 20.6605[sec]\r\n",
      "2024-08-27 16:24:28,953 - INFO - joeynmt.training - EPOCH 246\r\n",
      "2024-08-27 16:24:29,819 - INFO - joeynmt.training - Epoch 246, Step:    23315, Batch Loss:     0.061846, Batch Acc: 0.994660, Tokens per Sec:     3912, Lr: 0.000117\r\n",
      "2024-08-27 16:24:30,892 - INFO - joeynmt.training - Epoch 246, Step:    23320, Batch Loss:     0.069877, Batch Acc: 0.995860, Tokens per Sec:     3830, Lr: 0.000117\r\n",
      "2024-08-27 16:24:31,970 - INFO - joeynmt.training - Epoch 246, Step:    23325, Batch Loss:     0.062219, Batch Acc: 0.994315, Tokens per Sec:     3918, Lr: 0.000117\r\n",
      "2024-08-27 16:24:33,052 - INFO - joeynmt.training - Epoch 246, Step:    23330, Batch Loss:     0.055711, Batch Acc: 0.993917, Tokens per Sec:     3952, Lr: 0.000117\r\n",
      "2024-08-27 16:24:34,160 - INFO - joeynmt.training - Epoch 246, Step:    23335, Batch Loss:     0.046410, Batch Acc: 0.996410, Tokens per Sec:     3774, Lr: 0.000117\r\n",
      "2024-08-27 16:24:35,217 - INFO - joeynmt.training - Epoch 246, Step:    23340, Batch Loss:     0.055964, Batch Acc: 0.997216, Tokens per Sec:     4080, Lr: 0.000117\r\n",
      "2024-08-27 16:24:36,289 - INFO - joeynmt.training - Epoch 246, Step:    23345, Batch Loss:     0.054826, Batch Acc: 0.997936, Tokens per Sec:     4071, Lr: 0.000117\r\n",
      "2024-08-27 16:24:37,405 - INFO - joeynmt.training - Epoch 246, Step:    23350, Batch Loss:     0.055149, Batch Acc: 0.993932, Tokens per Sec:     3694, Lr: 0.000117\r\n",
      "2024-08-27 16:24:38,477 - INFO - joeynmt.training - Epoch 246, Step:    23355, Batch Loss:     0.055144, Batch Acc: 0.997702, Tokens per Sec:     3655, Lr: 0.000117\r\n",
      "2024-08-27 16:24:39,554 - INFO - joeynmt.training - Epoch 246, Step:    23360, Batch Loss:     0.054758, Batch Acc: 0.997550, Tokens per Sec:     3792, Lr: 0.000117\r\n",
      "2024-08-27 16:24:40,623 - INFO - joeynmt.training - Epoch 246, Step:    23365, Batch Loss:     0.058489, Batch Acc: 0.995379, Tokens per Sec:     4051, Lr: 0.000117\r\n",
      "2024-08-27 16:24:41,688 - INFO - joeynmt.training - Epoch 246, Step:    23370, Batch Loss:     0.053850, Batch Acc: 0.995748, Tokens per Sec:     3977, Lr: 0.000117\r\n",
      "2024-08-27 16:24:42,778 - INFO - joeynmt.training - Epoch 246, Step:    23375, Batch Loss:     0.053009, Batch Acc: 0.995327, Tokens per Sec:     3930, Lr: 0.000117\r\n",
      "2024-08-27 16:24:43,858 - INFO - joeynmt.training - Epoch 246, Step:    23380, Batch Loss:     0.057584, Batch Acc: 0.996374, Tokens per Sec:     3833, Lr: 0.000117\r\n",
      "2024-08-27 16:24:44,942 - INFO - joeynmt.training - Epoch 246, Step:    23385, Batch Loss:     0.050883, Batch Acc: 0.994920, Tokens per Sec:     4000, Lr: 0.000117\r\n",
      "2024-08-27 16:24:46,029 - INFO - joeynmt.training - Epoch 246, Step:    23390, Batch Loss:     0.062368, Batch Acc: 0.996471, Tokens per Sec:     3912, Lr: 0.000117\r\n",
      "2024-08-27 16:24:47,135 - INFO - joeynmt.training - Epoch 246, Step:    23395, Batch Loss:     0.063195, Batch Acc: 0.995197, Tokens per Sec:     3768, Lr: 0.000117\r\n",
      "2024-08-27 16:24:48,228 - INFO - joeynmt.training - Epoch 246, Step:    23400, Batch Loss:     0.061222, Batch Acc: 0.995221, Tokens per Sec:     4020, Lr: 0.000117\r\n",
      "2024-08-27 16:24:49,302 - INFO - joeynmt.training - Epoch 246, Step:    23405, Batch Loss:     0.062204, Batch Acc: 0.996836, Tokens per Sec:     4123, Lr: 0.000117\r\n",
      "2024-08-27 16:24:49,559 - INFO - joeynmt.training - Epoch 246, total training loss: 5.36, num. of seqs: 8065, num. of tokens: 80463, 20.5892[sec]\r\n",
      "2024-08-27 16:24:49,559 - INFO - joeynmt.training - EPOCH 247\r\n",
      "2024-08-27 16:24:50,413 - INFO - joeynmt.training - Epoch 247, Step:    23410, Batch Loss:     0.052587, Batch Acc: 0.997798, Tokens per Sec:     3743, Lr: 0.000117\r\n",
      "2024-08-27 16:24:51,468 - INFO - joeynmt.training - Epoch 247, Step:    23415, Batch Loss:     0.061628, Batch Acc: 0.996329, Tokens per Sec:     3874, Lr: 0.000117\r\n",
      "2024-08-27 16:24:52,544 - INFO - joeynmt.training - Epoch 247, Step:    23420, Batch Loss:     0.054050, Batch Acc: 0.995156, Tokens per Sec:     3839, Lr: 0.000117\r\n",
      "2024-08-27 16:24:53,608 - INFO - joeynmt.training - Epoch 247, Step:    23425, Batch Loss:     0.060805, Batch Acc: 0.995868, Tokens per Sec:     4097, Lr: 0.000117\r\n",
      "2024-08-27 16:24:54,665 - INFO - joeynmt.training - Epoch 247, Step:    23430, Batch Loss:     0.051802, Batch Acc: 0.997003, Tokens per Sec:     4106, Lr: 0.000117\r\n",
      "2024-08-27 16:24:55,725 - INFO - joeynmt.training - Epoch 247, Step:    23435, Batch Loss:     0.058144, Batch Acc: 0.997224, Tokens per Sec:     4081, Lr: 0.000117\r\n",
      "2024-08-27 16:24:56,836 - INFO - joeynmt.training - Epoch 247, Step:    23440, Batch Loss:     0.055752, Batch Acc: 0.996191, Tokens per Sec:     3546, Lr: 0.000117\r\n",
      "2024-08-27 16:24:57,909 - INFO - joeynmt.training - Epoch 247, Step:    23445, Batch Loss:     0.050999, Batch Acc: 0.995551, Tokens per Sec:     3982, Lr: 0.000117\r\n",
      "2024-08-27 16:24:58,984 - INFO - joeynmt.training - Epoch 247, Step:    23450, Batch Loss:     0.052913, Batch Acc: 0.996103, Tokens per Sec:     3823, Lr: 0.000117\r\n",
      "2024-08-27 16:25:00,063 - INFO - joeynmt.training - Epoch 247, Step:    23455, Batch Loss:     0.060519, Batch Acc: 0.994977, Tokens per Sec:     4062, Lr: 0.000117\r\n",
      "2024-08-27 16:25:01,126 - INFO - joeynmt.training - Epoch 247, Step:    23460, Batch Loss:     0.056885, Batch Acc: 0.995729, Tokens per Sec:     3965, Lr: 0.000117\r\n",
      "2024-08-27 16:25:02,181 - INFO - joeynmt.training - Epoch 247, Step:    23465, Batch Loss:     0.054382, Batch Acc: 0.997092, Tokens per Sec:     3916, Lr: 0.000117\r\n",
      "2024-08-27 16:25:03,259 - INFO - joeynmt.training - Epoch 247, Step:    23470, Batch Loss:     0.057118, Batch Acc: 0.996360, Tokens per Sec:     4081, Lr: 0.000117\r\n",
      "2024-08-27 16:25:04,332 - INFO - joeynmt.training - Epoch 247, Step:    23475, Batch Loss:     0.061672, Batch Acc: 0.996937, Tokens per Sec:     3959, Lr: 0.000117\r\n",
      "2024-08-27 16:25:05,398 - INFO - joeynmt.training - Epoch 247, Step:    23480, Batch Loss:     0.047124, Batch Acc: 0.995773, Tokens per Sec:     3995, Lr: 0.000117\r\n",
      "2024-08-27 16:25:06,476 - INFO - joeynmt.training - Epoch 247, Step:    23485, Batch Loss:     0.048397, Batch Acc: 0.996118, Tokens per Sec:     4065, Lr: 0.000117\r\n",
      "2024-08-27 16:25:07,576 - INFO - joeynmt.training - Epoch 247, Step:    23490, Batch Loss:     0.053104, Batch Acc: 0.996266, Tokens per Sec:     3899, Lr: 0.000117\r\n",
      "2024-08-27 16:25:08,651 - INFO - joeynmt.training - Epoch 247, Step:    23495, Batch Loss:     0.064847, Batch Acc: 0.996109, Tokens per Sec:     3825, Lr: 0.000117\r\n",
      "2024-08-27 16:25:09,715 - INFO - joeynmt.training - Epoch 247, Step:    23500, Batch Loss:     0.058875, Batch Acc: 0.996050, Tokens per Sec:     4051, Lr: 0.000117\r\n",
      "2024-08-27 16:25:09,716 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=23542\r\n",
      "2024-08-27 16:25:09,716 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 65.19it/s]\r\n",
      "2024-08-27 16:25:32,114 - INFO - joeynmt.prediction - Generation took 22.3959[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 45009.89 examples/s]\r\n",
      "2024-08-27 16:25:32,493 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:25:32,493 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.53, loss:   6.69, ppl: 807.08, acc:   0.29, 0.1664[sec]\r\n",
      "2024-08-27 16:25:32,494 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:25:32,641 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:25:32,641 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:25:32,641 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 16:25:32,933 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:25:33,078 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:25:33,078 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:25:33,078 - INFO - joeynmt.training - \tHypothesis: trois montée de l'enclos\r\n",
      "2024-08-27 16:25:33,368 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:25:33,513 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:25:33,513 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:25:33,513 - INFO - joeynmt.training - \tHypothesis: le service passagers n'est pas en activité actuellement\r\n",
      "2024-08-27 16:25:33,804 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:25:33,951 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:25:33,951 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:25:33,951 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 16:25:34,502 - INFO - joeynmt.training - Epoch 247, total training loss: 5.30, num. of seqs: 8065, num. of tokens: 80463, 20.3984[sec]\r\n",
      "2024-08-27 16:25:34,502 - INFO - joeynmt.training - EPOCH 248\r\n",
      "2024-08-27 16:25:35,358 - INFO - joeynmt.training - Epoch 248, Step:    23505, Batch Loss:     0.055221, Batch Acc: 0.996034, Tokens per Sec:     3848, Lr: 0.000117\r\n",
      "2024-08-27 16:25:36,437 - INFO - joeynmt.training - Epoch 248, Step:    23510, Batch Loss:     0.049298, Batch Acc: 0.997482, Tokens per Sec:     4054, Lr: 0.000117\r\n",
      "2024-08-27 16:25:37,555 - INFO - joeynmt.training - Epoch 248, Step:    23515, Batch Loss:     0.056302, Batch Acc: 0.997189, Tokens per Sec:     3818, Lr: 0.000117\r\n",
      "2024-08-27 16:25:38,644 - INFO - joeynmt.training - Epoch 248, Step:    23520, Batch Loss:     0.060446, Batch Acc: 0.997496, Tokens per Sec:     4036, Lr: 0.000117\r\n",
      "2024-08-27 16:25:39,718 - INFO - joeynmt.training - Epoch 248, Step:    23525, Batch Loss:     0.066137, Batch Acc: 0.994852, Tokens per Sec:     3801, Lr: 0.000117\r\n",
      "2024-08-27 16:25:40,796 - INFO - joeynmt.training - Epoch 248, Step:    23530, Batch Loss:     0.054848, Batch Acc: 0.996625, Tokens per Sec:     3849, Lr: 0.000117\r\n",
      "2024-08-27 16:25:41,887 - INFO - joeynmt.training - Epoch 248, Step:    23535, Batch Loss:     0.056645, Batch Acc: 0.996341, Tokens per Sec:     3760, Lr: 0.000117\r\n",
      "2024-08-27 16:25:43,169 - INFO - joeynmt.training - Epoch 248, Step:    23540, Batch Loss:     0.051240, Batch Acc: 0.996389, Tokens per Sec:     3242, Lr: 0.000117\r\n",
      "2024-08-27 16:25:44,267 - INFO - joeynmt.training - Epoch 248, Step:    23545, Batch Loss:     0.061707, Batch Acc: 0.992993, Tokens per Sec:     3772, Lr: 0.000117\r\n",
      "2024-08-27 16:25:45,347 - INFO - joeynmt.training - Epoch 248, Step:    23550, Batch Loss:     0.050795, Batch Acc: 0.996312, Tokens per Sec:     4019, Lr: 0.000117\r\n",
      "2024-08-27 16:25:46,425 - INFO - joeynmt.training - Epoch 248, Step:    23555, Batch Loss:     0.053694, Batch Acc: 0.995961, Tokens per Sec:     4138, Lr: 0.000117\r\n",
      "2024-08-27 16:25:47,543 - INFO - joeynmt.training - Epoch 248, Step:    23560, Batch Loss:     0.050841, Batch Acc: 0.995764, Tokens per Sec:     4013, Lr: 0.000117\r\n",
      "2024-08-27 16:25:48,634 - INFO - joeynmt.training - Epoch 248, Step:    23565, Batch Loss:     0.046235, Batch Acc: 0.997823, Tokens per Sec:     3794, Lr: 0.000117\r\n",
      "2024-08-27 16:25:49,721 - INFO - joeynmt.training - Epoch 248, Step:    23570, Batch Loss:     0.054623, Batch Acc: 0.996260, Tokens per Sec:     3938, Lr: 0.000117\r\n",
      "2024-08-27 16:25:50,810 - INFO - joeynmt.training - Epoch 248, Step:    23575, Batch Loss:     0.059962, Batch Acc: 0.997211, Tokens per Sec:     3951, Lr: 0.000117\r\n",
      "2024-08-27 16:25:51,904 - INFO - joeynmt.training - Epoch 248, Step:    23580, Batch Loss:     0.062734, Batch Acc: 0.996113, Tokens per Sec:     4000, Lr: 0.000116\r\n",
      "2024-08-27 16:25:52,988 - INFO - joeynmt.training - Epoch 248, Step:    23585, Batch Loss:     0.056990, Batch Acc: 0.995521, Tokens per Sec:     3917, Lr: 0.000116\r\n",
      "2024-08-27 16:25:54,054 - INFO - joeynmt.training - Epoch 248, Step:    23590, Batch Loss:     0.054186, Batch Acc: 0.997556, Tokens per Sec:     3840, Lr: 0.000116\r\n",
      "2024-08-27 16:25:55,144 - INFO - joeynmt.training - Epoch 248, Step:    23595, Batch Loss:     0.058834, Batch Acc: 0.995546, Tokens per Sec:     3917, Lr: 0.000116\r\n",
      "2024-08-27 16:25:55,296 - INFO - joeynmt.training - Epoch 248, total training loss: 5.18, num. of seqs: 8065, num. of tokens: 80463, 20.7780[sec]\r\n",
      "2024-08-27 16:25:55,297 - INFO - joeynmt.training - EPOCH 249\r\n",
      "2024-08-27 16:25:56,379 - INFO - joeynmt.training - Epoch 249, Step:    23600, Batch Loss:     0.061035, Batch Acc: 0.994578, Tokens per Sec:     3933, Lr: 0.000116\r\n",
      "2024-08-27 16:25:57,482 - INFO - joeynmt.training - Epoch 249, Step:    23605, Batch Loss:     0.054605, Batch Acc: 0.996748, Tokens per Sec:     3629, Lr: 0.000116\r\n",
      "2024-08-27 16:25:58,538 - INFO - joeynmt.training - Epoch 249, Step:    23610, Batch Loss:     0.054747, Batch Acc: 0.997270, Tokens per Sec:     3819, Lr: 0.000116\r\n",
      "2024-08-27 16:25:59,623 - INFO - joeynmt.training - Epoch 249, Step:    23615, Batch Loss:     0.047972, Batch Acc: 0.997416, Tokens per Sec:     3927, Lr: 0.000116\r\n",
      "2024-08-27 16:26:00,705 - INFO - joeynmt.training - Epoch 249, Step:    23620, Batch Loss:     0.046737, Batch Acc: 0.998208, Tokens per Sec:     4126, Lr: 0.000116\r\n",
      "2024-08-27 16:26:01,776 - INFO - joeynmt.training - Epoch 249, Step:    23625, Batch Loss:     0.052614, Batch Acc: 0.996749, Tokens per Sec:     4026, Lr: 0.000116\r\n",
      "2024-08-27 16:26:02,835 - INFO - joeynmt.training - Epoch 249, Step:    23630, Batch Loss:     0.062633, Batch Acc: 0.995612, Tokens per Sec:     3872, Lr: 0.000116\r\n",
      "2024-08-27 16:26:03,918 - INFO - joeynmt.training - Epoch 249, Step:    23635, Batch Loss:     0.059671, Batch Acc: 0.994573, Tokens per Sec:     3919, Lr: 0.000116\r\n",
      "2024-08-27 16:26:04,998 - INFO - joeynmt.training - Epoch 249, Step:    23640, Batch Loss:     0.047726, Batch Acc: 0.997458, Tokens per Sec:     4006, Lr: 0.000116\r\n",
      "2024-08-27 16:26:06,077 - INFO - joeynmt.training - Epoch 249, Step:    23645, Batch Loss:     0.051225, Batch Acc: 0.995806, Tokens per Sec:     3979, Lr: 0.000116\r\n",
      "2024-08-27 16:26:07,174 - INFO - joeynmt.training - Epoch 249, Step:    23650, Batch Loss:     0.049109, Batch Acc: 0.997878, Tokens per Sec:     3867, Lr: 0.000116\r\n",
      "2024-08-27 16:26:08,251 - INFO - joeynmt.training - Epoch 249, Step:    23655, Batch Loss:     0.059289, Batch Acc: 0.996818, Tokens per Sec:     3798, Lr: 0.000116\r\n",
      "2024-08-27 16:26:09,319 - INFO - joeynmt.training - Epoch 249, Step:    23660, Batch Loss:     0.055223, Batch Acc: 0.994952, Tokens per Sec:     3897, Lr: 0.000116\r\n",
      "2024-08-27 16:26:10,403 - INFO - joeynmt.training - Epoch 249, Step:    23665, Batch Loss:     0.058602, Batch Acc: 0.996294, Tokens per Sec:     3984, Lr: 0.000116\r\n",
      "2024-08-27 16:26:11,610 - INFO - joeynmt.training - Epoch 249, Step:    23670, Batch Loss:     0.055557, Batch Acc: 0.995027, Tokens per Sec:     3334, Lr: 0.000116\r\n",
      "2024-08-27 16:26:12,694 - INFO - joeynmt.training - Epoch 249, Step:    23675, Batch Loss:     0.055773, Batch Acc: 0.995216, Tokens per Sec:     4051, Lr: 0.000116\r\n",
      "2024-08-27 16:26:13,761 - INFO - joeynmt.training - Epoch 249, Step:    23680, Batch Loss:     0.053217, Batch Acc: 0.996711, Tokens per Sec:     3992, Lr: 0.000116\r\n",
      "2024-08-27 16:26:14,843 - INFO - joeynmt.training - Epoch 249, Step:    23685, Batch Loss:     0.057352, Batch Acc: 0.996743, Tokens per Sec:     3974, Lr: 0.000116\r\n",
      "2024-08-27 16:26:15,924 - INFO - joeynmt.training - Epoch 249, Step:    23690, Batch Loss:     0.051361, Batch Acc: 0.996568, Tokens per Sec:     3776, Lr: 0.000116\r\n",
      "2024-08-27 16:26:16,028 - INFO - joeynmt.training - Epoch 249, total training loss: 5.26, num. of seqs: 8065, num. of tokens: 80463, 20.7158[sec]\r\n",
      "2024-08-27 16:26:16,029 - INFO - joeynmt.training - EPOCH 250\r\n",
      "2024-08-27 16:26:17,148 - INFO - joeynmt.training - Epoch 250, Step:    23695, Batch Loss:     0.051902, Batch Acc: 0.995692, Tokens per Sec:     3747, Lr: 0.000116\r\n",
      "2024-08-27 16:26:18,228 - INFO - joeynmt.training - Epoch 250, Step:    23700, Batch Loss:     0.049217, Batch Acc: 0.996688, Tokens per Sec:     3916, Lr: 0.000116\r\n",
      "2024-08-27 16:26:19,308 - INFO - joeynmt.training - Epoch 250, Step:    23705, Batch Loss:     0.050286, Batch Acc: 0.994924, Tokens per Sec:     4015, Lr: 0.000116\r\n",
      "2024-08-27 16:26:20,386 - INFO - joeynmt.training - Epoch 250, Step:    23710, Batch Loss:     0.054949, Batch Acc: 0.994910, Tokens per Sec:     3647, Lr: 0.000116\r\n",
      "2024-08-27 16:26:21,475 - INFO - joeynmt.training - Epoch 250, Step:    23715, Batch Loss:     0.049260, Batch Acc: 0.995967, Tokens per Sec:     3645, Lr: 0.000116\r\n",
      "2024-08-27 16:26:22,543 - INFO - joeynmt.training - Epoch 250, Step:    23720, Batch Loss:     0.058694, Batch Acc: 0.996953, Tokens per Sec:     3999, Lr: 0.000116\r\n",
      "2024-08-27 16:26:23,640 - INFO - joeynmt.training - Epoch 250, Step:    23725, Batch Loss:     0.056405, Batch Acc: 0.996573, Tokens per Sec:     3993, Lr: 0.000116\r\n",
      "2024-08-27 16:26:24,714 - INFO - joeynmt.training - Epoch 250, Step:    23730, Batch Loss:     0.048511, Batch Acc: 0.996749, Tokens per Sec:     4009, Lr: 0.000116\r\n",
      "2024-08-27 16:26:25,793 - INFO - joeynmt.training - Epoch 250, Step:    23735, Batch Loss:     0.054452, Batch Acc: 0.996348, Tokens per Sec:     3811, Lr: 0.000116\r\n",
      "2024-08-27 16:26:26,906 - INFO - joeynmt.training - Epoch 250, Step:    23740, Batch Loss:     0.051715, Batch Acc: 0.996159, Tokens per Sec:     3979, Lr: 0.000116\r\n",
      "2024-08-27 16:26:27,965 - INFO - joeynmt.training - Epoch 250, Step:    23745, Batch Loss:     0.060999, Batch Acc: 0.996073, Tokens per Sec:     4090, Lr: 0.000116\r\n",
      "2024-08-27 16:26:29,024 - INFO - joeynmt.training - Epoch 250, Step:    23750, Batch Loss:     0.053553, Batch Acc: 0.995098, Tokens per Sec:     3854, Lr: 0.000116\r\n",
      "2024-08-27 16:26:30,075 - INFO - joeynmt.training - Epoch 250, Step:    23755, Batch Loss:     0.057166, Batch Acc: 0.995656, Tokens per Sec:     3946, Lr: 0.000116\r\n",
      "2024-08-27 16:26:31,159 - INFO - joeynmt.training - Epoch 250, Step:    23760, Batch Loss:     0.048379, Batch Acc: 0.997222, Tokens per Sec:     3987, Lr: 0.000116\r\n",
      "2024-08-27 16:26:32,217 - INFO - joeynmt.training - Epoch 250, Step:    23765, Batch Loss:     0.055077, Batch Acc: 0.994888, Tokens per Sec:     3886, Lr: 0.000116\r\n",
      "2024-08-27 16:26:33,310 - INFO - joeynmt.training - Epoch 250, Step:    23770, Batch Loss:     0.059008, Batch Acc: 0.995480, Tokens per Sec:     4050, Lr: 0.000116\r\n",
      "2024-08-27 16:26:34,389 - INFO - joeynmt.training - Epoch 250, Step:    23775, Batch Loss:     0.048553, Batch Acc: 0.996245, Tokens per Sec:     3706, Lr: 0.000116\r\n",
      "2024-08-27 16:26:35,471 - INFO - joeynmt.training - Epoch 250, Step:    23780, Batch Loss:     0.059007, Batch Acc: 0.995625, Tokens per Sec:     4016, Lr: 0.000116\r\n",
      "2024-08-27 16:26:36,548 - INFO - joeynmt.training - Epoch 250, Step:    23785, Batch Loss:     0.060535, Batch Acc: 0.996177, Tokens per Sec:     3887, Lr: 0.000116\r\n",
      "2024-08-27 16:26:36,653 - INFO - joeynmt.training - Epoch 250, total training loss: 5.23, num. of seqs: 8065, num. of tokens: 80463, 20.6074[sec]\r\n",
      "2024-08-27 16:26:36,653 - INFO - joeynmt.training - EPOCH 251\r\n",
      "2024-08-27 16:26:37,763 - INFO - joeynmt.training - Epoch 251, Step:    23790, Batch Loss:     0.056402, Batch Acc: 0.995570, Tokens per Sec:     3876, Lr: 0.000116\r\n",
      "2024-08-27 16:26:38,854 - INFO - joeynmt.training - Epoch 251, Step:    23795, Batch Loss:     0.059286, Batch Acc: 0.996149, Tokens per Sec:     4050, Lr: 0.000116\r\n",
      "2024-08-27 16:26:39,901 - INFO - joeynmt.training - Epoch 251, Step:    23800, Batch Loss:     0.053442, Batch Acc: 0.996760, Tokens per Sec:     3835, Lr: 0.000116\r\n",
      "2024-08-27 16:26:40,955 - INFO - joeynmt.training - Epoch 251, Step:    23805, Batch Loss:     0.058319, Batch Acc: 0.996869, Tokens per Sec:     3943, Lr: 0.000116\r\n",
      "2024-08-27 16:26:42,017 - INFO - joeynmt.training - Epoch 251, Step:    23810, Batch Loss:     0.051708, Batch Acc: 0.996832, Tokens per Sec:     3864, Lr: 0.000116\r\n",
      "2024-08-27 16:26:43,243 - INFO - joeynmt.training - Epoch 251, Step:    23815, Batch Loss:     0.056029, Batch Acc: 0.997380, Tokens per Sec:     3426, Lr: 0.000116\r\n",
      "2024-08-27 16:26:44,312 - INFO - joeynmt.training - Epoch 251, Step:    23820, Batch Loss:     0.070317, Batch Acc: 0.997340, Tokens per Sec:     3870, Lr: 0.000116\r\n",
      "2024-08-27 16:26:45,387 - INFO - joeynmt.training - Epoch 251, Step:    23825, Batch Loss:     0.059633, Batch Acc: 0.995869, Tokens per Sec:     3833, Lr: 0.000116\r\n",
      "2024-08-27 16:26:46,500 - INFO - joeynmt.training - Epoch 251, Step:    23830, Batch Loss:     0.055904, Batch Acc: 0.996845, Tokens per Sec:     3703, Lr: 0.000116\r\n",
      "2024-08-27 16:26:47,641 - INFO - joeynmt.training - Epoch 251, Step:    23835, Batch Loss:     0.050544, Batch Acc: 0.997040, Tokens per Sec:     3853, Lr: 0.000116\r\n",
      "2024-08-27 16:26:48,698 - INFO - joeynmt.training - Epoch 251, Step:    23840, Batch Loss:     0.048943, Batch Acc: 0.996564, Tokens per Sec:     3858, Lr: 0.000116\r\n",
      "2024-08-27 16:26:49,770 - INFO - joeynmt.training - Epoch 251, Step:    23845, Batch Loss:     0.054089, Batch Acc: 0.997162, Tokens per Sec:     3947, Lr: 0.000116\r\n",
      "2024-08-27 16:26:50,858 - INFO - joeynmt.training - Epoch 251, Step:    23850, Batch Loss:     0.052656, Batch Acc: 0.996871, Tokens per Sec:     3823, Lr: 0.000116\r\n",
      "2024-08-27 16:26:51,933 - INFO - joeynmt.training - Epoch 251, Step:    23855, Batch Loss:     0.053142, Batch Acc: 0.997244, Tokens per Sec:     4051, Lr: 0.000116\r\n",
      "2024-08-27 16:26:53,011 - INFO - joeynmt.training - Epoch 251, Step:    23860, Batch Loss:     0.052467, Batch Acc: 0.996167, Tokens per Sec:     3874, Lr: 0.000116\r\n",
      "2024-08-27 16:26:54,080 - INFO - joeynmt.training - Epoch 251, Step:    23865, Batch Loss:     0.057730, Batch Acc: 0.994443, Tokens per Sec:     4043, Lr: 0.000116\r\n",
      "2024-08-27 16:26:55,150 - INFO - joeynmt.training - Epoch 251, Step:    23870, Batch Loss:     0.070453, Batch Acc: 0.994216, Tokens per Sec:     4043, Lr: 0.000116\r\n",
      "2024-08-27 16:26:56,236 - INFO - joeynmt.training - Epoch 251, Step:    23875, Batch Loss:     0.067290, Batch Acc: 0.994354, Tokens per Sec:     4081, Lr: 0.000116\r\n",
      "2024-08-27 16:26:57,368 - INFO - joeynmt.training - Epoch 251, Step:    23880, Batch Loss:     0.071453, Batch Acc: 0.996363, Tokens per Sec:     3886, Lr: 0.000116\r\n",
      "2024-08-27 16:26:57,415 - INFO - joeynmt.training - Epoch 251, total training loss: 5.20, num. of seqs: 8065, num. of tokens: 80463, 20.7451[sec]\r\n",
      "2024-08-27 16:26:57,415 - INFO - joeynmt.training - EPOCH 252\r\n",
      "2024-08-27 16:26:58,508 - INFO - joeynmt.training - Epoch 252, Step:    23885, Batch Loss:     0.055671, Batch Acc: 0.995818, Tokens per Sec:     3953, Lr: 0.000116\r\n",
      "2024-08-27 16:26:59,575 - INFO - joeynmt.training - Epoch 252, Step:    23890, Batch Loss:     0.048749, Batch Acc: 0.996302, Tokens per Sec:     3804, Lr: 0.000116\r\n",
      "2024-08-27 16:27:00,717 - INFO - joeynmt.training - Epoch 252, Step:    23895, Batch Loss:     0.073237, Batch Acc: 0.996115, Tokens per Sec:     3835, Lr: 0.000116\r\n",
      "2024-08-27 16:27:01,828 - INFO - joeynmt.training - Epoch 252, Step:    23900, Batch Loss:     0.055151, Batch Acc: 0.996873, Tokens per Sec:     3743, Lr: 0.000116\r\n",
      "2024-08-27 16:27:02,904 - INFO - joeynmt.training - Epoch 252, Step:    23905, Batch Loss:     0.050606, Batch Acc: 0.995755, Tokens per Sec:     3726, Lr: 0.000116\r\n",
      "2024-08-27 16:27:03,984 - INFO - joeynmt.training - Epoch 252, Step:    23910, Batch Loss:     0.049451, Batch Acc: 0.997007, Tokens per Sec:     3714, Lr: 0.000116\r\n",
      "2024-08-27 16:27:05,051 - INFO - joeynmt.training - Epoch 252, Step:    23915, Batch Loss:     0.051604, Batch Acc: 0.996513, Tokens per Sec:     3764, Lr: 0.000116\r\n",
      "2024-08-27 16:27:06,145 - INFO - joeynmt.training - Epoch 252, Step:    23920, Batch Loss:     0.064577, Batch Acc: 0.996960, Tokens per Sec:     3913, Lr: 0.000116\r\n",
      "2024-08-27 16:27:07,268 - INFO - joeynmt.training - Epoch 252, Step:    23925, Batch Loss:     0.049593, Batch Acc: 0.997394, Tokens per Sec:     4101, Lr: 0.000116\r\n",
      "2024-08-27 16:27:08,339 - INFO - joeynmt.training - Epoch 252, Step:    23930, Batch Loss:     0.057139, Batch Acc: 0.996381, Tokens per Sec:     3875, Lr: 0.000116\r\n",
      "2024-08-27 16:27:09,413 - INFO - joeynmt.training - Epoch 252, Step:    23935, Batch Loss:     0.062812, Batch Acc: 0.995447, Tokens per Sec:     4092, Lr: 0.000116\r\n",
      "2024-08-27 16:27:10,476 - INFO - joeynmt.training - Epoch 252, Step:    23940, Batch Loss:     0.055161, Batch Acc: 0.997094, Tokens per Sec:     3887, Lr: 0.000116\r\n",
      "2024-08-27 16:27:11,543 - INFO - joeynmt.training - Epoch 252, Step:    23945, Batch Loss:     0.060360, Batch Acc: 0.994914, Tokens per Sec:     4056, Lr: 0.000116\r\n",
      "2024-08-27 16:27:12,617 - INFO - joeynmt.training - Epoch 252, Step:    23950, Batch Loss:     0.048601, Batch Acc: 0.996903, Tokens per Sec:     3911, Lr: 0.000116\r\n",
      "2024-08-27 16:27:13,764 - INFO - joeynmt.training - Epoch 252, Step:    23955, Batch Loss:     0.055561, Batch Acc: 0.994756, Tokens per Sec:     3659, Lr: 0.000116\r\n",
      "2024-08-27 16:27:14,923 - INFO - joeynmt.training - Epoch 252, Step:    23960, Batch Loss:     0.052782, Batch Acc: 0.996868, Tokens per Sec:     3583, Lr: 0.000116\r\n",
      "2024-08-27 16:27:16,006 - INFO - joeynmt.training - Epoch 252, Step:    23965, Batch Loss:     0.056722, Batch Acc: 0.994664, Tokens per Sec:     3982, Lr: 0.000116\r\n",
      "2024-08-27 16:27:17,109 - INFO - joeynmt.training - Epoch 252, Step:    23970, Batch Loss:     0.047033, Batch Acc: 0.997645, Tokens per Sec:     3854, Lr: 0.000116\r\n",
      "2024-08-27 16:27:18,197 - INFO - joeynmt.training - Epoch 252, Step:    23975, Batch Loss:     0.059022, Batch Acc: 0.994151, Tokens per Sec:     4088, Lr: 0.000116\r\n",
      "2024-08-27 16:27:18,244 - INFO - joeynmt.training - Epoch 252, total training loss: 5.22, num. of seqs: 8065, num. of tokens: 80463, 20.8126[sec]\r\n",
      "2024-08-27 16:27:18,244 - INFO - joeynmt.training - EPOCH 253\r\n",
      "2024-08-27 16:27:19,312 - INFO - joeynmt.training - Epoch 253, Step:    23980, Batch Loss:     0.049208, Batch Acc: 0.997874, Tokens per Sec:     3978, Lr: 0.000116\r\n",
      "2024-08-27 16:27:20,390 - INFO - joeynmt.training - Epoch 253, Step:    23985, Batch Loss:     0.061399, Batch Acc: 0.997009, Tokens per Sec:     4035, Lr: 0.000116\r\n",
      "2024-08-27 16:27:21,469 - INFO - joeynmt.training - Epoch 253, Step:    23990, Batch Loss:     0.053061, Batch Acc: 0.997047, Tokens per Sec:     4084, Lr: 0.000115\r\n",
      "2024-08-27 16:27:22,541 - INFO - joeynmt.training - Epoch 253, Step:    23995, Batch Loss:     0.051583, Batch Acc: 0.996031, Tokens per Sec:     3999, Lr: 0.000115\r\n",
      "2024-08-27 16:27:23,607 - INFO - joeynmt.training - Epoch 253, Step:    24000, Batch Loss:     0.048289, Batch Acc: 0.997150, Tokens per Sec:     3952, Lr: 0.000115\r\n",
      "2024-08-27 16:27:23,608 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=24042\r\n",
      "2024-08-27 16:27:23,608 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:21<00:00, 66.64it/s]\r\n",
      "2024-08-27 16:27:45,520 - INFO - joeynmt.prediction - Generation took 21.9099[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 43495.70 examples/s]\r\n",
      "2024-08-27 16:27:45,901 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:27:45,901 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.61, loss:   6.69, ppl: 807.81, acc:   0.29, 0.1656[sec]\r\n",
      "2024-08-27 16:27:45,903 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:27:46,050 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:27:46,050 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:27:46,050 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 16:27:46,339 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:27:46,485 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:27:46,486 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:27:46,486 - INFO - joeynmt.training - \tHypothesis: trois montée de l'enclos\r\n",
      "2024-08-27 16:27:46,797 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:27:46,941 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:27:46,942 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:27:46,942 - INFO - joeynmt.training - \tHypothesis: le service passagers n'est pas en activité actuellement\r\n",
      "2024-08-27 16:27:47,230 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:27:47,376 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:27:47,376 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:27:47,376 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 16:27:48,743 - INFO - joeynmt.training - Epoch 253, Step:    24005, Batch Loss:     0.050844, Batch Acc: 0.999014, Tokens per Sec:     3773, Lr: 0.000115\r\n",
      "2024-08-27 16:27:49,816 - INFO - joeynmt.training - Epoch 253, Step:    24010, Batch Loss:     0.050337, Batch Acc: 0.996909, Tokens per Sec:     3921, Lr: 0.000115\r\n",
      "2024-08-27 16:27:50,918 - INFO - joeynmt.training - Epoch 253, Step:    24015, Batch Loss:     0.050366, Batch Acc: 0.997018, Tokens per Sec:     3957, Lr: 0.000115\r\n",
      "2024-08-27 16:27:51,993 - INFO - joeynmt.training - Epoch 253, Step:    24020, Batch Loss:     0.061611, Batch Acc: 0.997069, Tokens per Sec:     4129, Lr: 0.000115\r\n",
      "2024-08-27 16:27:53,058 - INFO - joeynmt.training - Epoch 253, Step:    24025, Batch Loss:     0.056215, Batch Acc: 0.996840, Tokens per Sec:     3865, Lr: 0.000115\r\n",
      "2024-08-27 16:27:54,115 - INFO - joeynmt.training - Epoch 253, Step:    24030, Batch Loss:     0.048611, Batch Acc: 0.996172, Tokens per Sec:     3956, Lr: 0.000115\r\n",
      "2024-08-27 16:27:55,179 - INFO - joeynmt.training - Epoch 253, Step:    24035, Batch Loss:     0.049161, Batch Acc: 0.995652, Tokens per Sec:     3896, Lr: 0.000115\r\n",
      "2024-08-27 16:27:56,291 - INFO - joeynmt.training - Epoch 253, Step:    24040, Batch Loss:     0.048491, Batch Acc: 0.998083, Tokens per Sec:     3757, Lr: 0.000115\r\n",
      "2024-08-27 16:27:57,420 - INFO - joeynmt.training - Epoch 253, Step:    24045, Batch Loss:     0.059417, Batch Acc: 0.997092, Tokens per Sec:     3962, Lr: 0.000115\r\n",
      "2024-08-27 16:27:58,489 - INFO - joeynmt.training - Epoch 253, Step:    24050, Batch Loss:     0.051941, Batch Acc: 0.995020, Tokens per Sec:     3945, Lr: 0.000115\r\n",
      "2024-08-27 16:27:59,554 - INFO - joeynmt.training - Epoch 253, Step:    24055, Batch Loss:     0.053190, Batch Acc: 0.997031, Tokens per Sec:     3801, Lr: 0.000115\r\n",
      "2024-08-27 16:28:00,622 - INFO - joeynmt.training - Epoch 253, Step:    24060, Batch Loss:     0.053686, Batch Acc: 0.995919, Tokens per Sec:     3901, Lr: 0.000115\r\n",
      "2024-08-27 16:28:01,679 - INFO - joeynmt.training - Epoch 253, Step:    24065, Batch Loss:     0.056686, Batch Acc: 0.995112, Tokens per Sec:     4068, Lr: 0.000115\r\n",
      "2024-08-27 16:28:02,752 - INFO - joeynmt.training - Epoch 253, Step:    24070, Batch Loss:     0.053606, Batch Acc: 0.997820, Tokens per Sec:     3849, Lr: 0.000115\r\n",
      "2024-08-27 16:28:02,753 - INFO - joeynmt.training - Epoch 253, total training loss: 5.12, num. of seqs: 8065, num. of tokens: 80463, 20.4326[sec]\r\n",
      "2024-08-27 16:28:02,753 - INFO - joeynmt.training - EPOCH 254\r\n",
      "2024-08-27 16:28:03,824 - INFO - joeynmt.training - Epoch 254, Step:    24075, Batch Loss:     0.057859, Batch Acc: 0.997689, Tokens per Sec:     4056, Lr: 0.000115\r\n",
      "2024-08-27 16:28:04,893 - INFO - joeynmt.training - Epoch 254, Step:    24080, Batch Loss:     0.054355, Batch Acc: 0.996189, Tokens per Sec:     3928, Lr: 0.000115\r\n",
      "2024-08-27 16:28:05,970 - INFO - joeynmt.training - Epoch 254, Step:    24085, Batch Loss:     0.053763, Batch Acc: 0.995750, Tokens per Sec:     3936, Lr: 0.000115\r\n",
      "2024-08-27 16:28:07,070 - INFO - joeynmt.training - Epoch 254, Step:    24090, Batch Loss:     0.049951, Batch Acc: 0.996049, Tokens per Sec:     3914, Lr: 0.000115\r\n",
      "2024-08-27 16:28:08,126 - INFO - joeynmt.training - Epoch 254, Step:    24095, Batch Loss:     0.048143, Batch Acc: 0.997981, Tokens per Sec:     3756, Lr: 0.000115\r\n",
      "2024-08-27 16:28:09,177 - INFO - joeynmt.training - Epoch 254, Step:    24100, Batch Loss:     0.056675, Batch Acc: 0.996634, Tokens per Sec:     3958, Lr: 0.000115\r\n",
      "2024-08-27 16:28:10,247 - INFO - joeynmt.training - Epoch 254, Step:    24105, Batch Loss:     0.056685, Batch Acc: 0.995810, Tokens per Sec:     4017, Lr: 0.000115\r\n",
      "2024-08-27 16:28:11,318 - INFO - joeynmt.training - Epoch 254, Step:    24110, Batch Loss:     0.050842, Batch Acc: 0.996698, Tokens per Sec:     3961, Lr: 0.000115\r\n",
      "2024-08-27 16:28:12,396 - INFO - joeynmt.training - Epoch 254, Step:    24115, Batch Loss:     0.052318, Batch Acc: 0.996705, Tokens per Sec:     3944, Lr: 0.000115\r\n",
      "2024-08-27 16:28:13,464 - INFO - joeynmt.training - Epoch 254, Step:    24120, Batch Loss:     0.057458, Batch Acc: 0.995765, Tokens per Sec:     3983, Lr: 0.000115\r\n",
      "2024-08-27 16:28:14,512 - INFO - joeynmt.training - Epoch 254, Step:    24125, Batch Loss:     0.053953, Batch Acc: 0.997137, Tokens per Sec:     4001, Lr: 0.000115\r\n",
      "2024-08-27 16:28:15,575 - INFO - joeynmt.training - Epoch 254, Step:    24130, Batch Loss:     0.059058, Batch Acc: 0.995258, Tokens per Sec:     3972, Lr: 0.000115\r\n",
      "2024-08-27 16:28:16,833 - INFO - joeynmt.training - Epoch 254, Step:    24135, Batch Loss:     0.050044, Batch Acc: 0.996301, Tokens per Sec:     3440, Lr: 0.000115\r\n",
      "2024-08-27 16:28:17,910 - INFO - joeynmt.training - Epoch 254, Step:    24140, Batch Loss:     0.055665, Batch Acc: 0.995843, Tokens per Sec:     3802, Lr: 0.000115\r\n",
      "2024-08-27 16:28:18,969 - INFO - joeynmt.training - Epoch 254, Step:    24145, Batch Loss:     0.061710, Batch Acc: 0.993780, Tokens per Sec:     3947, Lr: 0.000115\r\n",
      "2024-08-27 16:28:20,022 - INFO - joeynmt.training - Epoch 254, Step:    24150, Batch Loss:     0.056333, Batch Acc: 0.996960, Tokens per Sec:     3752, Lr: 0.000115\r\n",
      "2024-08-27 16:28:21,125 - INFO - joeynmt.training - Epoch 254, Step:    24155, Batch Loss:     0.046685, Batch Acc: 0.997666, Tokens per Sec:     3888, Lr: 0.000115\r\n",
      "2024-08-27 16:28:22,207 - INFO - joeynmt.training - Epoch 254, Step:    24160, Batch Loss:     0.060213, Batch Acc: 0.996917, Tokens per Sec:     3898, Lr: 0.000115\r\n",
      "2024-08-27 16:28:23,306 - INFO - joeynmt.training - Epoch 254, Step:    24165, Batch Loss:     0.060429, Batch Acc: 0.995501, Tokens per Sec:     4050, Lr: 0.000115\r\n",
      "2024-08-27 16:28:23,408 - INFO - joeynmt.training - Epoch 254, total training loss: 5.20, num. of seqs: 8065, num. of tokens: 80463, 20.6385[sec]\r\n",
      "2024-08-27 16:28:23,408 - INFO - joeynmt.training - EPOCH 255\r\n",
      "2024-08-27 16:28:24,496 - INFO - joeynmt.training - Epoch 255, Step:    24170, Batch Loss:     0.055430, Batch Acc: 0.995437, Tokens per Sec:     3844, Lr: 0.000115\r\n",
      "2024-08-27 16:28:25,567 - INFO - joeynmt.training - Epoch 255, Step:    24175, Batch Loss:     0.053535, Batch Acc: 0.996801, Tokens per Sec:     3798, Lr: 0.000115\r\n",
      "2024-08-27 16:28:26,655 - INFO - joeynmt.training - Epoch 255, Step:    24180, Batch Loss:     0.051285, Batch Acc: 0.995299, Tokens per Sec:     3912, Lr: 0.000115\r\n",
      "2024-08-27 16:28:27,765 - INFO - joeynmt.training - Epoch 255, Step:    24185, Batch Loss:     0.052873, Batch Acc: 0.995777, Tokens per Sec:     4055, Lr: 0.000115\r\n",
      "2024-08-27 16:28:28,812 - INFO - joeynmt.training - Epoch 255, Step:    24190, Batch Loss:     0.058730, Batch Acc: 0.996698, Tokens per Sec:     3762, Lr: 0.000115\r\n",
      "2024-08-27 16:28:29,862 - INFO - joeynmt.training - Epoch 255, Step:    24195, Batch Loss:     0.049839, Batch Acc: 0.997777, Tokens per Sec:     3861, Lr: 0.000115\r\n",
      "2024-08-27 16:28:30,925 - INFO - joeynmt.training - Epoch 255, Step:    24200, Batch Loss:     0.069477, Batch Acc: 0.997140, Tokens per Sec:     3949, Lr: 0.000115\r\n",
      "2024-08-27 16:28:32,022 - INFO - joeynmt.training - Epoch 255, Step:    24205, Batch Loss:     0.065251, Batch Acc: 0.994890, Tokens per Sec:     3926, Lr: 0.000115\r\n",
      "2024-08-27 16:28:33,103 - INFO - joeynmt.training - Epoch 255, Step:    24210, Batch Loss:     0.051553, Batch Acc: 0.996574, Tokens per Sec:     4052, Lr: 0.000115\r\n",
      "2024-08-27 16:28:34,178 - INFO - joeynmt.training - Epoch 255, Step:    24215, Batch Loss:     0.054715, Batch Acc: 0.996012, Tokens per Sec:     3969, Lr: 0.000115\r\n",
      "2024-08-27 16:28:35,266 - INFO - joeynmt.training - Epoch 255, Step:    24220, Batch Loss:     0.052076, Batch Acc: 0.996085, Tokens per Sec:     3992, Lr: 0.000115\r\n",
      "2024-08-27 16:28:36,332 - INFO - joeynmt.training - Epoch 255, Step:    24225, Batch Loss:     0.047934, Batch Acc: 0.997082, Tokens per Sec:     3859, Lr: 0.000115\r\n",
      "2024-08-27 16:28:37,445 - INFO - joeynmt.training - Epoch 255, Step:    24230, Batch Loss:     0.048939, Batch Acc: 0.997865, Tokens per Sec:     3792, Lr: 0.000115\r\n",
      "2024-08-27 16:28:38,516 - INFO - joeynmt.training - Epoch 255, Step:    24235, Batch Loss:     0.050886, Batch Acc: 0.996895, Tokens per Sec:     3912, Lr: 0.000115\r\n",
      "2024-08-27 16:28:39,582 - INFO - joeynmt.training - Epoch 255, Step:    24240, Batch Loss:     0.053264, Batch Acc: 0.998798, Tokens per Sec:     3906, Lr: 0.000115\r\n",
      "2024-08-27 16:28:40,649 - INFO - joeynmt.training - Epoch 255, Step:    24245, Batch Loss:     0.070182, Batch Acc: 0.994574, Tokens per Sec:     3974, Lr: 0.000115\r\n",
      "2024-08-27 16:28:41,715 - INFO - joeynmt.training - Epoch 255, Step:    24250, Batch Loss:     0.050201, Batch Acc: 0.997680, Tokens per Sec:     4045, Lr: 0.000115\r\n",
      "2024-08-27 16:28:42,779 - INFO - joeynmt.training - Epoch 255, Step:    24255, Batch Loss:     0.051520, Batch Acc: 0.995728, Tokens per Sec:     3964, Lr: 0.000115\r\n",
      "2024-08-27 16:28:43,843 - INFO - joeynmt.training - Epoch 255, Step:    24260, Batch Loss:     0.047412, Batch Acc: 0.997297, Tokens per Sec:     3825, Lr: 0.000115\r\n",
      "2024-08-27 16:28:43,994 - INFO - joeynmt.training - Epoch 255, total training loss: 5.14, num. of seqs: 8065, num. of tokens: 80463, 20.5676[sec]\r\n",
      "2024-08-27 16:28:43,994 - INFO - joeynmt.training - EPOCH 256\r\n",
      "2024-08-27 16:28:45,086 - INFO - joeynmt.training - Epoch 256, Step:    24265, Batch Loss:     0.053613, Batch Acc: 0.995615, Tokens per Sec:     3981, Lr: 0.000115\r\n",
      "2024-08-27 16:28:46,166 - INFO - joeynmt.training - Epoch 256, Step:    24270, Batch Loss:     0.057626, Batch Acc: 0.996025, Tokens per Sec:     3963, Lr: 0.000115\r\n",
      "2024-08-27 16:28:47,364 - INFO - joeynmt.training - Epoch 256, Step:    24275, Batch Loss:     0.052177, Batch Acc: 0.997395, Tokens per Sec:     3526, Lr: 0.000115\r\n",
      "2024-08-27 16:28:48,490 - INFO - joeynmt.training - Epoch 256, Step:    24280, Batch Loss:     0.063366, Batch Acc: 0.994672, Tokens per Sec:     4173, Lr: 0.000115\r\n",
      "2024-08-27 16:28:49,577 - INFO - joeynmt.training - Epoch 256, Step:    24285, Batch Loss:     0.055108, Batch Acc: 0.995596, Tokens per Sec:     3762, Lr: 0.000115\r\n",
      "2024-08-27 16:28:50,664 - INFO - joeynmt.training - Epoch 256, Step:    24290, Batch Loss:     0.048573, Batch Acc: 0.996095, Tokens per Sec:     3769, Lr: 0.000115\r\n",
      "2024-08-27 16:28:51,755 - INFO - joeynmt.training - Epoch 256, Step:    24295, Batch Loss:     0.053690, Batch Acc: 0.996541, Tokens per Sec:     3980, Lr: 0.000115\r\n",
      "2024-08-27 16:28:52,870 - INFO - joeynmt.training - Epoch 256, Step:    24300, Batch Loss:     0.054185, Batch Acc: 0.995441, Tokens per Sec:     3738, Lr: 0.000115\r\n",
      "2024-08-27 16:28:53,950 - INFO - joeynmt.training - Epoch 256, Step:    24305, Batch Loss:     0.052571, Batch Acc: 0.994412, Tokens per Sec:     3815, Lr: 0.000115\r\n",
      "2024-08-27 16:28:55,046 - INFO - joeynmt.training - Epoch 256, Step:    24310, Batch Loss:     0.055701, Batch Acc: 0.995231, Tokens per Sec:     3829, Lr: 0.000115\r\n",
      "2024-08-27 16:28:56,138 - INFO - joeynmt.training - Epoch 256, Step:    24315, Batch Loss:     0.054748, Batch Acc: 0.997241, Tokens per Sec:     3652, Lr: 0.000115\r\n",
      "2024-08-27 16:28:57,250 - INFO - joeynmt.training - Epoch 256, Step:    24320, Batch Loss:     0.065150, Batch Acc: 0.995582, Tokens per Sec:     3873, Lr: 0.000115\r\n",
      "2024-08-27 16:28:58,339 - INFO - joeynmt.training - Epoch 256, Step:    24325, Batch Loss:     0.049301, Batch Acc: 0.996896, Tokens per Sec:     4142, Lr: 0.000115\r\n",
      "2024-08-27 16:28:59,410 - INFO - joeynmt.training - Epoch 256, Step:    24330, Batch Loss:     0.054558, Batch Acc: 0.996508, Tokens per Sec:     4014, Lr: 0.000115\r\n",
      "2024-08-27 16:29:00,474 - INFO - joeynmt.training - Epoch 256, Step:    24335, Batch Loss:     0.057720, Batch Acc: 0.994874, Tokens per Sec:     4037, Lr: 0.000115\r\n",
      "2024-08-27 16:29:01,546 - INFO - joeynmt.training - Epoch 256, Step:    24340, Batch Loss:     0.059259, Batch Acc: 0.997870, Tokens per Sec:     3943, Lr: 0.000115\r\n",
      "2024-08-27 16:29:02,621 - INFO - joeynmt.training - Epoch 256, Step:    24345, Batch Loss:     0.061613, Batch Acc: 0.995224, Tokens per Sec:     3896, Lr: 0.000115\r\n",
      "2024-08-27 16:29:03,695 - INFO - joeynmt.training - Epoch 256, Step:    24350, Batch Loss:     0.061559, Batch Acc: 0.996375, Tokens per Sec:     3857, Lr: 0.000115\r\n",
      "2024-08-27 16:29:04,770 - INFO - joeynmt.training - Epoch 256, Step:    24355, Batch Loss:     0.059205, Batch Acc: 0.995503, Tokens per Sec:     3724, Lr: 0.000115\r\n",
      "2024-08-27 16:29:04,771 - INFO - joeynmt.training - Epoch 256, total training loss: 5.22, num. of seqs: 8065, num. of tokens: 80463, 20.7600[sec]\r\n",
      "2024-08-27 16:29:04,771 - INFO - joeynmt.training - EPOCH 257\r\n",
      "2024-08-27 16:29:05,872 - INFO - joeynmt.training - Epoch 257, Step:    24360, Batch Loss:     0.047557, Batch Acc: 0.997886, Tokens per Sec:     3879, Lr: 0.000115\r\n",
      "2024-08-27 16:29:06,976 - INFO - joeynmt.training - Epoch 257, Step:    24365, Batch Loss:     0.053209, Batch Acc: 0.996750, Tokens per Sec:     3907, Lr: 0.000115\r\n",
      "2024-08-27 16:29:08,058 - INFO - joeynmt.training - Epoch 257, Step:    24370, Batch Loss:     0.047566, Batch Acc: 0.996964, Tokens per Sec:     3957, Lr: 0.000115\r\n",
      "2024-08-27 16:29:09,128 - INFO - joeynmt.training - Epoch 257, Step:    24375, Batch Loss:     0.044793, Batch Acc: 0.995668, Tokens per Sec:     3884, Lr: 0.000115\r\n",
      "2024-08-27 16:29:10,183 - INFO - joeynmt.training - Epoch 257, Step:    24380, Batch Loss:     0.059322, Batch Acc: 0.993151, Tokens per Sec:     3739, Lr: 0.000115\r\n",
      "2024-08-27 16:29:11,264 - INFO - joeynmt.training - Epoch 257, Step:    24385, Batch Loss:     0.051403, Batch Acc: 0.997259, Tokens per Sec:     4054, Lr: 0.000115\r\n",
      "2024-08-27 16:29:12,327 - INFO - joeynmt.training - Epoch 257, Step:    24390, Batch Loss:     0.055962, Batch Acc: 0.995244, Tokens per Sec:     3957, Lr: 0.000115\r\n",
      "2024-08-27 16:29:13,410 - INFO - joeynmt.training - Epoch 257, Step:    24395, Batch Loss:     0.059430, Batch Acc: 0.996535, Tokens per Sec:     4002, Lr: 0.000115\r\n",
      "2024-08-27 16:29:14,490 - INFO - joeynmt.training - Epoch 257, Step:    24400, Batch Loss:     0.054005, Batch Acc: 0.997141, Tokens per Sec:     3889, Lr: 0.000115\r\n",
      "2024-08-27 16:29:15,561 - INFO - joeynmt.training - Epoch 257, Step:    24405, Batch Loss:     0.050155, Batch Acc: 0.995595, Tokens per Sec:     4027, Lr: 0.000115\r\n",
      "2024-08-27 16:29:16,650 - INFO - joeynmt.training - Epoch 257, Step:    24410, Batch Loss:     0.054056, Batch Acc: 0.997050, Tokens per Sec:     4052, Lr: 0.000114\r\n",
      "2024-08-27 16:29:17,742 - INFO - joeynmt.training - Epoch 257, Step:    24415, Batch Loss:     0.052128, Batch Acc: 0.997568, Tokens per Sec:     3768, Lr: 0.000114\r\n",
      "2024-08-27 16:29:18,892 - INFO - joeynmt.training - Epoch 257, Step:    24420, Batch Loss:     0.062784, Batch Acc: 0.994916, Tokens per Sec:     3593, Lr: 0.000114\r\n",
      "2024-08-27 16:29:19,970 - INFO - joeynmt.training - Epoch 257, Step:    24425, Batch Loss:     0.050755, Batch Acc: 0.997868, Tokens per Sec:     3918, Lr: 0.000114\r\n",
      "2024-08-27 16:29:21,020 - INFO - joeynmt.training - Epoch 257, Step:    24430, Batch Loss:     0.056236, Batch Acc: 0.996428, Tokens per Sec:     4003, Lr: 0.000114\r\n",
      "2024-08-27 16:29:22,082 - INFO - joeynmt.training - Epoch 257, Step:    24435, Batch Loss:     0.060712, Batch Acc: 0.994715, Tokens per Sec:     3923, Lr: 0.000114\r\n",
      "2024-08-27 16:29:23,156 - INFO - joeynmt.training - Epoch 257, Step:    24440, Batch Loss:     0.056353, Batch Acc: 0.995882, Tokens per Sec:     4072, Lr: 0.000114\r\n",
      "2024-08-27 16:29:24,274 - INFO - joeynmt.training - Epoch 257, Step:    24445, Batch Loss:     0.055532, Batch Acc: 0.996137, Tokens per Sec:     3939, Lr: 0.000114\r\n",
      "2024-08-27 16:29:25,350 - INFO - joeynmt.training - Epoch 257, Step:    24450, Batch Loss:     0.051192, Batch Acc: 0.996089, Tokens per Sec:     3806, Lr: 0.000114\r\n",
      "2024-08-27 16:29:25,350 - INFO - joeynmt.training - Epoch 257, total training loss: 5.15, num. of seqs: 8065, num. of tokens: 80463, 20.5627[sec]\r\n",
      "2024-08-27 16:29:25,350 - INFO - joeynmt.training - EPOCH 258\r\n",
      "2024-08-27 16:29:26,423 - INFO - joeynmt.training - Epoch 258, Step:    24455, Batch Loss:     0.046725, Batch Acc: 0.997322, Tokens per Sec:     3842, Lr: 0.000114\r\n",
      "2024-08-27 16:29:27,509 - INFO - joeynmt.training - Epoch 258, Step:    24460, Batch Loss:     0.046884, Batch Acc: 0.996845, Tokens per Sec:     3795, Lr: 0.000114\r\n",
      "2024-08-27 16:29:28,570 - INFO - joeynmt.training - Epoch 258, Step:    24465, Batch Loss:     0.050659, Batch Acc: 0.997098, Tokens per Sec:     3899, Lr: 0.000114\r\n",
      "2024-08-27 16:29:29,644 - INFO - joeynmt.training - Epoch 258, Step:    24470, Batch Loss:     0.053426, Batch Acc: 0.997319, Tokens per Sec:     3823, Lr: 0.000114\r\n",
      "2024-08-27 16:29:30,683 - INFO - joeynmt.training - Epoch 258, Step:    24475, Batch Loss:     0.054512, Batch Acc: 0.996054, Tokens per Sec:     3907, Lr: 0.000114\r\n",
      "2024-08-27 16:29:31,734 - INFO - joeynmt.training - Epoch 258, Step:    24480, Batch Loss:     0.059360, Batch Acc: 0.996936, Tokens per Sec:     4040, Lr: 0.000114\r\n",
      "2024-08-27 16:29:32,811 - INFO - joeynmt.training - Epoch 258, Step:    24485, Batch Loss:     0.059239, Batch Acc: 0.995747, Tokens per Sec:     4149, Lr: 0.000114\r\n",
      "2024-08-27 16:29:33,882 - INFO - joeynmt.training - Epoch 258, Step:    24490, Batch Loss:     0.050347, Batch Acc: 0.995615, Tokens per Sec:     3835, Lr: 0.000114\r\n",
      "2024-08-27 16:29:34,957 - INFO - joeynmt.training - Epoch 258, Step:    24495, Batch Loss:     0.049847, Batch Acc: 0.997043, Tokens per Sec:     4091, Lr: 0.000114\r\n",
      "2024-08-27 16:29:36,019 - INFO - joeynmt.training - Epoch 258, Step:    24500, Batch Loss:     0.048297, Batch Acc: 0.998109, Tokens per Sec:     3987, Lr: 0.000114\r\n",
      "2024-08-27 16:29:36,020 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=24542\r\n",
      "2024-08-27 16:29:36,020 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:23<00:00, 61.22it/s]\r\n",
      "2024-08-27 16:29:59,872 - INFO - joeynmt.prediction - Generation took 23.8495[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44868.16 examples/s]\r\n",
      "2024-08-27 16:30:00,250 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:30:00,251 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.70, loss:   6.70, ppl: 810.66, acc:   0.29, 0.1658[sec]\r\n",
      "2024-08-27 16:30:00,556 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/24500.ckpt.\r\n",
      "2024-08-27 16:30:00,557 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/20000.ckpt\r\n",
      "2024-08-27 16:30:00,582 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:30:00,728 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:30:00,729 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:30:00,729 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 16:30:01,019 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:30:01,164 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:30:01,164 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:30:01,164 - INFO - joeynmt.training - \tHypothesis: trois éléphants\r\n",
      "2024-08-27 16:30:01,456 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:30:01,601 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:30:01,601 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:30:01,601 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 16:30:01,892 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:30:02,038 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:30:02,038 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:30:02,038 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 16:30:03,415 - INFO - joeynmt.training - Epoch 258, Step:    24505, Batch Loss:     0.045688, Batch Acc: 0.997403, Tokens per Sec:     3903, Lr: 0.000114\r\n",
      "2024-08-27 16:30:04,511 - INFO - joeynmt.training - Epoch 258, Step:    24510, Batch Loss:     0.049580, Batch Acc: 0.996729, Tokens per Sec:     3907, Lr: 0.000114\r\n",
      "2024-08-27 16:30:05,587 - INFO - joeynmt.training - Epoch 258, Step:    24515, Batch Loss:     0.051675, Batch Acc: 0.996481, Tokens per Sec:     3963, Lr: 0.000114\r\n",
      "2024-08-27 16:30:06,681 - INFO - joeynmt.training - Epoch 258, Step:    24520, Batch Loss:     0.053931, Batch Acc: 0.997042, Tokens per Sec:     4021, Lr: 0.000114\r\n",
      "2024-08-27 16:30:07,741 - INFO - joeynmt.training - Epoch 258, Step:    24525, Batch Loss:     0.050420, Batch Acc: 0.995107, Tokens per Sec:     3666, Lr: 0.000114\r\n",
      "2024-08-27 16:30:08,790 - INFO - joeynmt.training - Epoch 258, Step:    24530, Batch Loss:     0.067986, Batch Acc: 0.994526, Tokens per Sec:     3834, Lr: 0.000114\r\n",
      "2024-08-27 16:30:09,872 - INFO - joeynmt.training - Epoch 258, Step:    24535, Batch Loss:     0.050093, Batch Acc: 0.995354, Tokens per Sec:     4181, Lr: 0.000114\r\n",
      "2024-08-27 16:30:10,930 - INFO - joeynmt.training - Epoch 258, Step:    24540, Batch Loss:     0.056226, Batch Acc: 0.995583, Tokens per Sec:     4069, Lr: 0.000114\r\n",
      "2024-08-27 16:30:11,980 - INFO - joeynmt.training - Epoch 258, Step:    24545, Batch Loss:     0.050830, Batch Acc: 0.995009, Tokens per Sec:     3817, Lr: 0.000114\r\n",
      "2024-08-27 16:30:12,130 - INFO - joeynmt.training - Epoch 258, total training loss: 5.17, num. of seqs: 8065, num. of tokens: 80463, 20.4532[sec]\r\n",
      "2024-08-27 16:30:12,130 - INFO - joeynmt.training - EPOCH 259\r\n",
      "2024-08-27 16:30:13,212 - INFO - joeynmt.training - Epoch 259, Step:    24550, Batch Loss:     0.057125, Batch Acc: 0.997288, Tokens per Sec:     4108, Lr: 0.000114\r\n",
      "2024-08-27 16:30:14,268 - INFO - joeynmt.training - Epoch 259, Step:    24555, Batch Loss:     0.052131, Batch Acc: 0.995389, Tokens per Sec:     3905, Lr: 0.000114\r\n",
      "2024-08-27 16:30:15,350 - INFO - joeynmt.training - Epoch 259, Step:    24560, Batch Loss:     0.046089, Batch Acc: 0.995592, Tokens per Sec:     3985, Lr: 0.000114\r\n",
      "2024-08-27 16:30:16,406 - INFO - joeynmt.training - Epoch 259, Step:    24565, Batch Loss:     0.052423, Batch Acc: 0.997785, Tokens per Sec:     3848, Lr: 0.000114\r\n",
      "2024-08-27 16:30:17,488 - INFO - joeynmt.training - Epoch 259, Step:    24570, Batch Loss:     0.057977, Batch Acc: 0.996527, Tokens per Sec:     3729, Lr: 0.000114\r\n",
      "2024-08-27 16:30:18,554 - INFO - joeynmt.training - Epoch 259, Step:    24575, Batch Loss:     0.049301, Batch Acc: 0.996411, Tokens per Sec:     3922, Lr: 0.000114\r\n",
      "2024-08-27 16:30:19,616 - INFO - joeynmt.training - Epoch 259, Step:    24580, Batch Loss:     0.046573, Batch Acc: 0.996063, Tokens per Sec:     4071, Lr: 0.000114\r\n",
      "2024-08-27 16:30:20,706 - INFO - joeynmt.training - Epoch 259, Step:    24585, Batch Loss:     0.052047, Batch Acc: 0.996621, Tokens per Sec:     3803, Lr: 0.000114\r\n",
      "2024-08-27 16:30:21,838 - INFO - joeynmt.training - Epoch 259, Step:    24590, Batch Loss:     0.053520, Batch Acc: 0.996421, Tokens per Sec:     3704, Lr: 0.000114\r\n",
      "2024-08-27 16:30:22,899 - INFO - joeynmt.training - Epoch 259, Step:    24595, Batch Loss:     0.049667, Batch Acc: 0.995627, Tokens per Sec:     3880, Lr: 0.000114\r\n",
      "2024-08-27 16:30:23,961 - INFO - joeynmt.training - Epoch 259, Step:    24600, Batch Loss:     0.058990, Batch Acc: 0.997213, Tokens per Sec:     4057, Lr: 0.000114\r\n",
      "2024-08-27 16:30:25,031 - INFO - joeynmt.training - Epoch 259, Step:    24605, Batch Loss:     0.052870, Batch Acc: 0.996963, Tokens per Sec:     4004, Lr: 0.000114\r\n",
      "2024-08-27 16:30:26,103 - INFO - joeynmt.training - Epoch 259, Step:    24610, Batch Loss:     0.061406, Batch Acc: 0.994146, Tokens per Sec:     3828, Lr: 0.000114\r\n",
      "2024-08-27 16:30:27,238 - INFO - joeynmt.training - Epoch 259, Step:    24615, Batch Loss:     0.056491, Batch Acc: 0.996026, Tokens per Sec:     3771, Lr: 0.000114\r\n",
      "2024-08-27 16:30:28,318 - INFO - joeynmt.training - Epoch 259, Step:    24620, Batch Loss:     0.047786, Batch Acc: 0.996707, Tokens per Sec:     3940, Lr: 0.000114\r\n",
      "2024-08-27 16:30:29,379 - INFO - joeynmt.training - Epoch 259, Step:    24625, Batch Loss:     0.050070, Batch Acc: 0.996877, Tokens per Sec:     3925, Lr: 0.000114\r\n",
      "2024-08-27 16:30:30,435 - INFO - joeynmt.training - Epoch 259, Step:    24630, Batch Loss:     0.051313, Batch Acc: 0.996291, Tokens per Sec:     3830, Lr: 0.000114\r\n",
      "2024-08-27 16:30:31,507 - INFO - joeynmt.training - Epoch 259, Step:    24635, Batch Loss:     0.057436, Batch Acc: 0.996948, Tokens per Sec:     3975, Lr: 0.000114\r\n",
      "2024-08-27 16:30:32,567 - INFO - joeynmt.training - Epoch 259, Step:    24640, Batch Loss:     0.052748, Batch Acc: 0.995734, Tokens per Sec:     3985, Lr: 0.000114\r\n",
      "2024-08-27 16:30:32,772 - INFO - joeynmt.training - Epoch 259, total training loss: 5.20, num. of seqs: 8065, num. of tokens: 80463, 20.6245[sec]\r\n",
      "2024-08-27 16:30:32,772 - INFO - joeynmt.training - EPOCH 260\r\n",
      "2024-08-27 16:30:33,616 - INFO - joeynmt.training - Epoch 260, Step:    24645, Batch Loss:     0.051747, Batch Acc: 0.999077, Tokens per Sec:     3867, Lr: 0.000114\r\n",
      "2024-08-27 16:30:34,706 - INFO - joeynmt.training - Epoch 260, Step:    24650, Batch Loss:     0.055378, Batch Acc: 0.998141, Tokens per Sec:     3951, Lr: 0.000114\r\n",
      "2024-08-27 16:30:35,776 - INFO - joeynmt.training - Epoch 260, Step:    24655, Batch Loss:     0.049466, Batch Acc: 0.997592, Tokens per Sec:     3882, Lr: 0.000114\r\n",
      "2024-08-27 16:30:36,867 - INFO - joeynmt.training - Epoch 260, Step:    24660, Batch Loss:     0.054339, Batch Acc: 0.997842, Tokens per Sec:     3825, Lr: 0.000114\r\n",
      "2024-08-27 16:30:37,957 - INFO - joeynmt.training - Epoch 260, Step:    24665, Batch Loss:     0.046157, Batch Acc: 0.998301, Tokens per Sec:     3780, Lr: 0.000114\r\n",
      "2024-08-27 16:30:39,025 - INFO - joeynmt.training - Epoch 260, Step:    24670, Batch Loss:     0.051317, Batch Acc: 0.996276, Tokens per Sec:     4023, Lr: 0.000114\r\n",
      "2024-08-27 16:30:40,100 - INFO - joeynmt.training - Epoch 260, Step:    24675, Batch Loss:     0.052405, Batch Acc: 0.996322, Tokens per Sec:     4050, Lr: 0.000114\r\n",
      "2024-08-27 16:30:41,191 - INFO - joeynmt.training - Epoch 260, Step:    24680, Batch Loss:     0.055266, Batch Acc: 0.996962, Tokens per Sec:     3925, Lr: 0.000114\r\n",
      "2024-08-27 16:30:42,289 - INFO - joeynmt.training - Epoch 260, Step:    24685, Batch Loss:     0.052397, Batch Acc: 0.995939, Tokens per Sec:     3591, Lr: 0.000114\r\n",
      "2024-08-27 16:30:43,371 - INFO - joeynmt.training - Epoch 260, Step:    24690, Batch Loss:     0.048431, Batch Acc: 0.997329, Tokens per Sec:     3811, Lr: 0.000114\r\n",
      "2024-08-27 16:30:44,444 - INFO - joeynmt.training - Epoch 260, Step:    24695, Batch Loss:     0.050574, Batch Acc: 0.996622, Tokens per Sec:     3865, Lr: 0.000114\r\n",
      "2024-08-27 16:30:45,516 - INFO - joeynmt.training - Epoch 260, Step:    24700, Batch Loss:     0.047809, Batch Acc: 0.997011, Tokens per Sec:     4059, Lr: 0.000114\r\n",
      "2024-08-27 16:30:46,604 - INFO - joeynmt.training - Epoch 260, Step:    24705, Batch Loss:     0.050627, Batch Acc: 0.995833, Tokens per Sec:     3974, Lr: 0.000114\r\n",
      "2024-08-27 16:30:47,718 - INFO - joeynmt.training - Epoch 260, Step:    24710, Batch Loss:     0.053143, Batch Acc: 0.995504, Tokens per Sec:     3995, Lr: 0.000114\r\n",
      "2024-08-27 16:30:48,796 - INFO - joeynmt.training - Epoch 260, Step:    24715, Batch Loss:     0.055269, Batch Acc: 0.996904, Tokens per Sec:     3898, Lr: 0.000114\r\n",
      "2024-08-27 16:30:49,853 - INFO - joeynmt.training - Epoch 260, Step:    24720, Batch Loss:     0.053433, Batch Acc: 0.997420, Tokens per Sec:     4036, Lr: 0.000114\r\n",
      "2024-08-27 16:30:50,923 - INFO - joeynmt.training - Epoch 260, Step:    24725, Batch Loss:     0.051906, Batch Acc: 0.996354, Tokens per Sec:     3846, Lr: 0.000114\r\n",
      "2024-08-27 16:30:52,025 - INFO - joeynmt.training - Epoch 260, Step:    24730, Batch Loss:     0.055518, Batch Acc: 0.994172, Tokens per Sec:     3739, Lr: 0.000114\r\n",
      "2024-08-27 16:30:53,218 - INFO - joeynmt.training - Epoch 260, Step:    24735, Batch Loss:     0.057988, Batch Acc: 0.996068, Tokens per Sec:     3629, Lr: 0.000114\r\n",
      "2024-08-27 16:30:53,538 - INFO - joeynmt.training - Epoch 260, total training loss: 4.97, num. of seqs: 8065, num. of tokens: 80463, 20.7501[sec]\r\n",
      "2024-08-27 16:30:53,539 - INFO - joeynmt.training - EPOCH 261\r\n",
      "2024-08-27 16:30:54,411 - INFO - joeynmt.training - Epoch 261, Step:    24740, Batch Loss:     0.050700, Batch Acc: 0.995384, Tokens per Sec:     4240, Lr: 0.000114\r\n",
      "2024-08-27 16:30:55,467 - INFO - joeynmt.training - Epoch 261, Step:    24745, Batch Loss:     0.052135, Batch Acc: 0.997378, Tokens per Sec:     3978, Lr: 0.000114\r\n",
      "2024-08-27 16:30:56,590 - INFO - joeynmt.training - Epoch 261, Step:    24750, Batch Loss:     0.052538, Batch Acc: 0.997154, Tokens per Sec:     3757, Lr: 0.000114\r\n",
      "2024-08-27 16:30:57,681 - INFO - joeynmt.training - Epoch 261, Step:    24755, Batch Loss:     0.054789, Batch Acc: 0.996436, Tokens per Sec:     3859, Lr: 0.000114\r\n",
      "2024-08-27 16:30:58,831 - INFO - joeynmt.training - Epoch 261, Step:    24760, Batch Loss:     0.057464, Batch Acc: 0.996111, Tokens per Sec:     3358, Lr: 0.000114\r\n",
      "2024-08-27 16:30:59,902 - INFO - joeynmt.training - Epoch 261, Step:    24765, Batch Loss:     0.050096, Batch Acc: 0.997042, Tokens per Sec:     4104, Lr: 0.000114\r\n",
      "2024-08-27 16:31:00,962 - INFO - joeynmt.training - Epoch 261, Step:    24770, Batch Loss:     0.050783, Batch Acc: 0.996333, Tokens per Sec:     3861, Lr: 0.000114\r\n",
      "2024-08-27 16:31:02,024 - INFO - joeynmt.training - Epoch 261, Step:    24775, Batch Loss:     0.045251, Batch Acc: 0.997044, Tokens per Sec:     3826, Lr: 0.000114\r\n",
      "2024-08-27 16:31:03,106 - INFO - joeynmt.training - Epoch 261, Step:    24780, Batch Loss:     0.050068, Batch Acc: 0.996358, Tokens per Sec:     3809, Lr: 0.000114\r\n",
      "2024-08-27 16:31:04,174 - INFO - joeynmt.training - Epoch 261, Step:    24785, Batch Loss:     0.051630, Batch Acc: 0.996365, Tokens per Sec:     3863, Lr: 0.000114\r\n",
      "2024-08-27 16:31:05,257 - INFO - joeynmt.training - Epoch 261, Step:    24790, Batch Loss:     0.059302, Batch Acc: 0.996840, Tokens per Sec:     4095, Lr: 0.000114\r\n",
      "2024-08-27 16:31:06,338 - INFO - joeynmt.training - Epoch 261, Step:    24795, Batch Loss:     0.053075, Batch Acc: 0.996853, Tokens per Sec:     3823, Lr: 0.000114\r\n",
      "2024-08-27 16:31:07,442 - INFO - joeynmt.training - Epoch 261, Step:    24800, Batch Loss:     0.070854, Batch Acc: 0.995942, Tokens per Sec:     4023, Lr: 0.000114\r\n",
      "2024-08-27 16:31:08,496 - INFO - joeynmt.training - Epoch 261, Step:    24805, Batch Loss:     0.049459, Batch Acc: 0.997069, Tokens per Sec:     3886, Lr: 0.000114\r\n",
      "2024-08-27 16:31:09,570 - INFO - joeynmt.training - Epoch 261, Step:    24810, Batch Loss:     0.053762, Batch Acc: 0.997425, Tokens per Sec:     3982, Lr: 0.000114\r\n",
      "2024-08-27 16:31:10,645 - INFO - joeynmt.training - Epoch 261, Step:    24815, Batch Loss:     0.050342, Batch Acc: 0.996211, Tokens per Sec:     3928, Lr: 0.000114\r\n",
      "2024-08-27 16:31:11,715 - INFO - joeynmt.training - Epoch 261, Step:    24820, Batch Loss:     0.053016, Batch Acc: 0.996406, Tokens per Sec:     3905, Lr: 0.000114\r\n",
      "2024-08-27 16:31:12,774 - INFO - joeynmt.training - Epoch 261, Step:    24825, Batch Loss:     0.048250, Batch Acc: 0.995649, Tokens per Sec:     3907, Lr: 0.000114\r\n",
      "2024-08-27 16:31:13,841 - INFO - joeynmt.training - Epoch 261, Step:    24830, Batch Loss:     0.055983, Batch Acc: 0.996728, Tokens per Sec:     4015, Lr: 0.000114\r\n",
      "2024-08-27 16:31:14,158 - INFO - joeynmt.training - Epoch 261, total training loss: 5.04, num. of seqs: 8065, num. of tokens: 80463, 20.6030[sec]\r\n",
      "2024-08-27 16:31:14,159 - INFO - joeynmt.training - EPOCH 262\r\n",
      "2024-08-27 16:31:15,026 - INFO - joeynmt.training - Epoch 262, Step:    24835, Batch Loss:     0.053342, Batch Acc: 0.997928, Tokens per Sec:     3917, Lr: 0.000114\r\n",
      "2024-08-27 16:31:16,107 - INFO - joeynmt.training - Epoch 262, Step:    24840, Batch Loss:     0.048412, Batch Acc: 0.997941, Tokens per Sec:     4046, Lr: 0.000114\r\n",
      "2024-08-27 16:31:17,204 - INFO - joeynmt.training - Epoch 262, Step:    24845, Batch Loss:     0.049155, Batch Acc: 0.997895, Tokens per Sec:     3900, Lr: 0.000113\r\n",
      "2024-08-27 16:31:18,276 - INFO - joeynmt.training - Epoch 262, Step:    24850, Batch Loss:     0.046678, Batch Acc: 0.995182, Tokens per Sec:     4259, Lr: 0.000113\r\n",
      "2024-08-27 16:31:19,367 - INFO - joeynmt.training - Epoch 262, Step:    24855, Batch Loss:     0.052158, Batch Acc: 0.996396, Tokens per Sec:     3818, Lr: 0.000113\r\n",
      "2024-08-27 16:31:20,444 - INFO - joeynmt.training - Epoch 262, Step:    24860, Batch Loss:     0.064896, Batch Acc: 0.997488, Tokens per Sec:     4068, Lr: 0.000113\r\n",
      "2024-08-27 16:31:21,513 - INFO - joeynmt.training - Epoch 262, Step:    24865, Batch Loss:     0.047696, Batch Acc: 0.996639, Tokens per Sec:     3900, Lr: 0.000113\r\n",
      "2024-08-27 16:31:22,568 - INFO - joeynmt.training - Epoch 262, Step:    24870, Batch Loss:     0.052300, Batch Acc: 0.995690, Tokens per Sec:     3959, Lr: 0.000113\r\n",
      "2024-08-27 16:31:23,707 - INFO - joeynmt.training - Epoch 262, Step:    24875, Batch Loss:     0.055285, Batch Acc: 0.996256, Tokens per Sec:     3756, Lr: 0.000113\r\n",
      "2024-08-27 16:31:24,779 - INFO - joeynmt.training - Epoch 262, Step:    24880, Batch Loss:     0.054770, Batch Acc: 0.994681, Tokens per Sec:     3858, Lr: 0.000113\r\n",
      "2024-08-27 16:31:25,850 - INFO - joeynmt.training - Epoch 262, Step:    24885, Batch Loss:     0.046303, Batch Acc: 0.996999, Tokens per Sec:     4047, Lr: 0.000113\r\n",
      "2024-08-27 16:31:26,951 - INFO - joeynmt.training - Epoch 262, Step:    24890, Batch Loss:     0.055571, Batch Acc: 0.996609, Tokens per Sec:     3752, Lr: 0.000113\r\n",
      "2024-08-27 16:31:28,019 - INFO - joeynmt.training - Epoch 262, Step:    24895, Batch Loss:     0.061414, Batch Acc: 0.995888, Tokens per Sec:     4102, Lr: 0.000113\r\n",
      "2024-08-27 16:31:29,074 - INFO - joeynmt.training - Epoch 262, Step:    24900, Batch Loss:     0.055659, Batch Acc: 0.997680, Tokens per Sec:     3678, Lr: 0.000113\r\n",
      "2024-08-27 16:31:30,255 - INFO - joeynmt.training - Epoch 262, Step:    24905, Batch Loss:     0.046161, Batch Acc: 0.997314, Tokens per Sec:     3469, Lr: 0.000113\r\n",
      "2024-08-27 16:31:31,327 - INFO - joeynmt.training - Epoch 262, Step:    24910, Batch Loss:     0.054652, Batch Acc: 0.996890, Tokens per Sec:     3903, Lr: 0.000113\r\n",
      "2024-08-27 16:31:32,405 - INFO - joeynmt.training - Epoch 262, Step:    24915, Batch Loss:     0.045986, Batch Acc: 0.997683, Tokens per Sec:     4004, Lr: 0.000113\r\n",
      "2024-08-27 16:31:33,488 - INFO - joeynmt.training - Epoch 262, Step:    24920, Batch Loss:     0.053888, Batch Acc: 0.996075, Tokens per Sec:     3770, Lr: 0.000113\r\n",
      "2024-08-27 16:31:34,562 - INFO - joeynmt.training - Epoch 262, Step:    24925, Batch Loss:     0.052503, Batch Acc: 0.996146, Tokens per Sec:     3864, Lr: 0.000113\r\n",
      "2024-08-27 16:31:34,832 - INFO - joeynmt.training - Epoch 262, total training loss: 4.94, num. of seqs: 8065, num. of tokens: 80463, 20.6573[sec]\r\n",
      "2024-08-27 16:31:34,832 - INFO - joeynmt.training - EPOCH 263\r\n",
      "2024-08-27 16:31:35,692 - INFO - joeynmt.training - Epoch 263, Step:    24930, Batch Loss:     0.049675, Batch Acc: 0.996436, Tokens per Sec:     3933, Lr: 0.000113\r\n",
      "2024-08-27 16:31:36,786 - INFO - joeynmt.training - Epoch 263, Step:    24935, Batch Loss:     0.050026, Batch Acc: 0.997128, Tokens per Sec:     3824, Lr: 0.000113\r\n",
      "2024-08-27 16:31:37,865 - INFO - joeynmt.training - Epoch 263, Step:    24940, Batch Loss:     0.057771, Batch Acc: 0.995960, Tokens per Sec:     3903, Lr: 0.000113\r\n",
      "2024-08-27 16:31:38,945 - INFO - joeynmt.training - Epoch 263, Step:    24945, Batch Loss:     0.060397, Batch Acc: 0.997582, Tokens per Sec:     3832, Lr: 0.000113\r\n",
      "2024-08-27 16:31:40,026 - INFO - joeynmt.training - Epoch 263, Step:    24950, Batch Loss:     0.062329, Batch Acc: 0.995760, Tokens per Sec:     3927, Lr: 0.000113\r\n",
      "2024-08-27 16:31:41,096 - INFO - joeynmt.training - Epoch 263, Step:    24955, Batch Loss:     0.053893, Batch Acc: 0.997031, Tokens per Sec:     4096, Lr: 0.000113\r\n",
      "2024-08-27 16:31:42,187 - INFO - joeynmt.training - Epoch 263, Step:    24960, Batch Loss:     0.055370, Batch Acc: 0.997005, Tokens per Sec:     3982, Lr: 0.000113\r\n",
      "2024-08-27 16:31:43,260 - INFO - joeynmt.training - Epoch 263, Step:    24965, Batch Loss:     0.047446, Batch Acc: 0.997679, Tokens per Sec:     4017, Lr: 0.000113\r\n",
      "2024-08-27 16:31:44,331 - INFO - joeynmt.training - Epoch 263, Step:    24970, Batch Loss:     0.051420, Batch Acc: 0.996927, Tokens per Sec:     3954, Lr: 0.000113\r\n",
      "2024-08-27 16:31:45,391 - INFO - joeynmt.training - Epoch 263, Step:    24975, Batch Loss:     0.052983, Batch Acc: 0.996329, Tokens per Sec:     4112, Lr: 0.000113\r\n",
      "2024-08-27 16:31:46,439 - INFO - joeynmt.training - Epoch 263, Step:    24980, Batch Loss:     0.046813, Batch Acc: 0.995284, Tokens per Sec:     3850, Lr: 0.000113\r\n",
      "2024-08-27 16:31:47,547 - INFO - joeynmt.training - Epoch 263, Step:    24985, Batch Loss:     0.055630, Batch Acc: 0.997205, Tokens per Sec:     3878, Lr: 0.000113\r\n",
      "2024-08-27 16:31:48,644 - INFO - joeynmt.training - Epoch 263, Step:    24990, Batch Loss:     0.054229, Batch Acc: 0.996773, Tokens per Sec:     3958, Lr: 0.000113\r\n",
      "2024-08-27 16:31:49,696 - INFO - joeynmt.training - Epoch 263, Step:    24995, Batch Loss:     0.052452, Batch Acc: 0.995272, Tokens per Sec:     4021, Lr: 0.000113\r\n",
      "2024-08-27 16:31:50,773 - INFO - joeynmt.training - Epoch 263, Step:    25000, Batch Loss:     0.049876, Batch Acc: 0.997840, Tokens per Sec:     3872, Lr: 0.000113\r\n",
      "2024-08-27 16:31:50,774 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=25042\r\n",
      "2024-08-27 16:31:50,774 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 65.28it/s]\r\n",
      "2024-08-27 16:32:13,141 - INFO - joeynmt.prediction - Generation took 22.3649[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44016.70 examples/s]\r\n",
      "2024-08-27 16:32:13,522 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:32:13,522 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.61, loss:   6.69, ppl: 802.13, acc:   0.29, 0.1668[sec]\r\n",
      "2024-08-27 16:32:13,524 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:32:13,670 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:32:13,671 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:32:13,671 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 16:32:13,961 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:32:14,106 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:32:14,106 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:32:14,106 - INFO - joeynmt.training - \tHypothesis: trois tables\r\n",
      "2024-08-27 16:32:14,395 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:32:14,540 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:32:14,540 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:32:14,540 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 16:32:14,829 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:32:14,974 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:32:14,974 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:32:14,974 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 16:32:16,390 - INFO - joeynmt.training - Epoch 263, Step:    25005, Batch Loss:     0.059068, Batch Acc: 0.996615, Tokens per Sec:     3938, Lr: 0.000113\r\n",
      "2024-08-27 16:32:17,493 - INFO - joeynmt.training - Epoch 263, Step:    25010, Batch Loss:     0.050668, Batch Acc: 0.997401, Tokens per Sec:     3841, Lr: 0.000113\r\n",
      "2024-08-27 16:32:18,557 - INFO - joeynmt.training - Epoch 263, Step:    25015, Batch Loss:     0.053517, Batch Acc: 0.995943, Tokens per Sec:     3938, Lr: 0.000113\r\n",
      "2024-08-27 16:32:19,605 - INFO - joeynmt.training - Epoch 263, Step:    25020, Batch Loss:     0.052448, Batch Acc: 0.996127, Tokens per Sec:     3699, Lr: 0.000113\r\n",
      "2024-08-27 16:32:19,861 - INFO - joeynmt.training - Epoch 263, total training loss: 5.01, num. of seqs: 8065, num. of tokens: 80463, 20.5212[sec]\r\n",
      "2024-08-27 16:32:19,861 - INFO - joeynmt.training - EPOCH 264\r\n",
      "2024-08-27 16:32:20,717 - INFO - joeynmt.training - Epoch 264, Step:    25025, Batch Loss:     0.052351, Batch Acc: 0.996083, Tokens per Sec:     3895, Lr: 0.000113\r\n",
      "2024-08-27 16:32:21,796 - INFO - joeynmt.training - Epoch 264, Step:    25030, Batch Loss:     0.054433, Batch Acc: 0.997298, Tokens per Sec:     4119, Lr: 0.000113\r\n",
      "2024-08-27 16:32:22,893 - INFO - joeynmt.training - Epoch 264, Step:    25035, Batch Loss:     0.049915, Batch Acc: 0.997986, Tokens per Sec:     4074, Lr: 0.000113\r\n",
      "2024-08-27 16:32:23,947 - INFO - joeynmt.training - Epoch 264, Step:    25040, Batch Loss:     0.055695, Batch Acc: 0.996197, Tokens per Sec:     3995, Lr: 0.000113\r\n",
      "2024-08-27 16:32:25,015 - INFO - joeynmt.training - Epoch 264, Step:    25045, Batch Loss:     0.051015, Batch Acc: 0.996321, Tokens per Sec:     3819, Lr: 0.000113\r\n",
      "2024-08-27 16:32:26,168 - INFO - joeynmt.training - Epoch 264, Step:    25050, Batch Loss:     0.053016, Batch Acc: 0.996349, Tokens per Sec:     3568, Lr: 0.000113\r\n",
      "2024-08-27 16:32:27,294 - INFO - joeynmt.training - Epoch 264, Step:    25055, Batch Loss:     0.049405, Batch Acc: 0.997327, Tokens per Sec:     3987, Lr: 0.000113\r\n",
      "2024-08-27 16:32:28,381 - INFO - joeynmt.training - Epoch 264, Step:    25060, Batch Loss:     0.049889, Batch Acc: 0.996978, Tokens per Sec:     3959, Lr: 0.000113\r\n",
      "2024-08-27 16:32:29,455 - INFO - joeynmt.training - Epoch 264, Step:    25065, Batch Loss:     0.052723, Batch Acc: 0.997123, Tokens per Sec:     3887, Lr: 0.000113\r\n",
      "2024-08-27 16:32:30,526 - INFO - joeynmt.training - Epoch 264, Step:    25070, Batch Loss:     0.047532, Batch Acc: 0.995420, Tokens per Sec:     4082, Lr: 0.000113\r\n",
      "2024-08-27 16:32:31,600 - INFO - joeynmt.training - Epoch 264, Step:    25075, Batch Loss:     0.047520, Batch Acc: 0.997884, Tokens per Sec:     3961, Lr: 0.000113\r\n",
      "2024-08-27 16:32:32,707 - INFO - joeynmt.training - Epoch 264, Step:    25080, Batch Loss:     0.057348, Batch Acc: 0.996996, Tokens per Sec:     3912, Lr: 0.000113\r\n",
      "2024-08-27 16:32:33,764 - INFO - joeynmt.training - Epoch 264, Step:    25085, Batch Loss:     0.057331, Batch Acc: 0.996580, Tokens per Sec:     3874, Lr: 0.000113\r\n",
      "2024-08-27 16:32:34,818 - INFO - joeynmt.training - Epoch 264, Step:    25090, Batch Loss:     0.049065, Batch Acc: 0.998751, Tokens per Sec:     3802, Lr: 0.000113\r\n",
      "2024-08-27 16:32:35,882 - INFO - joeynmt.training - Epoch 264, Step:    25095, Batch Loss:     0.051835, Batch Acc: 0.996069, Tokens per Sec:     4069, Lr: 0.000113\r\n",
      "2024-08-27 16:32:36,980 - INFO - joeynmt.training - Epoch 264, Step:    25100, Batch Loss:     0.044972, Batch Acc: 0.997058, Tokens per Sec:     3717, Lr: 0.000113\r\n",
      "2024-08-27 16:32:38,039 - INFO - joeynmt.training - Epoch 264, Step:    25105, Batch Loss:     0.060915, Batch Acc: 0.995220, Tokens per Sec:     3757, Lr: 0.000113\r\n",
      "2024-08-27 16:32:39,105 - INFO - joeynmt.training - Epoch 264, Step:    25110, Batch Loss:     0.052652, Batch Acc: 0.996991, Tokens per Sec:     4056, Lr: 0.000113\r\n",
      "2024-08-27 16:32:40,177 - INFO - joeynmt.training - Epoch 264, Step:    25115, Batch Loss:     0.054732, Batch Acc: 0.996948, Tokens per Sec:     3975, Lr: 0.000113\r\n",
      "2024-08-27 16:32:40,435 - INFO - joeynmt.training - Epoch 264, total training loss: 4.92, num. of seqs: 8065, num. of tokens: 80463, 20.5574[sec]\r\n",
      "2024-08-27 16:32:40,435 - INFO - joeynmt.training - EPOCH 265\r\n",
      "2024-08-27 16:32:41,292 - INFO - joeynmt.training - Epoch 265, Step:    25120, Batch Loss:     0.047383, Batch Acc: 0.997850, Tokens per Sec:     3817, Lr: 0.000113\r\n",
      "2024-08-27 16:32:42,356 - INFO - joeynmt.training - Epoch 265, Step:    25125, Batch Loss:     0.049419, Batch Acc: 0.997648, Tokens per Sec:     3998, Lr: 0.000113\r\n",
      "2024-08-27 16:32:43,422 - INFO - joeynmt.training - Epoch 265, Step:    25130, Batch Loss:     0.047939, Batch Acc: 0.997819, Tokens per Sec:     3874, Lr: 0.000113\r\n",
      "2024-08-27 16:32:44,483 - INFO - joeynmt.training - Epoch 265, Step:    25135, Batch Loss:     0.051561, Batch Acc: 0.996805, Tokens per Sec:     3837, Lr: 0.000113\r\n",
      "2024-08-27 16:32:45,540 - INFO - joeynmt.training - Epoch 265, Step:    25140, Batch Loss:     0.053151, Batch Acc: 0.995204, Tokens per Sec:     3945, Lr: 0.000113\r\n",
      "2024-08-27 16:32:46,632 - INFO - joeynmt.training - Epoch 265, Step:    25145, Batch Loss:     0.053033, Batch Acc: 0.997426, Tokens per Sec:     3917, Lr: 0.000113\r\n",
      "2024-08-27 16:32:47,743 - INFO - joeynmt.training - Epoch 265, Step:    25150, Batch Loss:     0.045603, Batch Acc: 0.996966, Tokens per Sec:     3858, Lr: 0.000113\r\n",
      "2024-08-27 16:32:48,832 - INFO - joeynmt.training - Epoch 265, Step:    25155, Batch Loss:     0.050519, Batch Acc: 0.996245, Tokens per Sec:     3672, Lr: 0.000113\r\n",
      "2024-08-27 16:32:49,939 - INFO - joeynmt.training - Epoch 265, Step:    25160, Batch Loss:     0.055455, Batch Acc: 0.996267, Tokens per Sec:     3873, Lr: 0.000113\r\n",
      "2024-08-27 16:32:51,052 - INFO - joeynmt.training - Epoch 265, Step:    25165, Batch Loss:     0.052337, Batch Acc: 0.995352, Tokens per Sec:     3481, Lr: 0.000113\r\n",
      "2024-08-27 16:32:52,154 - INFO - joeynmt.training - Epoch 265, Step:    25170, Batch Loss:     0.053294, Batch Acc: 0.995629, Tokens per Sec:     3946, Lr: 0.000113\r\n",
      "2024-08-27 16:32:53,286 - INFO - joeynmt.training - Epoch 265, Step:    25175, Batch Loss:     0.062515, Batch Acc: 0.996549, Tokens per Sec:     3843, Lr: 0.000113\r\n",
      "2024-08-27 16:32:54,372 - INFO - joeynmt.training - Epoch 265, Step:    25180, Batch Loss:     0.053247, Batch Acc: 0.996145, Tokens per Sec:     3824, Lr: 0.000113\r\n",
      "2024-08-27 16:32:55,457 - INFO - joeynmt.training - Epoch 265, Step:    25185, Batch Loss:     0.055620, Batch Acc: 0.996871, Tokens per Sec:     4126, Lr: 0.000113\r\n",
      "2024-08-27 16:32:56,565 - INFO - joeynmt.training - Epoch 265, Step:    25190, Batch Loss:     0.047686, Batch Acc: 0.997337, Tokens per Sec:     4071, Lr: 0.000113\r\n",
      "2024-08-27 16:32:57,815 - INFO - joeynmt.training - Epoch 265, Step:    25195, Batch Loss:     0.057487, Batch Acc: 0.997047, Tokens per Sec:     3252, Lr: 0.000113\r\n",
      "2024-08-27 16:32:58,935 - INFO - joeynmt.training - Epoch 265, Step:    25200, Batch Loss:     0.055217, Batch Acc: 0.996800, Tokens per Sec:     3910, Lr: 0.000113\r\n",
      "2024-08-27 16:33:00,023 - INFO - joeynmt.training - Epoch 265, Step:    25205, Batch Loss:     0.059068, Batch Acc: 0.994647, Tokens per Sec:     3952, Lr: 0.000113\r\n",
      "2024-08-27 16:33:01,105 - INFO - joeynmt.training - Epoch 265, Step:    25210, Batch Loss:     0.053142, Batch Acc: 0.997605, Tokens per Sec:     3859, Lr: 0.000113\r\n",
      "2024-08-27 16:33:01,422 - INFO - joeynmt.training - Epoch 265, total training loss: 4.96, num. of seqs: 8065, num. of tokens: 80463, 20.9708[sec]\r\n",
      "2024-08-27 16:33:01,423 - INFO - joeynmt.training - EPOCH 266\r\n",
      "2024-08-27 16:33:02,315 - INFO - joeynmt.training - Epoch 266, Step:    25215, Batch Loss:     0.051508, Batch Acc: 0.996214, Tokens per Sec:     3867, Lr: 0.000113\r\n",
      "2024-08-27 16:33:03,410 - INFO - joeynmt.training - Epoch 266, Step:    25220, Batch Loss:     0.054744, Batch Acc: 0.997846, Tokens per Sec:     3820, Lr: 0.000113\r\n",
      "2024-08-27 16:33:04,540 - INFO - joeynmt.training - Epoch 266, Step:    25225, Batch Loss:     0.045348, Batch Acc: 0.998103, Tokens per Sec:     3733, Lr: 0.000113\r\n",
      "2024-08-27 16:33:05,622 - INFO - joeynmt.training - Epoch 266, Step:    25230, Batch Loss:     0.044565, Batch Acc: 0.998007, Tokens per Sec:     3711, Lr: 0.000113\r\n",
      "2024-08-27 16:33:06,741 - INFO - joeynmt.training - Epoch 266, Step:    25235, Batch Loss:     0.052852, Batch Acc: 0.996403, Tokens per Sec:     3482, Lr: 0.000113\r\n",
      "2024-08-27 16:33:07,836 - INFO - joeynmt.training - Epoch 266, Step:    25240, Batch Loss:     0.046735, Batch Acc: 0.996944, Tokens per Sec:     3884, Lr: 0.000113\r\n",
      "2024-08-27 16:33:08,911 - INFO - joeynmt.training - Epoch 266, Step:    25245, Batch Loss:     0.048279, Batch Acc: 0.995970, Tokens per Sec:     3927, Lr: 0.000113\r\n",
      "2024-08-27 16:33:09,990 - INFO - joeynmt.training - Epoch 266, Step:    25250, Batch Loss:     0.056832, Batch Acc: 0.994513, Tokens per Sec:     3889, Lr: 0.000113\r\n",
      "2024-08-27 16:33:11,083 - INFO - joeynmt.training - Epoch 266, Step:    25255, Batch Loss:     0.051555, Batch Acc: 0.997418, Tokens per Sec:     3899, Lr: 0.000113\r\n",
      "2024-08-27 16:33:12,191 - INFO - joeynmt.training - Epoch 266, Step:    25260, Batch Loss:     0.055484, Batch Acc: 0.996574, Tokens per Sec:     3955, Lr: 0.000113\r\n",
      "2024-08-27 16:33:13,297 - INFO - joeynmt.training - Epoch 266, Step:    25265, Batch Loss:     0.053455, Batch Acc: 0.994821, Tokens per Sec:     3843, Lr: 0.000113\r\n",
      "2024-08-27 16:33:14,390 - INFO - joeynmt.training - Epoch 266, Step:    25270, Batch Loss:     0.061580, Batch Acc: 0.996912, Tokens per Sec:     3852, Lr: 0.000113\r\n",
      "2024-08-27 16:33:15,479 - INFO - joeynmt.training - Epoch 266, Step:    25275, Batch Loss:     0.047873, Batch Acc: 0.996399, Tokens per Sec:     3829, Lr: 0.000113\r\n",
      "2024-08-27 16:33:16,592 - INFO - joeynmt.training - Epoch 266, Step:    25280, Batch Loss:     0.045158, Batch Acc: 0.997928, Tokens per Sec:     3904, Lr: 0.000113\r\n",
      "2024-08-27 16:33:17,702 - INFO - joeynmt.training - Epoch 266, Step:    25285, Batch Loss:     0.056633, Batch Acc: 0.997406, Tokens per Sec:     3824, Lr: 0.000112\r\n",
      "2024-08-27 16:33:18,782 - INFO - joeynmt.training - Epoch 266, Step:    25290, Batch Loss:     0.058584, Batch Acc: 0.997060, Tokens per Sec:     4096, Lr: 0.000112\r\n",
      "2024-08-27 16:33:19,863 - INFO - joeynmt.training - Epoch 266, Step:    25295, Batch Loss:     0.051346, Batch Acc: 0.998025, Tokens per Sec:     3749, Lr: 0.000112\r\n",
      "2024-08-27 16:33:20,953 - INFO - joeynmt.training - Epoch 266, Step:    25300, Batch Loss:     0.066119, Batch Acc: 0.994525, Tokens per Sec:     3855, Lr: 0.000112\r\n",
      "2024-08-27 16:33:22,054 - INFO - joeynmt.training - Epoch 266, Step:    25305, Batch Loss:     0.053163, Batch Acc: 0.997095, Tokens per Sec:     3755, Lr: 0.000112\r\n",
      "2024-08-27 16:33:22,438 - INFO - joeynmt.training - Epoch 266, total training loss: 4.91, num. of seqs: 8065, num. of tokens: 80463, 20.9978[sec]\r\n",
      "2024-08-27 16:33:22,438 - INFO - joeynmt.training - EPOCH 267\r\n",
      "2024-08-27 16:33:23,320 - INFO - joeynmt.training - Epoch 267, Step:    25310, Batch Loss:     0.065926, Batch Acc: 0.995817, Tokens per Sec:     3812, Lr: 0.000112\r\n",
      "2024-08-27 16:33:24,417 - INFO - joeynmt.training - Epoch 267, Step:    25315, Batch Loss:     0.045976, Batch Acc: 0.998143, Tokens per Sec:     3931, Lr: 0.000112\r\n",
      "2024-08-27 16:33:25,509 - INFO - joeynmt.training - Epoch 267, Step:    25320, Batch Loss:     0.046621, Batch Acc: 0.996411, Tokens per Sec:     3829, Lr: 0.000112\r\n",
      "2024-08-27 16:33:26,618 - INFO - joeynmt.training - Epoch 267, Step:    25325, Batch Loss:     0.052704, Batch Acc: 0.995445, Tokens per Sec:     3764, Lr: 0.000112\r\n",
      "2024-08-27 16:33:27,758 - INFO - joeynmt.training - Epoch 267, Step:    25330, Batch Loss:     0.048829, Batch Acc: 0.996873, Tokens per Sec:     3649, Lr: 0.000112\r\n",
      "2024-08-27 16:33:28,928 - INFO - joeynmt.training - Epoch 267, Step:    25335, Batch Loss:     0.055149, Batch Acc: 0.996951, Tokens per Sec:     3647, Lr: 0.000112\r\n",
      "2024-08-27 16:33:30,016 - INFO - joeynmt.training - Epoch 267, Step:    25340, Batch Loss:     0.048601, Batch Acc: 0.995316, Tokens per Sec:     3926, Lr: 0.000112\r\n",
      "2024-08-27 16:33:31,107 - INFO - joeynmt.training - Epoch 267, Step:    25345, Batch Loss:     0.053567, Batch Acc: 0.997441, Tokens per Sec:     3944, Lr: 0.000112\r\n",
      "2024-08-27 16:33:32,171 - INFO - joeynmt.training - Epoch 267, Step:    25350, Batch Loss:     0.059928, Batch Acc: 0.997933, Tokens per Sec:     3640, Lr: 0.000112\r\n",
      "2024-08-27 16:33:33,256 - INFO - joeynmt.training - Epoch 267, Step:    25355, Batch Loss:     0.044332, Batch Acc: 0.998323, Tokens per Sec:     3847, Lr: 0.000112\r\n",
      "2024-08-27 16:33:34,370 - INFO - joeynmt.training - Epoch 267, Step:    25360, Batch Loss:     0.054350, Batch Acc: 0.996058, Tokens per Sec:     3871, Lr: 0.000112\r\n",
      "2024-08-27 16:33:35,519 - INFO - joeynmt.training - Epoch 267, Step:    25365, Batch Loss:     0.046822, Batch Acc: 0.996134, Tokens per Sec:     3830, Lr: 0.000112\r\n",
      "2024-08-27 16:33:36,624 - INFO - joeynmt.training - Epoch 267, Step:    25370, Batch Loss:     0.053377, Batch Acc: 0.996126, Tokens per Sec:     3740, Lr: 0.000112\r\n",
      "2024-08-27 16:33:37,762 - INFO - joeynmt.training - Epoch 267, Step:    25375, Batch Loss:     0.051886, Batch Acc: 0.995166, Tokens per Sec:     3819, Lr: 0.000112\r\n",
      "2024-08-27 16:33:38,866 - INFO - joeynmt.training - Epoch 267, Step:    25380, Batch Loss:     0.047206, Batch Acc: 0.995766, Tokens per Sec:     3852, Lr: 0.000112\r\n",
      "2024-08-27 16:33:39,967 - INFO - joeynmt.training - Epoch 267, Step:    25385, Batch Loss:     0.049883, Batch Acc: 0.997114, Tokens per Sec:     3779, Lr: 0.000112\r\n",
      "2024-08-27 16:33:41,086 - INFO - joeynmt.training - Epoch 267, Step:    25390, Batch Loss:     0.042459, Batch Acc: 0.997780, Tokens per Sec:     4028, Lr: 0.000112\r\n",
      "2024-08-27 16:33:42,223 - INFO - joeynmt.training - Epoch 267, Step:    25395, Batch Loss:     0.052070, Batch Acc: 0.995763, Tokens per Sec:     3741, Lr: 0.000112\r\n",
      "2024-08-27 16:33:43,315 - INFO - joeynmt.training - Epoch 267, Step:    25400, Batch Loss:     0.045702, Batch Acc: 0.997629, Tokens per Sec:     3863, Lr: 0.000112\r\n",
      "2024-08-27 16:33:43,578 - INFO - joeynmt.training - Epoch 267, total training loss: 4.93, num. of seqs: 8065, num. of tokens: 80463, 21.1231[sec]\r\n",
      "2024-08-27 16:33:43,578 - INFO - joeynmt.training - EPOCH 268\r\n",
      "2024-08-27 16:33:44,460 - INFO - joeynmt.training - Epoch 268, Step:    25405, Batch Loss:     0.048500, Batch Acc: 0.997698, Tokens per Sec:     3956, Lr: 0.000112\r\n",
      "2024-08-27 16:33:45,538 - INFO - joeynmt.training - Epoch 268, Step:    25410, Batch Loss:     0.053077, Batch Acc: 0.995382, Tokens per Sec:     3819, Lr: 0.000112\r\n",
      "2024-08-27 16:33:46,617 - INFO - joeynmt.training - Epoch 268, Step:    25415, Batch Loss:     0.045091, Batch Acc: 0.997660, Tokens per Sec:     3964, Lr: 0.000112\r\n",
      "2024-08-27 16:33:47,730 - INFO - joeynmt.training - Epoch 268, Step:    25420, Batch Loss:     0.051295, Batch Acc: 0.996887, Tokens per Sec:     3754, Lr: 0.000112\r\n",
      "2024-08-27 16:33:48,822 - INFO - joeynmt.training - Epoch 268, Step:    25425, Batch Loss:     0.049999, Batch Acc: 0.996199, Tokens per Sec:     4098, Lr: 0.000112\r\n",
      "2024-08-27 16:33:49,888 - INFO - joeynmt.training - Epoch 268, Step:    25430, Batch Loss:     0.050722, Batch Acc: 0.996956, Tokens per Sec:     4010, Lr: 0.000112\r\n",
      "2024-08-27 16:33:50,983 - INFO - joeynmt.training - Epoch 268, Step:    25435, Batch Loss:     0.052042, Batch Acc: 0.997203, Tokens per Sec:     3918, Lr: 0.000112\r\n",
      "2024-08-27 16:33:52,051 - INFO - joeynmt.training - Epoch 268, Step:    25440, Batch Loss:     0.048267, Batch Acc: 0.996737, Tokens per Sec:     4021, Lr: 0.000112\r\n",
      "2024-08-27 16:33:53,128 - INFO - joeynmt.training - Epoch 268, Step:    25445, Batch Loss:     0.047246, Batch Acc: 0.997610, Tokens per Sec:     3885, Lr: 0.000112\r\n",
      "2024-08-27 16:33:54,191 - INFO - joeynmt.training - Epoch 268, Step:    25450, Batch Loss:     0.056556, Batch Acc: 0.996357, Tokens per Sec:     3875, Lr: 0.000112\r\n",
      "2024-08-27 16:33:55,252 - INFO - joeynmt.training - Epoch 268, Step:    25455, Batch Loss:     0.047029, Batch Acc: 0.998018, Tokens per Sec:     3808, Lr: 0.000112\r\n",
      "2024-08-27 16:33:56,332 - INFO - joeynmt.training - Epoch 268, Step:    25460, Batch Loss:     0.064103, Batch Acc: 0.996842, Tokens per Sec:     3813, Lr: 0.000112\r\n",
      "2024-08-27 16:33:57,470 - INFO - joeynmt.training - Epoch 268, Step:    25465, Batch Loss:     0.055950, Batch Acc: 0.996943, Tokens per Sec:     3737, Lr: 0.000112\r\n",
      "2024-08-27 16:33:58,589 - INFO - joeynmt.training - Epoch 268, Step:    25470, Batch Loss:     0.052697, Batch Acc: 0.996420, Tokens per Sec:     3749, Lr: 0.000112\r\n",
      "2024-08-27 16:33:59,763 - INFO - joeynmt.training - Epoch 268, Step:    25475, Batch Loss:     0.070518, Batch Acc: 0.997113, Tokens per Sec:     3541, Lr: 0.000112\r\n",
      "2024-08-27 16:34:00,904 - INFO - joeynmt.training - Epoch 268, Step:    25480, Batch Loss:     0.047882, Batch Acc: 0.997145, Tokens per Sec:     3686, Lr: 0.000112\r\n",
      "2024-08-27 16:34:02,008 - INFO - joeynmt.training - Epoch 268, Step:    25485, Batch Loss:     0.046454, Batch Acc: 0.996095, Tokens per Sec:     3712, Lr: 0.000112\r\n",
      "2024-08-27 16:34:03,106 - INFO - joeynmt.training - Epoch 268, Step:    25490, Batch Loss:     0.049521, Batch Acc: 0.995733, Tokens per Sec:     3845, Lr: 0.000112\r\n",
      "2024-08-27 16:34:04,176 - INFO - joeynmt.training - Epoch 268, Step:    25495, Batch Loss:     0.058049, Batch Acc: 0.996068, Tokens per Sec:     3807, Lr: 0.000112\r\n",
      "2024-08-27 16:34:04,600 - INFO - joeynmt.training - Epoch 268, total training loss: 5.03, num. of seqs: 8065, num. of tokens: 80463, 21.0056[sec]\r\n",
      "2024-08-27 16:34:04,600 - INFO - joeynmt.training - EPOCH 269\r\n",
      "2024-08-27 16:34:05,250 - INFO - joeynmt.training - Epoch 269, Step:    25500, Batch Loss:     0.056980, Batch Acc: 0.996828, Tokens per Sec:     3901, Lr: 0.000112\r\n",
      "2024-08-27 16:34:05,251 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=25542\r\n",
      "2024-08-27 16:34:05,251 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 65.50it/s]\r\n",
      "2024-08-27 16:34:27,543 - INFO - joeynmt.prediction - Generation took 22.2895[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44720.19 examples/s]\r\n",
      "2024-08-27 16:34:27,925 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:34:27,925 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.74, loss:   6.68, ppl: 795.23, acc:   0.29, 0.1693[sec]\r\n",
      "2024-08-27 16:34:28,231 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/25500.ckpt.\r\n",
      "2024-08-27 16:34:28,231 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/13500.ckpt\r\n",
      "2024-08-27 16:34:28,256 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:34:28,403 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:34:28,403 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:34:28,403 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 16:34:28,692 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:34:28,837 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:34:28,838 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:34:28,838 - INFO - joeynmt.training - \tHypothesis: trois point d'avance\r\n",
      "2024-08-27 16:34:29,126 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:34:29,270 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:34:29,270 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:34:29,271 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 16:34:29,560 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:34:29,705 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:34:29,705 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:34:29,705 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 16:34:31,200 - INFO - joeynmt.training - Epoch 269, Step:    25505, Batch Loss:     0.050819, Batch Acc: 0.997902, Tokens per Sec:     3560, Lr: 0.000112\r\n",
      "2024-08-27 16:34:32,305 - INFO - joeynmt.training - Epoch 269, Step:    25510, Batch Loss:     0.045635, Batch Acc: 0.997875, Tokens per Sec:     3837, Lr: 0.000112\r\n",
      "2024-08-27 16:34:33,385 - INFO - joeynmt.training - Epoch 269, Step:    25515, Batch Loss:     0.046360, Batch Acc: 0.996736, Tokens per Sec:     3973, Lr: 0.000112\r\n",
      "2024-08-27 16:34:34,445 - INFO - joeynmt.training - Epoch 269, Step:    25520, Batch Loss:     0.051566, Batch Acc: 0.998212, Tokens per Sec:     3695, Lr: 0.000112\r\n",
      "2024-08-27 16:34:35,533 - INFO - joeynmt.training - Epoch 269, Step:    25525, Batch Loss:     0.051210, Batch Acc: 0.995831, Tokens per Sec:     3972, Lr: 0.000112\r\n",
      "2024-08-27 16:34:36,611 - INFO - joeynmt.training - Epoch 269, Step:    25530, Batch Loss:     0.053801, Batch Acc: 0.996386, Tokens per Sec:     3854, Lr: 0.000112\r\n",
      "2024-08-27 16:34:37,747 - INFO - joeynmt.training - Epoch 269, Step:    25535, Batch Loss:     0.058448, Batch Acc: 0.997519, Tokens per Sec:     3905, Lr: 0.000112\r\n",
      "2024-08-27 16:34:38,812 - INFO - joeynmt.training - Epoch 269, Step:    25540, Batch Loss:     0.049766, Batch Acc: 0.996437, Tokens per Sec:     3957, Lr: 0.000112\r\n",
      "2024-08-27 16:34:39,906 - INFO - joeynmt.training - Epoch 269, Step:    25545, Batch Loss:     0.046991, Batch Acc: 0.997077, Tokens per Sec:     4066, Lr: 0.000112\r\n",
      "2024-08-27 16:34:40,973 - INFO - joeynmt.training - Epoch 269, Step:    25550, Batch Loss:     0.050763, Batch Acc: 0.995089, Tokens per Sec:     4008, Lr: 0.000112\r\n",
      "2024-08-27 16:34:42,059 - INFO - joeynmt.training - Epoch 269, Step:    25555, Batch Loss:     0.046369, Batch Acc: 0.996448, Tokens per Sec:     3892, Lr: 0.000112\r\n",
      "2024-08-27 16:34:43,142 - INFO - joeynmt.training - Epoch 269, Step:    25560, Batch Loss:     0.046486, Batch Acc: 0.998261, Tokens per Sec:     3721, Lr: 0.000112\r\n",
      "2024-08-27 16:34:44,224 - INFO - joeynmt.training - Epoch 269, Step:    25565, Batch Loss:     0.047229, Batch Acc: 0.997596, Tokens per Sec:     3846, Lr: 0.000112\r\n",
      "2024-08-27 16:34:45,310 - INFO - joeynmt.training - Epoch 269, Step:    25570, Batch Loss:     0.056029, Batch Acc: 0.997670, Tokens per Sec:     3957, Lr: 0.000112\r\n",
      "2024-08-27 16:34:46,361 - INFO - joeynmt.training - Epoch 269, Step:    25575, Batch Loss:     0.049824, Batch Acc: 0.997156, Tokens per Sec:     4016, Lr: 0.000112\r\n",
      "2024-08-27 16:34:47,465 - INFO - joeynmt.training - Epoch 269, Step:    25580, Batch Loss:     0.049721, Batch Acc: 0.997399, Tokens per Sec:     3834, Lr: 0.000112\r\n",
      "2024-08-27 16:34:48,534 - INFO - joeynmt.training - Epoch 269, Step:    25585, Batch Loss:     0.051451, Batch Acc: 0.996151, Tokens per Sec:     3890, Lr: 0.000112\r\n",
      "2024-08-27 16:34:49,586 - INFO - joeynmt.training - Epoch 269, Step:    25590, Batch Loss:     0.051641, Batch Acc: 0.998076, Tokens per Sec:     3958, Lr: 0.000112\r\n",
      "2024-08-27 16:34:50,104 - INFO - joeynmt.training - Epoch 269, total training loss: 4.81, num. of seqs: 8065, num. of tokens: 80463, 20.7427[sec]\r\n",
      "2024-08-27 16:34:50,104 - INFO - joeynmt.training - EPOCH 270\r\n",
      "2024-08-27 16:34:50,762 - INFO - joeynmt.training - Epoch 270, Step:    25595, Batch Loss:     0.047772, Batch Acc: 0.998483, Tokens per Sec:     4029, Lr: 0.000112\r\n",
      "2024-08-27 16:34:51,847 - INFO - joeynmt.training - Epoch 270, Step:    25600, Batch Loss:     0.052569, Batch Acc: 0.996891, Tokens per Sec:     3856, Lr: 0.000112\r\n",
      "2024-08-27 16:34:52,933 - INFO - joeynmt.training - Epoch 270, Step:    25605, Batch Loss:     0.047013, Batch Acc: 0.997177, Tokens per Sec:     3918, Lr: 0.000112\r\n",
      "2024-08-27 16:34:54,057 - INFO - joeynmt.training - Epoch 270, Step:    25610, Batch Loss:     0.053246, Batch Acc: 0.997801, Tokens per Sec:     3644, Lr: 0.000112\r\n",
      "2024-08-27 16:34:55,116 - INFO - joeynmt.training - Epoch 270, Step:    25615, Batch Loss:     0.052980, Batch Acc: 0.996984, Tokens per Sec:     4075, Lr: 0.000112\r\n",
      "2024-08-27 16:34:56,177 - INFO - joeynmt.training - Epoch 270, Step:    25620, Batch Loss:     0.049562, Batch Acc: 0.997133, Tokens per Sec:     3619, Lr: 0.000112\r\n",
      "2024-08-27 16:34:57,302 - INFO - joeynmt.training - Epoch 270, Step:    25625, Batch Loss:     0.045493, Batch Acc: 0.996835, Tokens per Sec:     3934, Lr: 0.000112\r\n",
      "2024-08-27 16:34:58,385 - INFO - joeynmt.training - Epoch 270, Step:    25630, Batch Loss:     0.049038, Batch Acc: 0.997443, Tokens per Sec:     3976, Lr: 0.000112\r\n",
      "2024-08-27 16:34:59,489 - INFO - joeynmt.training - Epoch 270, Step:    25635, Batch Loss:     0.050514, Batch Acc: 0.996859, Tokens per Sec:     3750, Lr: 0.000112\r\n",
      "2024-08-27 16:35:00,570 - INFO - joeynmt.training - Epoch 270, Step:    25640, Batch Loss:     0.043723, Batch Acc: 0.996307, Tokens per Sec:     3762, Lr: 0.000112\r\n",
      "2024-08-27 16:35:01,684 - INFO - joeynmt.training - Epoch 270, Step:    25645, Batch Loss:     0.055705, Batch Acc: 0.995640, Tokens per Sec:     3708, Lr: 0.000112\r\n",
      "2024-08-27 16:35:02,815 - INFO - joeynmt.training - Epoch 270, Step:    25650, Batch Loss:     0.051509, Batch Acc: 0.994538, Tokens per Sec:     3724, Lr: 0.000112\r\n",
      "2024-08-27 16:35:03,911 - INFO - joeynmt.training - Epoch 270, Step:    25655, Batch Loss:     0.061723, Batch Acc: 0.994145, Tokens per Sec:     3744, Lr: 0.000112\r\n",
      "2024-08-27 16:35:05,020 - INFO - joeynmt.training - Epoch 270, Step:    25660, Batch Loss:     0.056741, Batch Acc: 0.997542, Tokens per Sec:     4038, Lr: 0.000112\r\n",
      "2024-08-27 16:35:06,088 - INFO - joeynmt.training - Epoch 270, Step:    25665, Batch Loss:     0.062702, Batch Acc: 0.993986, Tokens per Sec:     3895, Lr: 0.000112\r\n",
      "2024-08-27 16:35:07,219 - INFO - joeynmt.training - Epoch 270, Step:    25670, Batch Loss:     0.046204, Batch Acc: 0.996195, Tokens per Sec:     3718, Lr: 0.000112\r\n",
      "2024-08-27 16:35:08,313 - INFO - joeynmt.training - Epoch 270, Step:    25675, Batch Loss:     0.061232, Batch Acc: 0.994793, Tokens per Sec:     4043, Lr: 0.000112\r\n",
      "2024-08-27 16:35:09,393 - INFO - joeynmt.training - Epoch 270, Step:    25680, Batch Loss:     0.053202, Batch Acc: 0.996467, Tokens per Sec:     3932, Lr: 0.000112\r\n",
      "2024-08-27 16:35:10,459 - INFO - joeynmt.training - Epoch 270, Step:    25685, Batch Loss:     0.052262, Batch Acc: 0.996447, Tokens per Sec:     3963, Lr: 0.000112\r\n",
      "2024-08-27 16:35:10,980 - INFO - joeynmt.training - Epoch 270, total training loss: 4.92, num. of seqs: 8065, num. of tokens: 80463, 20.8591[sec]\r\n",
      "2024-08-27 16:35:10,981 - INFO - joeynmt.training - EPOCH 271\r\n",
      "2024-08-27 16:35:11,624 - INFO - joeynmt.training - Epoch 271, Step:    25690, Batch Loss:     0.047975, Batch Acc: 0.995124, Tokens per Sec:     3849, Lr: 0.000112\r\n",
      "2024-08-27 16:35:12,688 - INFO - joeynmt.training - Epoch 271, Step:    25695, Batch Loss:     0.049148, Batch Acc: 0.995993, Tokens per Sec:     3754, Lr: 0.000112\r\n",
      "2024-08-27 16:35:13,759 - INFO - joeynmt.training - Epoch 271, Step:    25700, Batch Loss:     0.045937, Batch Acc: 0.996702, Tokens per Sec:     3970, Lr: 0.000112\r\n",
      "2024-08-27 16:35:14,833 - INFO - joeynmt.training - Epoch 271, Step:    25705, Batch Loss:     0.049329, Batch Acc: 0.997509, Tokens per Sec:     4113, Lr: 0.000112\r\n",
      "2024-08-27 16:35:15,897 - INFO - joeynmt.training - Epoch 271, Step:    25710, Batch Loss:     0.047528, Batch Acc: 0.997589, Tokens per Sec:     3898, Lr: 0.000112\r\n",
      "2024-08-27 16:35:16,991 - INFO - joeynmt.training - Epoch 271, Step:    25715, Batch Loss:     0.049800, Batch Acc: 0.997126, Tokens per Sec:     3821, Lr: 0.000112\r\n",
      "2024-08-27 16:35:18,069 - INFO - joeynmt.training - Epoch 271, Step:    25720, Batch Loss:     0.059041, Batch Acc: 0.996328, Tokens per Sec:     4043, Lr: 0.000112\r\n",
      "2024-08-27 16:35:19,128 - INFO - joeynmt.training - Epoch 271, Step:    25725, Batch Loss:     0.051129, Batch Acc: 0.996617, Tokens per Sec:     3910, Lr: 0.000112\r\n",
      "2024-08-27 16:35:20,201 - INFO - joeynmt.training - Epoch 271, Step:    25730, Batch Loss:     0.052240, Batch Acc: 0.997246, Tokens per Sec:     4064, Lr: 0.000112\r\n",
      "2024-08-27 16:35:21,269 - INFO - joeynmt.training - Epoch 271, Step:    25735, Batch Loss:     0.057049, Batch Acc: 0.997435, Tokens per Sec:     4018, Lr: 0.000112\r\n",
      "2024-08-27 16:35:22,328 - INFO - joeynmt.training - Epoch 271, Step:    25740, Batch Loss:     0.070318, Batch Acc: 0.996684, Tokens per Sec:     3990, Lr: 0.000111\r\n",
      "2024-08-27 16:35:23,415 - INFO - joeynmt.training - Epoch 271, Step:    25745, Batch Loss:     0.048143, Batch Acc: 0.996844, Tokens per Sec:     4082, Lr: 0.000111\r\n",
      "2024-08-27 16:35:24,495 - INFO - joeynmt.training - Epoch 271, Step:    25750, Batch Loss:     0.048374, Batch Acc: 0.998615, Tokens per Sec:     4014, Lr: 0.000111\r\n",
      "2024-08-27 16:35:25,562 - INFO - joeynmt.training - Epoch 271, Step:    25755, Batch Loss:     0.047747, Batch Acc: 0.996966, Tokens per Sec:     4021, Lr: 0.000111\r\n",
      "2024-08-27 16:35:26,628 - INFO - joeynmt.training - Epoch 271, Step:    25760, Batch Loss:     0.044006, Batch Acc: 0.997711, Tokens per Sec:     4101, Lr: 0.000111\r\n",
      "2024-08-27 16:35:27,724 - INFO - joeynmt.training - Epoch 271, Step:    25765, Batch Loss:     0.057915, Batch Acc: 0.995797, Tokens per Sec:     3693, Lr: 0.000111\r\n",
      "2024-08-27 16:35:28,793 - INFO - joeynmt.training - Epoch 271, Step:    25770, Batch Loss:     0.046417, Batch Acc: 0.997736, Tokens per Sec:     3719, Lr: 0.000111\r\n",
      "2024-08-27 16:35:29,861 - INFO - joeynmt.training - Epoch 271, Step:    25775, Batch Loss:     0.049255, Batch Acc: 0.997925, Tokens per Sec:     4066, Lr: 0.000111\r\n",
      "2024-08-27 16:35:30,938 - INFO - joeynmt.training - Epoch 271, Step:    25780, Batch Loss:     0.047200, Batch Acc: 0.998814, Tokens per Sec:     3917, Lr: 0.000111\r\n",
      "2024-08-27 16:35:31,361 - INFO - joeynmt.training - Epoch 271, total training loss: 4.85, num. of seqs: 8065, num. of tokens: 80463, 20.3639[sec]\r\n",
      "2024-08-27 16:35:31,361 - INFO - joeynmt.training - EPOCH 272\r\n",
      "2024-08-27 16:35:31,994 - INFO - joeynmt.training - Epoch 272, Step:    25785, Batch Loss:     0.048633, Batch Acc: 0.994878, Tokens per Sec:     4026, Lr: 0.000111\r\n",
      "2024-08-27 16:35:33,150 - INFO - joeynmt.training - Epoch 272, Step:    25790, Batch Loss:     0.045469, Batch Acc: 0.996331, Tokens per Sec:     3775, Lr: 0.000111\r\n",
      "2024-08-27 16:35:34,234 - INFO - joeynmt.training - Epoch 272, Step:    25795, Batch Loss:     0.046767, Batch Acc: 0.997923, Tokens per Sec:     3998, Lr: 0.000111\r\n",
      "2024-08-27 16:35:35,318 - INFO - joeynmt.training - Epoch 272, Step:    25800, Batch Loss:     0.049438, Batch Acc: 0.996762, Tokens per Sec:     3995, Lr: 0.000111\r\n",
      "2024-08-27 16:35:36,396 - INFO - joeynmt.training - Epoch 272, Step:    25805, Batch Loss:     0.046752, Batch Acc: 0.996343, Tokens per Sec:     4060, Lr: 0.000111\r\n",
      "2024-08-27 16:35:37,509 - INFO - joeynmt.training - Epoch 272, Step:    25810, Batch Loss:     0.048349, Batch Acc: 0.997400, Tokens per Sec:     3803, Lr: 0.000111\r\n",
      "2024-08-27 16:35:38,604 - INFO - joeynmt.training - Epoch 272, Step:    25815, Batch Loss:     0.046603, Batch Acc: 0.997186, Tokens per Sec:     3897, Lr: 0.000111\r\n",
      "2024-08-27 16:35:39,693 - INFO - joeynmt.training - Epoch 272, Step:    25820, Batch Loss:     0.053257, Batch Acc: 0.996696, Tokens per Sec:     3895, Lr: 0.000111\r\n",
      "2024-08-27 16:35:41,004 - INFO - joeynmt.training - Epoch 272, Step:    25825, Batch Loss:     0.045644, Batch Acc: 0.996216, Tokens per Sec:     3226, Lr: 0.000111\r\n",
      "2024-08-27 16:35:42,076 - INFO - joeynmt.training - Epoch 272, Step:    25830, Batch Loss:     0.064847, Batch Acc: 0.996683, Tokens per Sec:     3942, Lr: 0.000111\r\n",
      "2024-08-27 16:35:43,150 - INFO - joeynmt.training - Epoch 272, Step:    25835, Batch Loss:     0.047648, Batch Acc: 0.998640, Tokens per Sec:     4106, Lr: 0.000111\r\n",
      "2024-08-27 16:35:44,232 - INFO - joeynmt.training - Epoch 272, Step:    25840, Batch Loss:     0.045253, Batch Acc: 0.997841, Tokens per Sec:     3856, Lr: 0.000111\r\n",
      "2024-08-27 16:35:45,298 - INFO - joeynmt.training - Epoch 272, Step:    25845, Batch Loss:     0.059669, Batch Acc: 0.996341, Tokens per Sec:     3849, Lr: 0.000111\r\n",
      "2024-08-27 16:35:46,370 - INFO - joeynmt.training - Epoch 272, Step:    25850, Batch Loss:     0.051618, Batch Acc: 0.996417, Tokens per Sec:     4169, Lr: 0.000111\r\n",
      "2024-08-27 16:35:47,472 - INFO - joeynmt.training - Epoch 272, Step:    25855, Batch Loss:     0.052120, Batch Acc: 0.997211, Tokens per Sec:     3905, Lr: 0.000111\r\n",
      "2024-08-27 16:35:48,562 - INFO - joeynmt.training - Epoch 272, Step:    25860, Batch Loss:     0.049708, Batch Acc: 0.996331, Tokens per Sec:     4006, Lr: 0.000111\r\n",
      "2024-08-27 16:35:49,637 - INFO - joeynmt.training - Epoch 272, Step:    25865, Batch Loss:     0.062899, Batch Acc: 0.995820, Tokens per Sec:     4007, Lr: 0.000111\r\n",
      "2024-08-27 16:35:50,696 - INFO - joeynmt.training - Epoch 272, Step:    25870, Batch Loss:     0.047452, Batch Acc: 0.997713, Tokens per Sec:     3717, Lr: 0.000111\r\n",
      "2024-08-27 16:35:51,795 - INFO - joeynmt.training - Epoch 272, Step:    25875, Batch Loss:     0.055975, Batch Acc: 0.995837, Tokens per Sec:     3720, Lr: 0.000111\r\n",
      "2024-08-27 16:35:52,106 - INFO - joeynmt.training - Epoch 272, total training loss: 4.79, num. of seqs: 8065, num. of tokens: 80463, 20.7297[sec]\r\n",
      "2024-08-27 16:35:52,107 - INFO - joeynmt.training - EPOCH 273\r\n",
      "2024-08-27 16:35:52,976 - INFO - joeynmt.training - Epoch 273, Step:    25880, Batch Loss:     0.063539, Batch Acc: 0.996450, Tokens per Sec:     3910, Lr: 0.000111\r\n",
      "2024-08-27 16:35:54,059 - INFO - joeynmt.training - Epoch 273, Step:    25885, Batch Loss:     0.046645, Batch Acc: 0.997338, Tokens per Sec:     3818, Lr: 0.000111\r\n",
      "2024-08-27 16:35:55,134 - INFO - joeynmt.training - Epoch 273, Step:    25890, Batch Loss:     0.042896, Batch Acc: 0.997070, Tokens per Sec:     3812, Lr: 0.000111\r\n",
      "2024-08-27 16:35:56,243 - INFO - joeynmt.training - Epoch 273, Step:    25895, Batch Loss:     0.045623, Batch Acc: 0.997678, Tokens per Sec:     3884, Lr: 0.000111\r\n",
      "2024-08-27 16:35:57,372 - INFO - joeynmt.training - Epoch 273, Step:    25900, Batch Loss:     0.048099, Batch Acc: 0.997388, Tokens per Sec:     3732, Lr: 0.000111\r\n",
      "2024-08-27 16:35:58,435 - INFO - joeynmt.training - Epoch 273, Step:    25905, Batch Loss:     0.056719, Batch Acc: 0.996631, Tokens per Sec:     3910, Lr: 0.000111\r\n",
      "2024-08-27 16:35:59,510 - INFO - joeynmt.training - Epoch 273, Step:    25910, Batch Loss:     0.044555, Batch Acc: 0.997630, Tokens per Sec:     3928, Lr: 0.000111\r\n",
      "2024-08-27 16:36:00,584 - INFO - joeynmt.training - Epoch 273, Step:    25915, Batch Loss:     0.043586, Batch Acc: 0.997890, Tokens per Sec:     3974, Lr: 0.000111\r\n",
      "2024-08-27 16:36:01,656 - INFO - joeynmt.training - Epoch 273, Step:    25920, Batch Loss:     0.044557, Batch Acc: 0.994782, Tokens per Sec:     3939, Lr: 0.000111\r\n",
      "2024-08-27 16:36:02,731 - INFO - joeynmt.training - Epoch 273, Step:    25925, Batch Loss:     0.053462, Batch Acc: 0.995975, Tokens per Sec:     3928, Lr: 0.000111\r\n",
      "2024-08-27 16:36:03,831 - INFO - joeynmt.training - Epoch 273, Step:    25930, Batch Loss:     0.048110, Batch Acc: 0.996217, Tokens per Sec:     3851, Lr: 0.000111\r\n",
      "2024-08-27 16:36:05,013 - INFO - joeynmt.training - Epoch 273, Step:    25935, Batch Loss:     0.050175, Batch Acc: 0.998126, Tokens per Sec:     3611, Lr: 0.000111\r\n",
      "2024-08-27 16:36:06,093 - INFO - joeynmt.training - Epoch 273, Step:    25940, Batch Loss:     0.049657, Batch Acc: 0.995927, Tokens per Sec:     3868, Lr: 0.000111\r\n",
      "2024-08-27 16:36:07,195 - INFO - joeynmt.training - Epoch 273, Step:    25945, Batch Loss:     0.049739, Batch Acc: 0.996269, Tokens per Sec:     3892, Lr: 0.000111\r\n",
      "2024-08-27 16:36:08,259 - INFO - joeynmt.training - Epoch 273, Step:    25950, Batch Loss:     0.045754, Batch Acc: 0.997624, Tokens per Sec:     4356, Lr: 0.000111\r\n",
      "2024-08-27 16:36:09,368 - INFO - joeynmt.training - Epoch 273, Step:    25955, Batch Loss:     0.053808, Batch Acc: 0.996527, Tokens per Sec:     3637, Lr: 0.000111\r\n",
      "2024-08-27 16:36:10,449 - INFO - joeynmt.training - Epoch 273, Step:    25960, Batch Loss:     0.051187, Batch Acc: 0.998131, Tokens per Sec:     3962, Lr: 0.000111\r\n",
      "2024-08-27 16:36:11,554 - INFO - joeynmt.training - Epoch 273, Step:    25965, Batch Loss:     0.046707, Batch Acc: 0.995609, Tokens per Sec:     3917, Lr: 0.000111\r\n",
      "2024-08-27 16:36:12,644 - INFO - joeynmt.training - Epoch 273, Step:    25970, Batch Loss:     0.053158, Batch Acc: 0.996077, Tokens per Sec:     3745, Lr: 0.000111\r\n",
      "2024-08-27 16:36:12,901 - INFO - joeynmt.training - Epoch 273, total training loss: 4.83, num. of seqs: 8065, num. of tokens: 80463, 20.7784[sec]\r\n",
      "2024-08-27 16:36:12,902 - INFO - joeynmt.training - EPOCH 274\r\n",
      "2024-08-27 16:36:13,759 - INFO - joeynmt.training - Epoch 274, Step:    25975, Batch Loss:     0.050285, Batch Acc: 0.997680, Tokens per Sec:     4044, Lr: 0.000111\r\n",
      "2024-08-27 16:36:14,837 - INFO - joeynmt.training - Epoch 274, Step:    25980, Batch Loss:     0.049934, Batch Acc: 0.997665, Tokens per Sec:     3975, Lr: 0.000111\r\n",
      "2024-08-27 16:36:15,900 - INFO - joeynmt.training - Epoch 274, Step:    25985, Batch Loss:     0.048755, Batch Acc: 0.997461, Tokens per Sec:     3705, Lr: 0.000111\r\n",
      "2024-08-27 16:36:17,017 - INFO - joeynmt.training - Epoch 274, Step:    25990, Batch Loss:     0.054920, Batch Acc: 0.995874, Tokens per Sec:     3692, Lr: 0.000111\r\n",
      "2024-08-27 16:36:18,086 - INFO - joeynmt.training - Epoch 274, Step:    25995, Batch Loss:     0.048991, Batch Acc: 0.997514, Tokens per Sec:     3766, Lr: 0.000111\r\n",
      "2024-08-27 16:36:19,137 - INFO - joeynmt.training - Epoch 274, Step:    26000, Batch Loss:     0.050832, Batch Acc: 0.995703, Tokens per Sec:     3986, Lr: 0.000111\r\n",
      "2024-08-27 16:36:19,138 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=26042\r\n",
      "2024-08-27 16:36:19,138 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 65.46it/s]\r\n",
      "2024-08-27 16:36:41,443 - INFO - joeynmt.prediction - Generation took 22.3032[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44531.37 examples/s]\r\n",
      "2024-08-27 16:36:41,826 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:36:41,826 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.83, loss:   6.67, ppl: 791.07, acc:   0.29, 0.1681[sec]\r\n",
      "2024-08-27 16:36:41,827 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\r\n",
      "2024-08-27 16:36:42,138 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/26000.ckpt.\r\n",
      "2024-08-27 16:36:42,139 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/24500.ckpt\r\n",
      "2024-08-27 16:36:42,165 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:36:42,313 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:36:42,313 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:36:42,313 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 16:36:42,604 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:36:42,755 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:36:42,755 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:36:42,755 - INFO - joeynmt.training - \tHypothesis: trois point d'avance\r\n",
      "2024-08-27 16:36:43,054 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:36:43,199 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:36:43,199 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:36:43,200 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 16:36:43,492 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:36:43,639 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:36:43,640 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:36:43,640 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 16:36:45,022 - INFO - joeynmt.training - Epoch 274, Step:    26005, Batch Loss:     0.045580, Batch Acc: 0.997671, Tokens per Sec:     3981, Lr: 0.000111\r\n",
      "2024-08-27 16:36:46,102 - INFO - joeynmt.training - Epoch 274, Step:    26010, Batch Loss:     0.040560, Batch Acc: 0.998564, Tokens per Sec:     3869, Lr: 0.000111\r\n",
      "2024-08-27 16:36:47,206 - INFO - joeynmt.training - Epoch 274, Step:    26015, Batch Loss:     0.050893, Batch Acc: 0.996809, Tokens per Sec:     3976, Lr: 0.000111\r\n",
      "2024-08-27 16:36:48,288 - INFO - joeynmt.training - Epoch 274, Step:    26020, Batch Loss:     0.051218, Batch Acc: 0.995847, Tokens per Sec:     4232, Lr: 0.000111\r\n",
      "2024-08-27 16:36:49,358 - INFO - joeynmt.training - Epoch 274, Step:    26025, Batch Loss:     0.055824, Batch Acc: 0.996335, Tokens per Sec:     4081, Lr: 0.000111\r\n",
      "2024-08-27 16:36:50,432 - INFO - joeynmt.training - Epoch 274, Step:    26030, Batch Loss:     0.056593, Batch Acc: 0.996446, Tokens per Sec:     3933, Lr: 0.000111\r\n",
      "2024-08-27 16:36:51,538 - INFO - joeynmt.training - Epoch 274, Step:    26035, Batch Loss:     0.057813, Batch Acc: 0.995811, Tokens per Sec:     3888, Lr: 0.000111\r\n",
      "2024-08-27 16:36:52,613 - INFO - joeynmt.training - Epoch 274, Step:    26040, Batch Loss:     0.051756, Batch Acc: 0.997154, Tokens per Sec:     3925, Lr: 0.000111\r\n",
      "2024-08-27 16:36:53,700 - INFO - joeynmt.training - Epoch 274, Step:    26045, Batch Loss:     0.047012, Batch Acc: 0.997613, Tokens per Sec:     3857, Lr: 0.000111\r\n",
      "2024-08-27 16:36:54,772 - INFO - joeynmt.training - Epoch 274, Step:    26050, Batch Loss:     0.061493, Batch Acc: 0.996471, Tokens per Sec:     3969, Lr: 0.000111\r\n",
      "2024-08-27 16:36:55,821 - INFO - joeynmt.training - Epoch 274, Step:    26055, Batch Loss:     0.050025, Batch Acc: 0.996557, Tokens per Sec:     3879, Lr: 0.000111\r\n",
      "2024-08-27 16:36:56,944 - INFO - joeynmt.training - Epoch 274, Step:    26060, Batch Loss:     0.054240, Batch Acc: 0.996439, Tokens per Sec:     4000, Lr: 0.000111\r\n",
      "2024-08-27 16:36:58,017 - INFO - joeynmt.training - Epoch 274, Step:    26065, Batch Loss:     0.050627, Batch Acc: 0.996759, Tokens per Sec:     4032, Lr: 0.000111\r\n",
      "2024-08-27 16:36:58,170 - INFO - joeynmt.training - Epoch 274, total training loss: 4.82, num. of seqs: 8065, num. of tokens: 80463, 20.4456[sec]\r\n",
      "2024-08-27 16:36:58,170 - INFO - joeynmt.training - EPOCH 275\r\n",
      "2024-08-27 16:36:59,243 - INFO - joeynmt.training - Epoch 275, Step:    26070, Batch Loss:     0.044141, Batch Acc: 0.997445, Tokens per Sec:     4029, Lr: 0.000111\r\n",
      "2024-08-27 16:37:00,315 - INFO - joeynmt.training - Epoch 275, Step:    26075, Batch Loss:     0.052965, Batch Acc: 0.997867, Tokens per Sec:     3938, Lr: 0.000111\r\n",
      "2024-08-27 16:37:01,413 - INFO - joeynmt.training - Epoch 275, Step:    26080, Batch Loss:     0.051642, Batch Acc: 0.997036, Tokens per Sec:     3692, Lr: 0.000111\r\n",
      "2024-08-27 16:37:02,474 - INFO - joeynmt.training - Epoch 275, Step:    26085, Batch Loss:     0.046290, Batch Acc: 0.995775, Tokens per Sec:     4015, Lr: 0.000111\r\n",
      "2024-08-27 16:37:03,533 - INFO - joeynmt.training - Epoch 275, Step:    26090, Batch Loss:     0.046519, Batch Acc: 0.997183, Tokens per Sec:     3691, Lr: 0.000111\r\n",
      "2024-08-27 16:37:04,612 - INFO - joeynmt.training - Epoch 275, Step:    26095, Batch Loss:     0.050585, Batch Acc: 0.997361, Tokens per Sec:     3868, Lr: 0.000111\r\n",
      "2024-08-27 16:37:05,673 - INFO - joeynmt.training - Epoch 275, Step:    26100, Batch Loss:     0.047060, Batch Acc: 0.997008, Tokens per Sec:     3780, Lr: 0.000111\r\n",
      "2024-08-27 16:37:06,868 - INFO - joeynmt.training - Epoch 275, Step:    26105, Batch Loss:     0.050431, Batch Acc: 0.997079, Tokens per Sec:     3727, Lr: 0.000111\r\n",
      "2024-08-27 16:37:07,964 - INFO - joeynmt.training - Epoch 275, Step:    26110, Batch Loss:     0.048697, Batch Acc: 0.995928, Tokens per Sec:     3813, Lr: 0.000111\r\n",
      "2024-08-27 16:37:09,030 - INFO - joeynmt.training - Epoch 275, Step:    26115, Batch Loss:     0.048311, Batch Acc: 0.996445, Tokens per Sec:     3960, Lr: 0.000111\r\n",
      "2024-08-27 16:37:10,080 - INFO - joeynmt.training - Epoch 275, Step:    26120, Batch Loss:     0.049504, Batch Acc: 0.997494, Tokens per Sec:     3803, Lr: 0.000111\r\n",
      "2024-08-27 16:37:11,148 - INFO - joeynmt.training - Epoch 275, Step:    26125, Batch Loss:     0.049861, Batch Acc: 0.995869, Tokens per Sec:     4085, Lr: 0.000111\r\n",
      "2024-08-27 16:37:12,208 - INFO - joeynmt.training - Epoch 275, Step:    26130, Batch Loss:     0.046906, Batch Acc: 0.996930, Tokens per Sec:     3994, Lr: 0.000111\r\n",
      "2024-08-27 16:37:13,293 - INFO - joeynmt.training - Epoch 275, Step:    26135, Batch Loss:     0.049811, Batch Acc: 0.998833, Tokens per Sec:     3953, Lr: 0.000111\r\n",
      "2024-08-27 16:37:14,409 - INFO - joeynmt.training - Epoch 275, Step:    26140, Batch Loss:     0.055815, Batch Acc: 0.995134, Tokens per Sec:     3870, Lr: 0.000111\r\n",
      "2024-08-27 16:37:15,543 - INFO - joeynmt.training - Epoch 275, Step:    26145, Batch Loss:     0.052561, Batch Acc: 0.995884, Tokens per Sec:     3859, Lr: 0.000111\r\n",
      "2024-08-27 16:37:16,630 - INFO - joeynmt.training - Epoch 275, Step:    26150, Batch Loss:     0.063835, Batch Acc: 0.995961, Tokens per Sec:     3872, Lr: 0.000111\r\n",
      "2024-08-27 16:37:17,747 - INFO - joeynmt.training - Epoch 275, Step:    26155, Batch Loss:     0.045916, Batch Acc: 0.997013, Tokens per Sec:     3900, Lr: 0.000111\r\n",
      "2024-08-27 16:37:18,834 - INFO - joeynmt.training - Epoch 275, Step:    26160, Batch Loss:     0.043376, Batch Acc: 0.996265, Tokens per Sec:     3697, Lr: 0.000111\r\n",
      "2024-08-27 16:37:18,991 - INFO - joeynmt.training - Epoch 275, total training loss: 4.85, num. of seqs: 8065, num. of tokens: 80463, 20.8041[sec]\r\n",
      "2024-08-27 16:37:18,991 - INFO - joeynmt.training - EPOCH 276\r\n",
      "2024-08-27 16:37:20,077 - INFO - joeynmt.training - Epoch 276, Step:    26165, Batch Loss:     0.049789, Batch Acc: 0.996375, Tokens per Sec:     3825, Lr: 0.000111\r\n",
      "2024-08-27 16:37:21,159 - INFO - joeynmt.training - Epoch 276, Step:    26170, Batch Loss:     0.046687, Batch Acc: 0.997608, Tokens per Sec:     3867, Lr: 0.000111\r\n",
      "2024-08-27 16:37:22,269 - INFO - joeynmt.training - Epoch 276, Step:    26175, Batch Loss:     0.043606, Batch Acc: 0.997017, Tokens per Sec:     3926, Lr: 0.000111\r\n",
      "2024-08-27 16:37:23,349 - INFO - joeynmt.training - Epoch 276, Step:    26180, Batch Loss:     0.045247, Batch Acc: 0.996603, Tokens per Sec:     4094, Lr: 0.000111\r\n",
      "2024-08-27 16:37:24,406 - INFO - joeynmt.training - Epoch 276, Step:    26185, Batch Loss:     0.047201, Batch Acc: 0.997129, Tokens per Sec:     3955, Lr: 0.000111\r\n",
      "2024-08-27 16:37:25,462 - INFO - joeynmt.training - Epoch 276, Step:    26190, Batch Loss:     0.047129, Batch Acc: 0.996751, Tokens per Sec:     4084, Lr: 0.000111\r\n",
      "2024-08-27 16:37:26,575 - INFO - joeynmt.training - Epoch 276, Step:    26195, Batch Loss:     0.049220, Batch Acc: 0.996578, Tokens per Sec:     3943, Lr: 0.000111\r\n",
      "2024-08-27 16:37:27,677 - INFO - joeynmt.training - Epoch 276, Step:    26200, Batch Loss:     0.058480, Batch Acc: 0.997354, Tokens per Sec:     3776, Lr: 0.000111\r\n",
      "2024-08-27 16:37:28,767 - INFO - joeynmt.training - Epoch 276, Step:    26205, Batch Loss:     0.047672, Batch Acc: 0.996533, Tokens per Sec:     3970, Lr: 0.000111\r\n",
      "2024-08-27 16:37:29,846 - INFO - joeynmt.training - Epoch 276, Step:    26210, Batch Loss:     0.052032, Batch Acc: 0.996203, Tokens per Sec:     3910, Lr: 0.000110\r\n",
      "2024-08-27 16:37:30,921 - INFO - joeynmt.training - Epoch 276, Step:    26215, Batch Loss:     0.047683, Batch Acc: 0.996413, Tokens per Sec:     3892, Lr: 0.000110\r\n",
      "2024-08-27 16:37:31,982 - INFO - joeynmt.training - Epoch 276, Step:    26220, Batch Loss:     0.053018, Batch Acc: 0.996594, Tokens per Sec:     3876, Lr: 0.000110\r\n",
      "2024-08-27 16:37:33,039 - INFO - joeynmt.training - Epoch 276, Step:    26225, Batch Loss:     0.057638, Batch Acc: 0.995373, Tokens per Sec:     3684, Lr: 0.000110\r\n",
      "2024-08-27 16:37:34,108 - INFO - joeynmt.training - Epoch 276, Step:    26230, Batch Loss:     0.058721, Batch Acc: 0.995999, Tokens per Sec:     3977, Lr: 0.000110\r\n",
      "2024-08-27 16:37:35,165 - INFO - joeynmt.training - Epoch 276, Step:    26235, Batch Loss:     0.054532, Batch Acc: 0.996217, Tokens per Sec:     4004, Lr: 0.000110\r\n",
      "2024-08-27 16:37:36,238 - INFO - joeynmt.training - Epoch 276, Step:    26240, Batch Loss:     0.063374, Batch Acc: 0.995730, Tokens per Sec:     3930, Lr: 0.000110\r\n",
      "2024-08-27 16:37:37,368 - INFO - joeynmt.training - Epoch 276, Step:    26245, Batch Loss:     0.047365, Batch Acc: 0.997373, Tokens per Sec:     3705, Lr: 0.000110\r\n",
      "2024-08-27 16:37:38,488 - INFO - joeynmt.training - Epoch 276, Step:    26250, Batch Loss:     0.056183, Batch Acc: 0.995441, Tokens per Sec:     3726, Lr: 0.000110\r\n",
      "2024-08-27 16:37:39,554 - INFO - joeynmt.training - Epoch 276, Step:    26255, Batch Loss:     0.057048, Batch Acc: 0.995949, Tokens per Sec:     3939, Lr: 0.000110\r\n",
      "2024-08-27 16:37:39,656 - INFO - joeynmt.training - Epoch 276, total training loss: 4.93, num. of seqs: 8065, num. of tokens: 80463, 20.6482[sec]\r\n",
      "2024-08-27 16:37:39,656 - INFO - joeynmt.training - EPOCH 277\r\n",
      "2024-08-27 16:37:40,732 - INFO - joeynmt.training - Epoch 277, Step:    26260, Batch Loss:     0.057406, Batch Acc: 0.995487, Tokens per Sec:     3928, Lr: 0.000110\r\n",
      "2024-08-27 16:37:41,792 - INFO - joeynmt.training - Epoch 277, Step:    26265, Batch Loss:     0.047214, Batch Acc: 0.997122, Tokens per Sec:     3937, Lr: 0.000110\r\n",
      "2024-08-27 16:37:42,874 - INFO - joeynmt.training - Epoch 277, Step:    26270, Batch Loss:     0.049451, Batch Acc: 0.997481, Tokens per Sec:     4040, Lr: 0.000110\r\n",
      "2024-08-27 16:37:43,936 - INFO - joeynmt.training - Epoch 277, Step:    26275, Batch Loss:     0.046592, Batch Acc: 0.996054, Tokens per Sec:     3820, Lr: 0.000110\r\n",
      "2024-08-27 16:37:45,031 - INFO - joeynmt.training - Epoch 277, Step:    26280, Batch Loss:     0.046531, Batch Acc: 0.997904, Tokens per Sec:     3924, Lr: 0.000110\r\n",
      "2024-08-27 16:37:46,107 - INFO - joeynmt.training - Epoch 277, Step:    26285, Batch Loss:     0.043010, Batch Acc: 0.997123, Tokens per Sec:     3878, Lr: 0.000110\r\n",
      "2024-08-27 16:37:47,311 - INFO - joeynmt.training - Epoch 277, Step:    26290, Batch Loss:     0.051995, Batch Acc: 0.997890, Tokens per Sec:     3542, Lr: 0.000110\r\n",
      "2024-08-27 16:37:48,413 - INFO - joeynmt.training - Epoch 277, Step:    26295, Batch Loss:     0.046307, Batch Acc: 0.997733, Tokens per Sec:     4006, Lr: 0.000110\r\n",
      "2024-08-27 16:37:49,495 - INFO - joeynmt.training - Epoch 277, Step:    26300, Batch Loss:     0.048478, Batch Acc: 0.996324, Tokens per Sec:     4027, Lr: 0.000110\r\n",
      "2024-08-27 16:37:50,539 - INFO - joeynmt.training - Epoch 277, Step:    26305, Batch Loss:     0.045822, Batch Acc: 0.996583, Tokens per Sec:     3648, Lr: 0.000110\r\n",
      "2024-08-27 16:37:51,624 - INFO - joeynmt.training - Epoch 277, Step:    26310, Batch Loss:     0.043610, Batch Acc: 0.996678, Tokens per Sec:     3885, Lr: 0.000110\r\n",
      "2024-08-27 16:37:52,685 - INFO - joeynmt.training - Epoch 277, Step:    26315, Batch Loss:     0.046812, Batch Acc: 0.996841, Tokens per Sec:     3883, Lr: 0.000110\r\n",
      "2024-08-27 16:37:53,744 - INFO - joeynmt.training - Epoch 277, Step:    26320, Batch Loss:     0.050561, Batch Acc: 0.997598, Tokens per Sec:     3932, Lr: 0.000110\r\n",
      "2024-08-27 16:37:54,802 - INFO - joeynmt.training - Epoch 277, Step:    26325, Batch Loss:     0.046242, Batch Acc: 0.996529, Tokens per Sec:     4087, Lr: 0.000110\r\n",
      "2024-08-27 16:37:55,871 - INFO - joeynmt.training - Epoch 277, Step:    26330, Batch Loss:     0.048938, Batch Acc: 0.996254, Tokens per Sec:     3997, Lr: 0.000110\r\n",
      "2024-08-27 16:37:57,022 - INFO - joeynmt.training - Epoch 277, Step:    26335, Batch Loss:     0.048253, Batch Acc: 0.996130, Tokens per Sec:     3821, Lr: 0.000110\r\n",
      "2024-08-27 16:37:58,087 - INFO - joeynmt.training - Epoch 277, Step:    26340, Batch Loss:     0.045619, Batch Acc: 0.996195, Tokens per Sec:     3703, Lr: 0.000110\r\n",
      "2024-08-27 16:37:59,166 - INFO - joeynmt.training - Epoch 277, Step:    26345, Batch Loss:     0.056914, Batch Acc: 0.996757, Tokens per Sec:     4002, Lr: 0.000110\r\n",
      "2024-08-27 16:38:00,235 - INFO - joeynmt.training - Epoch 277, Step:    26350, Batch Loss:     0.041159, Batch Acc: 0.998047, Tokens per Sec:     3832, Lr: 0.000110\r\n",
      "2024-08-27 16:38:00,384 - INFO - joeynmt.training - Epoch 277, total training loss: 4.78, num. of seqs: 8065, num. of tokens: 80463, 20.7105[sec]\r\n",
      "2024-08-27 16:38:00,384 - INFO - joeynmt.training - EPOCH 278\r\n",
      "2024-08-27 16:38:01,465 - INFO - joeynmt.training - Epoch 278, Step:    26355, Batch Loss:     0.045828, Batch Acc: 0.997684, Tokens per Sec:     4009, Lr: 0.000110\r\n",
      "2024-08-27 16:38:02,543 - INFO - joeynmt.training - Epoch 278, Step:    26360, Batch Loss:     0.045923, Batch Acc: 0.997148, Tokens per Sec:     3906, Lr: 0.000110\r\n",
      "2024-08-27 16:38:03,618 - INFO - joeynmt.training - Epoch 278, Step:    26365, Batch Loss:     0.046731, Batch Acc: 0.998507, Tokens per Sec:     3741, Lr: 0.000110\r\n",
      "2024-08-27 16:38:04,681 - INFO - joeynmt.training - Epoch 278, Step:    26370, Batch Loss:     0.044405, Batch Acc: 0.997162, Tokens per Sec:     3983, Lr: 0.000110\r\n",
      "2024-08-27 16:38:05,757 - INFO - joeynmt.training - Epoch 278, Step:    26375, Batch Loss:     0.049901, Batch Acc: 0.996789, Tokens per Sec:     4054, Lr: 0.000110\r\n",
      "2024-08-27 16:38:06,849 - INFO - joeynmt.training - Epoch 278, Step:    26380, Batch Loss:     0.052897, Batch Acc: 0.997342, Tokens per Sec:     3791, Lr: 0.000110\r\n",
      "2024-08-27 16:38:07,941 - INFO - joeynmt.training - Epoch 278, Step:    26385, Batch Loss:     0.053567, Batch Acc: 0.997159, Tokens per Sec:     3870, Lr: 0.000110\r\n",
      "2024-08-27 16:38:09,131 - INFO - joeynmt.training - Epoch 278, Step:    26390, Batch Loss:     0.047291, Batch Acc: 0.995977, Tokens per Sec:     3554, Lr: 0.000110\r\n",
      "2024-08-27 16:38:10,185 - INFO - joeynmt.training - Epoch 278, Step:    26395, Batch Loss:     0.047621, Batch Acc: 0.996652, Tokens per Sec:     3970, Lr: 0.000110\r\n",
      "2024-08-27 16:38:11,234 - INFO - joeynmt.training - Epoch 278, Step:    26400, Batch Loss:     0.050759, Batch Acc: 0.995717, Tokens per Sec:     4010, Lr: 0.000110\r\n",
      "2024-08-27 16:38:12,321 - INFO - joeynmt.training - Epoch 278, Step:    26405, Batch Loss:     0.044436, Batch Acc: 0.996592, Tokens per Sec:     4051, Lr: 0.000110\r\n",
      "2024-08-27 16:38:13,388 - INFO - joeynmt.training - Epoch 278, Step:    26410, Batch Loss:     0.052796, Batch Acc: 0.995517, Tokens per Sec:     3763, Lr: 0.000110\r\n",
      "2024-08-27 16:38:14,461 - INFO - joeynmt.training - Epoch 278, Step:    26415, Batch Loss:     0.060036, Batch Acc: 0.996478, Tokens per Sec:     3973, Lr: 0.000110\r\n",
      "2024-08-27 16:38:15,553 - INFO - joeynmt.training - Epoch 278, Step:    26420, Batch Loss:     0.047063, Batch Acc: 0.996162, Tokens per Sec:     4060, Lr: 0.000110\r\n",
      "2024-08-27 16:38:16,641 - INFO - joeynmt.training - Epoch 278, Step:    26425, Batch Loss:     0.054689, Batch Acc: 0.995926, Tokens per Sec:     4062, Lr: 0.000110\r\n",
      "2024-08-27 16:38:17,727 - INFO - joeynmt.training - Epoch 278, Step:    26430, Batch Loss:     0.046310, Batch Acc: 0.998489, Tokens per Sec:     3658, Lr: 0.000110\r\n",
      "2024-08-27 16:38:18,824 - INFO - joeynmt.training - Epoch 278, Step:    26435, Batch Loss:     0.055371, Batch Acc: 0.997457, Tokens per Sec:     3948, Lr: 0.000110\r\n",
      "2024-08-27 16:38:19,912 - INFO - joeynmt.training - Epoch 278, Step:    26440, Batch Loss:     0.050675, Batch Acc: 0.996134, Tokens per Sec:     4045, Lr: 0.000110\r\n",
      "2024-08-27 16:38:20,998 - INFO - joeynmt.training - Epoch 278, Step:    26445, Batch Loss:     0.046861, Batch Acc: 0.996976, Tokens per Sec:     3656, Lr: 0.000110\r\n",
      "2024-08-27 16:38:21,047 - INFO - joeynmt.training - Epoch 278, total training loss: 4.77, num. of seqs: 8065, num. of tokens: 80463, 20.6455[sec]\r\n",
      "2024-08-27 16:38:21,047 - INFO - joeynmt.training - EPOCH 279\r\n",
      "2024-08-27 16:38:22,143 - INFO - joeynmt.training - Epoch 279, Step:    26450, Batch Loss:     0.052524, Batch Acc: 0.996610, Tokens per Sec:     3784, Lr: 0.000110\r\n",
      "2024-08-27 16:38:23,219 - INFO - joeynmt.training - Epoch 279, Step:    26455, Batch Loss:     0.049828, Batch Acc: 0.997258, Tokens per Sec:     3731, Lr: 0.000110\r\n",
      "2024-08-27 16:38:24,299 - INFO - joeynmt.training - Epoch 279, Step:    26460, Batch Loss:     0.048732, Batch Acc: 0.997436, Tokens per Sec:     3978, Lr: 0.000110\r\n",
      "2024-08-27 16:38:25,361 - INFO - joeynmt.training - Epoch 279, Step:    26465, Batch Loss:     0.064248, Batch Acc: 0.995842, Tokens per Sec:     4079, Lr: 0.000110\r\n",
      "2024-08-27 16:38:26,409 - INFO - joeynmt.training - Epoch 279, Step:    26470, Batch Loss:     0.052118, Batch Acc: 0.996978, Tokens per Sec:     3790, Lr: 0.000110\r\n",
      "2024-08-27 16:38:27,496 - INFO - joeynmt.training - Epoch 279, Step:    26475, Batch Loss:     0.049822, Batch Acc: 0.996144, Tokens per Sec:     3581, Lr: 0.000110\r\n",
      "2024-08-27 16:38:28,583 - INFO - joeynmt.training - Epoch 279, Step:    26480, Batch Loss:     0.051401, Batch Acc: 0.996250, Tokens per Sec:     3928, Lr: 0.000110\r\n",
      "2024-08-27 16:38:29,679 - INFO - joeynmt.training - Epoch 279, Step:    26485, Batch Loss:     0.051881, Batch Acc: 0.995712, Tokens per Sec:     4048, Lr: 0.000110\r\n",
      "2024-08-27 16:38:30,736 - INFO - joeynmt.training - Epoch 279, Step:    26490, Batch Loss:     0.061060, Batch Acc: 0.996606, Tokens per Sec:     3903, Lr: 0.000110\r\n",
      "2024-08-27 16:38:31,804 - INFO - joeynmt.training - Epoch 279, Step:    26495, Batch Loss:     0.055569, Batch Acc: 0.996829, Tokens per Sec:     3842, Lr: 0.000110\r\n",
      "2024-08-27 16:38:32,873 - INFO - joeynmt.training - Epoch 279, Step:    26500, Batch Loss:     0.045353, Batch Acc: 0.998303, Tokens per Sec:     3861, Lr: 0.000110\r\n",
      "2024-08-27 16:38:32,874 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=26542\r\n",
      "2024-08-27 16:38:32,874 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 66.16it/s]\r\n",
      "2024-08-27 16:38:54,945 - INFO - joeynmt.prediction - Generation took 22.0685[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44517.87 examples/s]\r\n",
      "2024-08-27 16:38:55,326 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:38:55,326 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.59, loss:   6.65, ppl: 773.76, acc:   0.29, 0.1663[sec]\r\n",
      "2024-08-27 16:38:55,328 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:38:55,473 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:38:55,473 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:38:55,473 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 16:38:55,765 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:38:55,910 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:38:55,910 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:38:55,910 - INFO - joeynmt.training - \tHypothesis: trois éléphants\r\n",
      "2024-08-27 16:38:56,198 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:38:56,344 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:38:56,345 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:38:56,345 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 16:38:56,654 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:38:56,818 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:38:56,818 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:38:56,818 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 16:38:58,192 - INFO - joeynmt.training - Epoch 279, Step:    26505, Batch Loss:     0.048445, Batch Acc: 0.996813, Tokens per Sec:     4059, Lr: 0.000110\r\n",
      "2024-08-27 16:38:59,257 - INFO - joeynmt.training - Epoch 279, Step:    26510, Batch Loss:     0.048450, Batch Acc: 0.996967, Tokens per Sec:     4027, Lr: 0.000110\r\n",
      "2024-08-27 16:39:00,324 - INFO - joeynmt.training - Epoch 279, Step:    26515, Batch Loss:     0.046789, Batch Acc: 0.996077, Tokens per Sec:     3823, Lr: 0.000110\r\n",
      "2024-08-27 16:39:01,401 - INFO - joeynmt.training - Epoch 279, Step:    26520, Batch Loss:     0.051762, Batch Acc: 0.997397, Tokens per Sec:     3927, Lr: 0.000110\r\n",
      "2024-08-27 16:39:02,478 - INFO - joeynmt.training - Epoch 279, Step:    26525, Batch Loss:     0.051427, Batch Acc: 0.995184, Tokens per Sec:     3857, Lr: 0.000110\r\n",
      "2024-08-27 16:39:03,548 - INFO - joeynmt.training - Epoch 279, Step:    26530, Batch Loss:     0.043057, Batch Acc: 0.996462, Tokens per Sec:     3966, Lr: 0.000110\r\n",
      "2024-08-27 16:39:04,619 - INFO - joeynmt.training - Epoch 279, Step:    26535, Batch Loss:     0.060545, Batch Acc: 0.995970, Tokens per Sec:     3941, Lr: 0.000110\r\n",
      "2024-08-27 16:39:05,660 - INFO - joeynmt.training - Epoch 279, Step:    26540, Batch Loss:     0.052236, Batch Acc: 0.997075, Tokens per Sec:     3943, Lr: 0.000110\r\n",
      "2024-08-27 16:39:05,976 - INFO - joeynmt.training - Epoch 279, total training loss: 4.85, num. of seqs: 8065, num. of tokens: 80463, 20.6756[sec]\r\n",
      "2024-08-27 16:39:05,977 - INFO - joeynmt.training - EPOCH 280\r\n",
      "2024-08-27 16:39:06,865 - INFO - joeynmt.training - Epoch 280, Step:    26545, Batch Loss:     0.048772, Batch Acc: 0.997233, Tokens per Sec:     3680, Lr: 0.000110\r\n",
      "2024-08-27 16:39:07,929 - INFO - joeynmt.training - Epoch 280, Step:    26550, Batch Loss:     0.051536, Batch Acc: 0.997272, Tokens per Sec:     3791, Lr: 0.000110\r\n",
      "2024-08-27 16:39:09,009 - INFO - joeynmt.training - Epoch 280, Step:    26555, Batch Loss:     0.043432, Batch Acc: 0.997665, Tokens per Sec:     3968, Lr: 0.000110\r\n",
      "2024-08-27 16:39:10,062 - INFO - joeynmt.training - Epoch 280, Step:    26560, Batch Loss:     0.049760, Batch Acc: 0.997985, Tokens per Sec:     3775, Lr: 0.000110\r\n",
      "2024-08-27 16:39:11,205 - INFO - joeynmt.training - Epoch 280, Step:    26565, Batch Loss:     0.047220, Batch Acc: 0.997457, Tokens per Sec:     3443, Lr: 0.000110\r\n",
      "2024-08-27 16:39:12,380 - INFO - joeynmt.training - Epoch 280, Step:    26570, Batch Loss:     0.049372, Batch Acc: 0.997166, Tokens per Sec:     3605, Lr: 0.000110\r\n",
      "2024-08-27 16:39:13,445 - INFO - joeynmt.training - Epoch 280, Step:    26575, Batch Loss:     0.046362, Batch Acc: 0.997368, Tokens per Sec:     3926, Lr: 0.000110\r\n",
      "2024-08-27 16:39:14,534 - INFO - joeynmt.training - Epoch 280, Step:    26580, Batch Loss:     0.047460, Batch Acc: 0.996616, Tokens per Sec:     4075, Lr: 0.000110\r\n",
      "2024-08-27 16:39:15,608 - INFO - joeynmt.training - Epoch 280, Step:    26585, Batch Loss:     0.050853, Batch Acc: 0.997276, Tokens per Sec:     4102, Lr: 0.000110\r\n",
      "2024-08-27 16:39:16,713 - INFO - joeynmt.training - Epoch 280, Step:    26590, Batch Loss:     0.052384, Batch Acc: 0.994644, Tokens per Sec:     3889, Lr: 0.000110\r\n",
      "2024-08-27 16:39:17,809 - INFO - joeynmt.training - Epoch 280, Step:    26595, Batch Loss:     0.050636, Batch Acc: 0.994590, Tokens per Sec:     3883, Lr: 0.000110\r\n",
      "2024-08-27 16:39:18,899 - INFO - joeynmt.training - Epoch 280, Step:    26600, Batch Loss:     0.059755, Batch Acc: 0.997201, Tokens per Sec:     3935, Lr: 0.000110\r\n",
      "2024-08-27 16:39:19,979 - INFO - joeynmt.training - Epoch 280, Step:    26605, Batch Loss:     0.045732, Batch Acc: 0.996922, Tokens per Sec:     3914, Lr: 0.000110\r\n",
      "2024-08-27 16:39:21,070 - INFO - joeynmt.training - Epoch 280, Step:    26610, Batch Loss:     0.042496, Batch Acc: 0.998016, Tokens per Sec:     3699, Lr: 0.000110\r\n",
      "2024-08-27 16:39:22,142 - INFO - joeynmt.training - Epoch 280, Step:    26615, Batch Loss:     0.045477, Batch Acc: 0.997162, Tokens per Sec:     3948, Lr: 0.000110\r\n",
      "2024-08-27 16:39:23,217 - INFO - joeynmt.training - Epoch 280, Step:    26620, Batch Loss:     0.054777, Batch Acc: 0.995678, Tokens per Sec:     4089, Lr: 0.000110\r\n",
      "2024-08-27 16:39:24,280 - INFO - joeynmt.training - Epoch 280, Step:    26625, Batch Loss:     0.046985, Batch Acc: 0.996523, Tokens per Sec:     4061, Lr: 0.000110\r\n",
      "2024-08-27 16:39:25,369 - INFO - joeynmt.training - Epoch 280, Step:    26630, Batch Loss:     0.052061, Batch Acc: 0.995705, Tokens per Sec:     4067, Lr: 0.000110\r\n",
      "2024-08-27 16:39:26,481 - INFO - joeynmt.training - Epoch 280, Step:    26635, Batch Loss:     0.049111, Batch Acc: 0.995971, Tokens per Sec:     3797, Lr: 0.000110\r\n",
      "2024-08-27 16:39:26,827 - INFO - joeynmt.training - Epoch 280, total training loss: 4.80, num. of seqs: 8065, num. of tokens: 80463, 20.8325[sec]\r\n",
      "2024-08-27 16:39:26,827 - INFO - joeynmt.training - EPOCH 281\r\n",
      "2024-08-27 16:39:27,684 - INFO - joeynmt.training - Epoch 281, Step:    26640, Batch Loss:     0.048871, Batch Acc: 0.999050, Tokens per Sec:     3707, Lr: 0.000110\r\n",
      "2024-08-27 16:39:28,746 - INFO - joeynmt.training - Epoch 281, Step:    26645, Batch Loss:     0.045533, Batch Acc: 0.997568, Tokens per Sec:     3875, Lr: 0.000110\r\n",
      "2024-08-27 16:39:29,836 - INFO - joeynmt.training - Epoch 281, Step:    26650, Batch Loss:     0.051534, Batch Acc: 0.996733, Tokens per Sec:     3932, Lr: 0.000110\r\n",
      "2024-08-27 16:39:30,903 - INFO - joeynmt.training - Epoch 281, Step:    26655, Batch Loss:     0.047274, Batch Acc: 0.998322, Tokens per Sec:     3912, Lr: 0.000110\r\n",
      "2024-08-27 16:39:31,964 - INFO - joeynmt.training - Epoch 281, Step:    26660, Batch Loss:     0.046666, Batch Acc: 0.998593, Tokens per Sec:     4020, Lr: 0.000110\r\n",
      "2024-08-27 16:39:33,026 - INFO - joeynmt.training - Epoch 281, Step:    26665, Batch Loss:     0.045166, Batch Acc: 0.997649, Tokens per Sec:     4006, Lr: 0.000110\r\n",
      "2024-08-27 16:39:34,087 - INFO - joeynmt.training - Epoch 281, Step:    26670, Batch Loss:     0.052609, Batch Acc: 0.996611, Tokens per Sec:     3898, Lr: 0.000110\r\n",
      "2024-08-27 16:39:35,164 - INFO - joeynmt.training - Epoch 281, Step:    26675, Batch Loss:     0.053930, Batch Acc: 0.996711, Tokens per Sec:     3956, Lr: 0.000110\r\n",
      "2024-08-27 16:39:36,238 - INFO - joeynmt.training - Epoch 281, Step:    26680, Batch Loss:     0.057506, Batch Acc: 0.997261, Tokens per Sec:     4079, Lr: 0.000110\r\n",
      "2024-08-27 16:39:37,353 - INFO - joeynmt.training - Epoch 281, Step:    26685, Batch Loss:     0.050701, Batch Acc: 0.997490, Tokens per Sec:     3933, Lr: 0.000110\r\n",
      "2024-08-27 16:39:38,427 - INFO - joeynmt.training - Epoch 281, Step:    26690, Batch Loss:     0.055450, Batch Acc: 0.995780, Tokens per Sec:     3974, Lr: 0.000109\r\n",
      "2024-08-27 16:39:39,513 - INFO - joeynmt.training - Epoch 281, Step:    26695, Batch Loss:     0.059843, Batch Acc: 0.997660, Tokens per Sec:     3935, Lr: 0.000109\r\n",
      "2024-08-27 16:39:40,595 - INFO - joeynmt.training - Epoch 281, Step:    26700, Batch Loss:     0.051683, Batch Acc: 0.996864, Tokens per Sec:     3834, Lr: 0.000109\r\n",
      "2024-08-27 16:39:41,662 - INFO - joeynmt.training - Epoch 281, Step:    26705, Batch Loss:     0.052825, Batch Acc: 0.996355, Tokens per Sec:     3862, Lr: 0.000109\r\n",
      "2024-08-27 16:39:42,895 - INFO - joeynmt.training - Epoch 281, Step:    26710, Batch Loss:     0.049920, Batch Acc: 0.997461, Tokens per Sec:     3515, Lr: 0.000109\r\n",
      "2024-08-27 16:39:44,028 - INFO - joeynmt.training - Epoch 281, Step:    26715, Batch Loss:     0.050498, Batch Acc: 0.997031, Tokens per Sec:     3866, Lr: 0.000109\r\n",
      "2024-08-27 16:39:45,121 - INFO - joeynmt.training - Epoch 281, Step:    26720, Batch Loss:     0.039563, Batch Acc: 0.998070, Tokens per Sec:     3796, Lr: 0.000109\r\n",
      "2024-08-27 16:39:46,186 - INFO - joeynmt.training - Epoch 281, Step:    26725, Batch Loss:     0.057936, Batch Acc: 0.996495, Tokens per Sec:     4019, Lr: 0.000109\r\n",
      "2024-08-27 16:39:47,278 - INFO - joeynmt.training - Epoch 281, Step:    26730, Batch Loss:     0.047804, Batch Acc: 0.997040, Tokens per Sec:     3716, Lr: 0.000109\r\n",
      "2024-08-27 16:39:47,582 - INFO - joeynmt.training - Epoch 281, total training loss: 4.67, num. of seqs: 8065, num. of tokens: 80463, 20.7379[sec]\r\n",
      "2024-08-27 16:39:47,582 - INFO - joeynmt.training - EPOCH 282\r\n",
      "2024-08-27 16:39:48,459 - INFO - joeynmt.training - Epoch 282, Step:    26735, Batch Loss:     0.047258, Batch Acc: 0.997126, Tokens per Sec:     3991, Lr: 0.000109\r\n",
      "2024-08-27 16:39:49,530 - INFO - joeynmt.training - Epoch 282, Step:    26740, Batch Loss:     0.043978, Batch Acc: 0.997598, Tokens per Sec:     3891, Lr: 0.000109\r\n",
      "2024-08-27 16:39:50,588 - INFO - joeynmt.training - Epoch 282, Step:    26745, Batch Loss:     0.047685, Batch Acc: 0.998764, Tokens per Sec:     3822, Lr: 0.000109\r\n",
      "2024-08-27 16:39:51,680 - INFO - joeynmt.training - Epoch 282, Step:    26750, Batch Loss:     0.046719, Batch Acc: 0.996205, Tokens per Sec:     3865, Lr: 0.000109\r\n",
      "2024-08-27 16:39:52,797 - INFO - joeynmt.training - Epoch 282, Step:    26755, Batch Loss:     0.045571, Batch Acc: 0.997351, Tokens per Sec:     4056, Lr: 0.000109\r\n",
      "2024-08-27 16:39:53,872 - INFO - joeynmt.training - Epoch 282, Step:    26760, Batch Loss:     0.049779, Batch Acc: 0.996319, Tokens per Sec:     4047, Lr: 0.000109\r\n",
      "2024-08-27 16:39:54,951 - INFO - joeynmt.training - Epoch 282, Step:    26765, Batch Loss:     0.050994, Batch Acc: 0.996091, Tokens per Sec:     4273, Lr: 0.000109\r\n",
      "2024-08-27 16:39:56,041 - INFO - joeynmt.training - Epoch 282, Step:    26770, Batch Loss:     0.044016, Batch Acc: 0.997300, Tokens per Sec:     4078, Lr: 0.000109\r\n",
      "2024-08-27 16:39:57,147 - INFO - joeynmt.training - Epoch 282, Step:    26775, Batch Loss:     0.051556, Batch Acc: 0.996429, Tokens per Sec:     3799, Lr: 0.000109\r\n",
      "2024-08-27 16:39:58,231 - INFO - joeynmt.training - Epoch 282, Step:    26780, Batch Loss:     0.045365, Batch Acc: 0.996180, Tokens per Sec:     3867, Lr: 0.000109\r\n",
      "2024-08-27 16:39:59,321 - INFO - joeynmt.training - Epoch 282, Step:    26785, Batch Loss:     0.049304, Batch Acc: 0.996870, Tokens per Sec:     3813, Lr: 0.000109\r\n",
      "2024-08-27 16:40:00,401 - INFO - joeynmt.training - Epoch 282, Step:    26790, Batch Loss:     0.046421, Batch Acc: 0.996862, Tokens per Sec:     3837, Lr: 0.000109\r\n",
      "2024-08-27 16:40:01,470 - INFO - joeynmt.training - Epoch 282, Step:    26795, Batch Loss:     0.047920, Batch Acc: 0.997924, Tokens per Sec:     4060, Lr: 0.000109\r\n",
      "2024-08-27 16:40:02,543 - INFO - joeynmt.training - Epoch 282, Step:    26800, Batch Loss:     0.057513, Batch Acc: 0.996870, Tokens per Sec:     3876, Lr: 0.000109\r\n",
      "2024-08-27 16:40:03,639 - INFO - joeynmt.training - Epoch 282, Step:    26805, Batch Loss:     0.059282, Batch Acc: 0.996990, Tokens per Sec:     3942, Lr: 0.000109\r\n",
      "2024-08-27 16:40:04,724 - INFO - joeynmt.training - Epoch 282, Step:    26810, Batch Loss:     0.043895, Batch Acc: 0.997897, Tokens per Sec:     3946, Lr: 0.000109\r\n",
      "2024-08-27 16:40:05,797 - INFO - joeynmt.training - Epoch 282, Step:    26815, Batch Loss:     0.048602, Batch Acc: 0.997264, Tokens per Sec:     3751, Lr: 0.000109\r\n",
      "2024-08-27 16:40:06,904 - INFO - joeynmt.training - Epoch 282, Step:    26820, Batch Loss:     0.050953, Batch Acc: 0.997799, Tokens per Sec:     3697, Lr: 0.000109\r\n",
      "2024-08-27 16:40:07,962 - INFO - joeynmt.training - Epoch 282, Step:    26825, Batch Loss:     0.048199, Batch Acc: 0.997138, Tokens per Sec:     3963, Lr: 0.000109\r\n",
      "2024-08-27 16:40:08,111 - INFO - joeynmt.training - Epoch 282, total training loss: 4.65, num. of seqs: 8065, num. of tokens: 80463, 20.5117[sec]\r\n",
      "2024-08-27 16:40:08,112 - INFO - joeynmt.training - EPOCH 283\r\n",
      "2024-08-27 16:40:09,174 - INFO - joeynmt.training - Epoch 283, Step:    26830, Batch Loss:     0.041488, Batch Acc: 0.998307, Tokens per Sec:     3907, Lr: 0.000109\r\n",
      "2024-08-27 16:40:10,234 - INFO - joeynmt.training - Epoch 283, Step:    26835, Batch Loss:     0.058367, Batch Acc: 0.995877, Tokens per Sec:     3891, Lr: 0.000109\r\n",
      "2024-08-27 16:40:11,301 - INFO - joeynmt.training - Epoch 283, Step:    26840, Batch Loss:     0.058519, Batch Acc: 0.996301, Tokens per Sec:     4057, Lr: 0.000109\r\n",
      "2024-08-27 16:40:12,369 - INFO - joeynmt.training - Epoch 283, Step:    26845, Batch Loss:     0.046533, Batch Acc: 0.995753, Tokens per Sec:     3971, Lr: 0.000109\r\n",
      "2024-08-27 16:40:13,500 - INFO - joeynmt.training - Epoch 283, Step:    26850, Batch Loss:     0.047694, Batch Acc: 0.997848, Tokens per Sec:     3700, Lr: 0.000109\r\n",
      "2024-08-27 16:40:14,622 - INFO - joeynmt.training - Epoch 283, Step:    26855, Batch Loss:     0.042945, Batch Acc: 0.997312, Tokens per Sec:     3648, Lr: 0.000109\r\n",
      "2024-08-27 16:40:15,691 - INFO - joeynmt.training - Epoch 283, Step:    26860, Batch Loss:     0.055839, Batch Acc: 0.997423, Tokens per Sec:     3999, Lr: 0.000109\r\n",
      "2024-08-27 16:40:16,809 - INFO - joeynmt.training - Epoch 283, Step:    26865, Batch Loss:     0.049858, Batch Acc: 0.997138, Tokens per Sec:     3751, Lr: 0.000109\r\n",
      "2024-08-27 16:40:17,888 - INFO - joeynmt.training - Epoch 283, Step:    26870, Batch Loss:     0.051441, Batch Acc: 0.997854, Tokens per Sec:     3891, Lr: 0.000109\r\n",
      "2024-08-27 16:40:18,959 - INFO - joeynmt.training - Epoch 283, Step:    26875, Batch Loss:     0.048848, Batch Acc: 0.996882, Tokens per Sec:     3895, Lr: 0.000109\r\n",
      "2024-08-27 16:40:20,033 - INFO - joeynmt.training - Epoch 283, Step:    26880, Batch Loss:     0.057220, Batch Acc: 0.996637, Tokens per Sec:     3879, Lr: 0.000109\r\n",
      "2024-08-27 16:40:21,103 - INFO - joeynmt.training - Epoch 283, Step:    26885, Batch Loss:     0.049877, Batch Acc: 0.995539, Tokens per Sec:     3984, Lr: 0.000109\r\n",
      "2024-08-27 16:40:22,176 - INFO - joeynmt.training - Epoch 283, Step:    26890, Batch Loss:     0.064114, Batch Acc: 0.996939, Tokens per Sec:     3962, Lr: 0.000109\r\n",
      "2024-08-27 16:40:23,252 - INFO - joeynmt.training - Epoch 283, Step:    26895, Batch Loss:     0.052591, Batch Acc: 0.996245, Tokens per Sec:     3960, Lr: 0.000109\r\n",
      "2024-08-27 16:40:24,319 - INFO - joeynmt.training - Epoch 283, Step:    26900, Batch Loss:     0.054752, Batch Acc: 0.995734, Tokens per Sec:     3737, Lr: 0.000109\r\n",
      "2024-08-27 16:40:25,407 - INFO - joeynmt.training - Epoch 283, Step:    26905, Batch Loss:     0.048544, Batch Acc: 0.996750, Tokens per Sec:     3961, Lr: 0.000109\r\n",
      "2024-08-27 16:40:26,468 - INFO - joeynmt.training - Epoch 283, Step:    26910, Batch Loss:     0.050357, Batch Acc: 0.996913, Tokens per Sec:     3971, Lr: 0.000109\r\n",
      "2024-08-27 16:40:27,570 - INFO - joeynmt.training - Epoch 283, Step:    26915, Batch Loss:     0.046798, Batch Acc: 0.997090, Tokens per Sec:     3745, Lr: 0.000109\r\n",
      "2024-08-27 16:40:28,642 - INFO - joeynmt.training - Epoch 283, Step:    26920, Batch Loss:     0.044233, Batch Acc: 0.995700, Tokens per Sec:     3907, Lr: 0.000109\r\n",
      "2024-08-27 16:40:28,855 - INFO - joeynmt.training - Epoch 283, total training loss: 4.85, num. of seqs: 8065, num. of tokens: 80463, 20.7263[sec]\r\n",
      "2024-08-27 16:40:28,855 - INFO - joeynmt.training - EPOCH 284\r\n",
      "2024-08-27 16:40:29,714 - INFO - joeynmt.training - Epoch 284, Step:    26925, Batch Loss:     0.043818, Batch Acc: 0.996877, Tokens per Sec:     3744, Lr: 0.000109\r\n",
      "2024-08-27 16:40:30,793 - INFO - joeynmt.training - Epoch 284, Step:    26930, Batch Loss:     0.051866, Batch Acc: 0.996746, Tokens per Sec:     3990, Lr: 0.000109\r\n",
      "2024-08-27 16:40:31,890 - INFO - joeynmt.training - Epoch 284, Step:    26935, Batch Loss:     0.053528, Batch Acc: 0.997853, Tokens per Sec:     3823, Lr: 0.000109\r\n",
      "2024-08-27 16:40:32,970 - INFO - joeynmt.training - Epoch 284, Step:    26940, Batch Loss:     0.046813, Batch Acc: 0.997696, Tokens per Sec:     4024, Lr: 0.000109\r\n",
      "2024-08-27 16:40:34,038 - INFO - joeynmt.training - Epoch 284, Step:    26945, Batch Loss:     0.052973, Batch Acc: 0.996717, Tokens per Sec:     3994, Lr: 0.000109\r\n",
      "2024-08-27 16:40:35,104 - INFO - joeynmt.training - Epoch 284, Step:    26950, Batch Loss:     0.047313, Batch Acc: 0.998351, Tokens per Sec:     3985, Lr: 0.000109\r\n",
      "2024-08-27 16:40:36,180 - INFO - joeynmt.training - Epoch 284, Step:    26955, Batch Loss:     0.044589, Batch Acc: 0.997117, Tokens per Sec:     3870, Lr: 0.000109\r\n",
      "2024-08-27 16:40:37,291 - INFO - joeynmt.training - Epoch 284, Step:    26960, Batch Loss:     0.047751, Batch Acc: 0.998220, Tokens per Sec:     3543, Lr: 0.000109\r\n",
      "2024-08-27 16:40:38,366 - INFO - joeynmt.training - Epoch 284, Step:    26965, Batch Loss:     0.046983, Batch Acc: 0.997506, Tokens per Sec:     4104, Lr: 0.000109\r\n",
      "2024-08-27 16:40:39,464 - INFO - joeynmt.training - Epoch 284, Step:    26970, Batch Loss:     0.049684, Batch Acc: 0.997111, Tokens per Sec:     4100, Lr: 0.000109\r\n",
      "2024-08-27 16:40:40,544 - INFO - joeynmt.training - Epoch 284, Step:    26975, Batch Loss:     0.050851, Batch Acc: 0.996575, Tokens per Sec:     3788, Lr: 0.000109\r\n",
      "2024-08-27 16:40:41,634 - INFO - joeynmt.training - Epoch 284, Step:    26980, Batch Loss:     0.053310, Batch Acc: 0.996935, Tokens per Sec:     3895, Lr: 0.000109\r\n",
      "2024-08-27 16:40:42,724 - INFO - joeynmt.training - Epoch 284, Step:    26985, Batch Loss:     0.059748, Batch Acc: 0.995494, Tokens per Sec:     3870, Lr: 0.000109\r\n",
      "2024-08-27 16:40:43,809 - INFO - joeynmt.training - Epoch 284, Step:    26990, Batch Loss:     0.043724, Batch Acc: 0.996848, Tokens per Sec:     3804, Lr: 0.000109\r\n",
      "2024-08-27 16:40:44,984 - INFO - joeynmt.training - Epoch 284, Step:    26995, Batch Loss:     0.042639, Batch Acc: 0.996858, Tokens per Sec:     3523, Lr: 0.000109\r\n",
      "2024-08-27 16:40:46,151 - INFO - joeynmt.training - Epoch 284, Step:    27000, Batch Loss:     0.049521, Batch Acc: 0.996251, Tokens per Sec:     3886, Lr: 0.000109\r\n",
      "2024-08-27 16:40:46,152 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=27042\r\n",
      "2024-08-27 16:40:46,152 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 65.47it/s]\r\n",
      "2024-08-27 16:41:08,454 - INFO - joeynmt.prediction - Generation took 22.2999[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 43795.80 examples/s]\r\n",
      "2024-08-27 16:41:08,832 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:41:08,833 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.96, loss:   6.63, ppl: 760.61, acc:   0.29, 0.1658[sec]\r\n",
      "2024-08-27 16:41:08,834 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\r\n",
      "2024-08-27 16:41:09,139 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/27000.ckpt.\r\n",
      "2024-08-27 16:41:09,140 - INFO - joeynmt.helpers - delete /kaggle/working/saved_model/dyu_fr/25500.ckpt\r\n",
      "2024-08-27 16:41:09,166 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:41:09,314 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:41:09,314 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:41:09,314 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 16:41:09,607 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:41:09,751 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:41:09,752 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:41:09,752 - INFO - joeynmt.training - \tHypothesis: trois tables blanches\r\n",
      "2024-08-27 16:41:10,039 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:41:10,185 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:41:10,185 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:41:10,185 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 16:41:10,477 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:41:10,622 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:41:10,622 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:41:10,622 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 16:41:11,990 - INFO - joeynmt.training - Epoch 284, Step:    27005, Batch Loss:     0.055981, Batch Acc: 0.996290, Tokens per Sec:     3754, Lr: 0.000109\r\n",
      "2024-08-27 16:41:13,066 - INFO - joeynmt.training - Epoch 284, Step:    27010, Batch Loss:     0.063754, Batch Acc: 0.997344, Tokens per Sec:     3850, Lr: 0.000109\r\n",
      "2024-08-27 16:41:14,141 - INFO - joeynmt.training - Epoch 284, Step:    27015, Batch Loss:     0.048781, Batch Acc: 0.996545, Tokens per Sec:     4040, Lr: 0.000109\r\n",
      "2024-08-27 16:41:14,453 - INFO - joeynmt.training - Epoch 284, total training loss: 4.77, num. of seqs: 8065, num. of tokens: 80463, 20.8204[sec]\r\n",
      "2024-08-27 16:41:14,454 - INFO - joeynmt.training - EPOCH 285\r\n",
      "2024-08-27 16:41:15,323 - INFO - joeynmt.training - Epoch 285, Step:    27020, Batch Loss:     0.044351, Batch Acc: 0.998470, Tokens per Sec:     3779, Lr: 0.000109\r\n",
      "2024-08-27 16:41:16,586 - INFO - joeynmt.training - Epoch 285, Step:    27025, Batch Loss:     0.046568, Batch Acc: 0.997915, Tokens per Sec:     3418, Lr: 0.000109\r\n",
      "2024-08-27 16:41:17,705 - INFO - joeynmt.training - Epoch 285, Step:    27030, Batch Loss:     0.053946, Batch Acc: 0.998141, Tokens per Sec:     3849, Lr: 0.000109\r\n",
      "2024-08-27 16:41:18,780 - INFO - joeynmt.training - Epoch 285, Step:    27035, Batch Loss:     0.049545, Batch Acc: 0.997068, Tokens per Sec:     3809, Lr: 0.000109\r\n",
      "2024-08-27 16:41:19,846 - INFO - joeynmt.training - Epoch 285, Step:    27040, Batch Loss:     0.052776, Batch Acc: 0.996535, Tokens per Sec:     4061, Lr: 0.000109\r\n",
      "2024-08-27 16:41:20,952 - INFO - joeynmt.training - Epoch 285, Step:    27045, Batch Loss:     0.050776, Batch Acc: 0.996899, Tokens per Sec:     3793, Lr: 0.000109\r\n",
      "2024-08-27 16:41:22,010 - INFO - joeynmt.training - Epoch 285, Step:    27050, Batch Loss:     0.045801, Batch Acc: 0.998272, Tokens per Sec:     3834, Lr: 0.000109\r\n",
      "2024-08-27 16:41:23,089 - INFO - joeynmt.training - Epoch 285, Step:    27055, Batch Loss:     0.068748, Batch Acc: 0.995895, Tokens per Sec:     3612, Lr: 0.000109\r\n",
      "2024-08-27 16:41:24,176 - INFO - joeynmt.training - Epoch 285, Step:    27060, Batch Loss:     0.053571, Batch Acc: 0.997067, Tokens per Sec:     4081, Lr: 0.000109\r\n",
      "2024-08-27 16:41:25,261 - INFO - joeynmt.training - Epoch 285, Step:    27065, Batch Loss:     0.050234, Batch Acc: 0.998079, Tokens per Sec:     3841, Lr: 0.000109\r\n",
      "2024-08-27 16:41:26,354 - INFO - joeynmt.training - Epoch 285, Step:    27070, Batch Loss:     0.051712, Batch Acc: 0.996774, Tokens per Sec:     3972, Lr: 0.000109\r\n",
      "2024-08-27 16:41:27,513 - INFO - joeynmt.training - Epoch 285, Step:    27075, Batch Loss:     0.058706, Batch Acc: 0.995688, Tokens per Sec:     3804, Lr: 0.000109\r\n",
      "2024-08-27 16:41:28,574 - INFO - joeynmt.training - Epoch 285, Step:    27080, Batch Loss:     0.045980, Batch Acc: 0.996411, Tokens per Sec:     3943, Lr: 0.000109\r\n",
      "2024-08-27 16:41:29,631 - INFO - joeynmt.training - Epoch 285, Step:    27085, Batch Loss:     0.046023, Batch Acc: 0.996121, Tokens per Sec:     3906, Lr: 0.000109\r\n",
      "2024-08-27 16:41:30,677 - INFO - joeynmt.training - Epoch 285, Step:    27090, Batch Loss:     0.056652, Batch Acc: 0.995770, Tokens per Sec:     3844, Lr: 0.000109\r\n",
      "2024-08-27 16:41:31,733 - INFO - joeynmt.training - Epoch 285, Step:    27095, Batch Loss:     0.053976, Batch Acc: 0.997056, Tokens per Sec:     3862, Lr: 0.000109\r\n",
      "2024-08-27 16:41:32,807 - INFO - joeynmt.training - Epoch 285, Step:    27100, Batch Loss:     0.044103, Batch Acc: 0.996963, Tokens per Sec:     3988, Lr: 0.000109\r\n",
      "2024-08-27 16:41:33,885 - INFO - joeynmt.training - Epoch 285, Step:    27105, Batch Loss:     0.052948, Batch Acc: 0.996589, Tokens per Sec:     4081, Lr: 0.000109\r\n",
      "2024-08-27 16:41:34,953 - INFO - joeynmt.training - Epoch 285, Step:    27110, Batch Loss:     0.045154, Batch Acc: 0.997116, Tokens per Sec:     3899, Lr: 0.000109\r\n",
      "2024-08-27 16:41:35,321 - INFO - joeynmt.training - Epoch 285, total training loss: 4.69, num. of seqs: 8065, num. of tokens: 80463, 20.8509[sec]\r\n",
      "2024-08-27 16:41:35,321 - INFO - joeynmt.training - EPOCH 286\r\n",
      "2024-08-27 16:41:36,173 - INFO - joeynmt.training - Epoch 286, Step:    27115, Batch Loss:     0.039952, Batch Acc: 0.997085, Tokens per Sec:     4049, Lr: 0.000109\r\n",
      "2024-08-27 16:41:37,278 - INFO - joeynmt.training - Epoch 286, Step:    27120, Batch Loss:     0.049836, Batch Acc: 0.997411, Tokens per Sec:     3847, Lr: 0.000109\r\n",
      "2024-08-27 16:41:38,356 - INFO - joeynmt.training - Epoch 286, Step:    27125, Batch Loss:     0.059145, Batch Acc: 0.996096, Tokens per Sec:     3805, Lr: 0.000109\r\n",
      "2024-08-27 16:41:39,425 - INFO - joeynmt.training - Epoch 286, Step:    27130, Batch Loss:     0.041383, Batch Acc: 0.998299, Tokens per Sec:     3849, Lr: 0.000109\r\n",
      "2024-08-27 16:41:40,502 - INFO - joeynmt.training - Epoch 286, Step:    27135, Batch Loss:     0.049969, Batch Acc: 0.996790, Tokens per Sec:     4050, Lr: 0.000109\r\n",
      "2024-08-27 16:41:41,566 - INFO - joeynmt.training - Epoch 286, Step:    27140, Batch Loss:     0.046603, Batch Acc: 0.997203, Tokens per Sec:     4038, Lr: 0.000109\r\n",
      "2024-08-27 16:41:42,646 - INFO - joeynmt.training - Epoch 286, Step:    27145, Batch Loss:     0.042033, Batch Acc: 0.996987, Tokens per Sec:     3997, Lr: 0.000109\r\n",
      "2024-08-27 16:41:43,717 - INFO - joeynmt.training - Epoch 286, Step:    27150, Batch Loss:     0.046094, Batch Acc: 0.998261, Tokens per Sec:     3760, Lr: 0.000109\r\n",
      "2024-08-27 16:41:44,784 - INFO - joeynmt.training - Epoch 286, Step:    27155, Batch Loss:     0.047042, Batch Acc: 0.996588, Tokens per Sec:     3848, Lr: 0.000109\r\n",
      "2024-08-27 16:41:45,854 - INFO - joeynmt.training - Epoch 286, Step:    27160, Batch Loss:     0.047512, Batch Acc: 0.998158, Tokens per Sec:     4062, Lr: 0.000109\r\n",
      "2024-08-27 16:41:46,984 - INFO - joeynmt.training - Epoch 286, Step:    27165, Batch Loss:     0.049669, Batch Acc: 0.996518, Tokens per Sec:     3814, Lr: 0.000109\r\n",
      "2024-08-27 16:41:48,312 - INFO - joeynmt.training - Epoch 286, Step:    27170, Batch Loss:     0.045532, Batch Acc: 0.996725, Tokens per Sec:     2992, Lr: 0.000109\r\n",
      "2024-08-27 16:41:49,394 - INFO - joeynmt.training - Epoch 286, Step:    27175, Batch Loss:     0.057111, Batch Acc: 0.997836, Tokens per Sec:     3847, Lr: 0.000109\r\n",
      "2024-08-27 16:41:50,455 - INFO - joeynmt.training - Epoch 286, Step:    27180, Batch Loss:     0.044258, Batch Acc: 0.998005, Tokens per Sec:     3779, Lr: 0.000109\r\n",
      "2024-08-27 16:41:51,521 - INFO - joeynmt.training - Epoch 286, Step:    27185, Batch Loss:     0.051174, Batch Acc: 0.996346, Tokens per Sec:     4112, Lr: 0.000108\r\n",
      "2024-08-27 16:41:52,613 - INFO - joeynmt.training - Epoch 286, Step:    27190, Batch Loss:     0.050154, Batch Acc: 0.995459, Tokens per Sec:     3833, Lr: 0.000108\r\n",
      "2024-08-27 16:41:53,681 - INFO - joeynmt.training - Epoch 286, Step:    27195, Batch Loss:     0.066224, Batch Acc: 0.995746, Tokens per Sec:     3963, Lr: 0.000108\r\n",
      "2024-08-27 16:41:54,760 - INFO - joeynmt.training - Epoch 286, Step:    27200, Batch Loss:     0.049246, Batch Acc: 0.996474, Tokens per Sec:     3946, Lr: 0.000108\r\n",
      "2024-08-27 16:41:55,819 - INFO - joeynmt.training - Epoch 286, Step:    27205, Batch Loss:     0.055544, Batch Acc: 0.996314, Tokens per Sec:     3845, Lr: 0.000108\r\n",
      "2024-08-27 16:41:56,260 - INFO - joeynmt.training - Epoch 286, total training loss: 4.76, num. of seqs: 8065, num. of tokens: 80463, 20.9219[sec]\r\n",
      "2024-08-27 16:41:56,260 - INFO - joeynmt.training - EPOCH 287\r\n",
      "2024-08-27 16:41:56,953 - INFO - joeynmt.training - Epoch 287, Step:    27210, Batch Loss:     0.050407, Batch Acc: 0.996533, Tokens per Sec:     3764, Lr: 0.000108\r\n",
      "2024-08-27 16:41:58,010 - INFO - joeynmt.training - Epoch 287, Step:    27215, Batch Loss:     0.046367, Batch Acc: 0.996915, Tokens per Sec:     3990, Lr: 0.000108\r\n",
      "2024-08-27 16:41:59,083 - INFO - joeynmt.training - Epoch 287, Step:    27220, Batch Loss:     0.049987, Batch Acc: 0.996272, Tokens per Sec:     3754, Lr: 0.000108\r\n",
      "2024-08-27 16:42:00,146 - INFO - joeynmt.training - Epoch 287, Step:    27225, Batch Loss:     0.059075, Batch Acc: 0.994657, Tokens per Sec:     4052, Lr: 0.000108\r\n",
      "2024-08-27 16:42:01,271 - INFO - joeynmt.training - Epoch 287, Step:    27230, Batch Loss:     0.049722, Batch Acc: 0.997148, Tokens per Sec:     3741, Lr: 0.000108\r\n",
      "2024-08-27 16:42:02,328 - INFO - joeynmt.training - Epoch 287, Step:    27235, Batch Loss:     0.051569, Batch Acc: 0.996018, Tokens per Sec:     4040, Lr: 0.000108\r\n",
      "2024-08-27 16:42:03,405 - INFO - joeynmt.training - Epoch 287, Step:    27240, Batch Loss:     0.049814, Batch Acc: 0.997645, Tokens per Sec:     3949, Lr: 0.000108\r\n",
      "2024-08-27 16:42:04,475 - INFO - joeynmt.training - Epoch 287, Step:    27245, Batch Loss:     0.047024, Batch Acc: 0.996926, Tokens per Sec:     3956, Lr: 0.000108\r\n",
      "2024-08-27 16:42:05,536 - INFO - joeynmt.training - Epoch 287, Step:    27250, Batch Loss:     0.044674, Batch Acc: 0.998302, Tokens per Sec:     3885, Lr: 0.000108\r\n",
      "2024-08-27 16:42:06,619 - INFO - joeynmt.training - Epoch 287, Step:    27255, Batch Loss:     0.050455, Batch Acc: 0.997041, Tokens per Sec:     3748, Lr: 0.000108\r\n",
      "2024-08-27 16:42:07,713 - INFO - joeynmt.training - Epoch 287, Step:    27260, Batch Loss:     0.058716, Batch Acc: 0.997260, Tokens per Sec:     3671, Lr: 0.000108\r\n",
      "2024-08-27 16:42:08,805 - INFO - joeynmt.training - Epoch 287, Step:    27265, Batch Loss:     0.045388, Batch Acc: 0.998814, Tokens per Sec:     3863, Lr: 0.000108\r\n",
      "2024-08-27 16:42:09,899 - INFO - joeynmt.training - Epoch 287, Step:    27270, Batch Loss:     0.047823, Batch Acc: 0.997107, Tokens per Sec:     4110, Lr: 0.000108\r\n",
      "2024-08-27 16:42:10,983 - INFO - joeynmt.training - Epoch 287, Step:    27275, Batch Loss:     0.044847, Batch Acc: 0.997431, Tokens per Sec:     3952, Lr: 0.000108\r\n",
      "2024-08-27 16:42:12,056 - INFO - joeynmt.training - Epoch 287, Step:    27280, Batch Loss:     0.045503, Batch Acc: 0.996371, Tokens per Sec:     3856, Lr: 0.000108\r\n",
      "2024-08-27 16:42:13,142 - INFO - joeynmt.training - Epoch 287, Step:    27285, Batch Loss:     0.048836, Batch Acc: 0.997468, Tokens per Sec:     4003, Lr: 0.000108\r\n",
      "2024-08-27 16:42:14,211 - INFO - joeynmt.training - Epoch 287, Step:    27290, Batch Loss:     0.056721, Batch Acc: 0.996307, Tokens per Sec:     4056, Lr: 0.000108\r\n",
      "2024-08-27 16:42:15,278 - INFO - joeynmt.training - Epoch 287, Step:    27295, Batch Loss:     0.052899, Batch Acc: 0.996965, Tokens per Sec:     4016, Lr: 0.000108\r\n",
      "2024-08-27 16:42:16,345 - INFO - joeynmt.training - Epoch 287, Step:    27300, Batch Loss:     0.054381, Batch Acc: 0.994860, Tokens per Sec:     3831, Lr: 0.000108\r\n",
      "2024-08-27 16:42:16,909 - INFO - joeynmt.training - Epoch 287, total training loss: 4.72, num. of seqs: 8065, num. of tokens: 80463, 20.6327[sec]\r\n",
      "2024-08-27 16:42:16,910 - INFO - joeynmt.training - EPOCH 288\r\n",
      "2024-08-27 16:42:17,553 - INFO - joeynmt.training - Epoch 288, Step:    27305, Batch Loss:     0.048503, Batch Acc: 0.996971, Tokens per Sec:     4130, Lr: 0.000108\r\n",
      "2024-08-27 16:42:18,723 - INFO - joeynmt.training - Epoch 288, Step:    27310, Batch Loss:     0.044892, Batch Acc: 0.997886, Tokens per Sec:     3640, Lr: 0.000108\r\n",
      "2024-08-27 16:42:19,884 - INFO - joeynmt.training - Epoch 288, Step:    27315, Batch Loss:     0.048441, Batch Acc: 0.996481, Tokens per Sec:     3674, Lr: 0.000108\r\n",
      "2024-08-27 16:42:20,964 - INFO - joeynmt.training - Epoch 288, Step:    27320, Batch Loss:     0.050081, Batch Acc: 0.998054, Tokens per Sec:     3812, Lr: 0.000108\r\n",
      "2024-08-27 16:42:22,051 - INFO - joeynmt.training - Epoch 288, Step:    27325, Batch Loss:     0.038906, Batch Acc: 0.997732, Tokens per Sec:     4056, Lr: 0.000108\r\n",
      "2024-08-27 16:42:23,140 - INFO - joeynmt.training - Epoch 288, Step:    27330, Batch Loss:     0.062589, Batch Acc: 0.996853, Tokens per Sec:     4089, Lr: 0.000108\r\n",
      "2024-08-27 16:42:24,212 - INFO - joeynmt.training - Epoch 288, Step:    27335, Batch Loss:     0.042881, Batch Acc: 0.998283, Tokens per Sec:     3804, Lr: 0.000108\r\n",
      "2024-08-27 16:42:25,292 - INFO - joeynmt.training - Epoch 288, Step:    27340, Batch Loss:     0.070819, Batch Acc: 0.996504, Tokens per Sec:     3710, Lr: 0.000108\r\n",
      "2024-08-27 16:42:26,393 - INFO - joeynmt.training - Epoch 288, Step:    27345, Batch Loss:     0.048218, Batch Acc: 0.996256, Tokens per Sec:     3884, Lr: 0.000108\r\n",
      "2024-08-27 16:42:27,524 - INFO - joeynmt.training - Epoch 288, Step:    27350, Batch Loss:     0.045248, Batch Acc: 0.997739, Tokens per Sec:     3914, Lr: 0.000108\r\n",
      "2024-08-27 16:42:28,606 - INFO - joeynmt.training - Epoch 288, Step:    27355, Batch Loss:     0.051799, Batch Acc: 0.997076, Tokens per Sec:     3792, Lr: 0.000108\r\n",
      "2024-08-27 16:42:29,683 - INFO - joeynmt.training - Epoch 288, Step:    27360, Batch Loss:     0.048746, Batch Acc: 0.996249, Tokens per Sec:     3967, Lr: 0.000108\r\n",
      "2024-08-27 16:42:30,832 - INFO - joeynmt.training - Epoch 288, Step:    27365, Batch Loss:     0.049028, Batch Acc: 0.997043, Tokens per Sec:     3827, Lr: 0.000108\r\n",
      "2024-08-27 16:42:31,961 - INFO - joeynmt.training - Epoch 288, Step:    27370, Batch Loss:     0.043224, Batch Acc: 0.997658, Tokens per Sec:     3784, Lr: 0.000108\r\n",
      "2024-08-27 16:42:33,033 - INFO - joeynmt.training - Epoch 288, Step:    27375, Batch Loss:     0.044086, Batch Acc: 0.997435, Tokens per Sec:     4003, Lr: 0.000108\r\n",
      "2024-08-27 16:42:34,099 - INFO - joeynmt.training - Epoch 288, Step:    27380, Batch Loss:     0.051049, Batch Acc: 0.995742, Tokens per Sec:     3967, Lr: 0.000108\r\n",
      "2024-08-27 16:42:35,172 - INFO - joeynmt.training - Epoch 288, Step:    27385, Batch Loss:     0.047031, Batch Acc: 0.997715, Tokens per Sec:     4082, Lr: 0.000108\r\n",
      "2024-08-27 16:42:36,257 - INFO - joeynmt.training - Epoch 288, Step:    27390, Batch Loss:     0.047478, Batch Acc: 0.996262, Tokens per Sec:     3951, Lr: 0.000108\r\n",
      "2024-08-27 16:42:37,385 - INFO - joeynmt.training - Epoch 288, Step:    27395, Batch Loss:     0.045073, Batch Acc: 0.994819, Tokens per Sec:     3593, Lr: 0.000108\r\n",
      "2024-08-27 16:42:37,741 - INFO - joeynmt.training - Epoch 288, total training loss: 4.64, num. of seqs: 8065, num. of tokens: 80463, 20.8148[sec]\r\n",
      "2024-08-27 16:42:37,741 - INFO - joeynmt.training - EPOCH 289\r\n",
      "2024-08-27 16:42:38,599 - INFO - joeynmt.training - Epoch 289, Step:    27400, Batch Loss:     0.042107, Batch Acc: 0.997825, Tokens per Sec:     3772, Lr: 0.000108\r\n",
      "2024-08-27 16:42:39,668 - INFO - joeynmt.training - Epoch 289, Step:    27405, Batch Loss:     0.043831, Batch Acc: 0.996684, Tokens per Sec:     3953, Lr: 0.000108\r\n",
      "2024-08-27 16:42:40,752 - INFO - joeynmt.training - Epoch 289, Step:    27410, Batch Loss:     0.046087, Batch Acc: 0.997166, Tokens per Sec:     3909, Lr: 0.000108\r\n",
      "2024-08-27 16:42:41,864 - INFO - joeynmt.training - Epoch 289, Step:    27415, Batch Loss:     0.052039, Batch Acc: 0.997009, Tokens per Sec:     3911, Lr: 0.000108\r\n",
      "2024-08-27 16:42:42,935 - INFO - joeynmt.training - Epoch 289, Step:    27420, Batch Loss:     0.043350, Batch Acc: 0.998328, Tokens per Sec:     3910, Lr: 0.000108\r\n",
      "2024-08-27 16:42:44,007 - INFO - joeynmt.training - Epoch 289, Step:    27425, Batch Loss:     0.046400, Batch Acc: 0.996984, Tokens per Sec:     4025, Lr: 0.000108\r\n",
      "2024-08-27 16:42:45,080 - INFO - joeynmt.training - Epoch 289, Step:    27430, Batch Loss:     0.048168, Batch Acc: 0.998806, Tokens per Sec:     3907, Lr: 0.000108\r\n",
      "2024-08-27 16:42:46,160 - INFO - joeynmt.training - Epoch 289, Step:    27435, Batch Loss:     0.046757, Batch Acc: 0.997457, Tokens per Sec:     4006, Lr: 0.000108\r\n",
      "2024-08-27 16:42:47,252 - INFO - joeynmt.training - Epoch 289, Step:    27440, Batch Loss:     0.045250, Batch Acc: 0.998620, Tokens per Sec:     3983, Lr: 0.000108\r\n",
      "2024-08-27 16:42:48,329 - INFO - joeynmt.training - Epoch 289, Step:    27445, Batch Loss:     0.052573, Batch Acc: 0.997429, Tokens per Sec:     3975, Lr: 0.000108\r\n",
      "2024-08-27 16:42:49,405 - INFO - joeynmt.training - Epoch 289, Step:    27450, Batch Loss:     0.046311, Batch Acc: 0.998063, Tokens per Sec:     3840, Lr: 0.000108\r\n",
      "2024-08-27 16:42:50,563 - INFO - joeynmt.training - Epoch 289, Step:    27455, Batch Loss:     0.047173, Batch Acc: 0.997400, Tokens per Sec:     3656, Lr: 0.000108\r\n",
      "2024-08-27 16:42:51,643 - INFO - joeynmt.training - Epoch 289, Step:    27460, Batch Loss:     0.050548, Batch Acc: 0.997400, Tokens per Sec:     3921, Lr: 0.000108\r\n",
      "2024-08-27 16:42:52,747 - INFO - joeynmt.training - Epoch 289, Step:    27465, Batch Loss:     0.048084, Batch Acc: 0.997883, Tokens per Sec:     3853, Lr: 0.000108\r\n",
      "2024-08-27 16:42:53,808 - INFO - joeynmt.training - Epoch 289, Step:    27470, Batch Loss:     0.047718, Batch Acc: 0.997301, Tokens per Sec:     3842, Lr: 0.000108\r\n",
      "2024-08-27 16:42:54,868 - INFO - joeynmt.training - Epoch 289, Step:    27475, Batch Loss:     0.054429, Batch Acc: 0.995670, Tokens per Sec:     3926, Lr: 0.000108\r\n",
      "2024-08-27 16:42:55,929 - INFO - joeynmt.training - Epoch 289, Step:    27480, Batch Loss:     0.048521, Batch Acc: 0.997324, Tokens per Sec:     3874, Lr: 0.000108\r\n",
      "2024-08-27 16:42:57,062 - INFO - joeynmt.training - Epoch 289, Step:    27485, Batch Loss:     0.050279, Batch Acc: 0.997395, Tokens per Sec:     3730, Lr: 0.000108\r\n",
      "2024-08-27 16:42:58,138 - INFO - joeynmt.training - Epoch 289, Step:    27490, Batch Loss:     0.046932, Batch Acc: 0.997213, Tokens per Sec:     4005, Lr: 0.000108\r\n",
      "2024-08-27 16:42:58,447 - INFO - joeynmt.training - Epoch 289, total training loss: 4.57, num. of seqs: 8065, num. of tokens: 80463, 20.6888[sec]\r\n",
      "2024-08-27 16:42:58,448 - INFO - joeynmt.training - EPOCH 290\r\n",
      "2024-08-27 16:42:59,300 - INFO - joeynmt.training - Epoch 290, Step:    27495, Batch Loss:     0.040650, Batch Acc: 0.998814, Tokens per Sec:     3980, Lr: 0.000108\r\n",
      "2024-08-27 16:43:00,361 - INFO - joeynmt.training - Epoch 290, Step:    27500, Batch Loss:     0.052250, Batch Acc: 0.996950, Tokens per Sec:     4021, Lr: 0.000108\r\n",
      "2024-08-27 16:43:00,362 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=27542\r\n",
      "2024-08-27 16:43:00,362 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 64.32it/s]\r\n",
      "2024-08-27 16:43:23,063 - INFO - joeynmt.prediction - Generation took 22.6989[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 43733.40 examples/s]\r\n",
      "2024-08-27 16:43:23,446 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:43:23,446 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.53, loss:   6.60, ppl: 734.18, acc:   0.29, 0.1685[sec]\r\n",
      "2024-08-27 16:43:23,448 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:43:23,596 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:43:23,597 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:43:23,597 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 16:43:23,890 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:43:24,034 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:43:24,034 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:43:24,034 - INFO - joeynmt.training - \tHypothesis: trois point d'avance\r\n",
      "2024-08-27 16:43:24,325 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:43:24,470 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:43:24,470 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:43:24,470 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 16:43:24,763 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:43:24,908 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:43:24,908 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:43:24,908 - INFO - joeynmt.training - \tHypothesis: une seule porte ne suffit pas\r\n",
      "2024-08-27 16:43:26,288 - INFO - joeynmt.training - Epoch 290, Step:    27505, Batch Loss:     0.040067, Batch Acc: 0.998273, Tokens per Sec:     3729, Lr: 0.000108\r\n",
      "2024-08-27 16:43:27,399 - INFO - joeynmt.training - Epoch 290, Step:    27510, Batch Loss:     0.042349, Batch Acc: 0.997658, Tokens per Sec:     3845, Lr: 0.000108\r\n",
      "2024-08-27 16:43:28,495 - INFO - joeynmt.training - Epoch 290, Step:    27515, Batch Loss:     0.045459, Batch Acc: 0.997589, Tokens per Sec:     3786, Lr: 0.000108\r\n",
      "2024-08-27 16:43:29,578 - INFO - joeynmt.training - Epoch 290, Step:    27520, Batch Loss:     0.041966, Batch Acc: 0.997829, Tokens per Sec:     3833, Lr: 0.000108\r\n",
      "2024-08-27 16:43:30,644 - INFO - joeynmt.training - Epoch 290, Step:    27525, Batch Loss:     0.041948, Batch Acc: 0.997843, Tokens per Sec:     3914, Lr: 0.000108\r\n",
      "2024-08-27 16:43:31,697 - INFO - joeynmt.training - Epoch 290, Step:    27530, Batch Loss:     0.051485, Batch Acc: 0.996138, Tokens per Sec:     3937, Lr: 0.000108\r\n",
      "2024-08-27 16:43:32,757 - INFO - joeynmt.training - Epoch 290, Step:    27535, Batch Loss:     0.056316, Batch Acc: 0.998304, Tokens per Sec:     3896, Lr: 0.000108\r\n",
      "2024-08-27 16:43:34,060 - INFO - joeynmt.training - Epoch 290, Step:    27540, Batch Loss:     0.052184, Batch Acc: 0.996212, Tokens per Sec:     3245, Lr: 0.000108\r\n",
      "2024-08-27 16:43:35,133 - INFO - joeynmt.training - Epoch 290, Step:    27545, Batch Loss:     0.047156, Batch Acc: 0.997344, Tokens per Sec:     3862, Lr: 0.000108\r\n",
      "2024-08-27 16:43:36,208 - INFO - joeynmt.training - Epoch 290, Step:    27550, Batch Loss:     0.052227, Batch Acc: 0.996752, Tokens per Sec:     4013, Lr: 0.000108\r\n",
      "2024-08-27 16:43:37,314 - INFO - joeynmt.training - Epoch 290, Step:    27555, Batch Loss:     0.048466, Batch Acc: 0.997764, Tokens per Sec:     3639, Lr: 0.000108\r\n",
      "2024-08-27 16:43:38,386 - INFO - joeynmt.training - Epoch 290, Step:    27560, Batch Loss:     0.059063, Batch Acc: 0.995468, Tokens per Sec:     3913, Lr: 0.000108\r\n",
      "2024-08-27 16:43:39,466 - INFO - joeynmt.training - Epoch 290, Step:    27565, Batch Loss:     0.049332, Batch Acc: 0.997178, Tokens per Sec:     3938, Lr: 0.000108\r\n",
      "2024-08-27 16:43:40,547 - INFO - joeynmt.training - Epoch 290, Step:    27570, Batch Loss:     0.053888, Batch Acc: 0.997277, Tokens per Sec:     3741, Lr: 0.000108\r\n",
      "2024-08-27 16:43:41,618 - INFO - joeynmt.training - Epoch 290, Step:    27575, Batch Loss:     0.049577, Batch Acc: 0.997130, Tokens per Sec:     3904, Lr: 0.000108\r\n",
      "2024-08-27 16:43:42,709 - INFO - joeynmt.training - Epoch 290, Step:    27580, Batch Loss:     0.045537, Batch Acc: 0.996872, Tokens per Sec:     4106, Lr: 0.000108\r\n",
      "2024-08-27 16:43:43,787 - INFO - joeynmt.training - Epoch 290, Step:    27585, Batch Loss:     0.050456, Batch Acc: 0.998051, Tokens per Sec:     3808, Lr: 0.000108\r\n",
      "2024-08-27 16:43:44,273 - INFO - joeynmt.training - Epoch 290, total training loss: 4.68, num. of seqs: 8065, num. of tokens: 80463, 20.9679[sec]\r\n",
      "2024-08-27 16:43:44,273 - INFO - joeynmt.training - EPOCH 291\r\n",
      "2024-08-27 16:43:44,930 - INFO - joeynmt.training - Epoch 291, Step:    27590, Batch Loss:     0.058316, Batch Acc: 0.995725, Tokens per Sec:     3941, Lr: 0.000108\r\n",
      "2024-08-27 16:43:46,006 - INFO - joeynmt.training - Epoch 291, Step:    27595, Batch Loss:     0.041695, Batch Acc: 0.997209, Tokens per Sec:     3665, Lr: 0.000108\r\n",
      "2024-08-27 16:43:47,101 - INFO - joeynmt.training - Epoch 291, Step:    27600, Batch Loss:     0.043950, Batch Acc: 0.998087, Tokens per Sec:     3821, Lr: 0.000108\r\n",
      "2024-08-27 16:43:48,189 - INFO - joeynmt.training - Epoch 291, Step:    27605, Batch Loss:     0.044410, Batch Acc: 0.998593, Tokens per Sec:     3923, Lr: 0.000108\r\n",
      "2024-08-27 16:43:49,257 - INFO - joeynmt.training - Epoch 291, Step:    27610, Batch Loss:     0.050678, Batch Acc: 0.994592, Tokens per Sec:     3637, Lr: 0.000108\r\n",
      "2024-08-27 16:43:50,314 - INFO - joeynmt.training - Epoch 291, Step:    27615, Batch Loss:     0.042398, Batch Acc: 0.997634, Tokens per Sec:     4004, Lr: 0.000108\r\n",
      "2024-08-27 16:43:51,371 - INFO - joeynmt.training - Epoch 291, Step:    27620, Batch Loss:     0.044852, Batch Acc: 0.997271, Tokens per Sec:     3813, Lr: 0.000108\r\n",
      "2024-08-27 16:43:52,536 - INFO - joeynmt.training - Epoch 291, Step:    27625, Batch Loss:     0.044921, Batch Acc: 0.999072, Tokens per Sec:     3702, Lr: 0.000108\r\n",
      "2024-08-27 16:43:53,633 - INFO - joeynmt.training - Epoch 291, Step:    27630, Batch Loss:     0.045829, Batch Acc: 0.997022, Tokens per Sec:     3983, Lr: 0.000108\r\n",
      "2024-08-27 16:43:54,695 - INFO - joeynmt.training - Epoch 291, Step:    27635, Batch Loss:     0.056454, Batch Acc: 0.997423, Tokens per Sec:     3654, Lr: 0.000108\r\n",
      "2024-08-27 16:43:55,781 - INFO - joeynmt.training - Epoch 291, Step:    27640, Batch Loss:     0.048385, Batch Acc: 0.997183, Tokens per Sec:     3926, Lr: 0.000108\r\n",
      "2024-08-27 16:43:56,921 - INFO - joeynmt.training - Epoch 291, Step:    27645, Batch Loss:     0.052558, Batch Acc: 0.997097, Tokens per Sec:     3628, Lr: 0.000108\r\n",
      "2024-08-27 16:43:57,988 - INFO - joeynmt.training - Epoch 291, Step:    27650, Batch Loss:     0.047949, Batch Acc: 0.997222, Tokens per Sec:     4053, Lr: 0.000108\r\n",
      "2024-08-27 16:43:59,076 - INFO - joeynmt.training - Epoch 291, Step:    27655, Batch Loss:     0.045419, Batch Acc: 0.996765, Tokens per Sec:     3978, Lr: 0.000108\r\n",
      "2024-08-27 16:44:00,139 - INFO - joeynmt.training - Epoch 291, Step:    27660, Batch Loss:     0.046067, Batch Acc: 0.996256, Tokens per Sec:     4021, Lr: 0.000108\r\n",
      "2024-08-27 16:44:01,210 - INFO - joeynmt.training - Epoch 291, Step:    27665, Batch Loss:     0.045613, Batch Acc: 0.997977, Tokens per Sec:     4156, Lr: 0.000108\r\n",
      "2024-08-27 16:44:02,259 - INFO - joeynmt.training - Epoch 291, Step:    27670, Batch Loss:     0.049800, Batch Acc: 0.997113, Tokens per Sec:     3966, Lr: 0.000108\r\n",
      "2024-08-27 16:44:03,324 - INFO - joeynmt.training - Epoch 291, Step:    27675, Batch Loss:     0.053606, Batch Acc: 0.996584, Tokens per Sec:     3850, Lr: 0.000108\r\n",
      "2024-08-27 16:44:04,386 - INFO - joeynmt.training - Epoch 291, Step:    27680, Batch Loss:     0.053734, Batch Acc: 0.997315, Tokens per Sec:     3861, Lr: 0.000108\r\n",
      "2024-08-27 16:44:05,042 - INFO - joeynmt.training - Epoch 291, total training loss: 4.67, num. of seqs: 8065, num. of tokens: 80463, 20.7526[sec]\r\n",
      "2024-08-27 16:44:05,043 - INFO - joeynmt.training - EPOCH 292\r\n",
      "2024-08-27 16:44:05,476 - INFO - joeynmt.training - Epoch 292, Step:    27685, Batch Loss:     0.042866, Batch Acc: 0.999402, Tokens per Sec:     3898, Lr: 0.000108\r\n",
      "2024-08-27 16:44:06,545 - INFO - joeynmt.training - Epoch 292, Step:    27690, Batch Loss:     0.043683, Batch Acc: 0.997092, Tokens per Sec:     3863, Lr: 0.000108\r\n",
      "2024-08-27 16:44:07,626 - INFO - joeynmt.training - Epoch 292, Step:    27695, Batch Loss:     0.048119, Batch Acc: 0.997151, Tokens per Sec:     3899, Lr: 0.000107\r\n",
      "2024-08-27 16:44:08,697 - INFO - joeynmt.training - Epoch 292, Step:    27700, Batch Loss:     0.047914, Batch Acc: 0.996220, Tokens per Sec:     3956, Lr: 0.000107\r\n",
      "2024-08-27 16:44:09,756 - INFO - joeynmt.training - Epoch 292, Step:    27705, Batch Loss:     0.052708, Batch Acc: 0.996866, Tokens per Sec:     4221, Lr: 0.000107\r\n",
      "2024-08-27 16:44:10,811 - INFO - joeynmt.training - Epoch 292, Step:    27710, Batch Loss:     0.050717, Batch Acc: 0.997505, Tokens per Sec:     3802, Lr: 0.000107\r\n",
      "2024-08-27 16:44:11,881 - INFO - joeynmt.training - Epoch 292, Step:    27715, Batch Loss:     0.045361, Batch Acc: 0.998117, Tokens per Sec:     3972, Lr: 0.000107\r\n",
      "2024-08-27 16:44:12,958 - INFO - joeynmt.training - Epoch 292, Step:    27720, Batch Loss:     0.051717, Batch Acc: 0.997007, Tokens per Sec:     4035, Lr: 0.000107\r\n",
      "2024-08-27 16:44:14,036 - INFO - joeynmt.training - Epoch 292, Step:    27725, Batch Loss:     0.047431, Batch Acc: 0.996041, Tokens per Sec:     3986, Lr: 0.000107\r\n",
      "2024-08-27 16:44:15,108 - INFO - joeynmt.training - Epoch 292, Step:    27730, Batch Loss:     0.053010, Batch Acc: 0.997157, Tokens per Sec:     3939, Lr: 0.000107\r\n",
      "2024-08-27 16:44:16,173 - INFO - joeynmt.training - Epoch 292, Step:    27735, Batch Loss:     0.051884, Batch Acc: 0.998125, Tokens per Sec:     4008, Lr: 0.000107\r\n",
      "2024-08-27 16:44:17,275 - INFO - joeynmt.training - Epoch 292, Step:    27740, Batch Loss:     0.042607, Batch Acc: 0.996967, Tokens per Sec:     3891, Lr: 0.000107\r\n",
      "2024-08-27 16:44:18,345 - INFO - joeynmt.training - Epoch 292, Step:    27745, Batch Loss:     0.048408, Batch Acc: 0.998240, Tokens per Sec:     3719, Lr: 0.000107\r\n",
      "2024-08-27 16:44:19,407 - INFO - joeynmt.training - Epoch 292, Step:    27750, Batch Loss:     0.046305, Batch Acc: 0.997057, Tokens per Sec:     3841, Lr: 0.000107\r\n",
      "2024-08-27 16:44:20,479 - INFO - joeynmt.training - Epoch 292, Step:    27755, Batch Loss:     0.050072, Batch Acc: 0.996538, Tokens per Sec:     4048, Lr: 0.000107\r\n",
      "2024-08-27 16:44:21,560 - INFO - joeynmt.training - Epoch 292, Step:    27760, Batch Loss:     0.055700, Batch Acc: 0.997880, Tokens per Sec:     3930, Lr: 0.000107\r\n",
      "2024-08-27 16:44:22,638 - INFO - joeynmt.training - Epoch 292, Step:    27765, Batch Loss:     0.047772, Batch Acc: 0.997680, Tokens per Sec:     3998, Lr: 0.000107\r\n",
      "2024-08-27 16:44:23,828 - INFO - joeynmt.training - Epoch 292, Step:    27770, Batch Loss:     0.044649, Batch Acc: 0.997754, Tokens per Sec:     3744, Lr: 0.000107\r\n",
      "2024-08-27 16:44:24,903 - INFO - joeynmt.training - Epoch 292, Step:    27775, Batch Loss:     0.048953, Batch Acc: 0.997703, Tokens per Sec:     4053, Lr: 0.000107\r\n",
      "2024-08-27 16:44:25,547 - INFO - joeynmt.training - Epoch 292, total training loss: 4.57, num. of seqs: 8065, num. of tokens: 80463, 20.4882[sec]\r\n",
      "2024-08-27 16:44:25,547 - INFO - joeynmt.training - EPOCH 293\r\n",
      "2024-08-27 16:44:25,978 - INFO - joeynmt.training - Epoch 293, Step:    27780, Batch Loss:     0.043201, Batch Acc: 0.999403, Tokens per Sec:     3922, Lr: 0.000107\r\n",
      "2024-08-27 16:44:27,076 - INFO - joeynmt.training - Epoch 293, Step:    27785, Batch Loss:     0.051406, Batch Acc: 0.998005, Tokens per Sec:     3652, Lr: 0.000107\r\n",
      "2024-08-27 16:44:28,150 - INFO - joeynmt.training - Epoch 293, Step:    27790, Batch Loss:     0.044485, Batch Acc: 0.997221, Tokens per Sec:     4024, Lr: 0.000107\r\n",
      "2024-08-27 16:44:29,231 - INFO - joeynmt.training - Epoch 293, Step:    27795, Batch Loss:     0.054832, Batch Acc: 0.998427, Tokens per Sec:     4116, Lr: 0.000107\r\n",
      "2024-08-27 16:44:30,320 - INFO - joeynmt.training - Epoch 293, Step:    27800, Batch Loss:     0.047456, Batch Acc: 0.996790, Tokens per Sec:     4006, Lr: 0.000107\r\n",
      "2024-08-27 16:44:31,387 - INFO - joeynmt.training - Epoch 293, Step:    27805, Batch Loss:     0.052093, Batch Acc: 0.997711, Tokens per Sec:     4096, Lr: 0.000107\r\n",
      "2024-08-27 16:44:32,458 - INFO - joeynmt.training - Epoch 293, Step:    27810, Batch Loss:     0.048455, Batch Acc: 0.997371, Tokens per Sec:     4264, Lr: 0.000107\r\n",
      "2024-08-27 16:44:33,516 - INFO - joeynmt.training - Epoch 293, Step:    27815, Batch Loss:     0.058886, Batch Acc: 0.994708, Tokens per Sec:     3933, Lr: 0.000107\r\n",
      "2024-08-27 16:44:34,599 - INFO - joeynmt.training - Epoch 293, Step:    27820, Batch Loss:     0.046658, Batch Acc: 0.997356, Tokens per Sec:     3843, Lr: 0.000107\r\n",
      "2024-08-27 16:44:35,687 - INFO - joeynmt.training - Epoch 293, Step:    27825, Batch Loss:     0.047658, Batch Acc: 0.996028, Tokens per Sec:     3937, Lr: 0.000107\r\n",
      "2024-08-27 16:44:36,844 - INFO - joeynmt.training - Epoch 293, Step:    27830, Batch Loss:     0.040910, Batch Acc: 0.995718, Tokens per Sec:     3434, Lr: 0.000107\r\n",
      "2024-08-27 16:44:37,913 - INFO - joeynmt.training - Epoch 293, Step:    27835, Batch Loss:     0.044871, Batch Acc: 0.997328, Tokens per Sec:     3850, Lr: 0.000107\r\n",
      "2024-08-27 16:44:38,975 - INFO - joeynmt.training - Epoch 293, Step:    27840, Batch Loss:     0.047238, Batch Acc: 0.996689, Tokens per Sec:     3986, Lr: 0.000107\r\n",
      "2024-08-27 16:44:40,067 - INFO - joeynmt.training - Epoch 293, Step:    27845, Batch Loss:     0.049802, Batch Acc: 0.996148, Tokens per Sec:     3804, Lr: 0.000107\r\n",
      "2024-08-27 16:44:41,141 - INFO - joeynmt.training - Epoch 293, Step:    27850, Batch Loss:     0.049321, Batch Acc: 0.995524, Tokens per Sec:     3958, Lr: 0.000107\r\n",
      "2024-08-27 16:44:42,225 - INFO - joeynmt.training - Epoch 293, Step:    27855, Batch Loss:     0.044777, Batch Acc: 0.998336, Tokens per Sec:     3881, Lr: 0.000107\r\n",
      "2024-08-27 16:44:43,318 - INFO - joeynmt.training - Epoch 293, Step:    27860, Batch Loss:     0.050247, Batch Acc: 0.996392, Tokens per Sec:     3808, Lr: 0.000107\r\n",
      "2024-08-27 16:44:44,407 - INFO - joeynmt.training - Epoch 293, Step:    27865, Batch Loss:     0.048886, Batch Acc: 0.997742, Tokens per Sec:     4066, Lr: 0.000107\r\n",
      "2024-08-27 16:44:45,483 - INFO - joeynmt.training - Epoch 293, Step:    27870, Batch Loss:     0.058085, Batch Acc: 0.996585, Tokens per Sec:     3813, Lr: 0.000107\r\n",
      "2024-08-27 16:44:46,179 - INFO - joeynmt.training - Epoch 293, total training loss: 4.54, num. of seqs: 8065, num. of tokens: 80463, 20.6173[sec]\r\n",
      "2024-08-27 16:44:46,180 - INFO - joeynmt.training - EPOCH 294\r\n",
      "2024-08-27 16:44:46,617 - INFO - joeynmt.training - Epoch 294, Step:    27875, Batch Loss:     0.047365, Batch Acc: 0.998179, Tokens per Sec:     3801, Lr: 0.000107\r\n",
      "2024-08-27 16:44:47,708 - INFO - joeynmt.training - Epoch 294, Step:    27880, Batch Loss:     0.045347, Batch Acc: 0.996533, Tokens per Sec:     3706, Lr: 0.000107\r\n",
      "2024-08-27 16:44:48,781 - INFO - joeynmt.training - Epoch 294, Step:    27885, Batch Loss:     0.053027, Batch Acc: 0.995705, Tokens per Sec:     4123, Lr: 0.000107\r\n",
      "2024-08-27 16:44:49,837 - INFO - joeynmt.training - Epoch 294, Step:    27890, Batch Loss:     0.049812, Batch Acc: 0.995407, Tokens per Sec:     3922, Lr: 0.000107\r\n",
      "2024-08-27 16:44:50,920 - INFO - joeynmt.training - Epoch 294, Step:    27895, Batch Loss:     0.051602, Batch Acc: 0.995822, Tokens per Sec:     3979, Lr: 0.000107\r\n",
      "2024-08-27 16:44:52,026 - INFO - joeynmt.training - Epoch 294, Step:    27900, Batch Loss:     0.043262, Batch Acc: 0.997606, Tokens per Sec:     3779, Lr: 0.000107\r\n",
      "2024-08-27 16:44:53,144 - INFO - joeynmt.training - Epoch 294, Step:    27905, Batch Loss:     0.060408, Batch Acc: 0.995381, Tokens per Sec:     3874, Lr: 0.000107\r\n",
      "2024-08-27 16:44:54,210 - INFO - joeynmt.training - Epoch 294, Step:    27910, Batch Loss:     0.057424, Batch Acc: 0.995628, Tokens per Sec:     4079, Lr: 0.000107\r\n",
      "2024-08-27 16:44:55,442 - INFO - joeynmt.training - Epoch 294, Step:    27915, Batch Loss:     0.047063, Batch Acc: 0.997563, Tokens per Sec:     3335, Lr: 0.000107\r\n",
      "2024-08-27 16:44:56,530 - INFO - joeynmt.training - Epoch 294, Step:    27920, Batch Loss:     0.052068, Batch Acc: 0.996965, Tokens per Sec:     3940, Lr: 0.000107\r\n",
      "2024-08-27 16:44:57,656 - INFO - joeynmt.training - Epoch 294, Step:    27925, Batch Loss:     0.047524, Batch Acc: 0.997096, Tokens per Sec:     3671, Lr: 0.000107\r\n",
      "2024-08-27 16:44:58,741 - INFO - joeynmt.training - Epoch 294, Step:    27930, Batch Loss:     0.045579, Batch Acc: 0.997634, Tokens per Sec:     3897, Lr: 0.000107\r\n",
      "2024-08-27 16:44:59,818 - INFO - joeynmt.training - Epoch 294, Step:    27935, Batch Loss:     0.045037, Batch Acc: 0.997872, Tokens per Sec:     3931, Lr: 0.000107\r\n",
      "2024-08-27 16:45:00,885 - INFO - joeynmt.training - Epoch 294, Step:    27940, Batch Loss:     0.044670, Batch Acc: 0.996590, Tokens per Sec:     3850, Lr: 0.000107\r\n",
      "2024-08-27 16:45:01,957 - INFO - joeynmt.training - Epoch 294, Step:    27945, Batch Loss:     0.047999, Batch Acc: 0.997344, Tokens per Sec:     3864, Lr: 0.000107\r\n",
      "2024-08-27 16:45:03,002 - INFO - joeynmt.training - Epoch 294, Step:    27950, Batch Loss:     0.052871, Batch Acc: 0.997091, Tokens per Sec:     3952, Lr: 0.000107\r\n",
      "2024-08-27 16:45:04,085 - INFO - joeynmt.training - Epoch 294, Step:    27955, Batch Loss:     0.047902, Batch Acc: 0.997819, Tokens per Sec:     3810, Lr: 0.000107\r\n",
      "2024-08-27 16:45:05,157 - INFO - joeynmt.training - Epoch 294, Step:    27960, Batch Loss:     0.052673, Batch Acc: 0.997244, Tokens per Sec:     3726, Lr: 0.000107\r\n",
      "2024-08-27 16:45:06,243 - INFO - joeynmt.training - Epoch 294, Step:    27965, Batch Loss:     0.043339, Batch Acc: 0.997985, Tokens per Sec:     4118, Lr: 0.000107\r\n",
      "2024-08-27 16:45:07,084 - INFO - joeynmt.training - Epoch 294, total training loss: 4.68, num. of seqs: 8065, num. of tokens: 80463, 20.8875[sec]\r\n",
      "2024-08-27 16:45:07,084 - INFO - joeynmt.training - EPOCH 295\r\n",
      "2024-08-27 16:45:07,528 - INFO - joeynmt.training - Epoch 295, Step:    27970, Batch Loss:     0.044329, Batch Acc: 0.997583, Tokens per Sec:     3765, Lr: 0.000107\r\n",
      "2024-08-27 16:45:08,622 - INFO - joeynmt.training - Epoch 295, Step:    27975, Batch Loss:     0.044458, Batch Acc: 0.998792, Tokens per Sec:     3785, Lr: 0.000107\r\n",
      "2024-08-27 16:45:09,714 - INFO - joeynmt.training - Epoch 295, Step:    27980, Batch Loss:     0.043218, Batch Acc: 0.997350, Tokens per Sec:     4150, Lr: 0.000107\r\n",
      "2024-08-27 16:45:10,789 - INFO - joeynmt.training - Epoch 295, Step:    27985, Batch Loss:     0.042666, Batch Acc: 0.998591, Tokens per Sec:     3961, Lr: 0.000107\r\n",
      "2024-08-27 16:45:11,866 - INFO - joeynmt.training - Epoch 295, Step:    27990, Batch Loss:     0.049444, Batch Acc: 0.998278, Tokens per Sec:     3779, Lr: 0.000107\r\n",
      "2024-08-27 16:45:12,950 - INFO - joeynmt.training - Epoch 295, Step:    27995, Batch Loss:     0.043897, Batch Acc: 0.996516, Tokens per Sec:     3974, Lr: 0.000107\r\n",
      "2024-08-27 16:45:14,031 - INFO - joeynmt.training - Epoch 295, Step:    28000, Batch Loss:     0.053731, Batch Acc: 0.996694, Tokens per Sec:     3920, Lr: 0.000107\r\n",
      "2024-08-27 16:45:14,032 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=28042\r\n",
      "2024-08-27 16:45:14,033 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:22<00:00, 64.84it/s]\r\n",
      "2024-08-27 16:45:36,551 - INFO - joeynmt.prediction - Generation took 22.5163[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 39664.81 examples/s]\r\n",
      "2024-08-27 16:45:36,947 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:45:36,948 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.40, loss:   6.61, ppl: 741.69, acc:   0.29, 0.1673[sec]\r\n",
      "2024-08-27 16:45:36,949 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:45:37,096 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:45:37,096 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:45:37,096 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 16:45:37,391 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:45:37,536 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:45:37,536 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:45:37,536 - INFO - joeynmt.training - \tHypothesis: trois éléphants\r\n",
      "2024-08-27 16:45:37,827 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:45:37,972 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:45:37,972 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:45:37,972 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 16:45:38,262 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:45:38,408 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:45:38,409 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:45:38,409 - INFO - joeynmt.training - \tHypothesis: une seule porte ne suffit pas\r\n",
      "2024-08-27 16:45:39,820 - INFO - joeynmt.training - Epoch 295, Step:    28005, Batch Loss:     0.047980, Batch Acc: 0.996867, Tokens per Sec:     3721, Lr: 0.000107\r\n",
      "2024-08-27 16:45:40,899 - INFO - joeynmt.training - Epoch 295, Step:    28010, Batch Loss:     0.047737, Batch Acc: 0.997653, Tokens per Sec:     3951, Lr: 0.000107\r\n",
      "2024-08-27 16:45:41,975 - INFO - joeynmt.training - Epoch 295, Step:    28015, Batch Loss:     0.047695, Batch Acc: 0.996162, Tokens per Sec:     3878, Lr: 0.000107\r\n",
      "2024-08-27 16:45:43,037 - INFO - joeynmt.training - Epoch 295, Step:    28020, Batch Loss:     0.050367, Batch Acc: 0.997941, Tokens per Sec:     3661, Lr: 0.000107\r\n",
      "2024-08-27 16:45:44,136 - INFO - joeynmt.training - Epoch 295, Step:    28025, Batch Loss:     0.050229, Batch Acc: 0.996184, Tokens per Sec:     3819, Lr: 0.000107\r\n",
      "2024-08-27 16:45:45,194 - INFO - joeynmt.training - Epoch 295, Step:    28030, Batch Loss:     0.054190, Batch Acc: 0.996349, Tokens per Sec:     3885, Lr: 0.000107\r\n",
      "2024-08-27 16:45:46,279 - INFO - joeynmt.training - Epoch 295, Step:    28035, Batch Loss:     0.058503, Batch Acc: 0.996815, Tokens per Sec:     4056, Lr: 0.000107\r\n",
      "2024-08-27 16:45:47,387 - INFO - joeynmt.training - Epoch 295, Step:    28040, Batch Loss:     0.047029, Batch Acc: 0.997119, Tokens per Sec:     3761, Lr: 0.000107\r\n",
      "2024-08-27 16:45:48,469 - INFO - joeynmt.training - Epoch 295, Step:    28045, Batch Loss:     0.048191, Batch Acc: 0.997308, Tokens per Sec:     3779, Lr: 0.000107\r\n",
      "2024-08-27 16:45:49,552 - INFO - joeynmt.training - Epoch 295, Step:    28050, Batch Loss:     0.050631, Batch Acc: 0.995270, Tokens per Sec:     3712, Lr: 0.000107\r\n",
      "2024-08-27 16:45:50,630 - INFO - joeynmt.training - Epoch 295, Step:    28055, Batch Loss:     0.051107, Batch Acc: 0.996904, Tokens per Sec:     3895, Lr: 0.000107\r\n",
      "2024-08-27 16:45:51,700 - INFO - joeynmt.training - Epoch 295, Step:    28060, Batch Loss:     0.058870, Batch Acc: 0.996489, Tokens per Sec:     3996, Lr: 0.000107\r\n",
      "2024-08-27 16:45:52,639 - INFO - joeynmt.training - Epoch 295, total training loss: 4.65, num. of seqs: 8065, num. of tokens: 80463, 20.8644[sec]\r\n",
      "2024-08-27 16:45:52,639 - INFO - joeynmt.training - EPOCH 296\r\n",
      "2024-08-27 16:45:52,871 - INFO - joeynmt.training - Epoch 296, Step:    28065, Batch Loss:     0.048378, Batch Acc: 0.995560, Tokens per Sec:     3962, Lr: 0.000107\r\n",
      "2024-08-27 16:45:53,974 - INFO - joeynmt.training - Epoch 296, Step:    28070, Batch Loss:     0.046338, Batch Acc: 0.996885, Tokens per Sec:     3789, Lr: 0.000107\r\n",
      "2024-08-27 16:45:55,057 - INFO - joeynmt.training - Epoch 296, Step:    28075, Batch Loss:     0.056015, Batch Acc: 0.996154, Tokens per Sec:     3843, Lr: 0.000107\r\n",
      "2024-08-27 16:45:56,159 - INFO - joeynmt.training - Epoch 296, Step:    28080, Batch Loss:     0.040001, Batch Acc: 0.998188, Tokens per Sec:     4008, Lr: 0.000107\r\n",
      "2024-08-27 16:45:57,412 - INFO - joeynmt.training - Epoch 296, Step:    28085, Batch Loss:     0.045420, Batch Acc: 0.998569, Tokens per Sec:     3349, Lr: 0.000107\r\n",
      "2024-08-27 16:45:58,511 - INFO - joeynmt.training - Epoch 296, Step:    28090, Batch Loss:     0.046909, Batch Acc: 0.998316, Tokens per Sec:     3783, Lr: 0.000107\r\n",
      "2024-08-27 16:45:59,602 - INFO - joeynmt.training - Epoch 296, Step:    28095, Batch Loss:     0.058210, Batch Acc: 0.995929, Tokens per Sec:     4056, Lr: 0.000107\r\n",
      "2024-08-27 16:46:00,676 - INFO - joeynmt.training - Epoch 296, Step:    28100, Batch Loss:     0.047734, Batch Acc: 0.997639, Tokens per Sec:     3949, Lr: 0.000107\r\n",
      "2024-08-27 16:46:01,743 - INFO - joeynmt.training - Epoch 296, Step:    28105, Batch Loss:     0.050643, Batch Acc: 0.997820, Tokens per Sec:     3872, Lr: 0.000107\r\n",
      "2024-08-27 16:46:02,804 - INFO - joeynmt.training - Epoch 296, Step:    28110, Batch Loss:     0.043237, Batch Acc: 0.997906, Tokens per Sec:     4054, Lr: 0.000107\r\n",
      "2024-08-27 16:46:03,888 - INFO - joeynmt.training - Epoch 296, Step:    28115, Batch Loss:     0.057466, Batch Acc: 0.997776, Tokens per Sec:     3733, Lr: 0.000107\r\n",
      "2024-08-27 16:46:04,958 - INFO - joeynmt.training - Epoch 296, Step:    28120, Batch Loss:     0.043506, Batch Acc: 0.997551, Tokens per Sec:     3820, Lr: 0.000107\r\n",
      "2024-08-27 16:46:06,034 - INFO - joeynmt.training - Epoch 296, Step:    28125, Batch Loss:     0.047085, Batch Acc: 0.997469, Tokens per Sec:     4042, Lr: 0.000107\r\n",
      "2024-08-27 16:46:07,128 - INFO - joeynmt.training - Epoch 296, Step:    28130, Batch Loss:     0.046594, Batch Acc: 0.997832, Tokens per Sec:     3796, Lr: 0.000107\r\n",
      "2024-08-27 16:46:08,192 - INFO - joeynmt.training - Epoch 296, Step:    28135, Batch Loss:     0.045082, Batch Acc: 0.998829, Tokens per Sec:     4016, Lr: 0.000107\r\n",
      "2024-08-27 16:46:09,260 - INFO - joeynmt.training - Epoch 296, Step:    28140, Batch Loss:     0.048283, Batch Acc: 0.997598, Tokens per Sec:     3901, Lr: 0.000107\r\n",
      "2024-08-27 16:46:10,323 - INFO - joeynmt.training - Epoch 296, Step:    28145, Batch Loss:     0.063017, Batch Acc: 0.995250, Tokens per Sec:     3766, Lr: 0.000107\r\n",
      "2024-08-27 16:46:11,394 - INFO - joeynmt.training - Epoch 296, Step:    28150, Batch Loss:     0.045323, Batch Acc: 0.997603, Tokens per Sec:     3897, Lr: 0.000107\r\n",
      "2024-08-27 16:46:12,468 - INFO - joeynmt.training - Epoch 296, Step:    28155, Batch Loss:     0.052385, Batch Acc: 0.997693, Tokens per Sec:     3636, Lr: 0.000107\r\n",
      "2024-08-27 16:46:13,536 - INFO - joeynmt.training - Epoch 296, Step:    28160, Batch Loss:     0.052395, Batch Acc: 0.997881, Tokens per Sec:     3978, Lr: 0.000107\r\n",
      "2024-08-27 16:46:13,537 - INFO - joeynmt.training - Epoch 296, total training loss: 4.54, num. of seqs: 8065, num. of tokens: 80463, 20.8784[sec]\r\n",
      "2024-08-27 16:46:13,537 - INFO - joeynmt.training - EPOCH 297\r\n",
      "2024-08-27 16:46:14,616 - INFO - joeynmt.training - Epoch 297, Step:    28165, Batch Loss:     0.044854, Batch Acc: 0.996222, Tokens per Sec:     3937, Lr: 0.000107\r\n",
      "2024-08-27 16:46:15,669 - INFO - joeynmt.training - Epoch 297, Step:    28170, Batch Loss:     0.049251, Batch Acc: 0.998052, Tokens per Sec:     3900, Lr: 0.000107\r\n",
      "2024-08-27 16:46:16,780 - INFO - joeynmt.training - Epoch 297, Step:    28175, Batch Loss:     0.046860, Batch Acc: 0.996132, Tokens per Sec:     3958, Lr: 0.000107\r\n",
      "2024-08-27 16:46:17,852 - INFO - joeynmt.training - Epoch 297, Step:    28180, Batch Loss:     0.046742, Batch Acc: 0.997597, Tokens per Sec:     3887, Lr: 0.000107\r\n",
      "2024-08-27 16:46:18,920 - INFO - joeynmt.training - Epoch 297, Step:    28185, Batch Loss:     0.052581, Batch Acc: 0.996849, Tokens per Sec:     3867, Lr: 0.000107\r\n",
      "2024-08-27 16:46:19,998 - INFO - joeynmt.training - Epoch 297, Step:    28190, Batch Loss:     0.049195, Batch Acc: 0.998836, Tokens per Sec:     3989, Lr: 0.000107\r\n",
      "2024-08-27 16:46:21,077 - INFO - joeynmt.training - Epoch 297, Step:    28195, Batch Loss:     0.044628, Batch Acc: 0.997088, Tokens per Sec:     3822, Lr: 0.000107\r\n",
      "2024-08-27 16:46:22,160 - INFO - joeynmt.training - Epoch 297, Step:    28200, Batch Loss:     0.048803, Batch Acc: 0.997812, Tokens per Sec:     3799, Lr: 0.000107\r\n",
      "2024-08-27 16:46:23,248 - INFO - joeynmt.training - Epoch 297, Step:    28205, Batch Loss:     0.042629, Batch Acc: 0.998346, Tokens per Sec:     3893, Lr: 0.000107\r\n",
      "2024-08-27 16:46:24,329 - INFO - joeynmt.training - Epoch 297, Step:    28210, Batch Loss:     0.040596, Batch Acc: 0.998481, Tokens per Sec:     3657, Lr: 0.000107\r\n",
      "2024-08-27 16:46:25,409 - INFO - joeynmt.training - Epoch 297, Step:    28215, Batch Loss:     0.047136, Batch Acc: 0.997390, Tokens per Sec:     3903, Lr: 0.000106\r\n",
      "2024-08-27 16:46:26,493 - INFO - joeynmt.training - Epoch 297, Step:    28220, Batch Loss:     0.040748, Batch Acc: 0.997244, Tokens per Sec:     4020, Lr: 0.000106\r\n",
      "2024-08-27 16:46:27,583 - INFO - joeynmt.training - Epoch 297, Step:    28225, Batch Loss:     0.050533, Batch Acc: 0.995885, Tokens per Sec:     4016, Lr: 0.000106\r\n",
      "2024-08-27 16:46:28,850 - INFO - joeynmt.training - Epoch 297, Step:    28230, Batch Loss:     0.044693, Batch Acc: 0.997773, Tokens per Sec:     3192, Lr: 0.000106\r\n",
      "2024-08-27 16:46:29,968 - INFO - joeynmt.training - Epoch 297, Step:    28235, Batch Loss:     0.054900, Batch Acc: 0.998363, Tokens per Sec:     3826, Lr: 0.000106\r\n",
      "2024-08-27 16:46:31,044 - INFO - joeynmt.training - Epoch 297, Step:    28240, Batch Loss:     0.038294, Batch Acc: 0.997062, Tokens per Sec:     3799, Lr: 0.000106\r\n",
      "2024-08-27 16:46:32,125 - INFO - joeynmt.training - Epoch 297, Step:    28245, Batch Loss:     0.048444, Batch Acc: 0.997420, Tokens per Sec:     3947, Lr: 0.000106\r\n",
      "2024-08-27 16:46:33,181 - INFO - joeynmt.training - Epoch 297, Step:    28250, Batch Loss:     0.052470, Batch Acc: 0.996682, Tokens per Sec:     4000, Lr: 0.000106\r\n",
      "2024-08-27 16:46:34,248 - INFO - joeynmt.training - Epoch 297, Step:    28255, Batch Loss:     0.046705, Batch Acc: 0.997401, Tokens per Sec:     3968, Lr: 0.000106\r\n",
      "2024-08-27 16:46:34,460 - INFO - joeynmt.training - Epoch 297, total training loss: 4.54, num. of seqs: 8065, num. of tokens: 80463, 20.9068[sec]\r\n",
      "2024-08-27 16:46:34,460 - INFO - joeynmt.training - EPOCH 298\r\n",
      "2024-08-27 16:46:35,340 - INFO - joeynmt.training - Epoch 298, Step:    28260, Batch Loss:     0.053982, Batch Acc: 0.997142, Tokens per Sec:     3991, Lr: 0.000106\r\n",
      "2024-08-27 16:46:36,418 - INFO - joeynmt.training - Epoch 298, Step:    28265, Batch Loss:     0.043795, Batch Acc: 0.997306, Tokens per Sec:     4139, Lr: 0.000106\r\n",
      "2024-08-27 16:46:37,528 - INFO - joeynmt.training - Epoch 298, Step:    28270, Batch Loss:     0.049707, Batch Acc: 0.998372, Tokens per Sec:     3875, Lr: 0.000106\r\n",
      "2024-08-27 16:46:38,607 - INFO - joeynmt.training - Epoch 298, Step:    28275, Batch Loss:     0.049093, Batch Acc: 0.998143, Tokens per Sec:     3997, Lr: 0.000106\r\n",
      "2024-08-27 16:46:39,663 - INFO - joeynmt.training - Epoch 298, Step:    28280, Batch Loss:     0.052313, Batch Acc: 0.997876, Tokens per Sec:     3568, Lr: 0.000106\r\n",
      "2024-08-27 16:46:40,738 - INFO - joeynmt.training - Epoch 298, Step:    28285, Batch Loss:     0.043000, Batch Acc: 0.995941, Tokens per Sec:     3901, Lr: 0.000106\r\n",
      "2024-08-27 16:46:41,819 - INFO - joeynmt.training - Epoch 298, Step:    28290, Batch Loss:     0.046850, Batch Acc: 0.997853, Tokens per Sec:     3878, Lr: 0.000106\r\n",
      "2024-08-27 16:46:42,896 - INFO - joeynmt.training - Epoch 298, Step:    28295, Batch Loss:     0.041340, Batch Acc: 0.998642, Tokens per Sec:     4105, Lr: 0.000106\r\n",
      "2024-08-27 16:46:43,956 - INFO - joeynmt.training - Epoch 298, Step:    28300, Batch Loss:     0.045611, Batch Acc: 0.996527, Tokens per Sec:     3806, Lr: 0.000106\r\n",
      "2024-08-27 16:46:45,020 - INFO - joeynmt.training - Epoch 298, Step:    28305, Batch Loss:     0.044165, Batch Acc: 0.997883, Tokens per Sec:     3995, Lr: 0.000106\r\n",
      "2024-08-27 16:46:46,082 - INFO - joeynmt.training - Epoch 298, Step:    28310, Batch Loss:     0.051467, Batch Acc: 0.997443, Tokens per Sec:     4054, Lr: 0.000106\r\n",
      "2024-08-27 16:46:47,180 - INFO - joeynmt.training - Epoch 298, Step:    28315, Batch Loss:     0.047112, Batch Acc: 0.996451, Tokens per Sec:     3853, Lr: 0.000106\r\n",
      "2024-08-27 16:46:48,238 - INFO - joeynmt.training - Epoch 298, Step:    28320, Batch Loss:     0.054137, Batch Acc: 0.996851, Tokens per Sec:     3902, Lr: 0.000106\r\n",
      "2024-08-27 16:46:49,310 - INFO - joeynmt.training - Epoch 298, Step:    28325, Batch Loss:     0.043004, Batch Acc: 0.998157, Tokens per Sec:     4052, Lr: 0.000106\r\n",
      "2024-08-27 16:46:50,380 - INFO - joeynmt.training - Epoch 298, Step:    28330, Batch Loss:     0.044587, Batch Acc: 0.997188, Tokens per Sec:     3991, Lr: 0.000106\r\n",
      "2024-08-27 16:46:51,440 - INFO - joeynmt.training - Epoch 298, Step:    28335, Batch Loss:     0.048636, Batch Acc: 0.997233, Tokens per Sec:     3752, Lr: 0.000106\r\n",
      "2024-08-27 16:46:52,532 - INFO - joeynmt.training - Epoch 298, Step:    28340, Batch Loss:     0.051321, Batch Acc: 0.995773, Tokens per Sec:     3685, Lr: 0.000106\r\n",
      "2024-08-27 16:46:53,620 - INFO - joeynmt.training - Epoch 298, Step:    28345, Batch Loss:     0.048166, Batch Acc: 0.998182, Tokens per Sec:     4050, Lr: 0.000106\r\n",
      "2024-08-27 16:46:54,697 - INFO - joeynmt.training - Epoch 298, Step:    28350, Batch Loss:     0.044938, Batch Acc: 0.997431, Tokens per Sec:     3979, Lr: 0.000106\r\n",
      "2024-08-27 16:46:54,965 - INFO - joeynmt.training - Epoch 298, total training loss: 4.48, num. of seqs: 8065, num. of tokens: 80463, 20.4886[sec]\r\n",
      "2024-08-27 16:46:54,965 - INFO - joeynmt.training - EPOCH 299\r\n",
      "2024-08-27 16:46:55,832 - INFO - joeynmt.training - Epoch 299, Step:    28355, Batch Loss:     0.047355, Batch Acc: 0.998550, Tokens per Sec:     3998, Lr: 0.000106\r\n",
      "2024-08-27 16:46:56,988 - INFO - joeynmt.training - Epoch 299, Step:    28360, Batch Loss:     0.052205, Batch Acc: 0.996737, Tokens per Sec:     3711, Lr: 0.000106\r\n",
      "2024-08-27 16:46:58,064 - INFO - joeynmt.training - Epoch 299, Step:    28365, Batch Loss:     0.048177, Batch Acc: 0.998331, Tokens per Sec:     3901, Lr: 0.000106\r\n",
      "2024-08-27 16:46:59,119 - INFO - joeynmt.training - Epoch 299, Step:    28370, Batch Loss:     0.046083, Batch Acc: 0.997567, Tokens per Sec:     3900, Lr: 0.000106\r\n",
      "2024-08-27 16:47:00,276 - INFO - joeynmt.training - Epoch 299, Step:    28375, Batch Loss:     0.042482, Batch Acc: 0.997400, Tokens per Sec:     3659, Lr: 0.000106\r\n",
      "2024-08-27 16:47:01,421 - INFO - joeynmt.training - Epoch 299, Step:    28380, Batch Loss:     0.040110, Batch Acc: 0.998027, Tokens per Sec:     3545, Lr: 0.000106\r\n",
      "2024-08-27 16:47:02,509 - INFO - joeynmt.training - Epoch 299, Step:    28385, Batch Loss:     0.046131, Batch Acc: 0.995500, Tokens per Sec:     3880, Lr: 0.000106\r\n",
      "2024-08-27 16:47:03,610 - INFO - joeynmt.training - Epoch 299, Step:    28390, Batch Loss:     0.039231, Batch Acc: 0.996503, Tokens per Sec:     3898, Lr: 0.000106\r\n",
      "2024-08-27 16:47:04,707 - INFO - joeynmt.training - Epoch 299, Step:    28395, Batch Loss:     0.050188, Batch Acc: 0.998312, Tokens per Sec:     3782, Lr: 0.000106\r\n",
      "2024-08-27 16:47:05,795 - INFO - joeynmt.training - Epoch 299, Step:    28400, Batch Loss:     0.047603, Batch Acc: 0.997900, Tokens per Sec:     3942, Lr: 0.000106\r\n",
      "2024-08-27 16:47:06,897 - INFO - joeynmt.training - Epoch 299, Step:    28405, Batch Loss:     0.039663, Batch Acc: 0.996894, Tokens per Sec:     3801, Lr: 0.000106\r\n",
      "2024-08-27 16:47:07,981 - INFO - joeynmt.training - Epoch 299, Step:    28410, Batch Loss:     0.053278, Batch Acc: 0.996820, Tokens per Sec:     4065, Lr: 0.000106\r\n",
      "2024-08-27 16:47:09,050 - INFO - joeynmt.training - Epoch 299, Step:    28415, Batch Loss:     0.057518, Batch Acc: 0.997411, Tokens per Sec:     3977, Lr: 0.000106\r\n",
      "2024-08-27 16:47:10,118 - INFO - joeynmt.training - Epoch 299, Step:    28420, Batch Loss:     0.056563, Batch Acc: 0.995934, Tokens per Sec:     3917, Lr: 0.000106\r\n",
      "2024-08-27 16:47:11,208 - INFO - joeynmt.training - Epoch 299, Step:    28425, Batch Loss:     0.057695, Batch Acc: 0.997350, Tokens per Sec:     4158, Lr: 0.000106\r\n",
      "2024-08-27 16:47:12,295 - INFO - joeynmt.training - Epoch 299, Step:    28430, Batch Loss:     0.042977, Batch Acc: 0.997928, Tokens per Sec:     3996, Lr: 0.000106\r\n",
      "2024-08-27 16:47:13,486 - INFO - joeynmt.training - Epoch 299, Step:    28435, Batch Loss:     0.050502, Batch Acc: 0.996393, Tokens per Sec:     3494, Lr: 0.000106\r\n",
      "2024-08-27 16:47:14,558 - INFO - joeynmt.training - Epoch 299, Step:    28440, Batch Loss:     0.049360, Batch Acc: 0.998555, Tokens per Sec:     3878, Lr: 0.000106\r\n",
      "2024-08-27 16:47:15,635 - INFO - joeynmt.training - Epoch 299, Step:    28445, Batch Loss:     0.047859, Batch Acc: 0.996454, Tokens per Sec:     3929, Lr: 0.000106\r\n",
      "2024-08-27 16:47:15,855 - INFO - joeynmt.training - Epoch 299, total training loss: 4.54, num. of seqs: 8065, num. of tokens: 80463, 20.8728[sec]\r\n",
      "2024-08-27 16:47:15,855 - INFO - joeynmt.training - EPOCH 300\r\n",
      "2024-08-27 16:47:16,751 - INFO - joeynmt.training - Epoch 300, Step:    28450, Batch Loss:     0.042857, Batch Acc: 0.998124, Tokens per Sec:     3584, Lr: 0.000106\r\n",
      "2024-08-27 16:47:17,804 - INFO - joeynmt.training - Epoch 300, Step:    28455, Batch Loss:     0.047531, Batch Acc: 0.996754, Tokens per Sec:     3804, Lr: 0.000106\r\n",
      "2024-08-27 16:47:18,877 - INFO - joeynmt.training - Epoch 300, Step:    28460, Batch Loss:     0.042189, Batch Acc: 0.998604, Tokens per Sec:     4008, Lr: 0.000106\r\n",
      "2024-08-27 16:47:19,940 - INFO - joeynmt.training - Epoch 300, Step:    28465, Batch Loss:     0.047386, Batch Acc: 0.998335, Tokens per Sec:     3959, Lr: 0.000106\r\n",
      "2024-08-27 16:47:20,996 - INFO - joeynmt.training - Epoch 300, Step:    28470, Batch Loss:     0.046880, Batch Acc: 0.997866, Tokens per Sec:     3994, Lr: 0.000106\r\n",
      "2024-08-27 16:47:22,058 - INFO - joeynmt.training - Epoch 300, Step:    28475, Batch Loss:     0.038035, Batch Acc: 0.997360, Tokens per Sec:     3926, Lr: 0.000106\r\n",
      "2024-08-27 16:47:23,128 - INFO - joeynmt.training - Epoch 300, Step:    28480, Batch Loss:     0.041739, Batch Acc: 0.996965, Tokens per Sec:     4008, Lr: 0.000106\r\n",
      "2024-08-27 16:47:24,206 - INFO - joeynmt.training - Epoch 300, Step:    28485, Batch Loss:     0.048861, Batch Acc: 0.997184, Tokens per Sec:     3955, Lr: 0.000106\r\n",
      "2024-08-27 16:47:25,272 - INFO - joeynmt.training - Epoch 300, Step:    28490, Batch Loss:     0.050985, Batch Acc: 0.997896, Tokens per Sec:     4016, Lr: 0.000106\r\n",
      "2024-08-27 16:47:26,332 - INFO - joeynmt.training - Epoch 300, Step:    28495, Batch Loss:     0.042501, Batch Acc: 0.997159, Tokens per Sec:     3986, Lr: 0.000106\r\n",
      "2024-08-27 16:47:27,432 - INFO - joeynmt.training - Epoch 300, Step:    28500, Batch Loss:     0.042415, Batch Acc: 0.996358, Tokens per Sec:     3999, Lr: 0.000106\r\n",
      "2024-08-27 16:47:27,432 - INFO - joeynmt.datasets - Sample random subset from validation data: n=1460, seed=28542\r\n",
      "2024-08-27 16:47:27,433 - INFO - joeynmt.prediction - Predicting 1460 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\r\n",
      "Predicting...: 100%|████████████████████████| 1460/1460 [00:23<00:00, 62.34it/s]\r\n",
      "2024-08-27 16:47:50,856 - INFO - joeynmt.prediction - Generation took 23.4211[sec].\r\n",
      "Filter: 100%|█████████████████████| 1471/1471 [00:00<00:00, 44655.46 examples/s]\r\n",
      "2024-08-27 16:47:51,238 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\r\n",
      "2024-08-27 16:47:51,239 - INFO - joeynmt.prediction - Evaluation result (greedy): bleu:   5.25, loss:   6.62, ppl: 748.08, acc:   0.29, 0.1692[sec]\r\n",
      "2024-08-27 16:47:51,240 - INFO - joeynmt.training - Example #0\r\n",
      "2024-08-27 16:47:51,387 - INFO - joeynmt.training - \tSource:     i tɔgɔ bi cogodɔ\r\n",
      "2024-08-27 16:47:51,387 - INFO - joeynmt.training - \tReference:  tu portes un nom de fantaisie\r\n",
      "2024-08-27 16:47:51,387 - INFO - joeynmt.training - \tHypothesis: comment t’appelles tu\r\n",
      "2024-08-27 16:47:51,679 - INFO - joeynmt.training - Example #1\r\n",
      "2024-08-27 16:47:51,825 - INFO - joeynmt.training - \tSource:     puɛn saba fɔlɔ\r\n",
      "2024-08-27 16:47:51,826 - INFO - joeynmt.training - \tReference:  trois points d’avance\r\n",
      "2024-08-27 16:47:51,826 - INFO - joeynmt.training - \tHypothesis: trois point d'avance\r\n",
      "2024-08-27 16:47:52,115 - INFO - joeynmt.training - Example #2\r\n",
      "2024-08-27 16:47:52,264 - INFO - joeynmt.training - \tSource:     tile bena\r\n",
      "2024-08-27 16:47:52,265 - INFO - joeynmt.training - \tReference:  le soleil s’est couché\r\n",
      "2024-08-27 16:47:52,265 - INFO - joeynmt.training - \tHypothesis: cependant à quarante ans...\r\n",
      "2024-08-27 16:47:52,569 - INFO - joeynmt.training - Example #3\r\n",
      "2024-08-27 16:47:52,715 - INFO - joeynmt.training - \tSource:     cogoya kelen\r\n",
      "2024-08-27 16:47:52,715 - INFO - joeynmt.training - \tReference:  mêmes mouvements\r\n",
      "2024-08-27 16:47:52,715 - INFO - joeynmt.training - \tHypothesis: un papier de musique\r\n",
      "2024-08-27 16:47:54,096 - INFO - joeynmt.training - Epoch 300, Step:    28505, Batch Loss:     0.050533, Batch Acc: 0.997787, Tokens per Sec:     4158, Lr: 0.000106\r\n",
      "2024-08-27 16:47:55,165 - INFO - joeynmt.training - Epoch 300, Step:    28510, Batch Loss:     0.043115, Batch Acc: 0.996456, Tokens per Sec:     3699, Lr: 0.000106\r\n",
      "2024-08-27 16:47:56,265 - INFO - joeynmt.training - Epoch 300, Step:    28515, Batch Loss:     0.051786, Batch Acc: 0.997811, Tokens per Sec:     3738, Lr: 0.000106\r\n",
      "2024-08-27 16:47:57,371 - INFO - joeynmt.training - Epoch 300, Step:    28520, Batch Loss:     0.052603, Batch Acc: 0.997104, Tokens per Sec:     3751, Lr: 0.000106\r\n",
      "2024-08-27 16:47:58,441 - INFO - joeynmt.training - Epoch 300, Step:    28525, Batch Loss:     0.050602, Batch Acc: 0.997890, Tokens per Sec:     3989, Lr: 0.000106\r\n",
      "2024-08-27 16:47:59,500 - INFO - joeynmt.training - Epoch 300, Step:    28530, Batch Loss:     0.052124, Batch Acc: 0.997390, Tokens per Sec:     3982, Lr: 0.000106\r\n",
      "2024-08-27 16:48:00,582 - INFO - joeynmt.training - Epoch 300, Step:    28535, Batch Loss:     0.051701, Batch Acc: 0.996314, Tokens per Sec:     3766, Lr: 0.000106\r\n",
      "2024-08-27 16:48:01,663 - INFO - joeynmt.training - Epoch 300, Step:    28540, Batch Loss:     0.053196, Batch Acc: 0.995938, Tokens per Sec:     3872, Lr: 0.000106\r\n",
      "2024-08-27 16:48:02,168 - INFO - joeynmt.training - Epoch 300, total training loss: 4.55, num. of seqs: 8065, num. of tokens: 80463, 20.7192[sec]\r\n",
      "2024-08-27 16:48:02,168 - INFO - joeynmt.training - Training ended after 300 epochs.\r\n",
      "2024-08-27 16:48:02,168 - INFO - joeynmt.training - Best validation result (greedy) at step    27000:   5.96 bleu.\r\n",
      "2024-08-27 16:48:02,487 - INFO - joeynmt.training - Checkpoint saved in /kaggle/working/saved_model/dyu_fr/28542.ckpt.\r\n",
      "2024-08-27 16:48:02,488 - INFO - joeynmt.training - Skipping test after training.\r\n"
     ]
    }
   ],
   "source": [
    "!python -m joeynmt train {data_dir}/config.yaml --skip-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01337f9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T16:48:06.239443Z",
     "iopub.status.busy": "2024-08-27T16:48:06.239027Z",
     "iopub.status.idle": "2024-08-27T16:48:06.246779Z",
     "shell.execute_reply": "2024-08-27T16:48:06.246006Z"
    },
    "papermill": {
     "duration": 1.072699,
     "end_time": "2024-08-27T16:48:06.248714",
     "exception": false,
     "start_time": "2024-08-27T16:48:05.176015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add the best model info on config file\n",
    "with (Path(model_dir) / \"config.yaml\").open('r') as f:\n",
    "    config = f.read()\n",
    "resume_config = config\\\n",
    "  .replace(f'#load_model: \"{model_dir}/best.ckpt\"',\n",
    "           f'load_model: \"{model_dir}/best.ckpt\"')\n",
    "\n",
    "resume_config = resume_config\\\n",
    "  .replace(f'model_file: \"{data_dir}/sp.model\"',\n",
    "           f'model_file: \"{model_dir}/sp.model\"')\n",
    "\n",
    "resume_config = resume_config\\\n",
    "  .replace(f'voc_file: \"{data_dir}/vocab.txt\"',\n",
    "           f'voc_file: \"{model_dir}/vocab.txt\"')\n",
    "\n",
    "with (Path(model_dir) / \"config.yaml\").open('w') as f:\n",
    "    f.write(resume_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cecb042",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T16:48:08.240697Z",
     "iopub.status.busy": "2024-08-27T16:48:08.239936Z",
     "iopub.status.idle": "2024-08-27T16:48:09.268259Z",
     "shell.execute_reply": "2024-08-27T16:48:09.266949Z"
    },
    "papermill": {
     "duration": 2.018414,
     "end_time": "2024-08-27T16:48:09.270565",
     "exception": false,
     "start_time": "2024-08-27T16:48:07.252151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp {data_dir}/vocab.txt  {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ffab2",
   "metadata": {
    "papermill": {
     "duration": 0.991745,
     "end_time": "2024-08-27T16:48:11.313890",
     "exception": false,
     "start_time": "2024-08-27T16:48:10.322145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7873.270604,
   "end_time": "2024-08-27T16:48:12.911472",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-27T14:36:59.640868",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "02c72d4f320d4e8790c08ac3d7453f70": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "06f25f8b0ea840ecbbeef4f346e320e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "09324d20af064771ae304bfd51481342": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bb87a9689a4c44f8b8b221fbca1e245c",
       "placeholder": "​",
       "style": "IPY_MODEL_70a9f584e7b145adb910f4a2a84a4f39",
       "value": "Computing checksums: 100%"
      }
     },
     "0a49d89ff5914da48061f941f4490110": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0c37e1577b5c42d0a9fdc881a462e524": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0e9224c1ba104de9a12e34ad340b9c43": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "15230538cdf24e26a9cd73e99300210a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "17ba65083bc548f9897d57f1177f76b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dac7021586d74fac826656a69da019c7",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_15230538cdf24e26a9cd73e99300210a",
       "value": 3.0
      }
     },
     "1c9ea3e327db45518ebfd7b92696d0a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a66e16e4874a426ab3d6f25655d2c4af",
       "placeholder": "​",
       "style": "IPY_MODEL_c8001931d16b4470a537a7cd83eff255",
       "value": " 1471/1471 [00:00&lt;00:00, 12344.58 examples/s]"
      }
     },
     "1d1e45dccf094b7ebcd71d039bb7d7d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_63d322666f6d497b8e5ff39aed13cf58",
        "IPY_MODEL_b354421a1cb9481c94ea1a3bcf7d2b61",
        "IPY_MODEL_3f21d83549bb4088b74790b69bc35507"
       ],
       "layout": "IPY_MODEL_d052c963098440e7b3c7b0778aa00165"
      }
     },
     "1e675c04fbe84b01a7d2a1c99d410273": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_996b351764404d89bfe1cc292b34ec29",
       "max": 1471.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ef4a34c17b584095a917b8d7aa384e75",
       "value": 1471.0
      }
     },
     "1f57539fbd184f8ba4dc8b8c13b37bf8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2ef92dd622074627af94317332544a3e",
       "placeholder": "​",
       "style": "IPY_MODEL_a27bf520838f464dae5684e9a441afdd",
       "value": " 1393/1393 [00:00&lt;00:00, 76681.44 examples/s]"
      }
     },
     "1f9b2cf7264b41dca3973b798aaf6c3f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_46add396b1c147a9b8a2fb97d832866a",
       "placeholder": "​",
       "style": "IPY_MODEL_cefb427a092b45ff89eefa31acf90cd6",
       "value": "Map: 100%"
      }
     },
     "2446300534494ea6acf1763e303fb582": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "24fee0f49e704d15ba0793fcd4766078": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "26c751a567544df1b2a4b5cf5df45a44": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8590cac2f5e042678ffa7f9e3bc58caf",
       "placeholder": "​",
       "style": "IPY_MODEL_b189dcb1150749a6b44a376727e5b308",
       "value": "Saving the dataset (1/1 shards): 100%"
      }
     },
     "2896afb9c426414aa23f26b270d5bee0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2a1a044c4d5c4b16b452a20ccfce66a2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2ca936852a664bc9961afe144e1cb33f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2d343b416e30448eb13cb39f61aee1b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2ef92dd622074627af94317332544a3e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3319a6c78c3f4ef78eeac2fa751f1f7a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3579109781b9425e91a6697eb39d14ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3689f3c88909451193bd9c1ae64292af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5fa1adb34c9c4f3fb5add8ff374e6e29",
        "IPY_MODEL_65180c170e7443ccbfdc2a60fe83044f",
        "IPY_MODEL_b5cf4f721b5c4da09438cea400174184"
       ],
       "layout": "IPY_MODEL_c51e9e58ffbe48cda09fd44ce3b1d6b4"
      }
     },
     "374d511afa4240a8a3545a11e0fc6e76": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3920db9b1e9b491d8e4fe85138694f26": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3a480f6b7f5a4546b752953b97e843a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3e52cfb67e964c2b80794f6f62063384": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3f00c31d39544229be29ea718278fefd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3f21d83549bb4088b74790b69bc35507": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_893c6d19f7894829b8bbf75a5997b0c4",
       "placeholder": "​",
       "style": "IPY_MODEL_d5ccce4eee1442f08eac89e356aa9ddc",
       "value": " 8065/8065 [00:00&lt;00:00, 146745.60 examples/s]"
      }
     },
     "402af233501a4a30985d61e3948399f9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "40bd87f9fa1b432b96a4a2cfe6efe0cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e1d4346a5a8f48458876cdd76d76d5f6",
       "placeholder": "​",
       "style": "IPY_MODEL_5a0d688c0b37428a9033e6f65e4f9c7e",
       "value": " 8065/8065 [00:00&lt;00:00, 341146.48 examples/s]"
      }
     },
     "43fc61f1a5c942d286acb20f76d6efb5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "46add396b1c147a9b8a2fb97d832866a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "46c9adfb89fb4262824fa047250857b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4758071a18ee488ebcb00b6ccd31c72a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a14d694dda384d6da6e751a5d130b1c3",
        "IPY_MODEL_f3506d59236243d48a37bc200991212a",
        "IPY_MODEL_5994d45b4a5245769bde40b076df47ce"
       ],
       "layout": "IPY_MODEL_88dad9afd4f84b70924a3351a1d03150"
      }
     },
     "485fc2f09e98465eb869430e83bd0e6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "487812bbca814b15832bf65fe6c49897": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "489cbbeb499c4a80a922ac0100e1c067": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_46c9adfb89fb4262824fa047250857b5",
       "max": 1393.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d7b307fbfb184807bb1a2be3e8c4ab1b",
       "value": 1393.0
      }
     },
     "49e50c1fa767449baad3021b6b4c09cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4a33aab5bfa74b8da550945528cc8415": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1f9b2cf7264b41dca3973b798aaf6c3f",
        "IPY_MODEL_acfe162d6a5c4a0785163ad5f99ddf51",
        "IPY_MODEL_4f324dd4c34647a2ab61b7f237714c33"
       ],
       "layout": "IPY_MODEL_5997a24324b7463da144bbb7a53dd9ec"
      }
     },
     "4b7c99bf86c64cbbbb7e23552324d352": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4cb342f75c6d42a5919bf6e58ad7a7fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4db01ab432734acc813d021aa3ad641e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_09324d20af064771ae304bfd51481342",
        "IPY_MODEL_17ba65083bc548f9897d57f1177f76b6",
        "IPY_MODEL_7b98aac5f90b499fa3a7f714a7595c81"
       ],
       "layout": "IPY_MODEL_3920db9b1e9b491d8e4fe85138694f26"
      }
     },
     "4f324dd4c34647a2ab61b7f237714c33": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0e9224c1ba104de9a12e34ad340b9c43",
       "placeholder": "​",
       "style": "IPY_MODEL_80ef1d4cd9634103893be172435c506e",
       "value": " 8065/8065 [00:00&lt;00:00, 12651.97 examples/s]"
      }
     },
     "4fcd27552ab84314b1609b30ea7de252": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fab03e75a96a4d8ea92e0ccab10d078f",
       "max": 102335.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_06f25f8b0ea840ecbbeef4f346e320e7",
       "value": 102335.0
      }
     },
     "50e1365a1c514d2ba85e7e46396c65b6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5222e371564b43c3b7959516163b4da0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "528465f248e74e64a055e0b734a72555": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_49e50c1fa767449baad3021b6b4c09cd",
       "placeholder": "​",
       "style": "IPY_MODEL_3579109781b9425e91a6697eb39d14ec",
       "value": " 530k/530k [00:00&lt;00:00, 2.05MB/s]"
      }
     },
     "52aaa06b5383435abb356133f27d0c2a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "58c4d65da5ef40cd8e186916384da6e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3e52cfb67e964c2b80794f6f62063384",
       "placeholder": "​",
       "style": "IPY_MODEL_5222e371564b43c3b7959516163b4da0",
       "value": " 1.80k/1.80k [00:00&lt;00:00, 160kB/s]"
      }
     },
     "5901a380cddd483bac21d22f0c82120d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_91e7099e58ac4bd1964bf722454dd02e",
       "max": 8065.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_adb88ae1336c47dda324242d8b5237a0",
       "value": 8065.0
      }
     },
     "5994d45b4a5245769bde40b076df47ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f6087740ad144f129a5207cf580b63bd",
       "placeholder": "​",
       "style": "IPY_MODEL_bf4f40549fe94e96b7b6ceb6ebe7fccc",
       "value": " 55.8k/55.8k [00:00&lt;00:00, 296kB/s]"
      }
     },
     "5997a24324b7463da144bbb7a53dd9ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "59eed6a279274e83adaea008926d80af": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5a0d688c0b37428a9033e6f65e4f9c7e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5a6e47ca43044cf191c143d43d0b6e99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5f0a40b37dda46a9ba51d52084534989": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5fa1adb34c9c4f3fb5add8ff374e6e29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2896afb9c426414aa23f26b270d5bee0",
       "placeholder": "​",
       "style": "IPY_MODEL_fdad9c4ce81c4bdbb7944fb8a239436f",
       "value": "Saving the dataset (1/1 shards): 100%"
      }
     },
     "62a15704dfe6495e9f5c427c1d4af600": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e90860854f934050b2ae60ada59b9b5f",
       "placeholder": "​",
       "style": "IPY_MODEL_819bc0eaafef4a45bb2418a14f2ee438",
       "value": "Map: 100%"
      }
     },
     "63d322666f6d497b8e5ff39aed13cf58": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7fe8c2718da044f7895550fe34c9f9c7",
       "placeholder": "​",
       "style": "IPY_MODEL_656854412f6c4284a35753995c787a38",
       "value": "Generating train split: 100%"
      }
     },
     "65180c170e7443ccbfdc2a60fe83044f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2446300534494ea6acf1763e303fb582",
       "max": 1393.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_487812bbca814b15832bf65fe6c49897",
       "value": 1393.0
      }
     },
     "656854412f6c4284a35753995c787a38": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6838a15a28db4f4eb7f31b1a4315f7f8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "68b6c9983203429fb3b3388ca266ee3c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6d326fbc46f24ef0af2b5339fd6bd9ce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6d53d8bc29af4b9288dfeacd51ac7832": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6ed23c25b0ac4989a98103a00b24f892": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a90b894b05c549549886a806a86ba7ab",
       "max": 1802.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5f0a40b37dda46a9ba51d52084534989",
       "value": 1802.0
      }
     },
     "6f1d45496dbc4c26ba649d24c15cf4ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d99ce9a79c784087a8ab69e45545a84a",
        "IPY_MODEL_9dbddcd3eff345f6957c19e042e41fb7",
        "IPY_MODEL_1c9ea3e327db45518ebfd7b92696d0a6"
       ],
       "layout": "IPY_MODEL_2a1a044c4d5c4b16b452a20ccfce66a2"
      }
     },
     "705b3d19ec16490ebff254351ceaee00": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "705eac40843742468cec21d567113dd8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "70a9f584e7b145adb910f4a2a84a4f39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "766dd89eb7904141874294d74fc152d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_abc582fcd4d14010b1ee3b466eaebd2b",
       "max": 1471.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d09573bc035f4bbda089944f77679a01",
       "value": 1471.0
      }
     },
     "76a1f327d28942f1b841e0718218231e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7b98aac5f90b499fa3a7f714a7595c81": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3f00c31d39544229be29ea718278fefd",
       "placeholder": "​",
       "style": "IPY_MODEL_5a6e47ca43044cf191c143d43d0b6e99",
       "value": " 3/3 [00:00&lt;00:00, 330.55it/s]"
      }
     },
     "7fe8c2718da044f7895550fe34c9f9c7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "80af6597a0d949619f2497b41ca87a0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0c37e1577b5c42d0a9fdc881a462e524",
       "max": 529641.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0a49d89ff5914da48061f941f4490110",
       "value": 529641.0
      }
     },
     "80ef1d4cd9634103893be172435c506e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "819bc0eaafef4a45bb2418a14f2ee438": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "84edb701364f4d5caf664fe9fb0beb33": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8b83f4e241524499a0daa2e914114247",
        "IPY_MODEL_766dd89eb7904141874294d74fc152d4",
        "IPY_MODEL_f41a9a3cbb7f4476b34596c4d279c4e2"
       ],
       "layout": "IPY_MODEL_4cb342f75c6d42a5919bf6e58ad7a7fe"
      }
     },
     "8590cac2f5e042678ffa7f9e3bc58caf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "85cfa9a2318340d38ad27644e8ebf9ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_db51bdc20b934d3cb2f240ffae33a055",
        "IPY_MODEL_6ed23c25b0ac4989a98103a00b24f892",
        "IPY_MODEL_58c4d65da5ef40cd8e186916384da6e0"
       ],
       "layout": "IPY_MODEL_cd5d00dd8b824a47b398667e30a77511"
      }
     },
     "87c27d82ff914642ac72d7b4c0942174": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "88dad9afd4f84b70924a3351a1d03150": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "893c6d19f7894829b8bbf75a5997b0c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8b83f4e241524499a0daa2e914114247": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ea8d6fa697394fb8984b0f9d4c872bb6",
       "placeholder": "​",
       "style": "IPY_MODEL_bff7cd0a3725441a92f264aa1f0e4adb",
       "value": "Generating validation split: 100%"
      }
     },
     "8d9db6c1125d486c89249605fec098cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_50e1365a1c514d2ba85e7e46396c65b6",
       "placeholder": "​",
       "style": "IPY_MODEL_705eac40843742468cec21d567113dd8",
       "value": "Generating test split: 100%"
      }
     },
     "91e7099e58ac4bd1964bf722454dd02e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "927310469c1c47f7bba2e059462866f7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "939d0e7b29c44302a5f3f2811efc67ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ad472bd9dce84f899917381f92792688",
        "IPY_MODEL_4fcd27552ab84314b1609b30ea7de252",
        "IPY_MODEL_9f2aace75a88485582e103a093f7c7f8"
       ],
       "layout": "IPY_MODEL_c9b7a966118b49c98c6475cef934b949"
      }
     },
     "93f389c106be40acad5d68a34f3cd931": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a862395935a34f2a900e6c714b2ba6f5",
       "placeholder": "​",
       "style": "IPY_MODEL_485fc2f09e98465eb869430e83bd0e6f",
       "value": " 1393/1393 [00:00&lt;00:00, 12236.82 examples/s]"
      }
     },
     "96c1128de1064b638c49af1342f87f08": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "996b351764404d89bfe1cc292b34ec29": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9b533da2ca6e403b827896df8d69e10f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9b7cb6ab18134f39903d01f58ff816c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e3a23cc621564c6596e170a1d9b35f2a",
       "placeholder": "​",
       "style": "IPY_MODEL_87c27d82ff914642ac72d7b4c0942174",
       "value": " 1471/1471 [00:00&lt;00:00, 87520.16 examples/s]"
      }
     },
     "9d39c8809ae34909adf031f1f788a919": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9dbddcd3eff345f6957c19e042e41fb7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6838a15a28db4f4eb7f31b1a4315f7f8",
       "max": 1471.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_52aaa06b5383435abb356133f27d0c2a",
       "value": 1471.0
      }
     },
     "9f2aace75a88485582e103a093f7c7f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_be712700fa0747c7bd5fd33c7eaf152f",
       "placeholder": "​",
       "style": "IPY_MODEL_b17bb0bb10054869ba3f0cd09d6980d6",
       "value": " 102k/102k [00:00&lt;00:00, 549kB/s]"
      }
     },
     "a14d694dda384d6da6e751a5d130b1c3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_705b3d19ec16490ebff254351ceaee00",
       "placeholder": "​",
       "style": "IPY_MODEL_cd1315315d0a4728b4a1c9fe33b69aa2",
       "value": "Downloading data: 100%"
      }
     },
     "a1bdd06931934549839af18fd421a022": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_26c751a567544df1b2a4b5cf5df45a44",
        "IPY_MODEL_5901a380cddd483bac21d22f0c82120d",
        "IPY_MODEL_40bd87f9fa1b432b96a4a2cfe6efe0cc"
       ],
       "layout": "IPY_MODEL_b9bc9574ddf24afc88be0adf7b6a3980"
      }
     },
     "a27bf520838f464dae5684e9a441afdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a66e16e4874a426ab3d6f25655d2c4af": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a821e4b10bb84eec839950193a0444ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cce785ae61b0486fbb3b8bb6824841a8",
        "IPY_MODEL_80af6597a0d949619f2497b41ca87a0d",
        "IPY_MODEL_528465f248e74e64a055e0b734a72555"
       ],
       "layout": "IPY_MODEL_3a480f6b7f5a4546b752953b97e843a3"
      }
     },
     "a862395935a34f2a900e6c714b2ba6f5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a90b894b05c549549886a806a86ba7ab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aa4be5572df848a6bb2b16cb32031b08": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_59eed6a279274e83adaea008926d80af",
       "max": 1393.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f577fab566374e038dc54a4597807a56",
       "value": 1393.0
      }
     },
     "abc582fcd4d14010b1ee3b466eaebd2b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "acfe162d6a5c4a0785163ad5f99ddf51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d7cb0fd954f948b4bd8e6f341cc0b8ac",
       "max": 8065.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ea3f8b5144634732947102d9d412d9fc",
       "value": 8065.0
      }
     },
     "ad472bd9dce84f899917381f92792688": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fb3d7ead43074155a59f41e4077c5266",
       "placeholder": "​",
       "style": "IPY_MODEL_4b7c99bf86c64cbbbb7e23552324d352",
       "value": "Downloading data: 100%"
      }
     },
     "adb88ae1336c47dda324242d8b5237a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b17bb0bb10054869ba3f0cd09d6980d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b189dcb1150749a6b44a376727e5b308": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b354421a1cb9481c94ea1a3bcf7d2b61": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_76a1f327d28942f1b841e0718218231e",
       "max": 8065.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_02c72d4f320d4e8790c08ac3d7453f70",
       "value": 8065.0
      }
     },
     "b5cf4f721b5c4da09438cea400174184": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_68b6c9983203429fb3b3388ca266ee3c",
       "placeholder": "​",
       "style": "IPY_MODEL_9d39c8809ae34909adf031f1f788a919",
       "value": " 1393/1393 [00:00&lt;00:00, 81677.90 examples/s]"
      }
     },
     "b9bc9574ddf24afc88be0adf7b6a3980": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bb87a9689a4c44f8b8b221fbca1e245c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "be712700fa0747c7bd5fd33c7eaf152f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bf4f40549fe94e96b7b6ceb6ebe7fccc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "bff7cd0a3725441a92f264aa1f0e4adb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c17ceeacb33247c5a91fcc857067e19d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8d9db6c1125d486c89249605fec098cb",
        "IPY_MODEL_aa4be5572df848a6bb2b16cb32031b08",
        "IPY_MODEL_1f57539fbd184f8ba4dc8b8c13b37bf8"
       ],
       "layout": "IPY_MODEL_fc556d761e374a14a6d506dba3673232"
      }
     },
     "c1cd8dc2b8b24ab9a9104b1def41dbc6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2d343b416e30448eb13cb39f61aee1b1",
       "placeholder": "​",
       "style": "IPY_MODEL_24fee0f49e704d15ba0793fcd4766078",
       "value": "Saving the dataset (1/1 shards): 100%"
      }
     },
     "c51e9e58ffbe48cda09fd44ce3b1d6b4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c8001931d16b4470a537a7cd83eff255": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c9b7a966118b49c98c6475cef934b949": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cce785ae61b0486fbb3b8bb6824841a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_374d511afa4240a8a3545a11e0fc6e76",
       "placeholder": "​",
       "style": "IPY_MODEL_6d53d8bc29af4b9288dfeacd51ac7832",
       "value": "Downloading data: 100%"
      }
     },
     "cd1315315d0a4728b4a1c9fe33b69aa2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "cd5d00dd8b824a47b398667e30a77511": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cefb427a092b45ff89eefa31acf90cd6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d052c963098440e7b3c7b0778aa00165": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d09573bc035f4bbda089944f77679a01": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d5ccce4eee1442f08eac89e356aa9ddc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d7b307fbfb184807bb1a2be3e8c4ab1b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d7cb0fd954f948b4bd8e6f341cc0b8ac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d99ce9a79c784087a8ab69e45545a84a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_43fc61f1a5c942d286acb20f76d6efb5",
       "placeholder": "​",
       "style": "IPY_MODEL_9b533da2ca6e403b827896df8d69e10f",
       "value": "Map: 100%"
      }
     },
     "dac7021586d74fac826656a69da019c7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "db51bdc20b934d3cb2f240ffae33a055": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_402af233501a4a30985d61e3948399f9",
       "placeholder": "​",
       "style": "IPY_MODEL_96c1128de1064b638c49af1342f87f08",
       "value": "Downloading readme: 100%"
      }
     },
     "dc69ab656ccb4ed0a4f0c851cf281bb6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c1cd8dc2b8b24ab9a9104b1def41dbc6",
        "IPY_MODEL_1e675c04fbe84b01a7d2a1c99d410273",
        "IPY_MODEL_9b7cb6ab18134f39903d01f58ff816c8"
       ],
       "layout": "IPY_MODEL_de595e4fcbf8418f85ef5e8eb21c3d4b"
      }
     },
     "de52cd7d8164442b85fab2b13448048c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "de595e4fcbf8418f85ef5e8eb21c3d4b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e1d4346a5a8f48458876cdd76d76d5f6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e3a23cc621564c6596e170a1d9b35f2a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e90860854f934050b2ae60ada59b9b5f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ea3f8b5144634732947102d9d412d9fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ea8d6fa697394fb8984b0f9d4c872bb6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eac31f04a700436c9f092db9acfbeaee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_62a15704dfe6495e9f5c427c1d4af600",
        "IPY_MODEL_489cbbeb499c4a80a922ac0100e1c067",
        "IPY_MODEL_93f389c106be40acad5d68a34f3cd931"
       ],
       "layout": "IPY_MODEL_de52cd7d8164442b85fab2b13448048c"
      }
     },
     "ef4a34c17b584095a917b8d7aa384e75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f3506d59236243d48a37bc200991212a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6d326fbc46f24ef0af2b5339fd6bd9ce",
       "max": 55816.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2ca936852a664bc9961afe144e1cb33f",
       "value": 55816.0
      }
     },
     "f41a9a3cbb7f4476b34596c4d279c4e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_927310469c1c47f7bba2e059462866f7",
       "placeholder": "​",
       "style": "IPY_MODEL_3319a6c78c3f4ef78eeac2fa751f1f7a",
       "value": " 1471/1471 [00:00&lt;00:00, 82371.92 examples/s]"
      }
     },
     "f577fab566374e038dc54a4597807a56": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f6087740ad144f129a5207cf580b63bd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fab03e75a96a4d8ea92e0ccab10d078f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fb3d7ead43074155a59f41e4077c5266": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fc556d761e374a14a6d506dba3673232": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fdad9c4ce81c4bdbb7944fb8a239436f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
